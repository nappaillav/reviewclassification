,Submission ID,Scores,Title,Abstract,result,mean
1033,1678,1.25    1.25    1    1.25    ,A Study on Different Feature Extraction Techniques for Audio-visual Speech Recognition,"Audio-visual speech recognition is the recognition of speech by using both acoustic and visual information. Visual speech provides useful information for improving the accuracy of automatic speech recognition both in humans and machines. In this paper we present a review on different features extraction techniques for audio-visual speech recognition. Also, we present various lip/mouth localization techniques since it is an important step for the correct feature extraction. The visual features such as geometric-based features and image transform based features are discussed here. And here we introduce image-transform based feature extraction techniques for Malayalam visual speech recognition.",1.0,1.1875
1316,2560,1    1.25    1.4    1.25    ,"Segmentation, Classification and Clustering Audio in a Broadcast News","The main goal of this paper is to collect audio from a live broadcast news and process it through various methods such as segmentation, classification and clustering. The algorithms used here are both accurate as well as computationally effective. The segmentation divides audio into homogeneous regions according to background conditions. After the segmentation stage each segment is classified using a speech / non-speech discriminator, tagging audio portions without speech, with too much noise. Later the segmented audio is classified based on gender. This helps a lot to improve speaker clustering. With this type of clustering, cluster distances will have a smaller distance matrix which effectively reduces the
search space. We are taking some trained cluster models which contains data about the speakers (anchors). The speaker cluster algorithm used here is modified BIC (Bayesian Information Criterion) which performs better than standard KL2 (Kullback-Leibler) and is much faster than full BIC. After speaker clustering, the resulting clusters are compared one by one against the special anchor cluster models to determine which of those belongs to one of the news anchors. This cluster comparison uses the KL2 distance metrics to measure
cluster similarity.",1.0,1.225
1085,1814,1.7    1    1.2    1.25    ,Signal Enhancement for Telugu Speech Recognition & Text Conversion for Speech & Hearing Impaired,"For a Speech and Hearing Impaired person, comprehending the real time speech through listening only is difficult. They requires real time speech to text conversion. Conversion of STT helps to understand the speech in real time by using visual reinforcement cues. The Speech enhancement techniques helps to obtain a clean speech signal from a noisy environment or a degraded speech. It refers to removing or reducing the background noise in order to obtain an improved quality of original speech signal. Degradation of speech signals is most common problem. A LPC based, weiner filter approach is used for speech enhancement for Telugu language. This method is then compared with several speech enhancement algorithms to obtain better speech quality. NOIZEUS speech database can be  used in order to compare different speech enhancement methods. The research  shows that obtained method provides better result and there is no information loss in original speech signal. Other enhancement methods reviewed in this paper for better noise cancellation. The output is a linear combination of the actual speech input to the system and a synthesized speech signal. The synthesized signal is generated using the LP coefficients and an estimate of the excitation from the current speech.",1.0,1.2875
1092,1829,1    1.45    1.45    ,Adaptive Machine Learning -an Innovative Hearing Aid Technology,"Traditional hearing aid systems have evolved from table sized bulky systems to miniaturise in the canal systems. The effectiveness of any hearing aid is highly accredited to its signal processing ability. Adaptability of a hearing aid into a vast array of listening situations is a crucial factor. But most of the current hearing aids fail to account and acclimatize to all these listening situations.
The current study aims to innovate a machine learning technology based algorithmic program for self adaptability in hearing aid users. . Hearing aid processing configurations for around 07 different listening conditions were mapped. An Artificial Neural Network was built to represent all these listening conditions. Real time input of different listening situations were taken & processed using the built ANN. As a result the ANN yielded configuration settings corresponding to that situation.
The proposed model of adaptive hearing aid programming was compared with the traditional single fit hearing system. Results revealed that the proposed method was preferred by the hearing aid users when compared to the standard programming methods. Therefore this study provides an insight and examines the application of artificial intelligence in hearing aid programming.",1.0,1.3
771,87,1.5    1.05    1.5    1.25    ,An LSTM Language Model for Wakirike Language,"In this paper, a character-based Recurrent Neural Network (RNN) language model is implemented for the Wakirike language as a strategy to address the under-resourced nature of the language for Automatic Speech Recognition (ASR) speech processing.  In this work, a comparable Long Short Term Memory (LSTM) deep RNN Language model is prescribed as a step towards faster and resource affordable ASR systems.  

The Language model implemented in this article while having a small text corpus size was found to be potentially powerful enough to be comparable with the state of the art language models yielding a better relative perplexity score of 2.6 when compared to the score of 3.2 on the n-gram model with smoothing.",1.0,1.325
1097,1848,1.4    1.35    1.8    1.25    ,Audio MNIST Classification Using SVM and KNN,"Most of the Automatic Speech Recognition (ASR) systems
are solving speaker, genre, sound event, sentence and digit identification.
This paper focuses on the English spoken digit classification
using support vector Machine(SVM) and K-Nearest-
Neighbour (KNN) techniques. A new Audio MNIST dataset is
considered for analysis.Current dataset consists of 1500 samples
(3 speakers*50 repetitions*10 digits) having sampling rate
8KHz. As the digit duration is varying from 300ms to 500ms,
different lengths of input samples are choosen for experimentation.
Popular MFCC feature vectors are used as input to the
SVM and KNN techniques.10% of dataset is taken randomly
as testing data to genearte confusion matrix for both techniques.
Finally it can be concluded that caluculated accuracy results
in Gaussian nature with respect to number of frames for
all testing data. For the present dataset best possible accuracy
as 97.59% is achieved for Radial Basis Function(RBF) in SVM.
Index Terms: Spoken digit recognition,SVM,KNN, Audio
MNIST,MFCC.",1.0,1.45
931,1415,1.55    1    1.9    1.65    1.25    ,"Recognition Performance of Wearable Speech Enhancement System under Noisy Environment: an Experiment Using Beamforming, ANR and VAD Algorithms","Wearable speech enhancement (WSE) has the ability to improve the recognition accuracy of the speech signals containing White Gaussian noise. Beamforming, adaptive noise reduction (ANR), and voice active detection (VAC) algorithms are used in WSE system to enhance the speech signals. In a recent work, a recognition accuracy of 63% was reported for 0db signal to noise ratio (SNR), which is not satisfactory for a robust speech recognition system. To improve the recognition accuracy, this research proposed the Beamforming, adaptive noise reduction, and voice active detection algorithms with the inclusion of -10db to 20db SNR. Using the AURORA speech database, the recognition accuracy was 5.6% at -10db SNR and 93.77% for 20db SNR on an average under all noisy environments. The proposed algorithms perform significantly better in higher noisy environment. The improvement to the low noise environment can be the future research direction.",1.0,1.47
990,1577,1.7    1.35    1.5    1.35    ,Interactive Eye Gaze Based Screening of Autism,"Autism Spectrum Disorder (ASD) is one among the leading childhood communication disorders across the world. Children with ASD orient less to socially salient stimuli, such as dynamic social images than typically developing children. Various early assessment programs have been instituted for early identification of autism but none of try to objectively evaluate autistic features in developing children. Early identification of ASD in children helps in early intervention and better prognosis.
In this paper we aim to build a social orienting task for screening autism using eye-tracking technology. The task consists of side by side presentation of dynamic social images and dynamic geometric images. Visual exploration patterns recorded from 60 typically developing children aged below 2 years formed the normative database.
Using OGAMA software, a pilot study was carried out on ten individuals with a confirmed diagnosis of Autism. The results of the pilot study revealed that the developed task was sensitive in identifying toddlers with autism. Further, a large-scale implementation of the proposed task will fine tune the intervention programs and aid in early intervention of children with Autism.",1.0,1.475
1277,2379,1.3    2.2    1.35    1.1    ,Deep Learning Based Speech Emotion Detection for Atypical Affect Sub-Challenge,"In the present work, we have developed a deep neural network based model to predict classes of emotions from utterances of differently-abled persons. Two hidden layers and ten activation units per hidden layer have been employed to develop the model. The input feature set file is the ComParE Acoustic Feature Set containing 6373 static features resulting from the computation of various functions over low-level descriptor (LLD) contours. At first, we selected first 10 features and last 6 features from the ComParE Acoustic Feature Set in a random way whereas in order to maintain a proper scaling, we normalized the feature values and 21.76% accuracy has been achieved. In another try, we selected first 52 features from the same feature set above and a better accuracy of 31.44% has been achieved using the same model. In order to improve the system further, we have selected the important features from those 6373 features using some well-known techniques like information gain and then we trained our system with those selected features.",1.0,1.4874999999999998
1184,2064,2.05    1.4    1.25    1.6    ,On BLSTM-RNN and TDNN-based Continuous Speech Recognition of Low-resourced Languages,"To date, continuous speech recognition (CSR) of low-resourced languages   (minority languages, dialects and varieties of a certain language with relatively few annotated speech data) performs unsatisfactorily. This research takes Indian English for example, explores the effective method in recognizing low-resourced languages. Based on the mainstream neural network algorithms BLSTM-RNN and TDNN, we employ MFCC and PLP to extract features respectively, thus come up with 6 acoustic models. Then the models are tested on the test set and results are analyzed in detail.   Through analysis, we suggest that when dealing with low-resourced languages, PLP is advised in extracting features and TDNN is advised in training the acoustic model.",1.0,1.5749999999999997
911,1362,1    2.25    1.6    ,Speech Recognition Applications for Use in a University “Smart Production Lab”.,"A standard application in the medical and judiciary/legal communities for diagnostic and reporting purposes, respectively, speech recognition technology is now being adopted for domestic purposes, with industrial applications under rapid development. Modern (DL HMM) systems have become highly accurate, but are still prone to error particularly due to two major factors: divergent phonetic output (i.e. dialect, accent, foreign accent, speech impediments) and background noise
The purpose of this work is to test how strongly an L2 accent in English, and a non-standard L1 German accent, will affect error rates on ASR on a standard application, developed by an Austrian business software provider, and an augmented reality device in a smart production technology lab at FH JOANNEUM University of Applied Sciences. The primary goal is to establish usability potential advanced speech recognition applications in the manufacturing industry. Further steps in this project shall include both quantitative and qualitative research for Masters’ theses, in collaboration with a network of industrial partners at our institute. The project should benefit digital manufacturing systems by reducing speech recognition errors from non-standard input. Furthermore, the research should reemphasise the importance of pronunciation and prosody in second language teaching.",1.0,1.6166666666666665
1009,1621,1.25    1.45    2.2    1.85    ,Evaluating Robustness and Accuracy of Semantic Decoder for Spoken Language Understanding,"Spoken Language Understanding (SLU) is an important component in human-computer interaction system which represents speech data into semantic form which is important to understand user’s intension. The human computer interaction system is most commonly placed in noisy environment. This introduces error in Automatic Speech Recognizer (ASR). SLU takes into account ASR errors. Thus robustness of SLU plays important role in efficient working of human computer interaction system. The SLU consist of semantic decoder and belief tracker. Recent line of research models semantic decoder as a classification task or sequence to sequence learning. 
The objective of this research is to evaluate robustness and accuracy of semantic decoder modeled as classification task. The classifiers that were incorporated in semantic decoder are support vector machine (SVM), stochastic gradient descent (SGD), relevance vector classifier (RVC) and variation relevance vector classifier. The results showed that the robustness of semantic decoder was poor when the data available for training was less.The SVM classifier showed maximum robustness and better accuracy as compared to other classifier. The dataset used for semantic decoder classifier is DSTC-II. For SVM, classifier the Inter Cross Entropy (ICE) and accuracy were 1.0 and 0.91. For RVC, ICE and accuracy were 4.055 and 0.52.",1.0,1.6875
884,1282,1.8    2.3    1.55    1.45    ,Delay-sum Beamforming to Enhancement Speech for Conference Devices,"The concept of microphone array processing finds its application in speech processing to enhance the SNR (Signal Noise Ratio) using signals from multiple microphones. Beamforming, also known as Spatial Filtering, is one such microphone array signal processing technique which enhances the SNR by adjusting the delay between the signal captured from multiple microphones. This paper describes the implementation of low complexity classical adaptive delay-sum beamformer for microphone array to enhance the speech SNR for conference speaker phone type of devices. Device considered in this paper has 4 microphones. The delay-sum beamformer works by adjusting the delay for each microphone signal based on the correlation of the microphone captured signals. False predictions of correlations are avoided by the decision averaging technique and energy tracking.",1.0,1.775
981,1550,1.55    2.45    2.1    1    ,Analysis of Articulatory System Based Speech Production for People with Speech Disabilities,"Based on our previous work more complicated situations e.g. words and sentences formulation, have been analyzed towards articulatory system (tongue based) speech production. The aim of the present work is to reconstruct the real-time speech synthesis system based on the gestures of the articulatory system (with a focus on the interaction of tongue with rest of the oral cavity) taken from a sensor based input.  A virtual prototype was developed by forming a Graphical User Interface (GUI) and the proposed algorithm was validated by a perceptual test. Speech production using an articulatory system and vocal tract model has been compared.
Index Terms: articulatory system, sensors, vocal tract model",1.0,1.775
1291,2437,1.8    2.35    1.5    1.5    ,A Model for Sibilant Sound Distortion Detection in Children,"The distortion of sibilant sounds is a common type of speech sound disorder in European Portuguese speaking children. Speech and language pathologists (SLP) use different types of speech production tasks to assess these distortions. One of these tasks consists of the sustained production of isolated sibilants. Using these sound productions, SLPs usually rely on auditory perceptual evaluation to assess the sibilant distortions. Here we propose to use an isolated sibilant machine learning model to help SLPs assessing these distortions.

Our model uses Mel frequency cepstral coefficient of isolated sibilant phone and it was trained with data from 145 children. The analysis of the false negatives detected by the model can give insight into whether the child has a sibilant sound production distortion. We were able to confirm that there exist some relation between the model classification results and the distortion assessment of professional SLPs. 69% of the distortion cases identified by the model are confirmed by an SLP as having some sort of distortion or are perceived as being the production of a different sound.",1.0,1.7875
1194,2107,1.55    2.95    1.4    1.25    ,Analysis of Suitable Features and Classifiers for Speech Recognition,Autism Spectral Disorder (ASD) is a developmental disorder that impairs the ability to communicate and interact. Attention Deficit Hyperactivity Disorder (ADHD) is a biological condition which makes it difficult for the children to pay attention and interact. A framework to detect ASD and ADHD using Speech Recognition is proposed. One of the signs of these disorders is that the subject has difficulty with communication or speaks and raises abnormal tone of voice. Researchers have focused on several different sets of features to recognize their speech. The objective of the proposed work is to find out an ensemble of features that can recognize speech with maximum accuracy. The analysis of existing feature sets is done to ascertain those ensemble of features that results in the maximum speech recognition rate. The experiments are carried out with several frequently used speech recognition data sets and the experimental results are presented.,1.0,1.7875
1166,2010,1.4    2.4    1.95    1.4    ,Automatic Speech Recognition for Manipuri Dialects(Kakching and Andro) Using DNN,"This paper proposes an approach to identify the differences between the Manipuri dialects (Kakching & Andro) and recognise the speech of different dialects. In this work, speech recognition is implemented in three different ways and compare their performances. Tonal differences are calculated by using speech features i.e., MFCCs and their derivatives as well as formant frequencies. It has been implemented using HMM-GMM with monophones, HMM-GMM with triphones and Deep neural network with 4 hidden layers with softmax adaptation layer and finally their performances are compared. By using DNN, WER is reduced from 15% to 13%, with softmax adaptation layer, WER is reduced to 10%.

Keywords: MFCC, HMM-GMM, DNN, Softmax adaptation layer, word error rate (WER)",1.0,1.7875
754,44,1    2.05    2.75    1.55    ,Production and Perception of Fake-singing,"These days armature people upload their own music videos to public data-cite, and they pretend to sing like a professional singer, however, the music sound and singing voice are from the original singers. We call it ""fake-singing"" in this paper. Their levels for fake-singing varies depending on their skills and attitude. In order to make a better fake-singing video, we investigated effects of breathing action and effects from first impressions. We noticed that fake-singers do not breathe in and/or actually breathe in a wrong timing. We also noticed that the first impression from our knowledge could shift the judgement of the fake-singing.
We examined (1) breathing action in fake-singing, and (2) perceivers' first impressions. Three armature singers participated for the recordings. Three kind of videos (real singing, fake-singing, and fake-singing with breathing). 
The results show that the breathing action is effective for the perceivers to get natural and congruent impressions. The first-impressions affects the evaluation for the fake-singing.",1.0,1.8375
1045,1708,1.7    1.5    1.5    2.7    ,Study of Multi-band Spectral Subtraction for Speech Enhancement in Magnitude and Power Spectral Domains.,"The spectral subtraction is a well-known method to reduce the background noise from noisy speech signal which assumes noise & signal are additive and uncorrelated. Hence an estimate of noise obtained from the noise only region of noisy speech is subtracted from the noisy speech to obtain an estimate of speech signal in which speech quality & intelligibility are enhanced. However in the normal speech communication scenario noise is mostly colored and does not affect the entire speech spectrum uniformly. To reduce this non-uniform effect of noise, kamath et al[6] proposed a method where the frame spectrum is divided into different frequency bands. They have computed oversubtraction factor & tweaking factor based on the band specific SNR as against frame SNR. Further, they have implemented the algorithm in the power domain. In this research activity, we have implemented multiband spectral subtraction method as proposed by Kamath et al[6] in the magnitude domain. We have experimented with babble noise, random noise, car noise & helicopter noise. The method presented by us results in reduced background noise & improved quality for babble noise & random noise. However, for helicopter and car noise this multi-band spectral subtraction in magnitude domain gives poor results",1.0,1.85
1312,2535,2.6    1.35    1.95    1.5    ,Classification of Emotions for Cognitively Impaired Subjects,"The study of emotions is an important task due to emotions affect the people’s behavior and reflect the social and interpersonal perception . It is a fact that emotions are an important aspect that plays a significant role in human decision-making processes. Our work is centered in the classification of emotions in speech signals in atypical situations like cognitively impaired subjects with mental, neurological, and/or physical disabilities and who may have different processes of interaction between them and the environment. The proposed methodology is based on wavelet decomposition and statistical feature extraction. Signal segmentation is performed by re-sampling the audio signal and windowing process. Next, each segment is decomposed through the discrete and stationary wavelet transforms and the results are compared. The descriptive characteristics of the decomposed signal are extracted and a supervised classification scheme is used. This methodology have been used for emotion recognition and classification in previous works with an accuracy between 80% and 90%. In this work we compare the performance of these algorithms using the provided database from cognitively impaired subjects.",1.0,1.85
1314,2555,1.25    1.7    2.85    1.6    ,Enhancement of Artificial Larynx Transducer Speech Using Subspace Filtering and Multidimensional Scaling,"Speaking is the fundamental activity in everybody's life. It becomes a pleasure if spoken with our own voice. But many human beings have problem in speaking and those disorders are termed with many names based on the cause like apraxia, dystheria, stammering and so on. Here the problems are caused because of the malfunctioning of our organs. There are cases were the larynx, the main part of the vocal systems is damaged and removed. Artificial Larynx Transducer(ALT), a small hand held device will be used by them. The sound produced by malfunctioning of the organ is called as Disordered speech and the speech produced by ALT is called Substituition voices. In this paper, an algorithm is developed to improve the quality of ALT speech. Initially the F0 contour, formant frequency deviations are used to categories the ALT speech and HE speech as well as used to train the system..The novelty of the system is to use optimum interpolation and decimation for making Principal Component Analysis to remove the noise.Multidimensional scaling(MDS) is used to estimate the distance vector between the training and test data set.
Index terms: Healthy Speech, ALT speech, F0 contour, F1 Formant deviation, Subspace filtering, MDS.",1.0,1.85
1224,2179,1.9    1.95    1.75    ,Speech Profile of Wilsons’ Disease: a Case Study,"Wilson Disease (WD) is a rare, inherited autosomal recessive disease which occurs due to mutation of the ATP7B gene resulting in copper toxicity. WD affects mostly liver, eyes, brain and combination of any. The present study aims to formulate the speech profile of WD and for same purpose a complete speech battery was performed including detailed Oral Peripheral assessment, Voice assessment, Cranial Nerve assessment, Dysarthria assessment, Swallowing assessment, Rate of speech assessment, articulation assessment and writing assessment. Client had less Maximum Phonation Time (13.2 sec) than normative values. On acoustic analysis, client had poor HNR and high shimmer values. Client had 38.88% of articulation error on H-PAT Client and had “slow rate of speech”.. Due to inadequate lip seal and lip strength client had drooling. Client had poor hand writing and misalignment. After completion of test battery we diagnosed the client as Mixed Dysarthria with dysphagia at oral preparatory stage. WD is affecting the client’s communication & daily activities and hence has impaired quality of life. This is one of the case reports exploring the speech and other related impairment in Wilson disease.",1.0,1.8666666666666665
1199,2121,1.55    2.15    1.4    2.4    ,"Acoustical Analysis of Affricates, Fricatives & Stops in Down’S Syndrome","Children with DS have anatomical and physiological differences in the mouth and throat region. They may also have hypotonic muscle, small oral cavity, high arched palate, respiratory difficulties. This may have an adverse effect on articulatory skills in comparison with typically developing child (TDC). So present study focus on acoustic analysis, (VOT, Formant Transitions including F1, F2, F3 and Friction Noise) of  fricatives, affricates and stops of children with Down’s Syndrome which will provide the speech profile of a particular child which help in further assessment and rehabilitation process. A total of 30 subjects were included in the study. Among 30 subjects 15 were DS children (i.e. 66.6% males and 33.3% females) whereas 15 were TDC (80% males and 20% females). Affricates, stops and fricatives in VCV combination (vowel-consonant-vowel) was asked to produce by subjects and analysis were made. The t test analysis was performed at 5% level of significance using SPSS 20.00 version software. Further percentile score, mean value and standard deviation were also computed. Result revealed that there is a significant difference in the articulatory skills of children with DS & TDC.",1.0,1.875
832,1148,1.5    1.6    3.15    1.3    ,A Discriminative Approach for Tonal Feature Extraction to Refine Punjabi Automatic Speech Recognition System.,"Among the Indo-Aryan languages, Punjabi language is the only tonal language which is spoken across the world by over 100 million native speakers. Although being very complex due to lexical tones, Punjabi language is very versatile spoken by a group of people in Punjab, India as well as Punjab, Pakistan. This leads to dialectal variations in the language. Punjabi language has three tones namely high, low and level tone. Although ample accomplishments have been made by the researchers in developing ASR systems for speech to text or text to speech applications, for tonal aspects of Punjabi ASR systems remains unexplored. Our research focuses on analyzing the pitch variations of the input speech pattern extracted on PRAAT to identify the meaning of the words having similar combination of consonants and vowels. The tonal features which are segmental and phonemic in nature are modeled based using DNN on KALDI toolkit. A discriminative feature extraction technique (DTFE) is proposed in the article which outperforms the fundamental F0 features. Improvement gains are observed in the ASR system based on the proposed features.",1.0,1.8875
1083,1810,1.15    1.65    1.75    3    ,"The INTERSPEECH 2018 ComParE Challenge: a MATLAB Based Atypical & Self-Assessed Affect, Crying & Heart Beats","In this paper we address all four sub-challenges put forth at the Interspeech 2018 Computational Paralinguistics challenge. We propose a system to classify atypical affect, self assessed affect, crying and heart beats. Based on the concepts of audio signal processing, we extracted features that were further processed by the state of the art machine learning algorithm of neural networks. The proposed system was implemented on a MATLAB environment and focused on automatic classifications of signals into specific classes. Based on their scores obtained after feature extraction an ANN was trained using the training dataset to represent different classes. The training process was repeated to obtain a cross word reference template (CWRT) for each class.
The proposed method of using a combination of ANN along with CWRT showed superior classification compared to earlier toolkits used for the same purpose.",1.0,1.8875
895,1321,1.7    2    1.65    2.55    ,"Experimental Analysis of the Changes in Speech While Normal Speaking, Walking, Running and Eating","Speech Communication is a mundane activity.People talk while
ambling, running, biking, eating and so on.This paper analyzes
the variability in speech parameters while a person speaking
when walking, eating or running with the normal speaking.
In this paper, we get information about the analysis of speech
signal while normally speaking and speaking along with walk-
ing, eating or running.The analysis of speech signal according
to minimum pitch,maximum pitch, mean energy intensity and
mean F1(first formant) has been carried out and also experi-
mented in Praat and then the problem is implemented in Mat-
lab.This paper metes out the variation in speech parameters in
different scenarios.The clustering is performed and the paper
presents an experimental comparison of two different features
of speech.",1.0,1.9749999999999999
941,1441,1.9    2.5    1.55    ,A New Method for Monaural Speech Separation Using Ideal Binary Mask,"This research work proposes a new computationally efficient method for monaural speech separation using Ideal Binary Mask (IBM). Speech separation systems using IBM in general consists of time-frequency (T-F) analysis by a bank of Gamma tone analysis filters, ideal binary mask computation using clean speech and noise and finally reconstruction of speech using the computed IBM via synthesis filter bank. This method involves post multiplication of IBM with the output of the synthesis filter bank. Which involves many computations without contributing anything to the final output with increased computational delay. This research work solves this issue by changing the order of operation in the reconstruction of speech signal from the noisy speech and improves the performance with minimal computational delay. The proposed method multiplies the computed IBM with T-F signals from the output of the analysis filter bank. This in turn makes many noise dominant frames to be zeros and enables the synthesis filter bank to produce the enhanced speech signal with minimal computational delay. The experimental results show that the proposed approach improves the intelligibility and quality of speech in terms of Short Time Objective Intelligibility (STOI) and Signal to Noise Ratio (SNR) respectively. The proposed method also reduces the computation time considerably as compared to the existing approach of monaural speech separation.",1.0,1.9833333333333334
1219,2167,1.9    1.55    2.15    2.5    ,Complementing the DTW Based Speaker Verification System with Knowledge of Specific Region of Interest,"In this paper, we propose complementing the traditional Dynamic Time Warping (DTW) based text dependent speaker verification system with local scores derived from the vicinity of speaker-identity-rich regions. The vowel-like regions are used to define portions along the warping path to be more important in terms of the speaker discriminating information content. Experiments have been performed to study the speaker mismatch scenario, on RSR2015 database, on the male set, considering three pass phrases. The proposed method showed absolute improvement of 0.88 to 1.82% over the baseline system.",1.0,2.025
837,1166,1.7    2.65    2.25    1.5    ,AMBIAURIS – a Multi-microphone Audio-recordings Database for Multi-speaker Spatial Localization and Separation,"The paper presents a multi-microphone audio-recordings database named Ambiauris that was collected for the development and evaluation of multi-speaker spatial localization, separation and distant speech recognition methods. A conventional multi-channel audio interface with four ceiling-mounted microphones was used for capturing audio recordings in a laboratory classroom. Several different spatial acoustic scenarios were designed and used to reflect different degrees of difficulty in automatic multi-speaker spatial localization and separation. The scenarios involve one to four individuals walking around the classroom, stopping at several reference locations, and uttering commands in English and/or making various sounds, such as clapping, snapping fingers, and moving chairs. The collected audio recordings were annotated and transcribed using EXMARaLDA Partitur Editor, where all the transcribed non-silence segments, which can overlap, were labelled with the identity and the corresponding reference location of the speakers who uttered commands or made the sounds. The paper also discusses how the Ambiauris database can be used in the development and evaluation of multi-speaker spatial localization and separation methods. Finally, an example of a system for spatial localization of speakers is presented, which was evaluated using the presented database. The Ambiauris database is publicly available and licensed under the Creative Commons - Attribution-NonCommercial-ShareAlike license.",1.0,2.025
1235,2227,1.65    2.7    1.8    ,Detecting Replay Attacks on Speaker Verification Systems,"Automatic Speaker Verification (ASV) systems are vulnerable to spoofing attacks, and a lot of research is focused in an attempt to improve the existing state-of-the-art by developing countermeasures against these attacks. Especially after two successful ASVspoof challenges and availability of standard databases, there has been enormous interest in researchers towards development of better countermeasures. Replay attacks, being low effort attacks, pose more threat to ASV systems. However, there is still a wide gap between performance in real-time and in case of small amount of training data. Conventional short-term features have not yet shown excellent results. In this paper, we review existing systems to counter replay attacks, and present our approach of detecting replay attacks using ASVspoof 2017
corpus. We experiment on various combinations of different filterbank features and claim that inverted mel-frequency cepstral coefficients (IMFCC) play a key role in discriminating replayed audio signals from genuine signals independent of any classifier.",1.0,2.05
1041,1697,1.4    2.35    2.7    1.55    2.5    ,Vowel Onset Point Detection Deep Learning,"In this paper, we have discussed about the Vowel Onset Point (VOP) for the Hindi language and its significance in the speech recognition. We have defined the vowel onset point and how it can be calculated. Alphabets in the Hindi language are the combination of the vowel and consonant part. In Hindi, we cant pronounce a consonant without a vowel. There is a very small region between consonant and vowel where transition happens from consonant to vowel. We have used characteristics of the sound files to get the vowel onset point. To calculate Vowel Onset Point, we have applied filtration process,after that, we can use the energy of the signal and different formants combined with epoch interval and itakura distance. Filtered energy and filtered formants can be used as cues for accurately detecting Vowel Onset Point within the range of +/- 30 milliseconds. In order to further increase the effectiveness of the proposed method, we have used Recurrent Neural Network variants to detect Vowel Onset Point which uses speech features and reference point calculated by filtered formants.",1.0,2.1
1006,1614,2.75    1.95    2.3    1.5    ,Aphasia Confidence Index (ACI) - an Objective Measure of Aphasic Symptoms,"Variability being a hallmark of aphasic syndromes there is a need for objectivity in a routine aphasic evaluation. Selecting domains and controlling parameter that needs to be worked upon for rehabilitation is still challenging for novice therapists. In this paper, we aim to build an objective tool that provides an assistive evaluation of aphasia. 
Using artificial neural networks, a confidence index on the type of aphasia and possible rehabilitation goals are highlighted. The study was carried out in two phases i.e. Phase I included the development of the tool using MATLAB software and Phase II of the study involved field testing of the developed tool.
 As results of phase I, Aphasic profiles of 49 participants were loaded onto MATLAB software and the ANN was trained. Results of phase II using Cohen's Kappa evaluation revealed an overall 0.916, indicating a positive agreement between the developed objective tool and traditional subjective evaluation. Hence this tool can help guide novice clinicians in decision making as well as planning appropriate intervention strategies.",1.0,2.125
830,1145,2.3    1.35    2.25    2.6    ,Acoustic Characteristics of Schwa Vowel and Release Vowel in Punjabi,"The research area of phonological study of Schwa /ə/ in Punjabi is a need of linguistic survey of Punjabi language. Punjabi is tonal language in which schwa occurs in only word-initial and word-medial. The word-medial schwa is generally used functionally to break the consonant clusters including geminated consonant clusters and is not represented orthographically. It also acts as tone bearing unit (TBU) in many such words. Consonantal release vowel Schwa is observed in isolated words with closed-syllable ending. 
The Prosodic pattern identification and their acoustic correlates is a complex problem. The ratio of first two formants also helps further in such identification. The spectrographic study of schwa needs to be carried out in the above context as the release vowel may have different acoustic properties than vowel schwa which may help in word spotting. The study is extended to the occurrence of release vowel in the same context in sentences for four male, four female native Punjabi speakers. The speech engines such as TTS can use it for speech processing provided the release vowel is found significantly impacting the phonological utterance. This study has been limited to tonal words which can help distinguish between a native and non-native Punjabi speaker.",1.0,2.125
1076,1789,2.1    2.05    1.1    3.3    ,Improved Continuous Hindi Speech Recognition Using Triphones,"Contextual information plays a very important role in the pronunciation of a phoneme. This paper presents context- dependent Triphone based continuous Hindi speech recognition system. As Triphone based acoustic modeling considers left and right context of monophones thus speech recognition results are improved by using Triphones.
Speech Recognition was performed using widely known Hidden Markov Model (HMM) based HTK toolkit. For feature extraction perceptual linear predictive (PLP) coefficient were used. 
Further HMMs were tied to share the same set of feature parameters. Decision tree based clustering was used for further refinements. Experiments were conducted for both monophones and Triphones based speaker dependent continuous Hindi Speech Recognition. Effects of two grammar definitions were analyzed. Maximum word correct percentage was achieved 69.11% using PLP coefficients with grammar definition (Gramdef1) using Triphone based acoustic modeling.",1.0,2.1375
934,1421,1.55    1.5    1.85    3.75    ,Data-Driven Model for Preparation of On-line Pronunciation Dictionaries,"Pronunciation varies from person to person. Interactive voice response devices deal with public using everyday speech and require to understand the varying accents. It is impossible to prepare pronunciation dictionaries covering all the accents of all human beings in the world. There is a need that these devices learn these accents and pronunciations from the interactions with its users like human beings do in everyday life. The pronunciation dictionaries should be lean and thin in terms space size as well as being current.

The existing models use supervised methods and are expensive in terms of time, cost, effort and huge in size and space requiring very processing power. We present an online model for preparation of lean, thin and current pronunciation dictionaries using data-driven adaptive methods. The new model measures the sound distances for automatic learning of new accents and prepares and updates its online pronunciation dictionaries to keep it current. The above model uses dynamic phone warping technique to measure the distances.

Experiments are conducted and results showed that the new pronunciations are recognized  with a very accuracy of eight-four percent. The model is generic and can be adopted for any language by using its phoneme sets.",1.0,2.1625
1207,2141,1.6    3.5    1.5    2.15    ,Speaker Change Point Detection Using Enhanced Spectrogram and Efficient Distance Metric,"Segmentation process is very popular in Speech recognition, word count, speaker indexing and speaker diarization process. This paper describes the speaker segmentation system which detects the speaker change point in an audio recording of two speakers with the help of feature extraction and proposed distance metric algorithms. In this new approach, pre-processing of audio stream includes noise reduction and speech compression by using discrete wavelet transform to decompose it into 4:1 ratio at level 2. After framing of compressed speech, features are extracted by nonlinear energy operator based enhanced spectrogram.  Finally, to detect speaker change point, dissimilarity measures are applied to find the distance between two frames. A sliding window is moved across the whole data stream to find the maximum distance that corresponds to speaker change point. The distance metrics incorporated are standard Bayesian Information Criteria (BIC), Kullback Leibler 2 (KL2) distance metric, T-Test and proposed distance metric algorithms to detect the speaker boundaries. At the end, results are evaluated with Recall, Precision and F-measure. Better result of 94.12% is shown by proposed distance metric with enhanced spectrogram as compare to BIC, KL2 and T-test algorithms.",1.0,2.1875
1270,2354,1.75    2.8    2.55    1.65    ,Speech Emotion Recognition Using Novel Cepstrum Statistics,"— This paper proposes a new feature set for speech emotion recognition that is based on the Mel Frequency Cepstral coefficients (MFCC), their entropies and their weighted means. Experiments on the benchmark SAVEE dataset show a high accuracy of speech emotion recognition confirming the efficacy of our proposed feature set. The SVM classifier is used for the classification and yield the highest results for the same.",1.0,2.1875
1072,1783,1.7    2.25    2.65    ,Automatic Speaker Recognition Using Stationary Wavelet Coefficients of LP Residual,"Automatic speaker recognition is a challenging task when the duration of the test speech is very short ie., a few seconds. Source features extracted from short speech utterences are shown to be effective for such cases. This paper proposes a system based on LP residual for text independent speaker recognition. Discrete Wavelet Transform (DWT) and Stationary Wavelet Transform (SWT) are used to parameterize the LP residual. These features are used for implementing an i-vector/PLDA based speaker recognition using Alize tool kit. Effectiveness of the system is evaluated for using 10 sec - 10 sec task of NIST SRE 2010 database. Speaker recognition using SWT level-3 results in an EER of 41.",1.0,2.1999999999999997
1222,2176,2.25    2.35    1.9    2.3    ,HiLAM Alligned KDA Classes for Online i-vector/DTW Framework for Text Dependent Speaker Verification System,"In this paper, we propose to use online-ivector/ DTW system with kernel discriminant analysis (KDA) where the class definition is derived of Hierarchical Acoustic Model (HiLAM). A finer speaker/phrase-segment definition of classes is achieved with the help of HiLAM so as to train KDA with finer discriminating power. Experiments have been performed on RSR2015 database and the proposed method showed improvement over the baseline system. Absolute reduction in EER values to the tune of 0.17% and 0.60% is observed for the speaker mismatch and the content mismatch scenarios respectively.",1.0,2.2
863,1215,2.8    1.4    2.5    2.15    ,CNN Based Deep Encoder and Multiple Shallow Decoders for Morphing Emotions in Speech,"Transforming emotions is integral to voice morphing. Current works rely on altering acoustic features such as pitch, frequency, and duration of the speech. We introduce a deep learning approach to transform emotions in natural speech without altering acoustic features. Through this paper, authors present a deep Convolutional Neural Network (CNN) based Encoder and multiple shallow CNN Decoders for morphing emotions. The Encoder provides a transformation from any emotion to a base emotion. This representation is then fed into the decoder of the required emotion to be expressed in the speech. The output obtained from the decoder is converted into a speech signal by reconstruction. Results were obtained on Toronto Emotional Speech Set (TESS) and verified by both subjective listening and L2 loss. Our study also highlights the advantages of using Rectified Linear Unit (ReLU) activation function for deep learning based acoustic models. The model was able to morph emotions in the speech without significant loss of content and performed better than the current state-of-the-art emotion morphing techniques in speech.",1.0,2.2125
1308,2510,1.8    1.65    3.3    2.15    ,Audio-Visual Correlates of Basic Emotions in Natural Human Interaction,"Understanding of the perception of emotions or affective states in human beings is important to develop an emotion aware systems that will work for realistic scenarios. In this paper, we are trying to find out the perception of emotions in naturalistic human interaction (audio-visual data) in terms of perceptual cues through subjective evaluation. For this purpose, we used a naturalistic audio-visual emotion database developed from TV broadcasts like soap-operas, movies and named as IIIT-H Audio-Visual Emotion (IIIT-H AVE) database. The database consists of audio, video and audio-visual data in English language. The perceptual tests are conducted in all three modes audio-only, video-only and audio-visual data for four basic emotions (anger, happy, neutral and sad) in category labeling and in dimensional approach labeling in two dimensions namely arousal (active or passive) and valence (positive or negative). Interestingly, the general patterns in the perception of emotions were remarkably different for different emotions and it gives us that the emotion specific features needs to be explored for the development of emotion aware systems, unlike the common feature set used in the literature for this purpose.",1.0,2.225
1178,2048,3.85    2.3    1.6    1.3    ,"Robust Improvement of Multi-emotion Labelling Using Valence, Arousal and Confusing-class Models","This paper reports classification performance improvement on seven emotions from the standard Emo-DB database using a valence/arousal-based disambiguation model. The model is developed using Russell’s Circumplex emotion wheel at level-2 and possibly feeds to confused-emotions (such as Happy and Anger) models at level-3. Models at all levels use low level descriptors (LLD) extracted from speech samples as well as aggregate features extracted from the LLDs. A valence/arousal based disambiguation model and confusion based emotion models operating on the proposed features gives the best average accuracy of 75.70% and unweighted average recall of 72.31% which is better than the state of the art. More importantly, it is found that there is an improvement in the recall measure for every class (without exception) compared to the base classifier.",1.0,2.2625
860,1206,3.4    1.9    1.5    ,Corpus Design Challenges and Analysis of Acoustic Information for Automatic Dialect Classification,"The primary challenge across the field of automatic speech communication relies upon the understanding of creation of such acoustic models that are capable of representing distinct differences throughout the spoken languages. The design of such systems is undoubtedly affected by the parameters such as an Individual’s age, gender, speaking style, dialectal variability, recording environment etc. This work investigates on the various challenges faced during designing an enriching corpus and analyzing the dialectal variations by tracking the variance of different acoustic parameters such as pitch contour, formant frequency, and intensity for different toneme sounds. The paper also makes an attempt to discuss the process and problem of tone assimilation (tone sandhi) in di/multi-syllable words of Punjabi language where the occurrence of more than one tone is being anticipated. The pitch variations of the input signal are performed using PRATT toolkit, to establish a relation between the pitch variations across the various tonemes.  The model is further evaluated using the ANOVA test for the comparative classification of dialects based on acoustic features. The entire work is based upon the four dialects of Punjabi language; classified as the 10th most spoken language across the world, with over 105 million native speakers.",1.0,2.2666666666666666
1264,2336,2.45    1.6    2.3    2.75    ,An Ensemble SVM-based Approach for Voice Activity Detection,"Voice activity detection (VAD), used as the front end of speech enhancement, speech and speaker recognition algorithms, determines the overall accuracy and efficiency of the algorithms. Therefore, a VAD with low complexity and high accuracy is highly desirable for speech processing applications. In this paper, we investigate the performance of supervised learning-based VAD system using neural network (NN) and support vector machine (SVM). Despite of high classification accuracy of support vector machines (SVM), trivial SVM is not suitable for classification of large data sets needed for a good VAD system because of high training complexity. To overcome this problem, a novel ensemble-based approach using SVM has been proposed in this paper. Although neural network performs better than single SVM-based VAD trained on a small portion of the training data, ensemble SVM gives accuracy comparable to neural network-based VAD. Ensemble SVM and NN give 88.74% and 86.28% accuracy respectively whereas the stand-alone SVM shows 57.05% accuracy on average on the test dataset.",1.0,2.2750000000000004
967,1508,1.5    2.9    2.25    2.45    ,An Experimental Study on Learning Hindi through Spoken English to Hindi Script (Devanagari) Translation and Text to Speech Conversion,"For English speakers in early stages of learning Hindi, formulating sentences in Hindi is often attempted by a verbatim translation of English words to corresponding Hindi words. Due to this reason, they are unable to learn Hindi sentences correctly. We tried to overcome this problem by taking English speech as input and translating the given English sentences and words into Hindi and then displaying its equivalent construct in Devanagari script. The system displays and speaks the same. It has been observed that a new language can be learned faster by frequently listening to the vocabulary and sentences of the language, that's why the given system provides the functionality of speaking the sentence in Hindi once it is represented in Devanagari script. The English sentences and words from the grammar tool books were given as input to the system for experimentation. We observe that while doing so the critical problem we encountered was translating English to Hindi. Sometimes insertion error for letters (only surfaced). The sentences represented using continuous tense and perfect continuous tense could not be translated correctly. The overall accuracy of system otherwise, is approximately 67%.",1.0,2.2750000000000004
1253,2278,1.55    2.45    2.85    ,Native Language Identification from South Indian English Speech,"Identification of the native language of a speaker from his speech in second language has vast applications. In this paper, automatic identification of the native South Indian language is proposed from the English speech of speakers in three popular South Indian languages: Kannada, Tamil and Telugu. The identification task is carried out first empirically, and then using the acoustic features and classifiers. A database of the English speech of speakers in three South Indian languages is collected for this study, in text-independent mode. Acoustic features linear predictive cepstral coefficients (LPCCs), perceptual linear predictive (PLP) coefficients and Mel-frequency cepstral coefficients (MFCCs) are used. The native language is identified automatically, by using two classifiers Gaussian Mixture Models (GMM) and GMM-UBM. The identification accuracy of 83% is obtained using GMM, which increased to 87.8% by using GMM-UBM, along with substantial reduction in the EER from 36.3% to 9.1%. The results obtained are encouraging when compared to the state-of-the-art accuracy at around 85%. Further, adding prosodic features to the feature vectors was attempted but with not much improvement in the identification accuracy. It is observed that identification of Kannada language from these speakers’ English speech is relatively easier, than that for Telugu language.",1.0,2.283333333333333
1261,2312,2.65    2.6    1.3    2.6    ,Infant Cry-Cause Severity Prediction Using Sub-band Spectral Analysis,"An infant cry is known to be the carrier of subtle information about their physical and mental states. Numerous attempts have been made towards understanding this acoustic means of communication for the neonates. In the current work, acoustic analysis towards characterising cry signals is done using Sub-band spectral analysis, divulging insightful spectral trends for the sub-bands limited by 4, 5 and 6 KHz frequency boundaries, with respect to lower order frequency bands. Also, cry-cause severity prediction towards the categories that help common people to relate with is attempted as part of this work. For this, different cry signals for various cry-causes are classified Severe and Non-severe, using a set of low-level descriptors like Sub-band spectral energies and their basic statistical functionals. A Cubic function based Support vector machine is implemented for classification of cry sounds that distinctly characterize cry-causes, using the aforementioned feature-set. The utility of these functions towards severity prediction is observed with respect to phonation intensity during infant cries. Observations made and results obtained in this work can further assist with the tasks related to infant cry-cause analysis.",1.0,2.2875
1047,1717,1.6    2.7    2.4    2.45    ,Spoofing Detection - a Challenge in ASV– a Survey,"In the world of information, acceptance of applications pertaining to secured handling of information demands reliability. In this regard, spoofing attack on Automatic Speaker Verification (ASV) system is an ongoing challenge to the research community. The major advancement in this field is the availability of common datasets to the research community. Literature pertaining to countermeasures has witnessed various methodologies and approaches proposed for the various acknowledged attacks i.e., voice conversion, speech synthesis and replay-attack. This survey is done categorizing the various proposed ASV countermeasure methodologies based on the contributing characteristics to ASV and those which tend to detect the spoofed signal.  The literature survey has witnessed that most of the works to detect spoofing attack has followed the approaches used for automatic speaker verification systems. Though many of these works have shown performance improvement based on the available metrics, much need to be achieved to detect known as well as unknown attacks. The work mentioned in this survey per se may give a clear insight into the available approaches leading to a better mean to discriminate the spoofed signal, thus directing one towards a better selection of features and classifiers apt for the spoofing countermeasure related research.",1.0,2.2875000000000005
828,1142,2.05    3.4    2.15    1.6    ,A Comparison of Features for Replay Attack Detection,"speaker verification (ASV) systems are still vulnerable to different kinds of spoofing attacks, especially replay attack due to high-quality playback devices. Many countermeasures have been developed recently. Most of the efforts focus on the search for more salient features and many new features have been proposed. Five kinds of features, namely mel-frequency cepstral coefficients (MFCCs), linear frequency cepstral coefficients (LFCCs), inverted mel-frequency cepstral coefficients (IMFCCs), constant Q cepstral coefficients (CQCCs) and bottleneck features were compared on the public ASVspoof 2017 and BTAS 2016 datasets in this paper. Our experimental results show that MFCCs and bottleneck features yield comparable results. Both of them significantly outperform others (including the recently proposed CQCCs). However, the number of filters and cepstral bins are essential to the success of MFCCs.",1.0,2.3
1139,1961,1.9    3.35    1.6    2.4    ,Modified Hybrid DNN-HMM Based Acoustic Model for Improving the Performance of ASR System,"In this paper, we attempt to improve the performance of an Automatic Speech Recognition (ASR) system by modelling the acoustic component using modified hybrid Deep Neural Network (DNN)-Hidden Markov Model (HMM) architecture.  The applications of ASR system is increasing at an exponential growth. However, an efficient ASR system is difficult to realize because of the complexity inherited in speech signal. The state of the art techniques used conventional statistical algorithms such as GMM to model the acoustic component of ASR system. The assumptions of linear relationships and normal distribution have limited the performance of GMM. Here, prosodic and spectral features along with the learning capability of DNN are used in building ASR system. Euclidean norm is proposed to model the non-linearity of the DNN network. Such non-linearity can capture the underlying characteristics of the pattern. An average natural gradient descent algorithm is adopted to optimize the network. The goal is to achieve an optimized network to represent the complex non linear relationship between the uttered phonemes of speech signal. The performance of developed ASR system is evaluated in terms of  Phone Error Rate (PER) using TIMIT. The experimental results show better performance of the proposed ASR system over existing approaches.",1.0,2.3125
1109,1879,1.45    3.5    1.55    2.85    ,Characterizing Autism Speech of Children in English Vowels Regions,"Autism spectrum disorder (ASD) is associated with children’s
speech. In ASD, children cannot express or understand any
emotional state, and hence are not able to communicate properly
with the society. Autism speech, i.e., speech of children
affected by ASD, has peculiar characteristics that parents or
doctors can perceive easily. But very less attention has been
paid so far towards its acoustic analysis. This study aims at
characterizing the autism speech in English vowel regions using
acoustic features. Autism speech data has been collected
for over a year, by recording speech samples of autistic children
every week, coming for treatment at an autism hospital.
English vowels regions are focused because of their relatively
longer duration. Changes in the excitation source characteristics
of autism speech signal are analyzed by examining differences
in the instantaneous fundamental frequency (F0) derived using
zero-frequency filtering method. Changes are examined in the
five vowels regions /a/, /e/, /i/, /o/ and /u/. Observations are validated
by examining changes in the production characteristics
signal energy. The mean F0 and signal energy values are higher
for female autism speech, as compared to male autism speech.
The study may help towards automatic detection of ASD from
the autism speech signal.",1.0,2.3375
839,1168,2.45    3.35    1.25    2.3    ,Evaluation of Speakers' Affective States Using Long Short-Term Memory Neural Networks,"Emotion recognition is an important aspect of natural human-computer interaction, however, achieving accurate classification is an ongoing challenge. Speech is one of the main modalities for emotion communication and it can be used as a marker for different emotional expressions.

Much progress has been made in the field of natural language processing in recent years, particularly with the advent of attentive neural networks. Attentive neural networks in general, and Long Short-Term Memory neural networks in particular, have the advantage of recognizing long-term dependencies in problems of understanding containing a temporal dimension. Building on previous studies which have demonstrated the effectiveness of LSTMs for emotion recognition, we combine state-of-the-art speech analysis tools with an LSTM network to maximise classification accuracy on diverse emotional data, including a corpus of atypical emotional expression.

In this study we aim to correctly label speaker affective state in two corpora, as part of the INTERSPEECH 2018 ComParE challenge: one corpus features individuals with disabilities, the other is comprised of a large number of recordings associated with self-assessed emotion. Prosodic features will be extracted with standard speech analysis tools, and the resulting features will be used to train an LSTM neural network to recognize emotion in speech.",1.0,2.3375000000000004
1104,1867,2.7    1.25    2.5    2.95    ,Optimal Code Switched Data Collection: an Approach for Low Resource Languages,"In a multilingual nation like India, code-switching is a natural way of conversation. Most of the Indic languages are low resourced with scarce database. Hence the development of a code-switched database with fair amount of code-switching requires a careful and optimal approach.  Though there has been several theoretical guidelines as well as textual analysis on the code switched data, the speech database collected mostly address only a particular category of people. This paper gives the analysis of the overall code-switching database collected with focus on Indic language Malayalam. Malayalam is the official language of the Kerala state with English as other tongue.  Various statistics of speech data collected with the background information of the speaker and the nature of code-switching is presented. This paper analyse the nature and extend of code-switch among different category of people. The speakers and areas of code-switching were classified using different parameters. The primary analysis of the Malayalam-English speech corpus is used to create a matrix which defines the optimal code switch instances vs the speaker category. This can be used as pointer for the critical process of optimal code-switched speech data collection which is a critical process for any low-resource languages.",1.0,2.35
1313,2550,2.25    2.2    2.6    ,Multi Model Authentication System Based on Audio and Visual Recognition,"With the increasing demand of security in many fastest growing secure applications, multimodal authentication is the most prominent authentication system. User authentication through speech and facial recognition is an important multimodal biometric option to enhance the security of a system. In this approach, we consider audio-video person authentication based on two sources of information which consists of an audio signal as well as the video signal. Single modality evidence has limitations in both security and robustness. Combining evidence from two modalities will increase the authenticity and integrity of the system. 
In our research, we had focused on audio-visual data of speakers and based on which all the experiments are carried out. Speech, as well as facial features of speakers, had been extracted separately from audio-visual data. Mel Frequency Cepstral Coefficients has been used for speech recognition and Viola Jones algorithm had been used for visual feature extraction. The Audio and Video processing are done in two separate phases using machine learning algorithms. The results of both the modalities are then combined at the decision level. It has been observed that multiple modalities of both Audio-Visual information give immensely good results compared to a standalone single modality.",1.0,2.35
1189,2097,2.85    2.7    2.55    1.45    ,Robust Speech Enhancement and Dereverberation Using Spherical Microphone Array,"Speech enhancement is necessary to increase intelligibility of speech signal to improve the accuracy of speech recognition systems. In this paper, a method for speech enhancement and dereverberation is presented. The proposed method is designed for a spherical microphone array, as of which acoustic model is also formulated in the spherical harmonics domain. Speech Enhancement is performed by super directivity beamformer which acts as a spatial filter with superior directivity factor. The proposed algorithm is used to achieve dereverberation by using plane-wave decomposition, based on DOA estimation and the separated waves are filtered for speech enhancement in the direct sound signal. The performance of the proposed algorithm is demonstrated using multi-speaker data collected in a real environment.",1.0,2.3875
1075,1786,2.5    1.2    3.5    2.4    ,Automatic Speech Recognition over GSM Networks with Narrowband Speech under Different Channel Conditions,"There are lot of efforts to make ASR as natural interface between humans and machines for services like enquiring for railway reservation, cost of commodity with Remote Speech Recognition (RSR). Client-server based ASR applications using communication networks can be divided to three modes. In th Network Speech Recognition (NSR). the user’s speech is compressed using conventional speech coders, and is transmitted to the server for ASR. NSR won’t require any changes to the existing infrastructure of network or to the mobile phones, except addition of a server for feature extraction and Speech Recognition. However, the speech coding for speech compression and channel noise will degrade the performance of ASR. The studies are carried out to evaluate the performance of ASR over VoIP and Wireless Networks under different channel noise conditions with the latest narrowband speech and channel coding standards. In this paper, the performance of the ASR using different channel noise conditions, when FR, EFR, HR and AMRNB speech and channel coding standards are used in the GSM wireless network are reported with critical analysis. A common ASR toolkit SPHINX and speech database TIMIT are used for evaluation of the performance Automatic Speech Recognition.",1.0,2.4
812,1095,2.55    2.5    3.4    1.2    ,Recognition of Emotion through Facial Expressions Using EMG Signal,"In human-computer interfacing and treatment of a person under depression, emotion recognition play important role. Facial expressions of a person reflect his emotional status without uttering a single word. EMG based emotion recognition systems are able to recognize true emotions of a person. 
Current research on EMG based emotion recognition reports accuracy in the range 69% to 91% in a particular emotional environment. Some emotions were recognized with high accuracy range, but in posed expressions. There is a scope for improvement for enhancing accuracy of emotion recognition in natural conversation. In this research work EMG dataset made available by Augsburg University and MIT laboratory for emotional environment is analyzed.  From 44 EMG signals six time- domain features including RMS mean, RMS maximum, RMS variance, mean absolute value, modified mean absolute value, and maximum peak value are calculated.  Elman neural network is used for classification of the emotions. The classification accuracy on 95.2% for Augsburg University dataset and 85.7% for MIT laboratory dataset is achieved. 
Index Terms: Emotion recognition, EMG, Elman neural network classifier",1.0,2.4124999999999996
808,1083,2.2    3.45    1.6    ,F0 Estimation Using LASSO Based Sparse TV-CAR Speech Analysis,"We have already proposed Time-Varying Complex AR (TV-CAR) speech analysis for an analytic signal based on MMSE, ELS or so on, and have evaluated the methods on speech processing such as speech coding, F0 estimation, speech enhancement and robust speech recognition. These methods are based on L2-norm optimization. Recently, sparse estimation is focused as big data can be used. L1-norm optimization is used to realize the sparse estimation. Sparse LPC analysis based on an L1-norm optimization has been proposed. However, it is not always efﬁcient for speech processing since speech does not provide the sparseness. In this paper, we propose sparse TV-CAR speech analysis based on LASSO(Least absolute shrinkage and selection operator). In order to evaluate the proposed analysis, F0 estimation using complex residual is realized by the IRAPT (Instantaneous RAPT) and compared by the estimation accuracy. The experimental results show that the proposed method can perform better for high level of additive Pink noise.",1.0,2.4166666666666665
1028,1669,2.9    2.25    3.45    1.2    ,Speaker Identification with Reference to Assamese Language Using Fusion Technique,"In this paper, a speaker identification system using fusion techniques in Assamese language is presented. The database has been built using fifteen Assamese speakers. The vocabulary for the database consists of ten Assamese words representing the Assamese digits from 0 to 9. Four sets of Assamese speaker’s dataset have been used in this study – clean data, noisy data with SNR 5, SNR 10 and SNR 15. The two fusion techniques are developed by combining three exiting techniques – Hidden Markov Model (HMM), Vector Quantization (VQ) and I-vector and speaker identification models have been developed using these techniques.",1.0,2.45
916,1383,2.35    2.05    2.2    3.3    ,Turn-Taking in a Concept Level Human Dialogue Simulation for Conversational Quality Estimation,"The experienced quality of telephone conversations depends not only on the parameters of the network  and degradations, but also on the type of conversation and its content. To asses the effects of time varying degradations like delay or packet-loss, the approach to simulate a human conversation on concept level is presented.
  Recent advances in user simulation for spoken dialogue systems, incremental dialogue processing and turn-taking prediction make such a simulation feasible. In this paper we present a framework that combines the techniques of user simulation, turn-taking modeling and incremental processing to simulate human conversation. Particularly, we simulate Short Conversation Tests and Random Number Verification tests that are often used for the assessment of conversational quality.
  We discuss how well the simulated conversations perform compared to real-world data in terms of the content of the dialogue and also in terms of the turn-taking behavior measured by double talk, mutual silence and the speaker alternation rate. We also show how the content of the dialogue and the turn-taking patterns influence each other.",1.0,2.475
1250,2272,2.3    2.35    3.5    1.8    ,Detection of Anxiety from Speech Using Vocal Air Pressure Distribution,"Anxiety disorder is a global health concern. It is of utmost importance in mental health monitoring.  With the wide acceptance of the tele-counseling centers, the need for non-contact detection of anxiety has great importance. This work is an attempt to detect anxiety from speech. Here the anxious speech was recorded just before the start of the compulsory student evaluation. It is very common that the students become anxious in these situations. As the anxiety decreases once the evaluation starts the recording was done just before the evaluation. The relaxed speech was recorded on a normal day after deep breath exercise. The vowels from the speech were found out and the vocal air pressure distribution at various sections of the vocal tract was estimated for each vowel. Principal Component Analysis was used to reduce the dimension of the number of vocal cord sections. Hidden Markov Model was used to find the temporal variation information and classify the pressure distribution as an anxious or relaxed vowel. The speeches from 9 speakers were taken. The model was trained and tested for each individual speaker. The work performed well for people with trait anxiety.",1.0,2.4875000000000003
844,1177,3.6    1.5    2.5    ,Signal Processing Cues to Improve Automatic Speech Recognition for Low Resource Indian Languages,"Building accurate acoustic models for low resource languages is the focus of this paper.   Acoustic models are likely to be accurate provided the phone boundaries are determined accurately. Conventional flat-start based Viterbi phone alignment (where only utterance level transcriptions are available) results in poor phone boundaries as the boundaries are not explicitly modeled in any statistical machine learning system.   The focus of the effort in this paper is to explicitly model phrase boundaries using acoustic cues obtained using signal processing.  A phrase is made up of a sequence of words, where each word is made up of a sequence of syllables.  Syllable boundaries are detected using signal processing.  The waveform corresponding to an utterance is spliced at phrase boundaries when it matches a syllable boundary. Gaussian mixture model - hidden Markov model (GMM-HMM) training is performed phrase by phrase, rather than utterance by utterance. Training using these short phrases yields better acoustic models.  This alignment is then fed to a DNN to enable better discrimination between phones. During the training process, the syllable boundaries (obtained using signal processing) are restored in every iteration. A relative improvement is observed in WER over the baseline Indian languages, namely, Gujarati, Tamil, and Telugu.",1.0,2.533333333333333
1132,1932,1.55    3.3    1.6    3.7    ,A Comparative Study of Accent Variations Using Vowel Sounds in Hindi and Odia Languages.,"This paper focuses on the accent variations in two different languages, with an emphasis on vowel sound analysis. In order to simplify the calculations and observational complicacies we can focus on the vowel content in a sentence uttered by an individual. It is found that a normal sentence consists mostly of vowels. In this work the first three formants, F1-F3 and fundamental frequency f0 of 10 Hindi and 11 Odia vowels, produced by 16 female and 16 male subjects were studied for linguistic comparison; there were very few dialectal variations in either language. To minimize the interference of various non linguistic factors, scaling and normalization was done over the data for better comparison of vowels. The cross-language comparison of the normalized data was analyzed from the geometric perspective using the Euclidean distance. The results show that the pronunciation   error was less due to the reduction of Euclidean distance between vowels of same IPA symbols. The findings of the study will likely be very useful in various speech processing tasks such as speech recognition and classification in different languages.",1.0,2.5374999999999996
976,1533,4.25    2.9    1.7    1.3    ,Automatic Analysis of Stuttered Speech Using Dynamic Time Warping for INTERSPEECH 2018,"Stuttering is characterised by repetition (of sounds, syllables, words or phrases), prolongation of sounds (audible and inaudible), interjections, revisions, incomplete phrases & broken words. The diagnosis and evaluation of the severity of a speech disorder is traditionally performed by experienced speech Language Pathologist using standardised tests. Recent experimental studies have focused on building time efficient automatic evaluation of stuttering. 
In this paper we propose a fully automatic stuttering evaluation system. A MATLAB based program was built to capture the audio recording of the signal and determine its fluency. For the development of training database, reading samples of 15 normal native speakers of Kannada along with 15 individuals with stuttering were taken. The tool uses dynamic time warping (DTW) to compare the segmented speech samples with the database in order to classify them. To validate the built program, simultaneous perceptual and automatic evaluation using the developed tool was carried out. The results indicated that the human made ratings of speech fluency disorder in read speech can be predicted using the proposed evaluation system. Future research could focus on the analysis of spontaneous speech by means of the developed tool.",1.0,2.5375
1142,1968,2.6    2.2    1.9    3.45    ,Binary Neural Network Trained with SADMM for Speaker Adaption,"Nowadays, with the development of mobile Internet, more and
more applications require small memory footprint, low power,
and high precision. In this paper, we first apply the state-of-the-
art Binarized Neural Network (BNNs) for wakeup and voice
command. Unlike traditional networks, the weights and activa-
tions of BNN are binary during training and inference, which
drastically reduce memory, real time and energy, but it always
loss accuracy. We solve the problem by using stochastic Alter-
nating Direction Method of Multipliers (SADMM) algorithm
instead of SGD to train our networks. What’s more, we use
self-learning to further improve the accuracy for specific speak-
ers. Our wakeup system can achieve 0.96 the acronym for Area
under curve (AUC) on signal noise ratio (SNR) of 30db with
230K (bit) memory, 141uW energy and 5 × faster on real time
processing on 28nm CMOS chips. Considering the design of
hardware, we do some optimizations on Batch normalization
layer. What is more, in order to improve the robustness and
reduce latency of our system, we propose simple yet effective
smoothing strategies for the posterior scores of neural networks.",1.0,2.5375000000000005
1057,1737,2.25    3.85    1.9    2.2    ,Implementation of Concatenation Technique for Text-To-Speech (TTS) Based Marathi Talking Calculator,"The indulgent acquaintance of mathematical basic concepts creates the pavement for numerous opportunities in life for every individual, including visually impaired people. The use of assertive technology for the disabled section of the society makes them more independent and avoid barriers in the field of education and employment. This research is focused to design
an Android-based application i.e. Talking Calculator for the Marathi native language. Marathi is an Indo-Aryan language spoken by approximately 6.99 million speakers in India, which is the third widely spoken language after Bengali and Telugu. The front end part of the application depicts the screen of a basic calculator with numerals displayed in Marathi. During runtime, each number is spoken as the specific key is pressed. It also speaks out the operation which is intended to be performed.
The concatenation technique is applied to speak out the value of decimal places of the output number. The result is spoken out with proper place value of a digit in Marathi. The
performance of the system is measured to the accuracy rate of 95.5%. The average run time complexity of the application is also calculated which is noted down to 2.64 Sec.",1.0,2.55
1089,1822,2.3    2.75    2.55    2.65    ,Speech Signal Based Broad Phoneme Classification and Search Space Reduction for Spoken Term Detection,"Depending on the acoustic properties, sound units can be grouped into six broad categories, namely, Silence (S), Vowels (V), Nasals (N), Fricatives (F), Approximants (A), and Plosives (P). This paper proposes a set of signal based features instead of conventional features such as Mel-Frequency Cepstral Coefficients (MFCCs) for broad phoneme classification. A DNN based broad phoneme classifier is trained to predict the phone class labels in continuous speech. These class labels are useful for applications like speech recognition, and Spoken 
Term Detection (STD). Across the TIMIT test data, the BPC gives an overall frame level accuracy of 75%. After smoothing the BPC labels, 83% of the broad phone regions are correctly detected by the system on an average. Effectiveness of the proposed broad phoneme classifier for search space reduction in STD is illustrated using TIMIT database.",1.0,2.5625
1328,2595,5.1    1.25    1.35    ,Deception Detection with Word Sense Disambiguation,"With the accessibility that one has to the internet, social media has become one of the major widespread tools of communication. With easy and rapid consumption of information on social media, this information has lead to devoid of supportive facts with an objective to mislead the readers. The dissemination of fake news, i.e., false and low quality facts and reports, has a negative influence on the society and lives of the individuals. Thus, fake news detection has become one of the major areas of interests and research. Due to a gradual rise in the number of sources of news and information, it is cumbersome to maintain a database differentiating reliable and unreliable sources. This study aims at automatic detection of fake news originating from linguistic cues. Linguistic cues can help to identify the articles written in sensational or the tabloid form of the journal. After a comprehensive review of the existing methodologies to detect fake news, we have overcome one of the major problems that the previous ideologies suffer. By retaining the sense of words in a sentence, we are able to detect linguistic cues much accurately, and thereby helping to distinguish fake news from the real one.",1.0,2.5666666666666664
854,1194,2.95    2.6    2.2    ,Assessing Performance of Bengali Speech Recognizers under Real World Conditions Using GMM-HMM and DNN Based Methods,"Real world Automatic Speech Recognition (ASR) system development in Indian scenario involves a number of technological challenges for conventional GMM-HMM and DNN based recent approaches. This requires rigorous performance review in different experimental setups under varying real world conditions. This paper reports our effort on ASR resource creation, transcription, system building and performance assessment for connected and continuous word applications in Bengali language (ranked seventh worldwide) using GMM-HMM and DNN framework on available open source toolkits. Baseline models are built from merging Bengali language dataset hosted on government sites with application specific 100 hours indigenous audio (for each application) collected under laboratory and target deployment scenario. After feedback analysis of live systems by real users, novel Error Handling Techniques like Signal Analysis and Decision, Confidence based  ASR output Polling and Runtime LM are implemented which results around 2% to 11% overall gain in WER with encouraging task success rates in final field trials. Results ultimately suggest that recent ASR approaches along with application, field, target user and runtime resource specific appropriate strategies will yield better acceptability of live ASR systems in India.",1.0,2.5833333333333335
1012,1624,1.35    1.95    2.7    2.3    4.65    ,Mood Recognition by Using Deep Learning :towards the Music Search,"The Indian classical music(ICM) has the power of inducing a specific type of emotion/rasa into the listener's mind. As almost every music piece is created to convey emotion, we prefer to listen to a song that suits our mood at that instant. Consequently, there is a need to classify music for automatic determination of perceived emotion. An excellent attempt has been made in the music information retrieval community to train a machine to automatically recognize the emotion of a music signal. But developing the automatic scheme to categorize ICM for expression of emotional content is lacking a precise outcome.
This paper presents a hierarchical framework of music emotion identification. The sets of features such that tone,  rhythm, harmony are extracted to represent characteristics of music clip. Deep learning approach is used for the task of automatic music mood detection. A set of segmental acoustic features is extracted to construct a  model using Deep Belief Neural Network(DBNN).  The emotional model used is the traditional Nava Rasa Model. We first identify the rasa using musical features and then mapped to emotions using Nava-Rasa Mood Model. The accuracy obtained is 93%.

Index Terms: Indian classical music, emotion/rasa, mood detection, DBNN",1.0,2.5900000000000003
930,1414,3.95    2.3    1.65    2.5    ,Text Prompted Speaker Verification Based on Phoneme Clustering with EMD and Cauchy-Schwarz Divergence,"For free-text prompted speaker verification where enrollment data is very short, the speaker model cannot be trained sufficiently. DNN and i-vector approaches work even worse due to lacking of data. GMM/HMM framework content matching is the state-of-the-art paradigm for short duration text-dependent speaker verification, in which models for individual lexical such as words, syllables, or phonemes are established for the background and speaker to make up mismatch. However, some of the phonemes do not occur in enrollment but happen in the testing recordings, and most of the phonemes have different preceding and succeeding phonemes, leading to coarticulation difference. These are called lexical and context mismatch. In this work, to overcome the data scarceness caused lexical mismatch and context mismatch, phoneme states are clustered applying Earth Mover’s Distance and Cauchy-Schwarz divergence as metrics. Performance improved as EER lowered 16.3%, minDCF08 lowered 15.6% for Earth Mover’s Distance metric, and EER lowered 14.3%, minDCF08 lowered 1.9% for Cauchy-Schwarz divergence metric.",1.0,2.6
810,1091,1.85    2.3    2.55    3.75    ,Study of Acoustic Correlates between Prosodic Features and Emotions in Marathi Language,"For designing intelligible and natural TTS systems researchers all over the world are studying relation between prosodic features and emotions in various languages. The objective of this research is to study acoustic correlates between prosodic features including duration, F0 and intensity for vowels and various emotions in Marathi language. Also, these prosodic features are analysed for vowels in words non-final and final syllables.  Pause durations occurring between words for various emotions are also studied. Marathi sentences spoken by a female speaker are recorded in five emotion styles: anger, fear, happy, sadness and neutral. For every sentence vowel mean duration, mean F0 and mean intensity are calculated. Long pauses, mid-pauses and short pauses for all the five emotions are analysed. The ANOVA analysis for F0 and intensity with p<0.001 indicate that these parameters can be used as cue for various emotion styles for Marathi language. Short vowel  duration and large number of short pauses are observed for angry emotion whereas long vowel duration and short, mid and  long pauses are observed equally for happy emotion. Above observations related to prosodic features and pauses would be useful to model Marathi TTS system.",1.0,2.6125
1292,2438,2.3    2.5    1.55    4.15    ,"Handset, Channel and Network Variability in Narrow-Band Telephone Speech Biometrics","The diversity of modern telephone communication introduces complicated distortions into a speech signal, and it is no longer sufficient to characterise the audio path in terms of a traditional Linear Time-Invariant (LTI) system with additive background noise.

It is important in the design of telephone-speech biometric systems that they are made robust to the presence of “real world” distortions such as noise and echo suppression, level and dynamic range “enhancements”, codec distortion, transmission errors, and packet loss concealment.

To investigate the degree of variability caused by current handsets, codecs, and transmission networks, we used a simple Interactive Voice Response (IVR) system to record a database of telephone recordings. The database includes calls from 89 adults, each using 5 different handsets and collected via different UK telephone networks.

This database was used to build a set of phonetic speaker models, and phoneme-level scores were calculated to indicate the effect of the respective conditions on speaker verification performance. Statistics of the speaker models were used to identify some common characteristics of the handsets and communication channels, and to identify which aspects of a biometric speech signal might be the most informative.",1.0,2.625
995,1587,3.1    1.25    2.7    3.45    ,Prosodic Features of Japanese News Speech Read by Newscasters,"Speech of news transcripts read by newscasters is distinctive from standard Japanese. The prosodic features of Japanese news speech read by newscasters were analyzed to clarify the “newscaster-ness” quantitatively. A thirty-second opening talk read by six professionals in a neutral tension was utilized for analysis. A TTS with a standard Japanese prosodic model was utilized to synthesize non-newscaster speech. The analyses gave the results that newscaster speech has the following characteristics: I) the duration of speech segments is shorter, II) the pause duration of commas is shorter and that of periods longer, and III) F0 around the syllables just before periods is lower, compared to non-newscaster speech.",1.0,2.625
1046,1709,3.4    2.35    2.1    2.75    ,The Effects of Prosody Conversion on Non-native Listeners' Comprehension of English Speech,"With the growth of the Internet and globalization, hardships in English speech communication became more serious for non-native speakers. It would be desirable to develop a speech communication device which can convert English speech into a more intelligible one for listeners. We hypothesized that English speech would be more intelligible to non-native listeners by being modified into the listeners’ native language prosody. In this study, English speech and non-native listeners were assumed for native speakers of English and Japanese, respectively. The hypothesis was realized by speech modification generated by STRAIGHT. The modification model marked modification of sentence F0 contours of native speakers’ speech so as to include Japanese characteristics that were gained by the comparison between English of native and Japanese speakers. The F0 contours of native speakers’ speeches were converted to equalize by minimizing the variance of mean F0 values of words within a sentence. Then, the modified speeches were evaluated on the basis of listening experiments involving Japanese listeners. The experiments consisted of dictation tests and preference tests, and resulted in an increase in the scores. The results, therefore, verified the hypothesis. This also revealed the potential feasibility of the speech conversion method which facilitates spoken English communication.",1.0,2.65
1251,2273,2.8    2.5    2.7    2.7    ,Classification of Emotion and Heartbeats Using DNNs and Gradient Boosted Decision Trees: IIIT-H Submission for the Atypical & Heart Beats Sub-challenge,"The INTERSPEECH COMPARE 2018: Atypical Affect subchallenge is about the classification of emotionally annotated speech of mentally impaired people. The Heart Beats(H) Sub-Challenge is about the classification of normal, mild and moderate or severe types of heartbeats. The main problem encountered in these two sub-challenges apart from the classification task is the class-imbalance present in the provided datasets. This class-imbalance problem in these two sub-challenges is addressed at the back-end part by exploring different sampling methodologies. Mel-frequency cepstral coefficients (MFCCs) features are considered at the front-end, deep neural networks (DNNs) and gradient boosted decision trees (GBDT) are investigated at the back-end as classifiers for the Atypical Affect sub-challenge. An 8-dimensional heart-beat specific feature set from the heart-beat rate, energy and Teager-energy contours are considered at the front-end, support vector machines (SVMs) along with the artificial neural networks (ANNs) are used for the classification purpose at the back-end for the Heart Beats(H) Sub-Challenge. The experimental results obtained from the two sub-challenges have shown higher classification accuracy and unweighted average recall (UAR) scores over the baseline system.",1.0,2.675
1300,2477,3.5    2.3    2.7    2.35    ,Code-switched Speech Recognition for Indian Languages: Multi-lingual and LID-switched Mono-lingual Approaches – a Critical Study,"We address the problem of efficient speech recognition for code-switched speech (for 2 Indian languages – Kannada and Urdu). We propose a multi-lingual phoneme recognition (Multi-PR) system using a multi-lingual phone-set derived from IPA based labeling convention, which offers seamless decoding of the code-switched speech. We show that this approach is superior to a more conventional front-end language-identification (LID)-switched mono-lingual phoneme recognition (LID-Mono) trained individually on each of the languages involved in the code-switched speech. We analyze the differences between these two systems, by showing that the LID-Mono approach suffers due to a trade-off between two conflicting factors – the need for short windows for detecting code-switching  at a high time resolution and the need for long windows needed for reliable language identification – which limits the overall performance of the Mono-LID system that suffers with high PERs at small windows (poor LID performance) and mismatched decoding conditions at long windows (due to poor code-switching detection time resolution). We show that the Multi-PR system, by virtue of not having to do a front-end LID switching and by using a multi-lingual phone-set, is not constrained by these conflicting factors and hence performs effectively on code-switched speech, offering low PERs than the Mono-LID system.",1.0,2.7125
1227,2200,3.9    1.65    2.8    2.5    ,Aggression in Hindi and English Speech: Acoustic Correlates and Automatic Identification,"In the present paper, we will present the results of an acoustic analysis of political discourse in Hindi and discuss some of the conventionalised acoustic features of aggressive speech regularly employed by the speakers of Hindi and English. The study is based on a corpus of slightly over 5 hours of political discourse and includes debates on news channel and political speeches.
Using this study, we develop an automatic classification system for identifying aggression English and Hindi speech, based solely on an acoustic model. The classifier achieves a remarkable F1 score of over 0.78 for both the languages. In this paper, we discuss the development of this classifier and discuss the errors that it makes.",1.0,2.7125
1025,1659,1.85    2.45    4.25    2.5    ,Feature Denoising Using CNNs for Robust Speech Recognition,"This paper proposes a simple convolutional neural network (CNN) based denoising system for speech enhancement (SE) in automatic speech recognition (ASR). The denoising network consists solely of convolutional layers and maps noisy filter bank features onto clean filter bank features. As foundation the speech recognizer from the Eesen toolkit of the Wall Street Journal (WSJ) example was used. In the experiments we found out, that the CNN denoising network can decrease the word error rate (WER) of the ASR system up to more than an absolute of 20 % in an environment with moderate signal-to-noise ratio (SNR). At the same time our denoising system increased the WER of speech recorded on high SNRs by only one percent. Additionally the denoising CNN generalizes onto multiple noise types and onto real world data. The results of our studies showed, that denoising audio using a CNN on the feature level is possible and can improve state-of-the-art speech recognition systems significantly for noisy environments while at the same time only slightly decreasing the performance for clean speech. Also the generalization capabilities of CNNs were demonstrated.",1.0,2.7625
885,1289,2.45    2    1.5    5.1    ,Modified FS1016 Speech Codec with Improvement in Perceptual Quality at a Lower Bit Rate,"The introduction of Code Excited Linear Prediction (CELP) based speech codecs was a major breakthrough in the field of speech coding. CELP based speech codecs use closed-loop Analysis-by-Synthesis method, to find out the best matching code-vector from the excitation codebooks. The FS1016 is a standardized codec based on the CELP paradigm which is the core of many speech coding standards that we use today. In this paper, a modified FS1016 speech codec using split vector quantization with reduced bit rate is introduced with less computational complexity and memory requirements and without any considerable degradation in speech quality. Further, bandwidth extension of speech using Sum Product Network and Hidden Markov Model is done at the receiver side, which improved the speech quality considerably without any increase in bit rate. The SPN models 20 log spectral bins of the wide band speech and the HMM can model the temporal evolution of the random variables. A noise removal block has been incorporated at the front-end of the speech codec that uses voice activity detection and spectral subtraction techniques to remove the noise from incoming speech signal. The quality of speech at the receiver with the proposed codec is better than with FS1016 standard.",1.0,2.7625
1105,1870,3.15    1.75    2.8    3.35    ,Text-Independent SER with Ensemble Neural Network and Attention,"Speech is the most natural form of communication which has both external information such as context and abundant internal information such as speaker identity and emotion. Speech emotion recognition or the influence of speech to human’s mind is important for human-computer interface to be more humanized. The difficult of speech classification is its ambiguity of target information in two aspect: 1. the target information is mixed with non-target information in utter naturally. 2. the bad expression of human would make target information itself incomplete.
Previous works on text-independent classification such as emotion recognition is mainly focus on feature engineering that the feature is delicate designed according to human’s experience while neural network has the capability to learn the method to extract target information from utter as pure as possible automatically during the training processing. 
In this paper, ensemble neural network is designed to solve the challenges of ComParE 2018 as an end2end system that directly output the predict result. Since the data bases of these challenges are all scale-limited, there is another way to use the neural network : extract feature by neural network and use SVM to do prediction.
Index terms: text-independent speech classification, ensemble neural network, attention",1.0,2.7625
1011,1623,2.7    2.5    3.4    2.5    ,Improving Statistical Parametric Speech Synthesis and Voice Conversion by Using GMM on Generative Adversarial Networks,"This paper evaluates recent Generative Adversarial Networks (GAN), for GAN-based Text-To-Speech (TTS) statistical parametric speech synthesis and voice conversion systems, to verify the quality of synthetic speech. After training a small amount of data in the multi-speaker deep neural network (DNN) system in our experiments, we observe GAN-based DNN system could benefit from joining the preceding DNN layers with an additional Gaussian Mixture Model (GMM) layer at the final stage, and optimize the conversion in the results.
      By extracting feature space, voice pitch, Mel Frequency Cepstral Coefficient (MFCC) and testing multivariate normalization, and applying Dynamic Time Warping for feature matching and GMM to analyze the speech content, the designed GAN-based synthetic system is built by taking speaker’s GMM metrics into the estimation to result in the final synthesis. Thus, we design an English synthesizer throughout statistical voice models, which convert and synthesize speech and voice from the source and the target speaker. 
      The proposed GAN-based system using GMM layer after its multi-layers DNN, which shows the better performance is achieved with conversions of cross-gender and user-oriented evaluation, compared to the quality of previous sole GAN-based speech synthesis and voice conversion systems.",1.0,2.775
892,1311,2.3    4.35    1.7    ,Cocktail Party Single Speaker Separation Using CNN Based Denoisng AutoEncoder,"The human auditory cortex is able to recognize and focus on selective voices in a noisy and multi-speaker environment. 
Traditionally, audio separation is modeled as information processing, which includes design of filters, hand-selected features, and computational modeling of the human auditory cortex. Recently, deep neural network frameworks have been shown to be effective in learning the useful representations of a target speaker from a mixture of speakers.
In this paper, we introduce a novel approach for the segregation of monaural sound mixtures based on Denoising Autoencoders (DAEs) using a convolution neural network (CNN). Specially, we explore a training scheme for an encoder-decoder network based on a Kullback–Leibler (KL) divergence cost function proposed by Lee et al [1].
We evaluate our model using a mixture of speakers created from the LibriSpeech ASR dataset. The performance of the reconstructed audio is evaluated by calculating the signal-to-noise ratio (SNR) with respect to clean audio of target speaker. We compared SNR calculated on samples from test data for KL divergence loss with L2 loss. The SNR difference between 3.146 dB for the L2 loss while it decreased to 0.628 dB in case of KL divergence loss for 512 FFT points.",1.0,2.783333333333333
1271,2356,2.85    2.55    2.5    3.25    ,Arabic Listeners' Intelligibility of English Phoneme Contrasts in Background Noise,"Second language (L2) listeners' speech perception is more affected by noise than that of first language (L1) listeners. The present study investigates the effect of l1 – l2 phonemic contrast transference in the presence of various levels of noise. The experiment was designed to test the English phonemic contrasts perceived by 20 native Egyptian listeners in various levels of noise. In the identification task, they were presented with minimal pairs of sentences. The sentences represents 11 vowels and 22 consonants (in initial and final position) contrasts and were combined with four different signal to noise ratios.
In general, increasing the background noise decreases the speech sounds' intelligibility particularly, in the perception of second language. The results have showed that sonorants were of higher intelligibility in noise than stops, while fricatives came as the last in order. Voiced consonants were more intelligible than the voiceless. Some of the English sounds were of less intelligibility in noise by Arabic native listeners; this is due either to the possibility of obscuring some of the phonetic features through the noise or to the phonemic differences between the two languages’ sound system.",1.0,2.7875
1051,1724,2.6    2.75    3.9    1.9    ,"Overcoming Pre-training in DNN-based Voice Conversion Using ELU, Dropout and Adam Optimization","Recently, Deep Neural Network (DNN)-based Voice Conversion (VC) techniques have become popular in the VC literature. These techniques require pre-training of DNN for better initialization of DNN parameters, which leads to the fast convergence. Greedy layer-wise pre-training of the stacked Restricted Boltzmann Machine (RBM) or the stacked De-noising AutoEncoder (DAE) is used with extra available speaker-pairs data. This pre-training is time-consuming, and requires a separate network to learn the parameters of the network. In this work, we propose to exploit recently advanced methods to train DNN. In particular, several recently proposed activation functions were used, among which the Exponential Linear Unit (ELU) performs better with dropout and Adam optimization. In this paper, we present our work on the first Voice Conversion Challenge (VCC) 2016 database. In particular, we achieved an improvement of 0.04 dB in Mel Cepstral Distortion (MCD) without any pre-training with our proposed architecture compared to the existing pre-trained baseline system. The subjective analysis also confirmed that our proposed VC system is able to overcome the need of pre-training in the area of VC without compromising on speech quality and speaker similarity.",1.0,2.7875
881,1276,2.7    2.05    2.2    4.2    ,Speakers Clustering with Stochastic VQ and Clustering Quality Estimator,"Short segments speaker clustering has significant importance both for diarization and applications such as short push-to-talk (PTT) segments clustering. In this paper we present a new way to cluster speech segments by applying a stochastic VQ with a cosine metric together with a speaker clustering quality estimator based on logistic regression. The VQ is performed on  codebooks of different sizes, and the choice of the best clustering result is estimated using the logistic regression. The algorithm is tested on a large range of speakers, from 2 to 60. The results are compared to those of the mean-shift clustering method, which was already tested for this task several times. The results are a bit bellow those of the cosine similarity measure-based mean-shift clustering. The advantage is in the run-time which is approximately 10 times faster.",1.0,2.7875
947,1464,2.55    3.3    2.55    ,Supervised Variational Relevance Learning (SUVREL) Applied to Voice Comparison and Classification,"An introductory survey of the application of the Supervised Variational Relevance Learning (SUVREL), as a preprocessing tool, to voice comparison and classification was made in conjunction with the Principal Component Analysis (PCA) classifier. SUVREL's algorithm produces an explicitly obtained metric tensor with low computational cost which can be possibly suitable to perform speaker identification over low processing devices, for instance, cell phones. The results demonstrate a considerable increase in the total variance explained by PCA's first three components obtained before and after preprocessing with SUVREL, on the original qualitiy data and after addition of white noise: 73.17% to 99.87% and 79.15% to 99.96% for data from 100 male subjects and 71.09% to 99.84% and 76.22% to 99.93% for data from 100 female subjects. Using an ellipsoidal representation of each subject in the PCA's space of scores, the volume intersection was evaluated as an indicator of subject's voice similarities. Percentual decrease in the overall volume intersection after preprocessing ranged from 6.65 ± 0.05 % to 20.44 ± 0.05 %.",1.0,2.7999999999999994
1044,1703,3.55    1.9    2.4    3.35    ,Infant Screening System Based on Cry Analysis,"Acoustical investigation of infant cries has been a clinical and research focus in the recent years. Findings of several studies reveal the importance of cry as a useful window for early detection of several diseases and communication difficulties such as hearing impairment, intellectual disabilities, cerebral palsy etc. This motivates us to use a minimal interface system that can automatically classify infant cries into normal and pathological with the help of state of the art machine learning strategies’.
In this paper, we propose a software program for screening infants based on their cries. The proposed system is able to detect & classify infant cries into normal and pathological based on the acoustic input. To build and train the system, infant cries of normal and Low Birth Weight (LBW) newborn within 7 days of birth were considered. A pain induced cry elicited using the routine intramuscular immunization was recorded using a standard Olympus LS-100 recorder which was held about 10 centimetres away from the infant’s mouth. The acoustic correlates of these cries were used to build the software tool. Artificial Neural Network was employed to improve its functionality. Therefore we propose a screening tool for further accessibility and large scale implementation.",1.0,2.8
1238,2234,1.4    4.2    2    3.6    ,Feature Analysis for Text Independent Speaker Verification,"Mel Frequency Cepstral Coefficients (MFCCs) are the most widely used acoustic features for both automatic speech recognition (ASR) and speaker verification. Speaker verification and speech recognition are different tasks, so spectral processing that is tuned to enhance performance of one may not enhance or may even degrade the other. In this paper, we analyze processing steps used for computation of cepstral coefficient features for the purpose of automatic speaker verification. Our study shows that the filter banks typically used to compute MFCCs for ASR should be modified for the task of automatic speaker verification. Based on our analysis, we replace voice activity detection (VAD) with a simple energy threshold, which works well at a low computational cost. We also propose a method to characterize the resonance frequencies (formants) using high order cepstral coefficients. As resonance frequencies are related to the unique characteristics of the vocal tract of the speaker, the proposed features improve speaker verification performance.",1.0,2.8
1082,1809,3.3    2.95    2.4    2.6    ,Effect of Frication Duration and Formant Transitions on the Perception of Fricatives in VCV Utterances,"Estimation of the place of articulation may be useful for visual
feedback of the articulatory efforts in speech training systems.
Place perception for fricatives is reported to primarily depend
on the spectral characteristics of the frication segment, and
also on the formant transitions in case of non-sibilants. A
study is conducted to investigate the relative importance of the
transitions and the frication duration on the perception of the
unvoiced fricatives /f, s, sh/ in vowel-consonant-vowel
utterances with the three cardinal vowels, using utterances
with naturally occurring transition segments and synthesized
frications with durations varied from 30 to 300 ms. For
frication less than 50 ms, the sounds were perceived as the
stop with the place corresponding to the transition. They were
perceived as fricatives for larger frication durations. For
sounds with transition corresponding to /f/, the perception was
not affected by the spectral characteristics of the frication. For
sounds with transitions corresponding to /s/ and /sh/, spectral
characteristics of the frication dominated the place perception.
The findings show that estimation of the place of articulation
of fricatives may be improved by combining the place
information obtained from the transition and frication
segments.",1.0,2.8125
852,1191,2.65    2.55    1.85    4.2    ,A Survey Investigating Usage of Virtual Personal Assistants,"Despite significant improvements in automatic speech recognition
and spoken language understanding - human interaction
with Virtual Personal Assistants (VPAs) through speech remains
irregular and sporadic. According to recent studies, currently
the usage of VPAs is constrained to basic tasks such as
checking facts, playing music, and obtaining weather updates.
In this paper, we present results of a survey (N = 118) that analyses
usage of VPAs by frequent and infrequent users. We investigate
how usage experience, performance expectations, and
privacy concerns differ between these two groups. The results
indicate that, compared with infrequent users, frequent users of
VPAs are more satisfied with their assistants, more eager to use
them in a variety of settings, yet equally concerned about their
privacy.",1.0,2.8125
1150,1982,1.3    3.45    4    2.55    ,Robust Recognition of Tone Specified Mizo Digits Using CNN-LSTM and Nonlinear Spectral Resolution,"In this work, we attempt Mizo digit recognition under degraded conditions, using spectrograms as visual inputs to a Convolutional Neural Network-Long Short-Term Memory (CNN-LSTM) network. As a tone language, each digit in Mizo is associated with a specific sequence of tones. To emphasize the tonal information, low frequency resolution is increased by applying a nonlinear spectral resolution method. The use of nonlinear spectral resolution improves the recognition rate of the system, as evident from the word error rate decrease of about 4% when the training data contains speech data with similar noise profiles as in the testing data. When the training data is clean, improvement in recognition rate is about 2%, using the nonlinear spectral resolution method. The proposed method, compared with the Deep Neural Network-Hidden Markov Model (DNN-HMM) based baseline system, gives an improvement of around 40% and 15% for 0 dB and 5 dB SNRs, respectively when noise profiles of speech sounds included in training and testing conditions are similar.",1.0,2.825
1257,2291,2.5    3.4    2.6    ,The Prosodic Plot of Dialogues: a Conceptual Framework to Trace Speakers' Role,"In this paper we present a proof-of-concept study which aims to model a conceptual framework to analyze structures of dialogues. We demonstrate our approach on a specific research question – how speaker's role is realized along the dialogue? To this end, we use a unified set of Map Task dialogues that are unique in the sense that each speaker participated twice – once as a follower and once as a leader, with the same interlocutor playing the other role. This pairwise setting enables to compare prosodic differences in three facets: Role, Speaker, and Session. For this POC, we analyze a basic set of prosodic features: Talk proportions, pitch, and intensity. To create comparable methodological framework for dialogues, we created three plots of the three prosodic features, in ten equal sized intervals along the session. We used a simple distance measure between the resulting ten dimensional vectors of each facet for each feature. The prosodic plots of these dialogues reveal the interactions and common behaviour across each facet, on the one hand, and allow to trace potential locations of extreme prosodic values, suggesting pivot points of each facet, on the other.",1.0,2.8333333333333335
1205,2139,2.8    2.35    3.35    ,Improving Accented Speech Recognition with Fast Acoustic Model Adaptation,"Even with advances in deep learning based methods, current speech recognition systems suffer a drastic degradation in performance when there is a mismatch in the speech accent of the test data. In this paper, we focus on handling the mismatch in accent via the acoustic model where many prominent accent-based variations can be captured. We develop a fast and effective method for accent adaptation that generalizes well to unseen accents during speech recognition. The method involves identifying the last few layers of the neural network-based acoustic model as the most sensitive components in accent adaptation and modifying them with a fine tuning strategy. Several speech recognition experiments are performed using acoustic models initialized using the English Switchboard corpus, which are tested with a variety of accent mismatched data. In these experiments, the proposed method of accent adaptation provides significant improvements even when the target accent is not used in adaptation. Furthermore, a detailed analysis is also presented on how these improvements are achieved.",1.0,2.8333333333333335
768,82,2.4    3.7    3.3    1.95    ,LSTM versus N-Gram Language Models in a Perplexity-driven Topic Classification Approach within the Air-Traffic-Control (ATC) Domain,"Considering the dimension of the daily, worldwide air traffic in conjunction with the variety of aircraft specific flight scenarios, it is impossible to find an overall language model, which suits for every single aircraft situation. Therefore we describe a way how to classify an air traffic control (ATC) command to a certain flight topic, by analyzing via the perplexity criterion of topic specific trained LMs. The identified topic(s) can be provided to the pilot as textual feedback in order to compare them with the heard and perceived ATC command. Topic detection also allows us to generate a more accurate pre- and post-ASR adaptation process. In future works aircraft specific recognition LMs have to be adapted, based on the found topics, to match their current flight situation best. Within this classification mechanism we evaluate the level of accuracy/robustness, regarding a proper topic prediction, by considering LSTM and N-GRAM models. We have shown that despite the small databases, the LSTM models achieve better perplexity results than the N-GRAMs. It has also been proved that the topic prediction accuracy of LSTM models is much better than in the case of N-GRAMs. The improvements are up to 45% absolute based on unseen real data.",1.0,2.8374999999999995
883,1279,3.55    1.25    3.55    3    ,AudioEmergency: Audio Event Detection for Surveillance Systems with Limited Data,"Due to the non-invasive and robust characteristics of audio surveillance systems, there is a fast growing interest in exploiting these smart systems in real life applications. However, given the limited amount of available annotated data, it is a challenge to develop an audio event detection system, which is robust in various environments. This  paper  presents  a novel  framework to generate  robust  models  for  audio surveillance applications with limited available data. The proposed hierarchical approach detects presence of an event using a Dilated Convolution Neural Network, and identifies the event boundaries via a novel feature modeling technique, named: Audio2Vec with bi-directional many to many Long Short-Term Memory (BLSTM) classifier. The proposed solution identifies the inherit relation between the audio states represented by the features and targeted audio events. Experimental results confirm the effectiveness of our solution both on publicly available datasets and two realistic applications in car and home scenarios.",1.0,2.8375
985,1564,3.3    2.1    2.65    3.3    ,Incorporating Speaker Normalizing Capabilities to an End-to-End Speech Recognition System,"Speaker normalization is employed to reduce the performance drop in Automatic speech recognition system (ASR) due to speaker variabilities. Traditional speaker normalization methods are mostly linear transforms over the input data estimated per speaker, such transforms would be efficient with sufficient data. In practical scenarios, only a single utterance from the test speaker is accessible. The present study explores speaker normalization methods for end-to-end speech recognition systems that could efficiently be performed even when single utterance from the unseen speaker is available. In this work, it is hypothesized that by suitably providing information about the speaker’s identity while training an end-to-end neural network, the capability to normalize the speaker variability could be incorporated into an ASR system. The efficiency of these normalization methods depends on the representation used for unseen speakers. In this work, the identity of the training speaker is represented in two different ways viz. i) by using a one-hot speaker code, ii) a weighted combination of all the training speakers identities. The unseen speakers from the test set are represented using a weighted combination of training speakers representations. Both the approaches have reduced the word error rate (WER) by 0.6, 1.3% WSJ corpus.",1.0,2.8375000000000004
1042,1698,2.35    2.5    3.2    3.35    ,Code-mixed Automatic Speech Recognition System for Call Center Application,"The major problems in building an automatic speech recognition (ASR) system for call center application are code-mixing, code-switching, and training data sparsity. In this paper, we propose multiple techniques to overcome these problems in training the acoustic and language models. In acoustic modeling, the code-switching and code-mixing are handled by mapping phones in foreign languages to phones in the target language. Similarly, in language modeling, it is handled by transliterating the foreign language words to target language script. The data sparsity problem is handled by leveraging the resources from the non code-mixed sources from both target language and foreign language along with the code-mixed call center data. The set of experiments performed with the call center data have shown significant performance improvements with proposed techniques.",1.0,2.85
1016,1640,2.3    2.55    3.45    3.1    ,Novel Spectral Root Cepstral Features for Replay Spoof Detection,"Replay poses a greater threat to the Automatic Speaker Verification
(ASV) system than any other spoofing attacks, as it neither
require any specific expertise nor a sophisticated equipment.
In this paper, we propose a novel countermeasure by modeling
the replayed speech as a convolution of genuine speech
with additional impulse responses (due to the microphone, loudspeaker,
recording and replay environment). In particular, we
propose the new feature set, namely, Magnitude-based Spectral
Root Cepstral Coefficients (MSRCC) and Phase-based Spectral
Root Cepstral Coefficients (PSRCC), that performs better
than the baseline system (CQCC), on ASVspoof 2017 challenge
database, which gives 29.18 % Equal Error Rate (EER) on the
evaluation set. The proposed feature set detects the effect of
these additional impulse responses, in the quefrency-domain.
Experiments performed on evaluation set using MSRCC and
PSRCC, with Gaussian Mixture Model (GMM) as a classifier
gives 18.61 % and 24.35 % EER, respectively. On the other
hand, Convolutional Neural Network (CNN) classifier gives
24.50 % and 26.81 % EER, respectively. The score-level fusion
of MSRCC and PSRCC gives reduced EER of 10.65 %
using GMM and 17.76 % using CNN classifier, indicates the
complementary information captured by the proposed feature
sets.",1.0,2.85
1117,1894,2.5    2.25    3.5    3.2    ,Neural Network-Based Small-Footprint Flexible Keyword Spotting,"Low resource keyword spotting (KWS) systems are an important technique, e.g., for implementing a wake word on mobile devices. In the past feed forward neural network (FFNN) systems using whole word models were proposed for this task. While these systems work well, they have the disadvantage that they need to train a deep neural network for every new keyword to be detected on large amounts of recordings of this keyword (e.g., 2,000 or more recordings of the new keyword). In this paper we present a modified KWS system that can be configured to detect arbitrary keywords without the need of any additional training data of the keyword. Further, as the resources available on mobile devices become more powerful and research in recurrent neural networks (RNNs) progresses, we also investigated the use of several different kinds of RNNs instead of an FFNN, leading to a more flexible and better performing KWS system that can be deployed on mobile devices.",1.0,2.8625
956,1490,3    3.45    2.1    2.9    ,Implementation of a Speech Enabled IVR System Using a Novel Technique of Artificial Bandwidth Extension for Better Recognition Accuracy,"New blends of Speech-Enabled Interactive Voice Response (SEIVR) Systems have been replacing the existing time-consuming menu driven IVRs. But there is a significant reduction in the quality and intelligibility of the speech signal when transmitted through the telecom networks which use narrowband codecs. Providing wideband quality signal without much modification of the existing network infrastructure can only be possible with a novel technique of Artificial BandWidth Extension (ABWE). In this paper, ABWE is implemented using a QMF filter bank which band split the input speech into LF and HF components and further, the HF components are compressed and encoded using a novel data hiding technique. In the reconstruction phase, an artificial wideband speech signal is generated through a QMF synthesis filter bank. For the implementation of the proposed model, a client-server approach with socket programming on a single machine has been used assuming no noise and no transmission errors. A comparative analysis has also been done to find out the root cause of degradation in performances of SEIVR systems. The simulations on the proposed SEIVR model with ABWE have shown significant improvement in the speech recognition accuracy and overall performance.",1.0,2.8625000000000003
1115,1890,1.1    4.3    1.9    4.2    ,Role of Different Speech Tasks in Voice Based Classification of Healthy Subjects and Patients with Amyotrophic Lateral Sclerosis,"We consider the problem of speech based automatic classification of healthy subjects and patients with amyotrophic lateral sclerosis (ALS). In particular, we examine the role of different speech tasks for this purpose. In this regard, we consider three speech tasks, viz., spontaneous speech, diadochokinesis and sustained phonation. Classification experiments are performed using speech data from 25 ALS patients and 25 healthy subjects using two kinds of classifiers, namely support vector machine (SVM) and deep neural network (DNN). Suprasegmental features are used for classification. Experimental results using both classifiers reveal that the spontaneous speech consistently performs better than diadochokinesis and sustained phonation tasks for discriminating ALS patients and healthy subjects.",1.0,2.875
848,1184,3.5    2.85    1.65    3.5    ,Minimum Audible Movement Angle in Virtual Auditory Environment: Effect of Stimulus Frequency,"Virtual auditory display (VAD) technology has advantages for the simulation of dynamic auditory environment, which has brought an increasing need for understanding auditory motion perception in virtual auditory environment (VAE).  Minimum audible movement angle (MAMA) is a fundamental measure for auditory dynamic spatial resolution in VAE. In realistic auditory environment (RAE), binaural auditory system is insensitive to motion information, and auditory dynamic spatial resolution for the sound stimulus with frequency below 1000 Hz is better than for the stimulus with frequency above 1000 Hz. This study investigated the effect of the auditory stimulus frequency (i.e., from 500 Hz to 2900 Hz) on MAMA in VAE. Results showed that in VAE, the stimulus frequency influenced auditory motion perception in the similar way as in RAE in terms of a frequency-threshold at 1000 Hz, and the dynamic spatial resolution in VAE was significantly poorer than that in RAE. The findings in this study suggested that the sensitivity of auditory motion perception to sound stimulus frequency in VAE was similar with that in RAE, while the binaural auditory system was even more insensitive to motion in VAE than in RAE.",1.0,2.875
1077,1790,1.65    3.35    2.8    3.7    ,Multilingual Multi-Task Learning for Low-Resource Acoustic Modeling,"The following study investigates low-resource multilingual
acoustic model training with Multi-Task Learning (MTL) for
Automatic Speech Recognition. The main question of this re-
search is: What is the best way to represent a source language
with MTL to improve performance on the target language? The
two parameters of interest are (1) the level of detail at which the
source language is modeled, and (2) the relative weighting of
source vs. target languages during backprop.

Results show that when the source task is weighted higher
than the target task, a more detailed task representation (ie. the
triphone) leads to better performance on the target language. On
the other hand, when the source task is weighted lower, then a
less detailed level of source task representation (ie. the mono-
phone) is better for performance in the target language. Given
all levels of detail in the source task, a 1-to-1 weighting ratio of
source-to-target leads to best results on average.

This study uses Kyrgyz (audiobook recordings) as a tar-
get language and English (LibriSpeech subset) as a source lan-
guage.",1.0,2.875
876,1261,2.45    3.35    3.45    2.25    ,Computational Inference of Candidates Oratory Performance in Employment Interviews Based on Candidates Vocal Analysis.,"Employment interviews is a popular tool to select the most suitable person for the job at hand. There has been a growing interest in understanding what factors are assessed in the employment interviews. It is a key issue in organizational psychology and can be addressed as a social computing problem.  We approach the problem from a face-to-face, nonverbal perspective where we propose a system which will assess how well did the candidate speak during the interview based on the candidates’ voice. This paper presents a computational framework for the automatic prediction of the speaking performance of the person through vocal analysis, that we refer as Oration Index. We have applied multi-classification algorithms on a audio dataset of real job interviews where candidates are subjected to five psychometric questionnaires. Our framework automatically predicts the ratings for constructs like Confidence, Fluency, Energetic and Clarity with an accuracy of 70 \% and higher,  that constitutes to be the major four attributes of Oration Index.  At the last step, we analyze and find the important vocal attributes that contribute in prediction of Oration Index attributes. Thus, our proposed framework will evaluate the speaking performance of the interviewee based on his/her voice using multi-classification algorithms.",1.0,2.875
1133,1934,2.55    5.15    1.6    2.2    ,Speech Detection in Practical Environments,"To address the task of speech detection in practical environments, this paper uses a database collected under practical environmental conditions, and presents a voice activity detection (VAD) algorithm suitable for such practical conditions. The standard algorithms such as Adaptive Multi-rate VAD option 2 (AMR-2), fails to discriminate speech and nonspeech effectively on this data. It is shown that speech-specific features such as instantaneous spectral variance and periodicity, helps in the identification of speech regions. The speech-specific features are derived from the recently proposed single frequency filtering (SFF) method. Performance of the proposed algorithm is compared with that of the AMR-2 method.",1.0,2.875
1309,2514,3.1    4.05    1.5    ,Activity Boundary Identification through Action Sequence Feature Analysis,"Research in Conversation Analysis, a sub-field of Sociology founded in the 1960s, describes three organizing features of talk-in-interaction: 1) turn-taking among speakers, 2) the repair of troubles in speaking, hearing and understanding, and 3) co-produced action sequences. Turn-taking is the vehicle that drives social action; repair enables progressivity to occur in talk-in-interaction; and action sequences are the building blocks of social interaction with question-answer pairs being the strongest form. Our analysis of dyadic and multi-party business meetings uses sequence organization in combination with turn-taking related silence metrics to predict the boundaries between activity-based and topically coherent phases of interaction.  The calculation of non-normative, longer silences, in addition to a density of local consecutive sequence boundary actions – both opening and closing actions -- denoted by key word detection, enables activity boundary identification. Four sequence boundary actions are discussed: 1) completion markers (e.g. okay, thank you), 2) assessments (e.g. great, that’s good, very helpful), 3) conversation management (e.g. go ahead, let’s listen, future tense), and 4) time management (e.g. for the sake of time, three weeks from now).  Through activity boundary identification the overall interactional contour of any conversational engagement can be described.",1.0,2.8833333333333333
1027,1667,1.65    2.55    3.95    3.4    ,Effective SVD-based Deep Network Compression for Automatic Speech Recognition,"Neural network improves speech recognition performance significantly, but its large size of parameters brings high computation and memory cost. To tackle this issue, we propose an efficient SVD-based compression method. Our method is performed in following process. Firstly, we analyse the property of matrices singular values to learn the sparsity of each layer,  and apply SVD on the most sparse layers to minish accuracy loss. Secondly, we re-train the restructured model using our data-adopted ""over-all"" fine-tune strategy. Finally, we use ""whole-net"" ""aggressively"" iterative training to push the model size to extreme to keep the accuracy. By performing our fine-tune free method, we can gain 50% compression ratio for single specific ""sparse"" layer after applying SVD, while maintaining almost the same accuracy. When fine-tune is allowed, our iterative fine-tune method can boost the compression ratio without accuracy loss. We evaluate the proposed approach on the two different LVCSR tasks. On AISHELL mandarin dataset, our proposed approach reduced parameters size of TDNN by 75%. Similar experimental result is obtained on TIMIT. Both results is gained with negligible accuracy loss.",1.0,2.8874999999999997
1211,2151,3.3    1.15    3.55    3.55    ,Exploring How Duration of Glottal Pulse Phases Relate to Jitter and Shimmer: Evidence from EGG Signals,"Synthetic glottal pulse waveforms are frequently used to generate voice waveforms in speech synthesizers according to the source-filter model. The introduction of jitter in the model makes several alternatives to differ, which would otherwise be equivalent. It has been reported that voice perturbation measures are severely influenced by the way jitter is modeled into the glottal pulse. Pathological voices usually display significant amounts of jitter, and voice perturbation measures are frequently used to evaluate the severity of the symptoms in pathological speech. The performance of the methods for measuring voice perturbation, on the other hand, is usually assessed using synthetic signals. Therefore, it is highly desired that the synthesis model used to introduce jitter closely resembles the way real life pathological signals behave.
This paper studies the relationships between jitter, shimmer and the duration of glottal pulse’s phases according to available electroglottographic signals. Results points towards the prevalence of a piecewise warping model of the influence of jitter into the glottal pulse phases, with glottal closure instants as pulse boundaries. Some implications of these results for the synthesis of pathological grade voices are discussed.",1.0,2.8874999999999997
1002,1604,1.45    1.8    4.05    4.3    ,Improvement Method of Speech Intelligibility by Enhancement of Spectral Variation,"It is hard to listen to an announcement in a noisy environments such as in a train while running. In this paper, to improve speech intelligibility in a noisy environment, we focused on the time variation of speech spectral representing spectral flux from the analysis result which clear speech has larger spectral flux than normal speech. Therefore, we propose speech enhancement method of spectral variation to improve the sound quality evaluation value of STOI. In the evaluation results of STOI score, enhanced speech by proposed method improved more than original speech in all the SNR conditions (SNR = -5, -10, -15 dB). Also, the significant difference in the dangerous rate of 5% was found in SNR = -10, -15 dB.",1.0,2.9
770,86,4.25    2.9    1.9    2.55    ,Improving Automatic Speech Recognition by Classifying Adults and Children Speakers into Separate Groups Using Speech Rate Rhythmicity Parameter,"When children's speech is transcribed using acoustic models trained on adults' data, a severely degraded recognition performance is obtained. Similar degradations are noted on recognizing adults' speech using an automatic speech recognition (ASR) system trained on children's speech. This problem can be overcome by using two separate ASR systems for the two groups of speakers. But this approach requires an effective technique to detect whether the given data is from adult or child speaker. In this paper, we present a very simple and novel technique to do the same. The proposed approach is based on speech-rate rhythmicity parameter (SRRP). Since the speaking-rates for adults and children differ significantly, the SRRP values are also very different for the two groups of speakers. Hence, by computing the SRRP value for a given speech utterance, it can be easily determined whether it is from adult or child speaker. The corresponding ASR system can then be used to achieve improved recognition performance. Alternatively, existing techniques for improving children's speech recognition on adult data trained systems can be directly applied once it is known that the data is from a child speaker. Both these aspects have been experimentally validated in this work.",1.0,2.9000000000000004
1026,1663,2.45    3.2    2.55    3.3    3.05    ,Excitation Source Features of Speech for Emotion Classification with Convolutional Neural Network,"The objective of this paper is to develop a speech-based emotion classification system. It is known that different emotions in speech arise due to changes in the production mechanism especially, in the excitation source. These changes in the excitation source cause amplitude variations in the harmonics of the speech signal segment. In the present study, harmonic features, excitation source features, and prosody features are used to classify different emotions. Harmonic amplitude variations are calculated using adaptive Quasi-Harmonic Analysis (QHA). The excitation source and prosody feature set consists of fundamental frequency (F0), Strength of Excitation (SoE), Energy of Excitation (EoE), duration features and their respective statistics. Convolutional Neural Network (CNN) is explored for the classification. The INTERSPEECH 2018 Computational Paralinguistics Challenge: Atypical Affect Sub-Challenge database (EmotAsS) is used for the study. As the database is imbalanced among the classes, the classifier may not be generalized. To overcome this problem, synthetic minority oversampling technique (SMOTE) is used. The SMOTEed feature set is used to train CNN and has given improved results when compared to the baseline system.",1.0,2.91
801,1066,2.05    4.3    1.9    3.4    ,End-to-End Model for Speech Enhancement by Joint Real and Imaginary Masking,"Recently, phase processing is attracting increasing interest in speech enhancement community. Some researchers integrate phase estimations module into speech enhancement
models by using complex-valued short-time Fourier transform(STFT) spectrogram based training targets, e.g. Complex Ratio Mask(cRM)[10] or estimating phase given enhanced
magnitudes merely by exploiting correlations between neighboring STFT frames, e.g. Griffin-Lim(GL) algorithm[1]. We would argue that defining a training target in time domain can
alleviate the inconsistent STFT spectrogram problem which cRM cannot address and estimate phase spectra by exploiting features of magnitude and phase spectra. Another important
factor in our method is the usage of fully convolutional neural network. It allows us to extract information from the STFT of audio signals and then resynthesize high-quality audio signals with efficiency. The experiments comparing our method with other methods are conducted to confirm both our objective function and our network architecture will have significant improvements in speech quality. From our experimental results,
we are assured that our method can enhance noisy speech audios with both efficiency and effectiveness.",1.0,2.9125
1014,1634,3.25    2.45    3.25    2.75    ,Single Frequency Filtering Representation of Speech for Blind Speech Separation Using Non-negative Matrix Factorization,"Blind Speech Separation (BSpS) is a problem of separating individual speech sources from the given speech mixtures of concurrent speakers, without any prior information of sources or mixing environment. Some BSpS algorithms perform Non-Negative Matrix Factorization (NMF) of Short-Time Fourier Transform (STFT) of mixtures. The performance of such existing algorithms depends on the choice of parameters such as type, shape and size of window, and hop size used to generate the STFT. In addition, artifacts are generated due to overlap of sources in STFT domain. To avoid the heuristics of choosing the parameters and to reduce artifacts in STFT based methods, we propose to use Single Frequency Filtering (SFF) based time-frequency representation of audio mixtures to perform BSpS. The database of Signal Separation Evaluation Campaign (SiSEC), dev1, consisting of stereo recordings of 3 and 4 concurrent speakers in reverberant conditions is used for evaluation. The performance is evaluated using PEASS toolkit. Our proposed SFF   based approach for speech separation shows an improvement in Overall Perceptual Score (OPS) compared to a STFT based Blind Speech Separation method.",1.0,2.925
1107,1875,2.55    2    2.15    5    ,Multilingual Acoustic Models for Low Resource Languages,"With the modeling power of deep networks, there has been an increasing emphasis on data-driven approaches to automatic speech recognition (ASR) in the recent past owing to the availability of substantial train data. This paper presents our work on developing robust acoustic models in ASR for languages with limited training examples using deep neural network (DNN) architectures. We explore multitasking and transfer learning approaches which use a common idea of using resources from other languages. These methods include: (i) using features that are language independent and encode phonetic classification information. We investigate features that generalize well across different languages, (ii) transfer learning approach called generalized distillation where the goal is to output a multinomial distribution which is a more achievable scenario than a pure classification task. Privileged information from a multilingual model that is assumed to know something about the true posterior distribution is used as a teacher. The experiments were performed on Phrasal as well as Conversational data for three Indian languages, namely, Gujarati, Telugu and Tamil.",1.0,2.925
792,1031,4.3    1.45    2.85    3.15    ,Unsupervised Discovery of Similar Audio Events via LSH,"When a call is being established by an automated application, such as an automated dialer, the first interest is to know if someone answered, if the line is busy, if it is an answering machine, etc. Call Progress Analysis algorithms are used to automatically classify the call into one of the aforementioned categories. Among others are “carrier audio messages” used by telephone service providers to predefine the outcome of the call. The normal approach used today to discover new carrier audio messages is to simply listen to a large set of audio recordings, which is highly inefficient and time consuming.  We explore in this paper the use of Locality Sensitive Hashing (LSH) to efficiently search for similar audio clips within a large dataset of audio recordings. We assume, without loss of generality, that the recurrence of similar audio events are more likely related carrier audio messages. We show by results obtained, that this approach can be very useful in the task of unsupervised discovery of new carrier audio messages over a large set of audio recordings.",1.0,2.9375
841,1170,3.55    3.2    2.5    2.5    ,Analysis of Vowel-to-Color Sentiment Association Characteristics Based on Speech Features,"Vowel-color association characteristics have been studied in the field of phonetics and perception. Though it has been reported that selected color categories after listening vowel categories have similar trends in multiple languages, their sentiment correlations have not yet been thoroughly studied from the viewpoint of speech features. We tried to find sentiment association characteristics between color parameters and speech features directly to have better understanding of cross-modal correlations and to find underlying principles for multimodal applications. Vowel samples uttered by 4 male and 3 female speakers were employed to associate colors after listening them by 34 subjects. Statistical analyses showed the advantage of employing RGB color parameters and speech formants directly to conventional color category to vowel category mapping. The selected color distributions in the F1-F2 plane clearly show that the acoustic speech resonance (i.e. F1 and F2) -RGB correlations can more consistently explain their sentiment correlations. Moreover, by incorporating our sentiment association experiment results using  formant-synthesized speech, their correlations can be attributed to F1 and F2 rather than vowel categories. We believe that this finding in cross-modal correlations will serve for not only scientific understanding but also further studies and applications using cross-modal information mapping.",1.0,2.9375
798,1056,4.15    2.4    2.6    3.35    2.2    ,Towards End-to-end Blind Bandwidth Extension with Wasserstein GAN Approach,"In this study, we propose an end-to-end framework that utilizes
Wasserstein Generative Adversarial Networks for blind bandwidth extension. While most blind bandwidth extension algoithms overlook the phoneme structure of the speech signal, our
proposed framework is built up from spectro-temporal re gions
of audio. We employ fully convolutional neural network to estimate high-quality audio from its degraded version directly. And the results are more natural and intelligible with the introduction of Wasserstein Generative Adversarial Networks(WGAN). The experiment results show that our framework is able to get a full-band audio signal with only a degraded narrowband version given.",1.0,2.94
1324,2584,2.85    2.7    4.25    2    ,Unsupervised Query by Example Spoken Term Detection Using Features Concatenated with Self-Organizing Map Distances,"In the task of unsupervised query by example spoken term detection (QbE-STD), we concatenate the features extracted by a Self-Organizing Map (SOM) with features learned by an unsupervised GMM based model at the feature level to enhance the performance. More specifically, The SOM features are represented by the distances between the current feature vector and the weight vectors of each SOM neuron learned in an unsupervised manner. After fetching these features, we apply sub-sequence Dynamic Time Warping (S-DTW) to detect the occurrences of keywords in the test data. We evaluate the performance of these features on the TIMIT English database. After concatenating the SOM features with the GMM based features together, we achieve an improvement of 7.77\% and 7.74\% on Mean Average Precision (MAP) and P@10 on average.",1.0,2.95
856,1196,4.25    3.4    2.5    1.7    ,Learning to Repair General Purpose ASR Output to Suit Domain Specific Use,"The availability of large amount of speech corpus and the ability to machine
learn have made general purpose automatic speech recognition (gpASR) very powerful.
However, these gpASR perform with limited success in a domain specific scenario
because of several reasons. Further, the accent of the speaker and the environmental conditions in which the speaker speaks a sentence may influence the speech engine to recognize certain words inaccurately. Many approaches to improve the accuracy of ASR output exist. However, in the context of a domain and the environment in which a speaker speaks the sentences, gpASR output needs a lot of improvement to provide effective speech interfaces to domain-specific systems. In this paper, motivated by some of our previous work, we propose an approach that combines machine learning (ML) and
bio-inspired evolutionary development (Evo-Devo) approaches to repair the output of a gpASR. The ML approach is able to learn and perform what can be called the acoustic
repair, while the Evo-Devo approach is able to repair the syntax and semantics
of the gpASR output.",1.0,2.9625
1229,2206,3.5    3.8    1.7    2.85    ,Gaussian Process Model of I-vectors for Indian Language Identification,"This paper explores Gaussian Process (GP) to model the compact i-vectors for the task of Indian language identification. i-Vectors are popular in speaker or language recognition task since it provides excellent performance without using any post processing or back-end techniques. Further improvement in performance are observed when it combines with other back-end system. This paper proposes the use of GP at the back-end to model the i-Vectors. GPs are Bayesian nonparametric models that are capable to capture highly nonlinear data relationship. Experiments have been conducted on NIT Silchar Language Database (NITS-LD), prepared for 12 Indian languages using All India Radio (AIR) broadcast news and OGI-MLTS database. Experimental analysis suggests that proposed method provides comparable performance with the baseline i-vector/ PLDA and i-vector/ SVM system.",1.0,2.9625
1287,2421,2.6    4.2    2.1    ,Fast Browsing of Large Quantities of Found Data,"We aim to explore prohibitively large audio collections rapidly using a method that exploits the insight that people can make fast judgements about lengthy sound recordings rapidly by listening to temporally disassembled audio (TDA) segments played simultaneously. We have shown proof-of-concept previously; here we develop the method and corroborate its usefulness. We conduct an experiment with untrained human annotators, and show that they are able to put in place meaningful annotation on a completely unknown 8 hour corpus in a matter of minutes.

The audio is temporally disassembled and spread out over a 2-dimensional map, and participants explore the resulting soundscape by hovering over different regions with a mouse. We used a collection of 11 State of the Unions given by 11 different US presidents, spread over half a century in time, for a corpus.

The results confirm that (a) participants can distinguish between different regions, and can describe the general contents of these regions; (b) the regions identified serve as labels describing the contents of the original audio collection; and (c) that the regions and labels can be used to segment the temporally reassembled audio into categories. We include an evaluation of the last step for completeness.",1.0,2.966666666666667
1221,2175,2.75    2.45    1.65    5.05    ,Combined Mel-Frequency Cepstral Coefficients and Spectral Centroid Magnitude Coefficients Features for Acoustic Scene Classification Using Deep Neural Network,"In this paper, we propose a feature set by concatenating Mel-
Frequency Cepstral Coefficients (MFCC) and Spectral Centroid
Magnitude Coefficients (SCMC) for Acoustic Scene Classifica-
tion (ASC) using Deep Neural Networks (DNN). MFCC are
used to capture the acoustic characteristics such as spectral en-
velope of acoustic scene in each frame. Also, it carries the av-
erage energy of the sub-band as a single dimension. SCMC are
used to capture distribution of energy in a sub-band effectively.
An experiment is carried out on Tampere University of Technol-
ogy (TUT) Acoustic Scenes 2017 Dataset. We used a DNN ar-
chitecture for utterance level classification. Performance of the
proposed system on a four-fold cross-validation setup is 80.2%,
which gives 5.4% relative improvement with respect to the base-
line system that uses log-Mel band energies with Multi-Layer
Perceptron model (MLP).",1.0,2.9749999999999996
1019,1643,3.3    4    2.05    2.6    ,Automatic Detection of Hypernasality Using Modified Group Delay Feature,"Hypernasality is a speech disorder in cleft palate (CP) children. Hypernasality detection is done by analyzing the vowels present in the speech stimuli. The presence of nasal peak in the vicinity of first formant $F_{1}$ is an important acoustic cue for the hypernasality detection. Generally, a two-step process is followed for hypernasality detection where first of all selection of vowel resign from stimuli is done using manually annotated points and then extraction of a feature from the selected vowel region and classification is done automatically. In this work, a method of hypernasality detection is proposed where the selection of vowel region, feature extraction and classification are done automatically. The method uses the vowel onset points (VOPs) for automatic selection of vowel region. The modified group delay feature (MODGDF) is then extracted from the vowel region. MODGDF is derived from the high resolution modified group delay spectrum which can resolve the closely spaced nasal peak and first formant $F_{1}$ and hence can capture the nasality evidence in a better way. When the feature is used for the hypernasality detection using support vector machine (SVM) classifier it gives an accuracy of 83.50 \% and 93.21 \% for /a/ and /i/ vowels respectively.",1.0,2.9875
1190,2101,2.15    3    3.25    3.55    ,Learning Dialogue Manager from Human-human Corpus with Discrepancy Awareness,"Dialogue policy learning is essential in Spoken Dialogue Systems (SDS) that predicts the next system actions in response to a current user utterance. Depending on the considered algorithm, the dialogue manager (DM) can be trained directly from the human-human dialogue corpora. Most existing researches equally treat every example i.e. slots and intents context with the corresponding system action, and train the DM from all the humans that defined as the system role in dataset. However, within a corpus, there will inevitably exist discrepancy between action styles of different humans that defined as system role, which may hurt the efficiency of training a human-like DM. To address this issue, we present Learning Dialogue Manager with Discrepancy Awareness (LDMDA), a method that training a deep recurrent neural network model for system action prediction using the examples of the target human acting as system role and extra ones with low discrepancy from other humans also acting as system role. Two simple but effective methods are proposed to identify behavior discrepancy. Based on the experiments on DSTC4 human-human dialogue corpus, we compare our model with baseline models and experiment results demonstrate the effectiveness of our approach.",1.0,2.9875
1169,2021,2.7    3.35    1.55    4.4    ,Towards Expressive Prosody Generation in TTS for Reading Aloud Applications,"Conversational interfaces involving text-to-speech (TTS) applications have improved expressiveness and overall naturalness to a reasonable extent in the last decades. Conversational features, such as speech acts, affective states and information structure have been instrumental to derive more expressive prosodic contours. However, synthetic speech is still perceived as monotonous, when a text that lacks those conversational features is read aloud in the interface, i.e. it is fed directly to the TTS application. In this paper, we propose a methodology for pre-processing raw texts before they arrive to the TTS application. The aim is to analyze syntactic and information (or communicative) structure, and then use the high-level linguistic features derived from the analysis to generate more expressive prosody in the synthesized speech. The proposed methodology encompasses a pipeline of four modules: (1) a tokenizer, (2) a syntactic parser, (3) a communicative parser, and (3) an SSML prosody converter. The implementation has been tested in an experimental setting for German, using web-retrieved articles. Perception tests show a considerable improvement in expressiveness of the synthesized speech when prosody is enriched automatically taking into account the communicative structure.",1.0,3.0
855,1195,2.15    5.05    1.5    3.3    ,Adaptive Optimization Based Neural Network for Classification of Stuttered Speech,"This work is intended to develop automatic recognition procedure to assess stuttering disfluencies (Repetitions, Prolongations and Blocks). For predicting the speech dysfluencies, we have employed an effective Adaptive Optimization based Artificial Neural Network (AOANN) approach. Moreover, the proposed technique employs the Mel Frequency Cepstral Coefficient (MFCC) features is implemented to test its effectiveness. The experimental investigations reveal that the proposed method shows promising results in distinguishing between three stuttering events repetitions, prolongations and blocks.",1.0,3.0
935,1426,2.5    3.45    3.55    2.5    ,Deep Speaker Embeddings with Convolutional Neural Network on Supervector for Text-independent Speaker Recognition,"Lexical content variability in different utterances is the key challenge for text-independent speaker verification. In this paper, rather than raw features, we investigate using supervector which has the capability to reduce the impact of lexical content mismatch among different utterances for supervised speaker embedding learning. A DNN acoustic model is used to align a feature sequence to a set of senones and generate centered and normalized first order statistics supervector. Statistics vectors from similar senones are placed together and reshaped to an image to maintain the local continuity and correlation. The supervector image is then fed into residual convolutional neural network. The deep speaker embedding features are the outputs of the last hidden layer of the network  and we employ a PLDA back-end for the subsequent modeling. Experimental results show that the proposed method outperforms the conventional GMM-UBM i-vector system and is complementary to the DNN-UBM i-vector system. The score level fusion system achieves 1.26% ERR and 0.260 DCF10 cost on the NIST SRE 10 extended core condition 5 task.",1.0,3.0
1193,2106,2.55    4.3    2.65    2.5    ,Improving the Generalized Performance of Deep Embedding for Text-Independent Speaker VeriﬁCation,"In this paper, we propose an effective approach to improve the generalized performance of a speaker embedding system based on a neural network. The deep embedding system, based on x-vector, can obtain a good discriminative capability between different speakers while in the training stage. However, the cross-entropy loss function and the one-hot label may limit the generalization ability of this kind of systems. This paper proposes a new training architecture by adding one filler node in the output layer to represent the out-of-domain speakers. Moreover, we adopt the attention mechanism in the proposed generalized x-vector system. From the results on the NIST 2010 SRE, we find that the proposed system achieves improved performance for the evaluation of varying duration conditions. On the full length condition, our proposed system obtains a 41.5% and 12.1% relative improvement over the baseline system (i-vector) and the standard x-vector system in equal error rate (EER).",1.0,3.0
1262,2316,2.15    4.25    2.05    3.55    ,A Survey on Replay Attack Detection for Automatic Speaker Verification (ASV) System,"In this paper, we present a brief survey of various approaches
for recently introduced replay attack detection for Automatic
Speaker Verification (ASV). The replay spoofing attack is the
most challenging task to detect as only few minutes of audio
samples are required to replay genuine speaker’s voice to get
access to the ASV systems. Due to large availability and the
widespread usage of the mobile/smart gadgets, recording devices,
it is easy and simple to record and replay the genuine
speaker’s voice. The challenging task, in replay spoof attack is
to detect the acoustical characteristics as there is a imperceptible
difference of the speech signal between the natural and the
replayed version. The speech signal recorded with the playback
device contains the convolutional and additive distortions from
the intermediate device. Background noise and channel degradations
seriously constrain the performance of the system. The
goal of this paper is to provide an overview literature of the replay
attack which is an emerging research problem in the field
of anti-spoofing. This paper presents critical analysis of stateof-
the-art techniques, various countermeasures, databases, also
aims to present current limitations and road map ahead, i.e., future
research directions in this technological challenging problems.",1.0,3.0
1263,2332,2.85    2.65    3.85    2.7    ,Experimental Comparison of Features and Classifiers for Replay Attack Detection,"Automatic speaker verification (ASV) has significantly matured over the past years. Notwithstanding, there is still concerns towards its vulnerability in real world scenarios. This paper presents different countermeasures approaches to protect ASV systems against replay spoofing attacks. Specifically, we investigate the performance of five features, including i-vector representation. For classification, we explore Gaussian mixture densities based on maximum-likelihood, support vector machines (SVM), multilayer perceptron (MLP) and cosine distance as classifiers. Experiments were performed on the ASVspoof 2017 dataset (Version 2.0), which provides training, development and evaluation sets with genuine recordings and spoof replay recordings. Our results are reported based on the evaluation set, with each model trained only on the training set and in a second experiment trained on training and development sets as well. Hence, two results are reported for each model. It is shown that the GMM-based systems achieve lower equal error rate (EER) when compared to the classifiers based on i-vectors. The best performance was achieved using the GMM model trained with DFTspec features. With these settings we were able to achieve 12\% EER when using only the training set to train our model and 11\% when the development set was included.",1.0,3.0125
1043,1699,3.95    1.2    4.45    2.45    ,Realization of English Lexical Stress Placement by Native (L1) Bengali Speakers: an Acoustic Analysis,"The current study investigates the native (L1) Bengali speakers’ acquisition of phonology of English lexical stress placement. For this purpose, this study compares 20 L1 Bengali speakers' use of acoustic correlates for the production of English lexical stress in context sentence and neutral frame sentence. From the results of this comparative study, it appears that L1 Bengali speakers were not able to achieve neutral frame sentence like control over duration, intensity, F0 and to a limited extent vowel quality in context sentence. As a result, unlike neutral frame sentence, L1 Bengali speakers did not realize English lexical stress contrast in context sentence. This analysis reveals that the difference between the neutral frame and context sentences regarding the use of acoustic cues of English lexical stress contrast by L1 Bengali speakers was probably due to the influence of Bengali phonology of lexical stress placement on L1 Bengali speakers’ English speech.",1.0,3.0125
1141,1965,2.2    1.8    5.05    ,Effect of Dysphonic Voice on the Speech Identification Scores of Children with Cochlear Implants,"Effect of Dysphonic Voice on the Speech Identification Scores of Children with Cochlear Implants

Dysphonia impairs the signal quality reportedly affecting the speech perception skills of children with typical hearing.Owing to the scarcity of research, the present study aims to study the effect of dysphonic voice on speech identification scores of children with cochlear implants. A preferred list of 28 phonetically balanced words recorded in dysphonic and normal voice, were used to obtain the speech identification scores at 10 dBSL and 30 dBSL of sixteen 4-9.11yrs old children with cochlear implants.Results reveal that speech identification score improves significantly, with increase in sensation levels during dysphonic voice presentation from 58%(±0.24) at 10 dB SL to 73%(±0.15)at 30 dBSL and not significantly during normal voice presentation from 63%( ±0.26) at 10 dBSL to 76%(±0.19) at 30 dBSL. Comparison of speech identification scores between stimulus presentations in  normal and dysphonic voice  did not reveal a significant difference either at 10 dBSL(t (30)=0.568, p=.58) or at 30 dBSL(t(30)=0.326,p= -0.747).Thus, increased sensational levels can improve the perception of stimulus with degraded voice quality.",1.0,3.016666666666667
1304,2493,4.05    2.8    3.3    1.95    ,Speech Emotion Recognition Using Attention Based CNN-LSTM-DNN Models,"In this paper, we present a novel approach for speech emotion
recognition using attention-based CNN-LSTM-DNN models.
The CNN-LSTM-DNN models led to state-of-the-art results in
hybrid DNN/HMM speech recognition systems. In our architecture,
the convolutional layers (CNN) extract acoustic features,
the bi-directional long short-term memory (BLSTM) layers
handle the sequential phenomena of the speech signal. This
is followed by an attention layer, which extracts a summary vector
that is fed to the fully connected dense layer (DNN), which
finally connects to a softmax layer. The results on an Arabic
speech emotion recognition task show that Our innovative approach
can lead to significant improvements over strong baseline
systems.",1.0,3.0249999999999995
514,1950,3.15    2.3    1.65    5    ,Phase-locked Loop Based Phase Estimation in Single Channel Speech Enhancement,"Conventional speech enhancement techniques are based on the modification of noisy spectral magnitude. In the reconstruction of the enhanced signal, noisy phase is combined with the modified noisy spectral magnitude. Recent studies on the importance of phase in enhancement process shows that the clean speech phase improves the quality of the enhanced signal. This work focused on Phase-Locked Loop (PLL) based time-domain approach for estimating the clean speech phase from noisy speech signal. The proposed technique is compared with the conventional approaches where noisy phase is used in the reconstruction of the enhanced signal. Here, Log-Likelihood Ratio (LLR), Weighted Spectral Slope (WSS) distance and Perceptual Evaluation of Speech Quality (PESQ) are used as performance measures. From experimental results, it is observed that the speech quality and intelligibility improved significantly with the proposed method over existing methods.",0.0,3.025
1071,1779,2.6    2.55    4.05    2.9    ,Lattice Modification Using Manner of Articulation Knowledge to Improve Speech Recognition Performance,"Deep neural networks (DNNs) based acoustic modeling has become the core component for all state-of-the-art speech recognition systems. The optimal word sequence generated depends not only on acoustic modeling, but also on mechanisms to find the best hypothesis from N -best lists. The N -most likely sentence hypotheses are represented using word lattice. The consideration of hypotheses other than one corresponding to the best path, increases the chances of finding the correct transcription. Lattice rescoring methods can achieve better results, but it requires higher order knowledge sources such as 3-gram or 4-gram language models. In our work, we have explored the knowledge of manner of articulation for rescoring the arc cost in the word lattice. The proposed idea is applied on Libri Speech and vysta dial English open source corpus, and gained nearly 0.7% improvement in word error rates over state of the art methods.",1.0,3.025
1090,1824,5    2.75    1.4    2.95    ,The QUT Speech Diarization System for the First DIHARD Challenge,"We present details of the QUT submission to the First DIHARD challenge, which is focussed on speaker diarisation on a diverse set of challenging domains. We present two systems; our first system uses a Siamese convolutional neural network (CNN) to learn embedding where speech segment belonging to the same speaker identity are close to each other and those belonging to different speakers are separated by a margin parameter. Our second system uses Automatic Speech Recognition (ASR) based on Deep Neural Network (DNN) to provide bottleneck (BN) features for extracting i-vectors; the ASR DNN is also used for providing feature statistics for training total-variability matrix. We have also implemented a baseline i-vector system to compare with the above two novel implementations. Our baseline i-vector baseline system achieves a diarization error rate (DER) of 33.15%. The improvement of our two innovative systems over baseline performance is reported in the paper.",1.0,3.0250000000000004
1067,1767,2.35    3.45    3.65    2.65    ,A Hybrid Approach to Inverse Text Normalization of Numeric Entities in ASR Systems,"In Automatic Speech Recognition (ASR) systems, a speech recognizer outputs the spoken-form of input utterance. Inverse Text Normalization (ITN) is the process of converting this spoken form utterance to a well formatted written form. Due to long distance language dependencies, numeric entity formatting becomes a challenging task in ITN when the context is indirect and lies far from the position of numeric entity. In this paper, we address the problem of classifying the numeric entities in an utterance to respective domains for appropriate formatting. For this, we propose a hybrid approach comprising of a set of rules and a statistical model. The rule based component of our model leverages the grammatical rules and structure of language while the statistical model uses Logistic Regression classifier. We show that our hybrid model outperforms the conventional rule based system by 6.1% and purely statistical systems (LSTM) by 4.6%. We further propose to extend our hybrid approach to handle spoken forms having multiple numeric entities with contexts distributed along the entire utterance. Our approach performs well on the utterances having multiple numeric entities as well.",1.0,3.0250000000000004
918,1388,2.65    2.4    2.6    4.45    ,Evaluating On-device ASR on Field Recordings from an Interactive Reading Companion,"Many applications designed to assess and improve oral reading fluency use automated speech recognition (ASR) to provide feedback to students, teachers, and parents. Most such applications rely on a distributed architecture with the speech recognition component located in the cloud. For interactive applications, this approach requires a reliable Internet connection that may not always be available. We investigate whether on-device ASR can be used for a virtual reading companion  using recordings obtained from children both in a controlled environment and in the field. Our limited evaluation makes us cautiously optimistic about the feasibility of using on-device ASR for our application.",1.0,3.0250000000000004
757,54,2.5    3.2    2.2    4.25    ,Scalable Language Model Adaptation for Spoken Dialogue Systems,"Language models (LM) for interactive speech recognition systems are trained on large amounts of data and the model parameters are optimized on past user data. New application intents and interaction types are released for these systems over time, imposing challenges to adapt their LMs as the existing training data is no longer sufficient to model the future user interactions. It is unclear how to adapt LMs to new application intents without degrading the performance on existing applications. In this paper, we propose a solution to (a) estimate n-gram based LMs directly from hand-written grammar and (b) use constrained optimization to optimize the system parameters for future use cases, while not degrading the performance on past usage. We evaluated our approach on new applications intents for a personal assistant system and find that the adaptation improves the word error rate by up to 15% on new applications without degrading the word error rate of the existing applications.",1.0,3.0375
1158,1999,2.45    2.85    5.1    1.75    ,Competency Evaluation in Text Independent Voice Mimicry Using Group Delay Functions.,"In this paper, modified group delay feature is utilized to identify the most competent imitator, who mimics a particular person. Most approaches rely on prosodic features to detect mimicry in speech recognition. In this paper, phase based features are used phase based  spectral feature is used in the analysis of text independent mimicked speech recognition. The experiment evaluates the competence of 5 artists in mimicking 10 celebrities and ranks them according to the scores using a classifier. This is validated using a perception test based on the mean opinion score (MOS). If the artist with highest MOS score is identified as rank-1 by the proposed system, a hit occurs. In the  classification phase, Gaussian mixture model (GMM) based classifier makes the decision based on the log-likelihood score. Top X-hit is used as the metric for evaluation. On top-1 rate, the performance of the modified group delay functions is superior (70 %) to the baseline MFCC based system (40 %).",1.0,3.0375
693,2436,3.2    4.05    3.35    1.55    ,Intent Discovery through Unsupervised Semantic Text Clustering,"Conversational systems need to understand spoken language to be able to converse with a human in a meaningful coherent manner. This understanding (Spoken Language understanding - SLU) of the human language is operationalized through identifying intents and entities.  While classification methods that rely on labeled data are often used for SLU, creating large supervised data sets is extremely tedious and time consuming. This paper presents a practical approach to automate the process of intent discovery on unlabeled data sets of human language text through clustering techniques. We explore a range of representations for the texts and various clustering methods to validate the clustering stability through quantitative metrics like Adjusted Random Index (ARI). A final alignment of the clusters to the semantic intent is determined through consensus labelling. Our experiments on public datasets demonstrate the effectiveness of our approach generating homogeneous clusters with 89\% cluster accuracy, leading to better semantic intent alignments. Furthermore, we illustrate that the clustering offer an alternate and effective way to mine sentence variants that can aid the bootstrapping of SLU models.",0.0,3.0375
880,1275,3.05    1.4    4.75    3    ,Phone-Level Embeddings for Unit Selection Speech Synthesis,"Deep neural networks have become the state of the art in speech synthesis. They have been used to directly predict signal parameters or provide unsupervised speech segment descriptions through embeddings. In this paper, we present two models to extract phone-level embeddings for unit selection speech synthesis. The first one relies on a feed-forward DNN, the other one on an LTSM. The resulting embeddings enable replacing usual expert-based target costs by an euclidean distance in the embedding space. This work is conducted on a French corpus of an 11 hours audiobook. Perceptual tests show the produced speech is preferred over a unit selection method where the target cost is defined by an expert. They also show that the embeddings are general enough to be used for different speech styles without quality loss. Furthermore, objective measures and a perceptual test on statistical parametric speech synthesis show that our models perform comparably to state-of-the-art models for parametric signal generation, in spite of necessary simplifications, namely late time integration and information compression.",1.0,3.05
954,1482,2.9    3.05    4.25    2    ,Poetic Meter Classification Using Feature Fusion-DTW Framework,"Meter, a set of well defined rules gives rhythm to the poetry. In this paper, a meter classification scheme is proposed, which is based on the fusion of musical texture features such as dynamic, spectral, beat histogram related, melodic and tonality features computed from the recited poems. The performance of the proposed system is evaluated
using a newly created poetic corpus in Malayalam, one of the classical languages in India. Initially, a baseline mel-frequency cepstral coefficient (MFCC) system is implemented. In the second phase, experiment is repeated with musical texture features. Finally, the experiment is extended to the early fusion of MFCC with the musical texture feature set. Dynamic time warping (DTW) based pattern matching is employed in all the three phases. Whilst MFCC system reports an overall accuracy of 41.6%, the second phase results in an accuracy of 49.16%. In the third phase, the classification accuracy is improved to
68.33 %. The experimental study shows the promise of low level and high level musical texture features in poetic meter classification.",1.0,3.05
1113,1887,2.3    1.6    4.15    4.15    ,The Sound of Silence: How Voice Activity Detection Influences Speech Quality Monitoring,"Real-time speech quality assessment is important for VoIP applications such as Google Hangouts, Microsoft Skype, and Apple FaceTime. Conventionally, subjective listening tests are used to quantify speech quality but are impractical for real-time monitoring scenarios. Objective speech quality assessment metrics can predict human judgement of perceived speech quality. Originally designed for narrowband telephony applications, ITU-T P.563 is a single-ended or non-intrusive speech quality assessment that predicts speech quality without access to a reference signal. This paper investigates the suitability of P.563 in Voice over Internet Protocol (VoIP) scenarios and specifically the influence of silences on the predicted speech quality. The performance of P.563 was evaluated using two datasets: TCD-VoIP containing speech with degradations commonly experience with VoIP and ITU-T P-series Supplement 23, which contains degradations common to regular telephony. The predictive capability of P.563 was established for both datasets by comparing with subjective listening test results. The effect of pre-processing the signal to remove silences using Voice Activity Detection (VAD) was evaluated for five VAD algorithms: energy, energy and spectral centroid, Mahalanobis distance, weighted energy and weighted spectral centroid. Analysis shows P.563 prediction accuracy improves for VoIP speech conditions when the silences were removed using a weighted spectral centroid VAD.",1.0,3.0500000000000003
1307,2507,4.3    2.2    2.7    ,"In-domain, Cross-Dialect Data Augmentation of Children's Speech","Most publicly available corpora of children's speech are limited in vocabulary and number of hours of data. The augmentation of children's data is a critical step in the development of acoustic models and the adaptation of existing models using the limited data available. Typically, the data used to augment a corpus comes from speakers of the same dialect. This work explores the use of data from related dialects to augment a resource-poor dialect. The study augments a small, unpublished, corpus of child-produced Jamaican English with data from established British (PF-STAR) or American (CMU Kids Speech) corpora of children's speech. The study seeks to answer two questions: firstly, does ASR performance improve when Jamaican data is augmented using speech from related dialects; and secondly, which, if any, of the related dialects leads to the greater improvement. The results show that models created by adding small amounts of Jamaican data to the PF-STAR corpus improves the recognition of Jamaican English, reducing the WER by 58.1\% when compared to recognition using baseline PF-STAR models. For the CMU corpus, the improvement was 59.6\% over the baseline. Both augmented models gave WERs within 2.1\% of the models trained on Jamaican only data.",1.0,3.0666666666666664
791,1029,2.65    5    1.55    ,Automatic Accent Classification Using Automatically Detected Pronunciation Variants,"In this study we aim to identify the region of origin of speakers of Dutch speakers on the basis of their accent. We use a large crowd-sourced data set which contains recorded acoustic Dutch pronunciations from a large number of speakers, but also perceptual human judgements on where each speaker is from. Our approach consists of combining multiple sentence-level accent classifiers which were trained on the basis of alternative pronunciation variants (obtained using a forced alignment system incorporating pronunciation variation) for all words in the sentence. Our results indicated that our system is able to classify two Dutch regional accents (Groningen + Drenthe vs. Limburg) with an accuracy of about 78.6%. When distinguishing three Dutch regional accents (Groningen + Drenthe, Twente + Achterhoek, and Limburg), the overall accuracy was about 54.8%. While relatively low, the accuracy of our system was higher than human classification performance.",1.0,3.066666666666667
1029,1670,4    2.7    2.8    2.35    3.5    ,Investigating Phase-Aware Single Channel Source Separation Using Non-Negative Matrix Factorization and Its Variants for Overlapping Speech Signal,"Speech recognition shows considerable degradation in quality in a cock-tail party environment. This problem may be mitigated by audio source separation as a pre-processor to speech recognition. Various other applications like audio forensics, speaker verification, instrument identification, hearing aids, etc. require it as a pre-processor for better results. 
There are various techniques available for single channel audio source separation, but the technique based on Non-negative Matrix Factorization (NMF) is widely used. In NMF, reconstruction of the separated time domain signal is done by applying inverse Fourier transform and overlap-and-add routine to the composition of estimated magnitude spectrum and mixture signal phase. This mixture signal phase causes audible artifacts in the reconstructed signal. To replace the mixture signal phase with clean signal phase, estimation of phase was incorporated either at the spectral modification stage or signal reconstruction stage as per recent research.
In this paper, phase-aware single channel source separation, is investigated. Their separation performance is compared with phase-unaware approaches based on NMF and its variants for a single-channel two-talker mixed signal. For carrying out investigations GRID speech corpus database was used. The quality of separated speech was judged by varying parameters such as number of bases, STFT window size etc.",1.0,3.07
1140,1963,1.85    4.25    3.15    ,Combining Complementary Acoustic Features Using Multi-channel Deep Fusion Architecture,"Deep learning architectures have been effective in solving complex computer vision problems and have potential towards complex acoustic tasks such as acoustic scenes recognition (ASR) and audio tagging. Several researchers have leveraged low-level and high-level acoustic features when used with state-of-the-art techniques, however, most of the current literature lacks an explanation behind the use of these acoustic features. In this work, we explore complementary properties inhibited by some acoustic features which can be exploited by deep learning models.
We present a multi-channel architecture where each channel is formed using deep convolution or recurrent layers. To transfer the information across channels, we introduce similarity matrices that aid in alignment of frames and pooling. 
The output of each channel is merged using interaction parameters which aggregate representations non-linearly at each level of abstraction. This aggregation of audio features proves to be effective for classifying the acoustic scenes. Finally, we evaluate the performance of the proposed architecture on three benchmark datasets  - DCASE-2016 (ASR), LITIS Rouen (ASR) and CHIME-2016 (tagging). The experimental results suggest that the presented architecture outperforms the standard baselines and achieves state-of-the-art performance on the task of ASR and audio tagging.",1.0,3.0833333333333335
1183,2063,1.9    5.05    2.65    2.8    ,CESAR: a New Metric for Measuring the Complexity of a Code-switched Corpus - Application to the Maghrebi Arabic,"In this paper we address the common phenomenon in daily conversation of Maghrebi people: code-switching or code-mixing. This issue consists of the alternation between languages during communication or writing. In  this work we want to measure the complexity of this phenomenon into the three Maghrebi corpus we harvested from Youtube. Each of them contains at least 17 million of words.  Several metrics have been proposed in the litterature for code-switching, but no one takes into account the degree of mixture accordingly to a reference language.  That is why, we propose a new metric named CESAR (CodE Switching According to a Reference language) which consists of measuring the degree of language mixtures, relatively to a reference language. CESAR has been tested on the three collected corpora by considering local dialects such as reference languages. The results show that the Algerian dialect is more polluted by other languages  than the two other dialects.",1.0,3.0999999999999996
1059,1741,4.3    4.05    1.5    2.55    ,Segment Based Modeling of Speech Using CNN with a Phase Space Representation as Input,"Can Convolutional Neural Networks learn better speech features over raw signals ( by themselves ) or do some transforms help? Reconstructed phase space(RPS) features capture nonlinear aspects of speech and closely match speech production. In our case, feature maps constructed with speech segments embedded in the Reconstructed Phase Space, are fed as input to the CNN. A Gaussian Mixture Model(GMM) trained over these RPS features is taken as baseline. Experiments are conducted on the TIMIT phoneme classification task using CNN models and compared with the baseline GMM. Our method using CNN obtained a higher accuracy over the baseline and we show interesting learned representations of the network in training. As the RPS and the state space of raw speech are known to be diffeomorphic in nature, our results show scope for newer pipelines in ASR design with the main advantage being the ease and ability of using a single fixed size representation for any varying phoneme segment length and avoiding frame-based processing altogether.",1.0,3.0999999999999996
1177,2047,2    1.65    5.1    3.65    ,Investigating Manifold Learning Technique for Robust Speech Recognition,"Developments of robustness techniques are imperative to the
success of automatic speech recognition (ASR) systems when
being confronted with varying sources of environmental
interference or disturbance. Recent studies have shown that
exploring low-dimensional structures of speech features can
help produce robust features to enhance ASR performance.
Along this research direction, we hypothesize that the intrinsic
structures of speech features lying on a low-dimensional
manifold subspace embedded in their original high–
dimensional ambient space. As such, noise components can be
ruled out by projecting noisy speech features into the prelearned
basis space of manifold structures. Specifically, we
endeavor to explore the intrinsic geometric low-dimensional
manifold structures inherent in modulation spectra of speech
features, in the hope to obtain more noise-robust speech
features. The main contribution of this paper is that we propose
a novel use of the graph-regularization based method to
enhance speech features by preserving the inherent manifold
structures of modulation spectra and excluding irrelevant ones.
Furthermore, we also compare our approach with several wellpracticed
methods that also explores low-dimensional
structures of data instances thoroughly. Extensive experiments
conducted on the Aurora-4 database and task demonstrate the
promising performance of the proposed approach.",1.0,3.1
1024,1657,2.65    2.55    4.3    2.9    ,Gender Identification from Speech Signal Using Multicomponent AFM Signal Model,"Speech is the best way to communicate information from a speaker to a listener. The listeners can identify details about the speaker's personality, age, gender, accent, emotions etc. In this paper, we address the gender identification using the  multicomponent multi-tone amplitude and frequency modulated (AFM) signal model, which is suitable for a long duration of signal and robust to various recording condition. Gender identification is used to detect the gender of the speaker by his/her voice. The gender classification is done using the support vector machines (SVM) with the k-fold cross validation (CV). The extracted features of the separated voiced phoneme from the different word spoken by male and female are classified. The first experimental result shows that the proposed model is suitable for text independent and the second experiment shows that the model is well applicable for emotion independent, environment independent and microphone independent gender identification. The model is precise with the high probability of correct classification in both cases for gender identification.",1.0,3.1
1230,2207,4.4    3.3    3.2    1.5    ,On Construction of the ASR-oriented Indian English Pronunciation Dictionary Based on an Adaptation Strategy,"Indian English (IE) has developed some of its own distinctive features, especially phonologically, from other varieties of English. An ASR system simply trained on British English (BE) or American English (AE) speech data and using the BE/AE pronunciation dictionary may perform terribly depending on the position of the IE speaker in the cline of bilingualism. An applicable IE ASR system needs a comprehensive, linguistically-guided IE pronunciation dictionary so as to achieve the effective mapping between the acoustic model and language model. There is no publicly available IE pronunciation dictionaries to date. The teaching-oriented and speech-technology-oriented English pronunciation dictionaries cannot fully represent the pronunciation of IE. This paper thoroughly analyzes and summarizes the pan-Indian phonological features of IE, design the phoneme set and adapt the Longman Pronunciation Dictionary (LPD, BE-accented) and CMUdict (AE-accented) to two IE pronunciation dictionaries separately. Then the four dictionaries(LPD, CMUdict, LPD-adapted IE pronunication dictionary and CMUdict-adapted pronunciation dictionary) are applied in an IE ASR system and test its performance.",1.0,3.1
1296,2452,5    3.4    2.45    1.55    ,Sibilant Sounds Classification with Deep Neural Networks,"Mel frequency cepstral coefficients (MFCCs) and support vector machines have been previously used for classification of phonemes. Recently, the use of deep neural networks have given considerable improvements in classification for a variety of use cases, from image classification to speech and language processing. Here we propose to use deep convolutional neural networks to classify sibilant phonemes of European Portuguese.

Many children suffering from speech sound disorders cannot pronounce the sibilant phonemes correctly. We have developed a serious game that allows children to practice the European Portuguese sibilant sounds without any type of supervision. This allows the children to practice the correct production of these sounds more often, which may lead to faster improvements of their speech.

Since the game does not depend on adult supervision, it needs an automatic sibilant classifier. A previous version of the game, that used MFCCs and support vector machines, achieved accuracy test scores of 90,72%. The proposed deep neural networks use the raw MFCCs as features. In order to use the MFCCs as features for the convolutions, we compared their use as one and two dimensions input. Our final deep learning model achieves classification scores of 94,42%, surpassing our previous work.",1.0,3.1000000000000005
1236,2230,4.3    3.45    2.6    2.1    ,Acceleration of Posteriorgram Based Query by Example by Simplifying Local Distance Computitation,"Spoken term detection (STD) is the task of finding matched sections in speech data with a query consisting of one or more words. Query-by-example has also been an important topic in spoken term detection, which covers spoken queries. A previous study examined posteriorgrams, which are sequences of output probabilities generated by a deep neural network from speech queries and speech data. Although posteriorgram matching between a spoken query and speech data high retrieval accuracy, it requires a long time to search for a spoken query among speech data. Reducing retrieval time is thus a crucial problem in posteriorgram matching. In this paper, we propose a method for reducing retrieval time in posteriorgram matching. In posteriorgram matching, a local distance computation takes the most computation time because it requires inner product calculation between high dimensional two posteriors at every frame. In this paper, we reduce the time by simplifying local distance computation. To address this problem, we propose a method to replace the posteriorgram of a spoken query to a sequence of state numbers and omit the local distance computation. Evaluation experiments have been carried out using open test collections, and the results have demonstrated the effectiveness of the proposed method.",1.0,3.1125
972,1520,2.65    3.4    3.55    2.85    ,Comparing Confusions of ASR Systems and Human Listeners in Noisy Speech,"Comparing confusions of humans and ASR systems is important for understanding human processing and improving ASR, especially in adverse conditions. However, few sets of human reference data are available in the form of confusion matrices. Another complication is the lack of procedures for presenting and interpreting differences between confusion patterns. In this paper, we focus on the impact of the features representations and the method for computing state posterior probabilities on the difference between the confusions patterns of several automatic systems and human reference data. We compare conventional Mel-frequency spectral coefficients with modulation spectrum features. For computing posterior probabilities we compare sparse coding with neural networks. The human reference data come from an experiment in which ten humans listened to digit sequences from the Aurora-2 corpus, noisified by airport, subway and babble noise. We compare the listeners' confusions in the 0 dB and -5 dB signal-to-noise conditions with the confusions made by our ASR systems.

We also discuss methods for comparing differences between confusion patterns. Formal distance measures between matrices are not suitable. We propose a method that replaces a scalar difference measure by a visual representation of the relevant differences between confusion patterns.",1.0,3.1125
1126,1917,4.35    2.4    2.6    3.1    ,Acoustic Analysis of Rhotic Approximant in Dysarthric Speech,"In Dysarthria, the range of articulatory dynamics will be reduced with a degree of dysarthria severity level. It is understood that the dysarthric speaker experiences difficulty in producing phones involving complex articulatory gestures. In this work,
vocal tract resonating characteristics of rhotic approximant in American English is studied in the context of the dysarthric speech. This work has been carried out in two phases. In the first phase, acoustic cues to articulatory mapping associated with rhotic approximant are investigated. Next, an algorithm for automatic detection of rhotic approximant is implemented and its consistency across different speakers is investigated on TIMIT
database. In the second phase, rhotic approximant detection algorithm is used for acoustic analysis of dysarthric speech on UASPEECH database. The third formant’s slope and proximity of the third formant with the second formant are proposed to link dysarthria severity. The correspondence between the dysarthric speech severity level and acoustic features of rhotic approximant is highlighted.",1.0,3.1125
772,89,3.75    2.25    4.35    2.15    ,Coarse-to-Fine Pseudo-Label Generation for RCNN-Based Text Clustering,"We propose a coarse-to-fine pseudo-label generation for recurrent convolutional neural networks (RCNN) model training. The K-nearest neighbors(KNN) algorithm and consensus analysis are served as coarse-pass and fine-pass, respectively, for pseudo-label generation. The RCNN models are trained in a supervised fashion with the pseudo-labeled documents. After obtaining the deep feature representations of all of the documents through the trained RCNN, a conventional algorithm is used to cluster them. The experimental results on two public databases show that the proposed approach significantly boosts the performance of text clustering.",1.0,3.125
787,1012,2.5    4.1    3.5    2.4    ,"On the Role of Linear, Mel and Inverse-Mel Filterbank in the Context of Automatic Speech Recognition","In the context of automatic speech recognition (ASR), the power spectrum is generally warped to the Mel-scale during front-end speech parameterization. This is motivated by the fact that, human perception of sound is nonlinear. The Mel-filterbank provide better resolution for low-frequency contents while a greater degree of averaging happens in the high-frequency range. The work presented in this paper aims at studying the role of linear, Mel and inverse-Mel filterbanks in the context of speech recognition. It is well known that, when speech data is from  high-pitched speakers like children, there is a significant amount of relevant information in the high-frequency region. Hence, down-sampling the information in that range through Mel-filterbank reduces the recognition performance. On the other hand, employing inverse-Mel or linear-filterbanks are expected to be more effective in such cases. The same has been experimentally validated in this work. To do so, an ASR system is developed on adults' speech and tested using data from adult as well as child speakers. Significantly improved recognition rates are noted for children's as well adult females' speech when linear or inverse-Mel filterbank is used. The use of linear filters results in a relative improvement of 21% over the baseline.",1.0,3.125
1054,1731,3.45    3.6    2.8    2.65    ,Neural Networks-based Automatic Speech Recognition for Agricultural Commodity in Gujarati Language,"In this paper, we present a development of Automatic Speech Recognition (ASR) system as a part of a speech-based access for an agricultural commodity in the Gujarati language. We
proposed to use neural networks for language modeling, acoustic modeling, and feature learning from the raw speech signals. The speech database of agricultural commodities was collected from the farmers belonging to various villages of Gujarat state (India). The database has various dialectal variations and real noisy acoustic environments. Acoustic modeling is performed using Time Delay Neural Networks (TDNN). The auditory feature representation is learned using Convolutional Restricted Boltzmann Machine (ConvRBM) and Teager Energy Operator (TEO). The language model (LM) rescoring is performed using Recurrent Neural Networks (RNN). RNNLM rescoring provides an absolute reduction of 0.69-1.18 in % WER for all the feature sets compared to the bi-gram LM. The system combination of ConvRBM and Mel filterbank further improved the performance of ASR compared to the baseline TDNN with Mel filterbank features (5.4 % relative reduction in WER). The statistical significance of proposed approach is justified using a bootstrap-based % Probability of Improvement (POI) measure.",1.0,3.1250000000000004
762,71,2.8    3.25    3.35    ,A Deep Learning Based Speech to Text Conversion and Question Answering System for Malayalam Language,"Malayalam is a language spoken predominantly in the state of Kerala, India. It is one of the 22 scheduled languages of India and was designated as one of the 6 Classical Languages (“Classical Language in India"" is an official status within the Republic of India, awarded by the Government of India based on certain criteria) in India in 2013.
In this paper we are proposing a system which can take Malayalam speech as the input and generate the reply as a text in Malayalam script. This system can be run in two modes. In the first mode, this works as a question answering system and in the second, as a Malayalam speech to Malayalam script converter. We use deep learning and Natural Language Processing techniques to implement this. This system uses deep learning to improve its ability in answering questions. A major part of this is training a neural network model with an appropriate dataset. By making use of this it gives appropriate replies to the queries. The experiment results show an accuracy of 86% for our system.",1.0,3.1333333333333333
779,994,3.45    4.1    2.6    2.4    ,Improving the Quality of Standard GMM-based Voice Conversion Systems Used in Real-time Applications,"Voice  conversion  (VC) can  be  described as  finding  a mapping  function  which  transforms  the features extracted from the source speaker  to  those  of  the  target  speaker.  Gaussian  mixture  model (GMM)  based  conversion  is the most   commonly  used technique in VC, but  it  is  often sensitive to overfitting and oversmoothing. To address these issues, we propose a secondary classification by applying a K-means classification in each class obtained by a primary classification in order to obtain a more precise local conversion function.
This proposal avoids the need for complex training algorithms because  the mapping local functions are determined at the same time.
 The proposed approach consists  of  a  Fourier cepstral analysis,  followed  by  a training phase in order to find the local mapping  functions  which  transform  the vocal tract  characteristics  of  the  source speaker  into  those  of  the  target  speaker.
The  converted parameters together  with  the excitation and phase  extracted from the target training space  using a sophisticated frame index selection are  used  in the synthesis step to generate a converted speech  with target speech characteristics. 

Objective and subjective experiments prove that the proposed technique outperforms  the baseline GMM   approach while reducing  dramatically the training and transformation computational times.",1.0,3.1375
1037,1684,3.5    4    2.6    2.45    ,Does L2 Vowel Inventory Enhances the Perceptibility of Novel Vowel Contrasts?,"This study aims to investigate bilingualism effect on the non-native speech perception by comparing the discriminability of Korean vowel contrasts by monolingual native Moroccan Arabic speakers and bilinguals of MA and French, we examined whether L2 vowel inventory enhances the perceptibility of non-native vowel contrasts.
The reason behind testing MA monolinguals and bilinguals’ perception of Korean vowel contrasts was motivated by the fact that MA’s vowel inventory is smaller than that of Korean, whereas French has larger vowel inventory than both MA and Korean. Moreover, unlike Korean and French, MA does not allow a single vowel to be a syllabic unit. Based on these facts and the assumption that a larger L2 vowel inventory facilitates perception of non-native contrasts, we hypothesized that bilinguals would benefit from French phonology in perceiving Korean vowel contrasts. To test this, an ABX discrimination task was conducted. As a result, bilinguals outperformed monolinguals, and this tendency was shown in phonemic level.
A plausible interpretation of bilinguals’ better performance is that the larger the L2 inventory size, the better is discriminability of non-native vowel contrasts. but this would work only when the L2 phonological system makes them overcome L1 restrictions.",1.0,3.1375
1155,1995,2.65    3.4    3    3.5    ,Combining Deep Learning with Sentence Templates for Speech Recognition in Restricted Domains: Language Models Really Matter,"Sentence Template language models are used to syntactically correct hypotheses generated by Cloud-based Automatic Speech Recognition (ASR) systems like Google’s Search by Voice. In this work, we train a local ASR based on a recently introduced DeepSpeech 2 architecture and measure the performance of post-processing the hypotheses with a Sentence Template language model. We also post-process the hypotheses of Google’s ASR while achieving a similar performance. This shows that the model can be successfully transferred to a local
ASR and significantly reduce the word error rate. We also compare the performance of internal language modeling with post-processing. The results show a similar performance, indicating that post-processing is still useful. Comparing the processing time of the local ASR compared to Google’s Cloud-based ASR, the local ASR only needs 0.53s on average, while Google takes 1.47s. This enables faster and more natural speech processing, e.g. in the Human-Robot-Interaction domain.",1.0,3.1375
1052,1726,4.3    3.3    2.55    2.5    ,Genesys Submission to Low Resource ASR Challenge for Indian Languages,"This paper describes the experiments performed on Indian languages Telugu, Tamil and Gujarati as a part of Interspeech 2018 low resource speech recognition challenge. 
This paper used Kaldi toolkit for building acoustic and language models. State of the art acoustic modeling approaches has been evaluated for improving
accuracy with low resource data. The approaches are: Lattice-Free MMI, sequence discriminative training, multi-lingual, and bottleneck features. 
Similar to acoustic modeling, well tested and published language modeling approaches were evaluated. Following language modeling toolkits used during experimentation; IRSTLM, Kaldi LM, SRILM, and RNN LM. Rover has been tried to improve the accuracy further. 
During our experimentation, our error analysis showed that word accuracy is widely affected due to compound word to split word confusion, colloquial to written text confusion. We submitted the best system trained by Lattice-Free MMI and SRILM methods. 
We obtained word error rate of 18.7\% for Telugu, 17.2\% for Tamil, and 21.6\% for Gujarati.",1.0,3.1624999999999996
1129,1925,2.45    4.15    3.45    2.6    ,Dialect Identification Using Chroma and Spectral Shape Features,"The present work proposes an automatic dialect identification (ADI) system using chroma and spectral shape features. Different dialects of a language are spoken with varying pronunciation styles in specific geographical regions. The dialects then, after the passage of time is considered to represent those particular regions where they are predominantly spoken. A comprehensive study of the five dialects of  Dravidian Kannada language has been taken up. Modified chroma features are extracted since they aggregate all spectral information related to given pitch class into a single coefficient. Later, eight dialect significant spectral shape features are derived. Single classifier and ensemble based support vector machines (SVM and ESVM) are employed for classification. The effectiveness of the proposed features for ADI system is demonstrated with Kannada dialect dataset with five prominent dialects and also with a standard intonation variation in English (IViE) dataset which consists of nine dialects of British English. The proposed features have shown better performance over traditional MFCC features with both datasets. A better error rate (ERR) of 15\% and 5.22\% is achieved with Kannada and IViE dialect datasets respectively. Obtained results have demonstrated better performances even under limited training data and also with noisy conditions.",1.0,3.1625
1226,2183,2.6    2.8    4.1    3.15    ,Evolutionary Cepstral Filterbank Optimization for Cry Recognition in Infants,"Mel-frequency cepstral coefficients have long been the most widely used type of speech representation. They were introduced to
incorporate biologically inspired characteristics into artificial speech recognizers. Recently, the introduction of new alternatives
to the classic mel-scaled filterbank has led to improvements in the performance of phoneme recognition in adverse conditions.
In this work we propose a new bioinspired approach for the optimization of the filterbanks, in order to find a robust speech
representation and we apply the method to a new task in the affective computing area, namely the cry recognition in infants. 
Our approach—which relies on evolutionary algorithms—reduces the number of parameters to optimize by using spline functions to shape the filterbanks. The success rates of a classifier based on hidden Markov models are used as the fitness measure. 
The results show that the proposed method is able to find optimized filterbanks for the cry recognition task.",1.0,3.1625
794,1040,4.85    2.65    3.9    1.25    ,Pay Attention to the Metadata: Sentence Modeling That Utilizes Syntactic Properties,"Sentence modeling is an important feature engineering for document classification. Various feature extraction and summarization algorithms have been adopted for efficient and effective classification of sentence, e.g. dense word vectors and neural network classifiers. Recently, the concept of attention for effective analysis of images has been applied to various NLP tasks and has shown significant performance. In this paper, we pay attention to syntactic properties in terms of metadata. Unlike conventional attention models, which may be considered as a division of word-based approaches, our model simply adds a selection layer to highlight syntactic components that may appear more than once. The proposed algorithm shows improved performance compared to the baselines in the tasks in syntax-semantics, suggesting possibility of extension to other fields.",1.0,3.1625
1110,1880,2.7    3.45    5    1.5    ,Robust Front-end Processing for Emotion Recognition in Noisy Speech,"Since the emotion recognition performances degrade drastically for noisy speech, we propose a robust front-end processing to reduce the effect of noise. The front-end consists of a novel energy based voice activity detector (VAD), which discards silence or noisy frames. We show that the proposed VAD results in significant performance gain and outperforms the more complex Recurrent Neural Network (RNN) based VAD as well as popular Non-negative Matrix Factorization (NMF) technique. Moreover, the proposed VAD can be used alongside the NMF technique to further improve the performance. The emotion recognition is done by extracting large number of statistical features from low level audio descriptors and we have used state-of-the-art classifiers. Extensive experimentation on noisy (5 types of noise: Babble, F-16, Factory, Volvo, and HF-channel from the Noisex-92 database) speech contaminated at 5 different SNR levels (0, 5, 10, 15, 20dB) have been carried out to measure the performance of the proposed front-end techniques.",1.0,3.1625
871,1235,2.3    2.4    4.8    ,Optimal Combination of Multivariate Filter Feature Selection and Classifier for Speech-based Depression Detection,"In this paper, we have focused to improve the performance of speech based uni-modal depression detection system, which is non-invasive, involves low cost and computation time in comparison to multi-modal systems. The performance of a decision system mainly depends on the choice of feature selection method and the classifier. We have investigated the combination of four well-known multivariate filter methods (minimum Redundancy Maximum Relevance, Scatter Ratio, Mahalanobis Distance, Fast Correlation Based feature selection) and four well-known classifiers (k-Nearest Neighbour, Linear Discriminant classifier, Decision Tree, Support Vector Machine) to obtain a minimal set of relevant and non-redundant features to improve the performance. This will speed-up the acquisition of features from speech and build the decision system with low cost and complexity. 
Experimental results on the high and low level features of a recent work on the DAICWOZ dataset demonstrate superior performance of the combination of Scatter Ratio and LDC as well as that of Mahalanobis Distance and LDC, in comparison to other combinations and existing  speech based depression results, for both gender-independent and gender based studies. Further, these combinations have also outperformed a few multimodal systems. It was noted that low level features are more discriminatory and provide better f1 score.",1.0,3.1666666666666665
1131,1931,3.85    4    2.6    2.25    ,Deep Residual Networks for Classification of Indian Languages into Tonal and Non-tonal Category,"Motivated by the success of deep residual networks (ResNets) in speech recognition, this paper proposes ResNets for classification of Indian languages into tonal and non-tonal category. ResNet is a special type of deep learning architecture that exhibits a capability to produce better performance due to its increasing depth of network architecture, higher training speed and immunity from vanishing gradient problem. In this paper usefulness of pitch chroma, MFCC and also their combination have been investigated. The performance of the system is analyzed for the features extracted of two different analysis units: syllable and utterance. The experiments have been carried out on NIT Silchar language database (NITS-LD) which is developed for 12 Indian languages using All India radio broadcast news. Experimental analysis suggests that proposed ResNets developed at syllable level outperforms the other models namely, artificial neural network (ANN), convolutional neural networks (CNN) and cascade CNN/long short-term memory (LSTM) recurrent neural networks (RNNs) with an accuracy of 91.6%, 89.21% and 85.68% for 30s, 10s, 3s test data.",1.0,3.175
1003,1605,2.8    3.35    3.4    ,On the Difficulty of a Distributional Semantics of Spoken Language,"The bulk of research in the area of speech processing concerns itself with supervised approaches to  transcribing spoken language into text. In the domain of unsupervised learning most work on speech has focused on discovering relatively low level constructs such as phoneme inventories or word-like units. This is in contrast to research on written language, where there is a large body of work on unsupervised induction of semantic representations of words and whole sentences and longer texts. In this study we examine the challenges of adapting these approaches from written to spoken language. We conjecture that unsupervised learning of spoken language semantics becomes possible if we abstract from the surface variability. We simulate this setting by using a dataset of utterances spoken by a realistic but uniform synthetic voice. We evaluate two simple unsupervised models which, to varying degrees of success, learn semantic representations of speech fragments. Finally we suggest possible routes toward transferring our methods to the domain of unrestricted natural speech.",1.0,3.1833333333333336
1093,1835,2.45    2.5    3.9    3.9    ,On the Evolution of EEG Representations in Language Learning,"The early stages of language learning consists of learning new words from an unfamiliar language through phases of listening and articulating the words of the language over time. In this study, we design an experimental setup to understand the evolution of neural representations in a language learning task. The human subjects in the experiment are performing the task of listening and reproducing a set of words from a familiar language (English) and an unfamiliar language (Japanese). During the task, electroencephalogram (EEG) signals are recorded in addition to the spoken audio signals. With a detailed analysis on the recorded EEG signals and the audio signals, we propose to uncover the language learning process by highlighting the difference between these signals for the known and the unknown language. Upon exposure to multiple trials of the same word from the unfamiliar language, we find that the inter-trial distance between EEG signals (as well as the spoken audio signals) decreases, indicating a consistency in the neural representations. In addition, the brain regions (in terms of EEG channel
locations) responsible for the learning task are identified.",1.0,3.1875
1073,1784,2.75    4.25    2.45    3.3    ,Speaker Identification Using Tensor Decomposition of Acoustic Models,"This paper explores speaker identification based on the speaker adaptation via multilinear decomposition of a speaker model.Tucker decomposition of the third order mean Tensor of training speaker yields three subspaces corresponding to each mode. The mean of the mixtures for speakers is expressed as a product of the mixture space and a weight matrix comprising of the other two spaces.We update only the mean of the mixtures for the adaptation stage . During testing,  log likelihood is used to identify the scores for the test speakers.Experiments conducted on the TIMIT and NIST 2003 databases shows comparable performance with the baseline even in channel mismatch conditions of the development and enrollment dataset. Furthermore, using higher order tensors, we can easily adapt this problem to include noise and environment factors as well.",1.0,3.1875
1198,2120,4.15    2.35    2.75    3.5    ,Linguistic Familiarity Has Differential Nonlinear Effects on Age Perception Accuracy for Different Speaker Ages,"Evidence suggests that listeners are able to use voice cues to estimate speakers' ages. However, estimation accuracy varies greatly depending on characteristics of both speaker and listener. Previous research with second language (L2) speakers and listeners of English shows that linguistic familiarity affects age estimation across languages, with higher accuracy when listening to native speakers of one’s own native language.
The present study investigates how linguistic familiarity affects L2 age estimation accuracy over a range of speaker ages. Generalized additive mixed models of estimation error indicated a central tendency effect: younger speakers were overestimated and older speakers underestimated. Furthermore, there was a nonlinear interaction between speaker age and linguistic familiarity (match, close and far languages). Young speakers (22-27 years) were overestimated significantly more by both the familiar and distant language groups compared to the match language group. The mid age range was perceived as significantly older by the far compared to the match language group. 
These results suggest that a) age cues carried in the voice are at least partially language-specific, b) language-specific cues can  carry over to the L2 and c) language-specific effects in voice perception differ nonlinearly with speaker age.",1.0,3.1875
1259,2303,3.75    3.25    3.45    2.35    ,Animating 3D Talking Heads Using 2D Videos,"In visual speech animation, lip motion accuracy is of paramount importance for speech intelligibility, especially for the hard of hearing or foreign language learners. We present an approach to 3D lip animation that uses 2D videos of a speaker to drive a 3D talking head through fitting a morphable model. Lip motion data is extracted from videos of a real speaker and the 3D morphable model is generated by applying principal component analysis (PCA) to the corresponding vertices of a collection of head poses created using commercial software. We report on a series of experiments that investigate how the match between a real speaker's facial features in video and the features in the animated 3D model affects the resulting lip motions. Our experiments consider matches between all and a subset of facial features and report on measurements of lip movement based on tracking specific lip features.",1.0,3.1999999999999997
1163,2007,2    2.35    3.35    5.1    ,Enhancement of Speech Corrupted by Additive Noise Using Multistage Spectral Subtraction,"Noisy environment degrades the quality of the speech there by reducing listener’s ability to understand the speech. This problem is major concern in any communication system. Hence there is a need to enhance the speech which is corrupted by the background noise. Goal of speech enhancement is to process the noisy speech signal to improve quality of the speech by reducing the noise without affecting intelligibility of the speech. Spectral subtraction is the basic method which is simple and better suited to enhance speech corrupted by additive noise. But it introduces musical noise which is annoying. In this paper an enhancement method is proposed to reduce background noise and annoying musical noise. To reduce musical noise, enhancement algorithm must consider the nonstationarity of the signal. The proposed method consists of number of spectral subtraction stages. Using different window length at each stage it is possible to capture most of signal tones. Hence nonstationarity problem is addressed resulting in minimum musical noise. The remaining noise is eliminated by modifying the phase spectrum empirically. The subjective listening tests, objective measurements (Increment in segmental SNR, Log likelihood ratio, Itakura Saito distance) and spectrogram analysis show reduction in both background and musical noise.",1.0,3.1999999999999997
1273,2367,4.2    3.7    1.55    3.35    ,Exploring Joint Spectro-temporal Features for Automatic Dialect Identification,"Most state-of-the-art dialect identification systems today use
some form of Mel-frequency cepstral coefficients (MFCCs) as
their acoustic feature representation. In the work presented
here, we explore 2D-DCT based joint spectro-temporal based
features in the task of dialect identification. Conventional
MFCCs can only model the temporal dynamics up to 30-50
msec, which may not be sufficient for dialect identification.
Dialectal information is rather embedded in the long tempo-
ral modulation. In contrast to MFCCs, it has been proven that
joint spectro-temporal based features can capture the explicit
spectro-temporal modulation in the speech signal. Apart from
this, they have additional advantage of noise robustness. Here,
DNN based systems are designed with two standard databases,
TIMIT and IARPA Babel Assamese Language Pack respec-
tively. We evaluate the performance of 2D-DCT based joint
spectro-temporal features over the MFCCs to find that these fea-
ture analysis outperform traditional features based on MFCCs.
Motivation for using 2D-DCT feature is that they can capture
joint dynamic spectro-temporal modulations than conventional
MFCCs.",1.0,3.2
1214,2159,4.1    3.5    2.35    2.9    ,The Effect of Corpus Size on Preserving Voice for Those to Be Treated with Glossectomy and Laryngectomy,"Oral and/or laryngeal cancer can have devastating consequences on one’s ability to communicate. While surgical interventions such as total or partial glossectomy and/or laryngectomy treat the cancer, survivors are rendered speechless and/or voiceless. They must rely on devices such as an electrolarynx which generates robotic sounding speech or a surgically inserted speaking valve which requires continual medical surveillance and care. A text-to-speech (TTS) synthesis-based prosthesis offers a novel alternative that is non-invasive and yet personalized. Unfortunately, patients with oral and/or laryngeal cancer rarely have more than a few weeks between diagnosis and treatment. In this paper, we describe an efficient and streamlined method of preserving speech and voice prior to surgical intervention. We then assess the impact of varying amounts of speaking corpora and TTS training methods for building personalized speech generating prostheses for two survivors of oral and/or laryngeal cancer. Our findings suggest that an adapted model requires up to 50% fewer training sentences to yield comparable results to the non-adapted model. Thus, crowdsourcing high quality spoken corpora from diverse and distinctive sounding speakers would enhance the ability to produce personalized TTS voices that reflect the likeness of the survivor while maintaining clarity.",1.0,3.2125
1175,2039,1.65    4.25    4.85    2.1    ,IIT Guwahati Systems for Low Resource Speech Recognition Challenge for Indian Languages,"This paper describes the automatic speech recognition (ASR) systems submitted to Interspeech 2018 Special Session on `Low Resource Speech Recognition Challenge for Indian Languages' by the Indian Institute of Technology (IIT) Guwahati. The said challenge focuses on three low resource Indian languages: 
Gujarati, Tamil and Telugu. The ASR systems are created by employing the recent advancements in neural network techniques. The salient contributions made in this work include: (i) language-specific tuning of the number of senones (tri-phones) (ii) successful employment of the $i$-vector-based features in training the acoustic models and, (iii) exploring the  time-delay neural network (TDNN) and long short term memory (LSTM) based deep neural network (DNN) architectures.",1.0,3.2125
1074,1785,4.1    2.1    3.45    ,Neural Correlates of Lexical Access in Adults with Stuttering,"The aim of the present study was to compare the lexical-semantic activation between Kannada speaking adults with and without stuttering (AWS and AWNS). The participants were divided into two groups. The first group comprised of 15 adults with stuttering, the second group consisted of 15 neuro-typical participants. Participants were subjected to primed lexical decision task, where they had to decide if the target stimulus was a word or non-word. The target stimulus was preceded by a prime which was either semantically related or unrelated to the target. Electrophysiological (Amplitude and latency measures of N400, reflecting the semantic processing) measures were obtained. For electrophysiological recording, continuous EEG recording was done using the Compumedics-Neuroscan instrument with the SynAmps2 amplifier. The latency on N400 was significantly longer in AWS compared to AWNS, however, the amplitude of N400 scores did not differ between the two groups of participants. Overall, the results suggest a deficit in the neural processing of lexical access in adults with stuttering.",1.0,3.2166666666666663
802,1067,3.65    3.55    2.45    3.25    ,Focal Loss and Double-Edge-Triggered Detector for Robust Small-Footprint Keyword Spotting,"Keyword spotting (KWS) system constitutes a critical component of human-computer interfaces, which detects the specific keyword from a continuous stream of audio. The goals for KWS are having a small memory and footprint, while providing the high detection accuracy at a low false alarm rate. The DNN-based KWS system faces a large class imbalance during training because the amount of data available for the keyword is usually much less than the background speech and non-speech, which overwhelms training and leads to the degenerate model. In this paper, we explore the focal loss for the training of a small-footprint KWS system. It can automatically down-weight the contribution of easy samples during training and focus the model on hard samples, which naturally solves the class imbalance and allows us to efficiently utilize all data available. Furthermore, we propose the double-edge-triggered detecting method for the repeated keyword, which significantly reduces the false alarm rate relative to the single threshold method. Systematic experiments demonstrate significant further improvements compared to the baseline system.",1.0,3.2249999999999996
1091,1826,4.7    2.65    2.7    2.85    ,Utterance and Syllable Level Prosodic Features for Automatic Emotion Recognition,"This paper describes an automatic emotion recognition (AER) system developed for the INTERSPEECH 2018 ComParE Atypical Affect Sub-Challenge. This system combines features extracted at three different levels of speech, namely, utterance level, syllable level and frame level to recognize its emotional content. The prosodic features at utterance level and syllable level are extracted after identifying speech/non-speech intervals, followed by syllable level segmentation.  Prosodic features chosen include parameters for representing dynamics of pitch and energy, along with duration information. Frame level features are represented using Mel Frequency Cepstral Coefficients (MFCC). Three separate classifiers are built using Deep Neural Networks (DNN). The decision scores based on the utterance level, syllable level and frame level are fused to identify the emotion of a test utterance. The proposed system gives an Unweighted Average Recall (UAR) of 30.25% for utterance level prosodic features, 28.87% for syllable level prosodic features and 30.7% for frame level spectral features. Fusion of scores by merely adding the scores gives an overall UAR of 37.7% which is at par with the results reported on the development set for the Sub-Challenge.",1.0,3.225
820,1119,3.3    4.15    2.05    3.45    ,Double Talk Detection Based on Deep Neural Network,"We propose a deep neural network (DNN) based model for state classification, and evaluate it on the task of double-talk detection (DTD), which is an essential component of acoustic echo cancellation or suppression. We find that the DNN's discriminate ability significantly outperforms that of the Gauss mixture model in this specific task. Offline features, which are independent from the previous frames and obtained without auxiliary algorithm, are used to extract robust imformation combined with the online features to distinguish the single-talk state or double-talk state. The experimental results demonstrate the effectiveness of the DNN-based model for DTD. The proposed algorithm achieves higher detect accuracy and is more robust to echo path changes compared to the conventionl DTD algorithms.",1.0,3.2375
887,1292,4    3.35    2.85    2.75    ,Improving ASR Error Detection with RNNLM Adaptation,"Applications of automatic speech recognition (ASR) such as broadcast transcription and dialog systems, can be helped by the ability to detect errors in the ASR output. The field of ASR error detection has emerged as a way to detect and subsequently correct ASR errors. The most common approach for ASR error detection is features-based, where a set of features are extracted from the ASR output and used to train a classifier to predict correct/incorrect labels.

Language models (LMs), either from the ASR decoder or externally trained, can be used to provide features to an ASR error detection system, through scores computed on the ASR output. Recently, recurrent neural network language models (RNNLMs) features were proposed for ASR error detection with improvements to the classification rate, thanks to their ability to model longer-range context.

RNNLM adaptation, through the introduction of auxiliary features that encode domain, has been shown to improve ASR performance. This work investigates whether RNNLM adaptation techniques can also improve ASR error detection performance in the context of multi-genre broadcast ASR. The results show that an overall improvement of about 1% in the F-measure can be achieved using adapted RNNLM features.",1.0,3.2375
1124,1915,1.55    5.1    2.8    3.5    ,Improving the Performance of Keyword Spotting System for Children's Speech through Prosody Modification,"Searching for words of interest from a speech sequence is referred to as keyword spotting (KWS). A myriad of techniques have been proposed over the years for effectively spotting keywords from adults' speech. However, not much work has been reported on KWS for children's speech. The speech data for adult and child speakers differs significantly due to physiological differences between the two groups of speakers. Consequently, the performance of a KWS system trained on adults' speech degrades severely when used by children due to the acoustic mismatch. In this paper, we present our efforts towards improving the performance of keyword spotting systems for children's speech. In this regard, we have explored prosody modification in order to reduce  the acoustic mismatch resulting from the differences in pitch and speaking-rate. The prosody modification technique explored in this paper is the one based on glottal closure instant (GCI) events. The approach based on zero-frequency filtering (ZFF) is used to compute the GCI locations. The experimental evaluations presented in this paper show that, significantly improved performances are obtained by explicitly modifying pitch and speaking-rate.",1.0,3.2375
964,1503,4.75    2.35    1.85    4    ,Variation in Excitation Source Characteristics for Shouted and Normal Speech,"The objective of present work is to examine the effect of shouted and normal speech on the excitation source characteristics. Differenced electroglottogram (DEGG) is analyzed in the context of different vowels. In this paper, two novel excitation source features namely, open phase triangle area (OPTA) and flatness of glottal cycle (FoGC) are proposed. The effect of open phase duration and slope of DEGG signal are captured by the proposed OPTA feature. The FoGC feature measures the change in source characteristics due to the strength of excitation (SoE) and pitch period. These features are estimated by processing the DEGG signal. Due to unavailability of DEGG signal in many speech processing tasks, integrated linear prediction residual (ILPR) signal is used as an approximation to the excitation source signal for estimating proposed features. In both cases namely, DEGG and ILPR, it is observed that the proposed excitation source features have a higher value for normal speech compared to shouted speech.",1.0,3.2375
951,1478,3.25    3.5    3.45    2.8    ,Performance Comparison of Cepstral Features for Language Identification Using Convolutional Recurrent Neural Networks,"In this paper, we consider the problem of automatic language identification, and propose the viability of front-end features such as MFCC, and the cepstral features derived with multitapered magnitude (MTMAG) and multitapered modified group delay function (MTMOGDF), using the recently proposed convolutional recurrent neural network (CRNN)-based image classifiers. The CRNN exploits both the spatial and temporal information between the image blocks, and within a given image, for the classification task. The images to these CRNN-based classifiers are constructed based on the considered cepstral-domain features. We conduct a detailed set of experiments on the news database with five International languages, along with the Akashvani news data base with ten Indian languages. The insights  from the resultant confusion matrices are inconclusive in the favor of a particular feature. In other words, each of the chosen features perform well for only a set of the chosen languages. However, evaluating the average performance metrics such as equal error rate (EER) and F1 score shows that the MTMOGDF-driven CRNN outperforms those that were designed using MFCC and MTMAG features. In particular,  the MTMOGDF-driven CRNN achieves nearly 3 dB reduction in EER, with a slight loss of performance in F1 score.",1.0,3.25
582,2125,3.95    5.15    1.95    1.95    ,Robust Mizo Continuous Speech Recognition,"Mizo is an under-resourced tonal language that is mainly spoken in North-East India. It has 4 canonical tones along with a tone-sandhi. In Mizo language, a majority of the words contain tone information. As a result of that, it exhibits higher acoustic variability like other tonal languages in the world. In this work, we investigate the impact of tonal information on robust Mizo continuous speech recognition (CSR). First, separate baseline CSR systems are developed employing the Mel-frequency cepstral coefficient (MFCC) based acoustic features and salient acoustic modeling paradigms. For further improvement, the tonal information has been incorporated in each of the CSR systems. For this purpose, 3-dimensional tonal features are derived which include pitch, pitch-difference, and probability of voicing values. Our experimental study reveals that with the inclusion of tonal information, the robustness of Mizo CSR system gets enhanced across all acoustic modeling paradigms. This trend is attributed to lesser degradation in the fundamental frequency information than the vocal tract information under noisy conditions.",0.0,3.25
815,1106,3.5    4.1    2.3    3.15    ,Deeper Time Delay Neural Networks for Effective Acoustic Modeling,"Time delay neural networks (TDNNs) have been shown to be an efficient network architecture for modeling long temporal contexts in speech recognition. Meanwhile, the training times of TDNNs are much less, compared with other long temporal contexts models based on recurrent neural networks such as long short term memory (LSTM). In this paper, we propose deeper architectures to improve the modeling power of TDNNs. At each TDNN layer that needs spliced input, we increase the number of transforms so that the lower layers can provide more salient features for upper layers. Dropout is found to be an effective way to prevent the model from overfitting once the depth of the model is substantially increased. The proposed architectures significantly improvements the recognition accuracy in Switchboard and AMI.",1.0,3.2624999999999997
847,1183,3.4    2.3    3.4    4    ,Robust Method for Sound Source Localization under Noise and Reverberant Environment,"The performance of the traditional sound source localization methods decrease seriously under noisy and reverberant environment. We propose an improved method to deal with the problem, which mainly has three steps. Firstly, the signal is processed by the multi-channel weighted prediction error (WPE) method to reduce the influence of reverberation. Secondly, considering the time-frequency (T-F) sparseness of speech signal, the reliable T-F points are selected by the noise tracking method. Finally, we utilize the directivity of high frequency domain and the stability of the low frequency domain. In this paper, we use the steered response power-phase transform (SRP-PHAT) to prove the effectiveness of the improved algorithm. The results on several test sets have demonstrated the robustness of the improved method in real environment.",1.0,3.275
968,1510,3.3    3.4    3.9    2.5    ,Fractional Time Delay Estimation from Broadband Signals like Speech,"Classical correlation-based techniques estimate time delay between signals as an integer multiple of the sampling period, where the integer corresponds to the lag at the peak in the correlation sequence. In this paper, we propose a method for fractional lag estimation by drawing support from the multiple correlation sequences that can be obtained by decomposing the signal into components corresponding to individual frequencies using a novel method called single frequency filtering. We demonstrate that it is possible to obtain the lag to an accuracy of 10% of the sampling interval in 87% of the cases and to an accuracy of 20% of the sampling interval in 97% of the cases.",1.0,3.275
877,1263,3.6    3.05    3.9    2.55    ,Time Delay Recurrent Neural Network for Speech Recognition,"In Automatic Speech Recognition(ASR), Time Delay Neural Network (TDNN) has been proven to be an efficient network structure for its strong ability in context modeling. In addition, as a feed-forward neural architecture, it is faster to train TDNN, compared with recurrent neural networks such as Long Short-Term Memory (LSTM). However, different from recurrent neural networks, the context in TDNN is carefully designed and is limited. Although stacking Long Short-Term Memory (LSTM) together with TDNN in order to extend the context information have been proven to be useful, it is too complex and is hard to train. In this paper, we focus on directly extending the context modeling capability of TDNNs by adding recurrent connections. Several new network architectures were investigated. The results on the Switchboard show that the best model significantly outperforms the base line TDNN system and is comparable with TDNN-LSTM architecture. In addition, the training process is much simpler than that of TDNN-LSTM.",1.0,3.2750000000000004
1135,1949,2.1    2.65    3.55    4.8    ,Improve End-to-end Model Design for Imbalanced Speech Emotion Classification,"Data imbalance is a common problem in classification task, classfier usually performs worse on categories with few samples than those with much. In this paper we propose attention gated recurrent convolutional neural network (Attention GCNN) for Crying Sub-Challenge of ComParE 2018. In the proposed model, a bidirectional gated recurrent unit neural network (GRU) with attention will model the audio in general and calculate the preferred frames through attention mechanism. An one convolutional layer neural network (CNN1L) will then extract patterns corresponding to each categories. Compared with the end-to-end baseline model, attention GCNN show strong performance on validation set constructed by three subjects out of ten. Through Attention GCNN we will show the effectiveness of two model design strategies for imbalance data classification: attention mechanism and shallow models.",1.0,3.2750000000000004
1123,1911,2.7    2.25    5.65    2.5    ,Semi-automatic Annotation of a Chinese L2 Speech Corpus,"One of the main obstacles to progress in Computer assisted pronunciation training (abbreviated as CAPT) technology has been the lack of sufficient annotated non-native second language data. It is generally assumed that non-native annotation is very time and labor consuming with poorer quality. To address these problem, this paper is largely methodological and focused on the question of how to improve the non-native annotation quality. We proposed a semi-automatic way which the annotators could refer to an optional result and make a final decision. To investigate its effect, Semi-automatic label was compared with manual label which undertook a role of control group. The quality of annotation was estimated by Inter-annotator Consistency, Full hit, Near hit, Miss, False alarm and Correct acceptance. The results show that mean consistency rate of manual labels is 86.6%, and 83.6% for semi-automatic label. But the semi-automatic label has higher Full hit (11.7%) and lower Miss (4.5%) while keeping similar correct acceptance (78.1%) with the manual label(78.7%), which shows that Semi-automatic annotation perform better.",1.0,3.2750000000000004
1087,1817,1.3    3.35    5.2    ,The Acoustic-camera Corpus for Speech in Noisy Environments.,"To successfully take verbal instructions from humans, it is important for robots to know both what is being said and where the speaker addressing it is. However, in a noisy environment it is still difficult to understand speakers, especially when there are multiple speakers talking at the same time. The contribution of this corpus is that it can be used for speaker separation with the presence of real world noise and reverberation. The speech has been recorded with a microphone array containing 72 microphones, allowing beam-forming to locate the speakers. In addition to this, we have used Microsoft Kinect V2 to record a ground truth of the location. This information can be used to do speaker separation and localisation in noisy and reverberant environments. A comparison using a baseline algorithm demonstrates that examples from this corpus present a more challenging speech separation task than those in the 3rd CHiME challenge corpus",1.0,3.2833333333333337
914,1380,3.55    4.2    2.3    3.1    ,Towards a Real-World Baby Cry Decoder,"Real-time decoding of a baby’s immediate need is a challenge for new parents and caregivers. The goal of our project is a creating a baby cry decoder capable of distinguishing between the different types of baby cries in real-world conditions. In this study, we describe a series of experiments designed to establish the accuracy of popular machine learning algorithms on the categorization of baby cries. We apply the lessons learned from our previous baby cry classification studies on 5 types of baby needs from a dataset containing 82 baby cries. Here, we expand to 7 types of baby needs with a database containing 13373 baby cries, recorded in a real hospital NICU. We perform best feature selection and report correlated accuracies.
Index Terms: baby cries, best feature selection, computational paralinguistics, machine learning, SPLANN",1.0,3.2875
1032,1674,3.3    2.65    2.8    4.45    ,Combining Phase-based Features for Replay Spoof Detection System,"Automatic Speaker Verification (ASV) system is used to verify
the claimed identity of a speaker based on speech samples.
The spoofing attacks increased due to significant technological
advances which motivated the authors to investigate different
countermeasures. Replay is one of the spoofing attacks where
the ASV system is fooled with the help of pre-recorded speech
samples of a target speaker. Both magnitude-based and phasebased
features get affected by the quality of intermediate devices,
the noise level of recording and playback environments.
Only a few studies have reported the use of phase-based features
for replay spoof detection. In this paper, we explore the
relative significance of various phase-based features for replay
spoof detection. The magnitude-based features are chosen and
perform score-level fusion among phase-based features and between
the magnitude and phase-based features to capture the
possible complementary information. Among various possible
combination of magnitude and phase-based features, the Equal
Error Rate (EER) reduced significantly (i.e., to 11.90 %) than
the individual features set alone, while the score-level fusion
among phase-based features gave EER of 12.98 % on the evaluation
set of ASV Spoof 2017 Challenge database.",1.0,3.3
1218,2166,2.7    3.55    4.2    2.75    ,Deep Neural Network Based Acoustic Scene Classification Using Score Fusion of MFCC and IMFCC,"In this study, we propose an Acoustic Scene Classification
based on Deep Neural Networks (DNN). For the past few years,
Mel-Frequency Cepstral Coefficients (MFCC) features has been
used for speech recognition, speaker recognition, acoustic scene
classification, etc. Due to the design of Mel filter bank, MFCC
features are used to capture the acoustic scene characteristics
effectively in the low-frequency regions. In this paper, Inverse
Mel Frequency Cepstral Coefficients (IMFCC) are used as com-
plementary to Mel filter bank structure which adds finesse to the
extraction of acoustic scene characteristic cues present in the
high-frequency regions. Hence, both MFCC and IMFCC fea-
tures are used to capture acoustic information in the audio fre-
quency range effectively. An experiment is performed on Tam-
pere University of Technology (TUT) Acoustic Scenes 2017
Dataset. DNN architecture at utterance level classification with
supervised learning is adopted. Scores from the DNN models
corresponding to MFCC and IMFCC features are fused to test
the model. The average accuracy obtained by our proposed sys-
tem on a four-fold cross-validation setup is 80.02% resulting in
5.22% relative improvement with respect to the baseline sys-
tem based on Acoustic Scene Classification using log-Mel band
energies with Multilayer Perceptron Model (MLP).",1.0,3.3
1186,2076,3.75    3.4    2.75    3.35    ,Speech Enhancement Using Binary Masks Predicted from Sff Outputs,"Speech enhancement is necessary to improve quality and intelligibility of speech signals.  Most of the enhancement algorithms modify the spectrum of noisy signal to enhance speech or suppress noise. Such methods can be viewed as weighting of the noisy spectrum with a gain factor. The gain factor can also be 1 or 0 as in binary masking which is shown to improve the intelligibility of speech signal. In this paper, a gain factor is derived using single frequency filtering (SFF) approach, which has high spectral resolution. In SFF, the spectral envelopes of speech signal are extracted for each frequency at every time instant. The derived envelopes are compensated for noise by using a weight factor for each frequency which assigns lower weight to the frequencies that are much effected by noise than others. The difference operation on variance of SFF envelope highlights  the high SNR regions in time-domain. The evidence from difference of variance signal and noise compensated envelopes is used to modify SFF outputs. The enhanced signal is resynthesized by summing the modified SFF outputs at each instant of time for the derived frequencies. The proposed algorithm is evaluated for different noises at various levels of degradation.",1.0,3.3125
1147,1977,3.45    5.1    2.35    2.35    ,Training Large Scale NLU Systems on Asynchronized Data,"Natural Language Understanding (NLU) systems are designed to serve multiple domains and training them involves pooling a cross-domain corpus.
A common dataset is required so that domain specific NLU components (e.g., domain classifier, intent classifier, named entity recognizers and calibrators), trained in parallel, are well calibrated against each other.
In this paper, we investigate the need to provide a synchronized dataset for training domains components across all domains.
The motivation of our experiments is to avoid a data synchronization step for model training and providing a more granular control of data used in training the domain components.
In particular, a lack of need to co-ordinate changes in training data has business value towards achieving a faster NLU system updates.
We conduct two experiments to investigate the need to synchronize datasets across domains: (i) using asynchronized data for training the each domain's recognizers, except training the calibrators on a common dataset and, (ii) also training the domain calibrators on asynchronized data.
We realize this asynchrony in the training data by synthetically perturbing the constitution of the utterances, as well as introducing a real use case updates in training data.
We identify the conditions under which an asynchronous training is viable.",1.0,3.3125
1206,2140,2.85    3.35    2.85    4.2    ,Submission from CMU for Low Resource Speech Recognition Challenge,"In this paper we present the approaches we followed for speech recognition in the context of INTERSPEECH2018 Low resource speech recognition challenge. We have submitted systems for all the three languages in the challenge: Gujarati, Telugu and Tamil. The recognition systems were trained using three kinds of acoustic features: Mel Frequency Cepstral Coefficients (MFCCs), log Mel-scale Filter Bank energies (FBANK) and Perceptual linear predictive (PLP) features. We have explored the use of data augmentation at both acoustic and linguistic levels to achieve robust performance with limited data resources. The best system using a single feature set achieves 14.8% word error rate while a fusion system using multiple features yields 14.3 %. Dataset augmentation via speed perturbation appears to add significant robustness to the system. Although the linguistic augmentation techniques did not achieve as good a result as acoustic augmentation techniques, the two approaches had good synergy when combined together in a fused system.",1.0,3.3125
900,1330,3.4    3.4    3.15    ,Discriminating between High-Arousal and Low-Arousal Emotional Speech,"Identification of emotions from human speech can be attempted
by focusing upon three aspects of emotional speech:
valence, arousal and dominance. In this paper, changes in the
production characteristics of emotional speech are examined to
discriminate between the high-arousal and low-arousal emotions,
and amongst emotions within each of these categories.
Basic emotions anger, happy and fear are examined in high-arousal,
and neutral speech and sad emotion in low-arousal
emotional speech. Discriminating changes are examined first
in the excitation source characteristics, i.e., instantaneous fundamental
frequency (F0) derived using the zero-frequency filtering
(ZFF) method. Differences observed in the spectrograms
are then validated by examining changes in the combined characteristics
of the source and the vocal tract filter, i.e., strength of
excitation (SoE), derived using ZFF method, and signal energy
features. Emotions within each category are distinguished by
examining changes in two scarcely explored discriminating features,
namely, zero-crossing rate and the ratios amongst the spectral
sub-band energies computed using short-time Fourier transform.
Effectiveness of these features in discriminating emotions
is validated using two emotion databases, Berlin EMO-DB
(German) and IIT-KGP-SESC (Telugu). Proposed features
exhibit excellent results in discriminating these emotions. This
study can be helpful towards automatic classification of emotions
from emotional speech.",1.0,3.3166666666666664
1228,2202,5.05    2.45    4.2    1.6    ,Syllabification Strategies of Chinese Learners of Russian as Foreign Language: Interference of L1 Syllable Patterns,"Experimental study deals with syllabification behavior of Chinese learners of the Russian language as foreign language while reading Russian word set with various types of multi-component consonantal clusters. Experimental pool of participants comprised 15 native speakers of Chinese with 4-4,5 experience of Russian classes within China. Results of syllabification experiment enable to define cross-language influence of syllabic patterns and phonotactic restrictions of the Chinese language entailing substantial foreign accent effects in the Russian speech of Chinese people. We suggest that most prominent source of accent is insertion of additional vowel(s) into consonantal cluster increasing thus number of syllables in the word. The utmost potential for changing syllabic composition of a word through formation of additional syllable in the Russian speech of Chinese learners demonstrates consonant [r]. The opposite phenomenon, i.e. deletion of a syllable in the word occurs regularly in syllabification behavior for words with vocalic hiatus. In this case, a diphthong / diphthongoid may appear, or one of the neighboring vowels could be omitted. This and the other evidence observed in syllabification experiments with Chinese learners of Russian as L2 confirms hypothesis on syllable structure as fundamental cause of inter-language interference.",1.0,3.3249999999999997
1223,2177,3    2.45    3.55    4.3    ,Fundamental Frequency Slope as a Cue to Word Segmentation in Natural Continuous Speech,"Listeners use several cues to segment the continuous speech stream into words. In our previous study, the fundamental frequency (f0) value was identified as guiding the segmentation process of French homophonic sequences. This paper examines more deeply the influence of f0 slope in the segmentation process. 
The slope and/or the mean f0 value of the /a/ vowel in the natural consonant-initial production 'la mie' were manipulated in order to test whether it influences perceived segmentation, i.e. la mie ‘the crumb’ or l’amie ‘the friend’. In our experiment, participants listened to phonemically identical sequences such as /selami/, c’est la mie / l’amie ‘it’s the crumb / friend’ and had to perform a two-alternative forced choice task. 
The main result showed that f0 slope alone can bias speech segmentation and that its influence increases with the increasing of the mean f0 value. This finding brings evidence of the relative importance of the f0 slope in the segmentation of French ambiguous sequences and strengthens the idea of the robust role of early intonational cues in word segmentation in French.",1.0,3.325
821,1123,2.5    2.7    4.3    3.8    ,"Boosted Trees for Atypical Affect, Crying, Heartbeat, and Self-assessed Signal Classification","The field of paralinguistics is developing rapidly along with its various applications. In this paper, we cover the four sub-challenges of the Interspeech ComParE challenge 2018, where the goal is to recognize the emotion of speech from disabilities, crying signals, heartbeat signals, and to predict the Likert scale of free speech. We approach these challenges in such a way that does not require large-scale data sets for suitable model training as a part of general classification problems. In this paper, we propose simple and effective algorithm that enables the model to learn higher order interaction between features even from small size data sets, which has proven to be one of the main issues that deep learning techniques and general paralinguistic tasks currently face. We first extracted the ComParE baseline feature, emobase2010 feature and prosody feature then trained them on each corpus in a supervised scheme. Finally, we used the trained trees to deduce the appropriate label for each task. Evaluation is measured in terms of weighted average recall (WAR) and unweighted average recall (UAR) in every task. This scalable boosted tree-based framework is applied on all Interspeech ComParE sub-challenge corpora, proving to outperform or be competitive compared to challenge baselines.",1.0,3.325
782,1002,4.8    2.2    3.6    2.7    ,The CSU-K DNN-Based System for the 2Nd Edition Spoken CALL Shared Task,"This paper presents the set-up and results of the DNN-based
Cooperative State University Karlsruhe (CSU-K) system 
for the 2nd edition of the shared spoken CALL ESL task, 
which added 6.698 utterances to the first edition of the task. 
School children participate in a dialogue system, that is then 
required to judge the utterances' correctness. 
Input features to the presented DNN-based system 
were distance measures between prompt-responses and 
input utterances based on the outputs of a Doc2Vec implementation 
that was retrained on the shared task data.
The paper presents a DNN architecture proposed for the 
system of the CSU-K shared task tutorial. 
It achieved a D-value of 13.70 on the 2nd edition 
evaluation test set.",1.0,3.325
1048,1718,3.5    2.65    2.15    5    ,Exploiting Short Text Keywords to Enhance Spontaneous Speech Driven Content Retrieval,"Content retrieval systems that accept spontaneously spoken queries as its input enable users to easily submit long queries, which provide rich clues for its backend retrieval process. On the other hands, the spontaneous nature means that they are harder to recognize reliably. In this work, we investigate supplementary use of short text keywords to enhance speech driven content retrieval systems. Here, we expect that, even though users can type only a short text, it can be used as reliable clues to complement the less reliable but richer spontaneously spoken queries. We propose two combination methods of such two modes of queries with different benefits. We also propose a method of estimating interpolation weight of the combination automatically by seeing only given two queries on the fly. Through our experimental evaluation, we show that the complementary use of spoken and text queries can actually improve the retrieval performance, and also that our proposed estimation method of the interpolation weight can balance well the two modes of queries automatically, resulting in achieving the performance comparable to when using the uniform oracle weight.",1.0,3.325
824,1133,3.75    2.75    4.1    2.7    ,An Analysis of Decoding for Attention-based End-to-end Mandarin Speech Recognition,"Many of the current state-of-the-art Mandarin Large Vocabulary Continuous Speech Recognition (LVCSR) Systems are built either with a hybrid of Deep Neural Network (DNN) and Hidden Markov Models (HMM) or with Neural Network model trained with Connectionist Temporal Classification (CTC) criterion. 
    In both of these models, decoding is conducted by Weighted Finite State Transducer (WFST) that searches the word sequence which matches best with the speech given the acoustic and language models. Recently, attention-based end-to-end method is becoming more and more popular for Mandarin speech recognition. This new method advocates replacing complex data processing pipelines with a single neural network trained in an end-to-end fashion.
      In this paper, we investigate the decoding process for attention-based Mandarin models using syllable and character as acoustic modeling units and discuss how to combine word information into the decoding process. We also conduct a detailed analysis on various factors that affect the performance of decoding, including beam size, label smoothing, softmax temperature, attention smoothing and coverage.",1.0,3.325
1329,2596,3.3    3.5    3.2    ,Speech Source Localization and Separation with a PCA-based Preprocessing Method,"Bin-wise time delay is a valuable clue for time-frequency (TF) masking based speech localization and separation. It can effectively solve the spatial aliasing problem with the histogram technique. However, this kind of method suffers from the degradation caused by non-directional noise. The estimated direction of signal may not be accurate and the direction-free noise cannot be handled easily. This paper proposes a principal component analysis (PCA) based preprocessing method to deal with these interferences. Short-time Fourier transform (STFT) is utilized to transform the signal into TF domain. A novel PCA strategy is carried out within each frame to form a selection of TF-units which are dominated by directional signals. Speech source localization and separation are performed only on these selected TF-units. As a result, the effects of non-directional noise are suppressed. Some experiments were conducted under different conditions, showing that the performance of speech source localization and separation methods can be remarkably improved with the proposed method.",1.0,3.3333333333333335
760,64,3.35    3.5    2.3    4.25    ,Implementation of Frame Constrained Scoring for Lightweight Speaker Verification System,"GMM-UBM and i-vector are widely adopted as compact and effective systems for speaker verification. This paper compares the performance of GMM-UBM and i-vector for short duration text-dependent task, and describes our advances in lightweight text-dependent speaker verification system for short duration utterances. Top N frame scoring and frame constrained scoring based on GMM-UBM and i-vector are proposed to improve the performance of speaker verification systems. In GMM-UBM, constrained frames whose scores are supposed to be representative for speech segments are selected. It is thought that it effectively reduces the noise influence and somewhat compensates the deficiency of VAD. In i-vector, the constrained frames from GMM-UBM are appended to enhance the noise robust. The methods can be implemented in the voice wakeup engine with keyword spotting and meet the run-time requirement ideally. Performance on our ""Hi Bixby"" English and Korean test cases under different kinds of environments are compared. Results show the performance is improved by more than 36% compared with GMM-UBM, and more than 28% compared with i-vector on both test cases.",1.0,3.3499999999999996
1325,2588,2.65    4.05    2.7    4    ,Data Selection for Mandarin-English Code-Switched Speech Recognition,"Sufficient amount of training data is required for building the state-of-the-art speech recognition systems. However, it remains a problem to get sufficient data for some application domains such as code-switching scenarios. This work proposes effective approaches to select informative and representative speech and text data from monolingual resources hence to improve the performance of Mandarin-English recognition. The selected speech data is used for building initial deep neural network (DNN) based acoustic model (AM). Cross-lingual transfer strategy is then applied on the AM to produce the final AM. Comparing to training initial DNN-AM using large scale of data, the proposed approach is more efficient. The proposed text data selection approach greedily selects Mandarin-English code-switching data for building language model (LM). The LM is then interpolated with a domain specific LM, to achieve better recognition performance.",1.0,3.3499999999999996
1114,1889,4.25    2.65    2.65    3.85    ,Features and Network Initialization Approaches for Speech-Based Depression Detection,"We study two questions relevant for depression detection. First, we compare performance of different acoustic features. Second, we investigate the effects of neural network pre-training for contexts involving small datasets. We consider a binary classification problem, with ground truth labels obtained by thresholding the scores obtained from a standard self-assessment questionnaire. We perform a speaker-independent study comparing the results obtained for different feature sets fed to a neural network based classifier with that of a baseline support vector machine based classifier used in Audio-Visual Emotion Challenge 2016 Depression sub-challenge. We find that noise robust features such as damped oscillator cepstral coefficients (DOCC) which have been shown useful for automatic speech recognition (ASR), can result in better performance than more standard features for this task. Another current challenge for research on depression is small datasets. We investigate the applicability of pre-training a neural network model. We show that initializing the weights of the neural network model with that of a stacked auto-encoder designed to reconstruct the feature vectors extracted from a larger dataset results in improvement in recognition performance compared with random initialization.",1.0,3.35
1063,1761,2.8    2.5    2.6    2.95    5.95    ,A Complex-valued ICA Algorithm Based on L-BFGS,"Abstract Complex-valued independent component analysis (ICA) is a celebrated method in blind separation of complex-valued signals. One theoretical drawback of ICA, as predicted by the Cramer-Rao lower bound (CRLB), is that its cost function becomes increasingly flat when the sources approach Gaussian. Therefore, fixed-point iteration, a quasi-Newton method, is the preferred algorithm to implement ICA. Unfortunately, fixed-point iteration relies on several approximations of the Hessian matrix which may not hold in practice. In this paper, we proposed to transform the complex-valued ICA constrained optimization problems into unconstrained optimization problems which can be solved by limited-memory Broyden–Fletcher–Goldfarb–Shanno update (L-BFGS). The proposed algorithm can separate mixed sub-Gaussian, super-Gaussian, circular, and non-circular sources. Simulations show promising results.",1.0,3.3600000000000003
1213,2154,2.5    4.2    4.2    2.55    ,Combining Evidences from Variable Teager Energy Source and Mel Cepstral Features for Classification of Normal vs. Pathological Voices,"In this paper, novel Variable length Teager Energy Operator (VTEO) based Mel frequency cepstral coefficients, namely, VTMFCC is proposed for automatic classification of normal and pathological voices. Experiments have been carried out using this proposed feature set, Mel Frequency Cepstral Coefficients (MFCC) and their score-level fusion. A classification was primarily performed using a discriminatively-trained 2nd order polynomial classifier on a subset of the MEEI database for a feature dimension of 12. The equal error rate (EER) on fusion was reduced by 3.2 % than MFCC alone which was used as the baseline. The classification accuracy was analyzed for different dimensions of the feature vector. Furthermore, results obtained for the 2nd order classifier were compared with results obtained from the 3rd order polynomial classifier for different feature dimensions. In addition, the effectiveness of dynamic features, in particular, delta, delta-delta and shifted delta cepstral features have been investigated for this particular problem.  It has been observed that score-level fusion (with equal weights) of proposed feature set and state-of-the-art MFCC gives better classification performance than MFCC alone for various evaluation factors considered in this paper.",1.0,3.3625
961,1498,3.3    2.7    2.35    5.1    ,Fast Gated Recurrent Neural Networks for Speech Synthesis,"The recurrent neural network (RNN) has attracted a lot of attention in the research area of speech synthesis. To produce human-like sounds, various approaches using RNN have been successfully applied to the acoustic modeling on speech synthesis. However, the long computing time of these approaches is still the main concern. In this research, we develop fast gated recurrent neural network for speech synthesis based on gated recurrent unit (GRU). Our method removes some state units in state equation of GRU. Compared to the baseline of GRU, our results obtained are twice faster without reducing the accuracy result.",1.0,3.3625
1049,1719,2.9    4.3    1.25    5    ,Analysis of Prosodic Correlates of Emotional Speech Data,"The study of expressive speech styles remains an important topic as to their parameters detection or prediction in speech processing. In this paper, we analyze prosodic correlates for six emotion styles (anger, disgust, joy, fear, surprise and sadness), using data uttered by two speakers. As each sentence pronounced in an emotional style has also been uttered in a neutral style, this allows us to focus the analysis on the way pronunciations and prosodic parameters are modified in emotional speech, compared to neutral style, for the two speakers. The analysis focuses on speech pronunciation modifications, and on local prosodic behavior, with an emphasis set on the analysis of the prosody over prosodic groups and breathing groups. Thus, this paper presents and analyses several speech characteristics, and compares them between neutral and emotional speech. This includes pronunciation modifications, speaking rate, presence of pauses in sentences, and fundamental frequency behavior over prosodic groups.",1.0,3.3625
804,1071,2.8    2.7    4.4    3.55    ,DNN-based Speech Synthesis for Arabic: Modelling and Evaluation,"This paper investigates the use of deep neural networks (DNN) for Arabic speech synthesis. In parametric speech synthesis, whether HMM-based or DNN-based, each speech segment is described with a set of contextual features. These contextual features correspond to linguistic, phonetic and prosodic information that may affect the pronunciation of the segments. Consonant gemination and vowel quantity are two particular and important phenomena in Arabic language. Hence, it is worth investigating if those phenomena must be handled by using specific speech units, or if their specification in the contextual features is enough. Consequently four modelling approaches are evaluated by considering geminated consonants (respectively long vowels) either as fully-fledged phoneme units or as the same phoneme as their simple (respectively short) counterparts. Although no significant difference has been observed in previous studies relying on HMM-based modelling, this paper examines these modelling variants in the framework of DNN-based speech synthesis. Listening tests are conducted to evaluate the four modelling approaches, and to assess the performance of DNN-based Arabic speech synthesis with respect to previous HMM-based approach.",1.0,3.3625
927,1410,2.2    2.7    4.95    3.6    ,Two-Level Deep and Convolutional Neural Network Fusion Method with Multi-Task Learning for Binaural Sound Source Localization,"In binaural sound source localization, front-back confusions often occur as a challenging problem in full azimuths range, particularly in the noisy or reverberant environments. To address it, a novel algorithm fusing two-level deep and convolutional neural network based on multi-task learning is proposed. Front-back features can be leveraged as additional information for localization task. Firstly, joint features of interaural level differences (ILD) and cross-correlation function (CCF) within a lag range are extracted from binaural signals. Secondly, convolutional neural network is used for the front-back classification task, and deep neural network is used for azimuth classification task with the extracted CCF-ILD features. In addition, we propose an angle loss function to avoid the overfitting problem and generalize well in adverse acoustic conditions. Finally, two branches are concatenated and followed by an output layer. Experiments show that our methods get the state-of-art performance in noisy and reverberant environments.",1.0,3.3625000000000003
873,1253,4.2    3.3    2.65    3.35    ,On Language Modeling of Hindi-English Code-Switched Data,"In bilingual communities, usually the speakers switch between two languages during verbal discourse and this phenomenon is referred to as code-switching. It is also common in chats and messages posted on social media like Facebook, Twitter, WhatsApp, etc. Code-switching poses some interesting challenges to the speech signal processing community, in particular, those involved in speech recognition, language modeling, and language identification. In recent times, this topic has generated a lot of interest. Yet, the research activity is somewhat limited due to lack of domain-specific resources. This work explores effective language modeling of Hindi-English code-switched data by employing the factored $N$-gram language model (LM). The existing parts-of-speech (POS) and language identification (LID) textual features are employed as factors. Additionally, the code-switching instants are also used as a feature and it is referred to as code-switch location (CSL) feature. For this study, the code-switched text database is derived by crawling a few web-blogs.  The salient contributions of this work include: (i) creation of Hindi-English code-switching database, (ii) a substantial reduction in perplexity with the use of POS and LID features and, (iii) additive gain in perplexity with the use of the proposed CSL feature.",1.0,3.375
1010,1622,2.65    2.55    4.95    ,A Formal Analysis about the Peaky Behavior of CTC,"We provide a formal analysis of the peaky behavior
of the Connectionist temporal classification (CTC) model and its training criterion
as well as hidden Markov models (HMM) and the generalized criterion
with the full sum over all alignments where the CTC criterion is a special case of.
The analysis leads to a new understanding of the role of the blank label,
and a better understanding of the convergence behavior
with CTC and the generalized full-sum criterion
with on-the-fly realignments.
We refer to experiments in the literature which confirm our analysis,
as well as experiments on Switchboard to demonstrate
the convergence and peaky behavior.",1.0,3.383333333333333
886,1290,5    2.3    2.85    ,Urdu Speech Recognition Using Hidden Markov Models and Deep Neural Networks,"We present the first high accuracy, publicly available speech recognition system for read Urdu, a language spoken by 163 million people throughout Pakistan, India and several other countries. We compare performance of the state-of-the-art speech recognition techniques applied to a medium vocabulary (6,000 word types) read speech Urdu dataset using KALDI automatic speech recognition toolkit. Our ASR system, called Guftaar Shanaas, uses the ARL speech corpus (LDC2007S03, 200 hours, 160K speech segments) that is available publicly through Linguistic Data Consortium (LDC). We describe in detail the construction of the three basic components of a speech recognition system: an acoustic model, a language model, and a pronunciation lexicon for Urdu language. Our best model uses sequentially trained DNNs that achieve a word error rate (WER) of 1.44% using the language model based on the training transcripts of ARL speech corpus, and a WER of 13.14% using an independent language model. We have made our KALDI scripts and pipeline publicly available that can be used to regenerate all resources for Urdu ASR given a speech corpus. To our knowledge, this is the first ever effort to share sizeable and reproducible speech recognition results and resources for Urdu.",1.0,3.3833333333333333
1023,1656,3.25    3.9    2.85    3.55    ,Hypernasality Severity Analysis Using Spectral and Residual Features,"In this work, spectral and linear prediction (LP) residual characteristics of normal, mild and moderate-severe hypernasal vowel /i/ are analyzed for finding their nature in these three levels of hypernasality severity. The coupling of nasal tract with the oral tract during the production of hypernasal vowels introduces additional nasal pole-zero pair in these vowels, which deviates their spectral characteristic compared to normal vowels. Further, the inappropriate modeling of this deviated spectrum by LP model due to the presence of zeros gives the deviated residual characteristic also. The spectral and residual analysis is done using vocal tract constriction (VTC) evidence, power spectral entropy (PSE) and peak to side-lobe ratio (PSR) features. The first two features capture the spectral deviation in terms of low-frequency dominance and spectral flatness in hypernasal vowel spectrum respectively whereas the third feature capture the residual deviation in terms of presence of undesirable signal components in the residual signal. The analysis result reveals that the value of VTC evidence increases whereas the value of PSE and PSR features decreases as the severity of hypernasality increases.",1.0,3.3875
1144,1971,2.2    4.15    3.45    3.75    ,Speech Emotion Recognition of Mentally and Physically Disabled Individuals,"This report presents our participation in the Atypical Affect Sub-Challenge of the INTERSPEECH 2018 Computational Paralinguistics Challenge (ComParE 2018). The task was to recognize four emotions (neutral, happiness, sadness and anger) in the EmotAsS dataset comprised of the speech utterances of individuals with reduced capabilities. 
We obtain a score of 40.8% (unweighted accuracy) with an ensemble of two models. The first one is a neural network pretrained on the IEMOCAP dataset and fed with raw spectrograms. Its architecture is composed of four convolutional layers followed by a bidirectional long short-term memory layer. The second model is an SVM trained on the ComParE acoustic feature set.
The neural network alone performed with unweighted accuracy of 39.2%, to be compared with 28.0% of CNN+LSTM network presented in the ComParE baseline paper.
We employed several techniques to enhance performance. First, we undersampled the neutral-labeled subset and oversampled the sad- and angry-labeled subsets. We also performed data augmentation, using vocal tract length perturbation for the neural network and SMOTE for the SVM. Layer-wise learning rate adjustment was used for the neural network optimization. Finally, ensembling of the neural network with the SVM was performed with a single dense layer.",1.0,3.3875
853,1193,2.35    4.2    2.7    4.3    ,Subword-Based End to End Mixed Language Speech Recognition,"Sequence-to-sequence model with attention has recently gained a lot of interest as a potential way to simplify the process of building ASR systems.
   Some work in English speech recognition has also achieved state-of-arts result with attention-based model using subword as modeling unit while other work in Chinese speech recognition achieved promising result with attention-based model using Chinese character as modeling unit.
  These results inspire us to wonder whether it's possible to build an attention-based model that utilizes both Chinese character and English subword to deal with the problem of Chinese-English mixed speech recognition.
   In this paper, we create a novel attention-based model that uses Chinese character and English subword as modeling unit. Using this new model, we achieves a character error rate (CER) of 10.85% on our internal DidiReading testset.
  By comparison, our baseline attention based character modeling method achieves a CER of 32.41%.",1.0,3.3875
1208,2142,5.2    2.6    3.9    1.85    ,Accidentally on Purpose? Disfluencies in Stand-up Comedy,"Understanding how expert speakers structure their speech and use disfluencies can enrich speech synthesis and analysis. We propose to study the use of disfluencies in stand-up comedy, which sounds spontaneous but is structured, rehearsed and repeated. Comedians' speech often contains apparent errors and filled pauses, unlike that of actors reading from a text. Because a professional comedian touring a show provides a similar experience to every audience, deviation and disfluency between performances are unlikely to be communicative failures. 

In a case study, we compare absolute transcripts from live performances of a rehearsed show to one another and to the performer's text. Using subsequence matching on transcripts of five performances of a stand-up comedy we map consistent subsequences, revealing aspects of show structure, and identify how subsequences are broken. The results show that filled pauses occur significantly more frequently immediately before and after matching subsequences, suggesting that disfluent speech could play a structural role in stand-up comedy as well as be part of the prepared material itself.",1.0,3.3875
1174,2038,3.3    2.65    3.4    4.2    ,An Improved DNN-based Spectral Feature Mapping That Removes Noise and Reverberation for Robust Automatic Speech Recognition,"Reverberation and additive noise have detrimental effects on the performance of automatic speech recognition systems. In this paper we explore the ability of a DNN-based spectral feature mapping to remove the effects of reverberation and additive noise. Experiments with the CHiME-2 database show that this DNN can achieve an average reduction in WER of 4.5%, when compared to the baseline system, at SNRs equal to -6 dB, -3 dB, 0 dB and 3 dB, and just 0.8% at greater SNRs of 6 dB and 9 dB. These results suggest that this DNN is more effective in removing additive noise than reverberation. To improve the DNN performance, we combine it with the weighted prediction error (WPE) method that shows a complementary behavior.  While this combination provided a reduction in WER of approximately 11% when compared with the baseline, the observed improvement is not as great as that obtained using WPE alone. However, modifications to the DNN training process were applied and an average reduction in WER equal to 18.3% was achieved when compared with the baseline system. Furthermore, the improved DNN combined with WPE achieves a reduction in WER of 7.9% when compared with WPE alone.",1.0,3.3875
1185,2070,4.35    3.45    3.35    2.4    ,Auditory-visual Speech Perception in Bipolar Disorder: Lip-reading Can Stall While Integration Remains Intact,"This study preliminarily investigated how people with bipolar disorder integrate auditory and visual speech information and whether any differences existed between the manic and depressive episodes. It was predicted that bipolar groups’ auditory-visual speech integration would be less robust than that of the control group and further, the manic phase of bipolar disorder would integrate visual speech information more readily than their depressive episode counterparts. To examine these predictions, the McGurk effect paradigm was employed using auditory-visual speech (AV) as well as auditory-only (AO) and visual-only (lip-reading, VO) stimuli. Results showed that the disordered and non-disordered groups differed only VO (lip-reading) stimuli and that how auditory-visual speech integration is made by the disordered group calls for further research whereby behavioural and physiological data should be collected simultaneously.",1.0,3.3875
1152,1989,4.3    2.05    3.55    3.7    ,Effect of Spectral Resolution on Speech Perception in Cochlear Implant Simulations: a Preliminary ERP Study,"Cochlear implant (CI) has become an effective treatment solution for profound-to-severe hearing-impaired patients over the last decades. This study aimed at evaluating the effect of spectral resolution (i.e., the number of channels) on CI speech perception via vocoder simulations. While many early behavioral experiments have shown the effect of spectral resolution on speech understanding in CIs, this study objectively evaluated the effect of spectral resolution based on the acoustic change complex (ACC). Three numbers of channels (i.e., 1, 2 and 4) were used to process the vocoded stimuli, and two conditions were employed to elicit the ACC in this study. The number of channels was changed from four to two and one, yielding two conditions of 4-to-2 and 4-to-1, respectively. Experimental results showed that both conditions evoked the ACC responses, while the ACC response elicited under the 4-to-1 condition had no significant difference compared with that evoked under the 4-to-2 condition, indicating that vocoded stimuli with poor spectral resolutions may not yield notable difference in their ACC responses. These results provided neurophysiological evidence for analyzing the effect of spectral resolution on speech perception.

Index Terms: Speech perception, spectral resolution, acoustic change complex.",1.0,3.3999999999999995
866,1218,3.45    5.25    3.35    1.55    ,DNN-Based Missing Feature Reconstruction for Robust Speech Recognition,"We explicitly conduct missing feature reconstruction using a deep neural network (DNN) in this study. Towards the hypothetical scenario of two interfering speech sources, the reconstruction is established as the local regression between the reliable components and the unreliable components. After the masking-based source separation, the DNN is firstly trained via mapping from the separated speech to the source speech in the time-frequency domain. Subsequently, the masks for separation are reused to maintain the reliable components. The missing features are estimated by combining the DNN output and the separated speech. The proposed method is applicable for both the binary and the ratio masking based separation. For evaluation, three objective measures and one phoneme recognition benchmark were selected as proof of concept. The experimental results confirmed the DNN-based method's potential for robust speech recognition.",1.0,3.4
811,1092,3.45    2.7    4.1    3.35    ,Acoustic Event Detection with Sequential Attention and Soft Boundary Information,"Acoustic event detection is to perceive the surrounding auditory sound and is popularly performed by the multi-label classification based approaches. The concatenated acoustic features of consecutive frames and the hard boundary labels are adopted as the input and output respectively. However, the different input frames are treated equally and the hard boundary based outputs are error-prone. To deal with these, this paper proposes to utilize the sequential attention together with the soft boundary information. Experimental results on the latest TUT Sound Event database demonstrate the superior performance of the
proposed technique.",1.0,3.4
800,1064,4.25    2.6    4.35    2.4    ,Determining Number of Speakers from Multispeaker Speech Signals,"A new method for time delay estimation (TDE) from mixed source (speaker) signals collected at two spatially separated microphones is proposed in this paper. The key idea in this proposal is that the crosscorrelation between corresponding segments of the mixed source signals is computed using the outputs of single frequency filtering (SFF) at each frequency, rather than using the waveforms directly. The advantage of SFF output is that it will have high signal to noise regions in both time and frequency domains, and also it gives multiple evidences, one from each of the SFF output. The multiple evidences can be combined to obtain robustness in the TDE. The estimated time delays can be used to determine the number of speakers present in mixed signals. The TDE is shown to be robust against different levels of additive noise degradation, and also against different types of degradation. The results are shown for mixed speech signals collected at two spatially separated microphones in a live laboratory environment, when the mixed signals contain speech from several spatially distributed speakers.",1.0,3.4
793,1036,5.05    3.3    2.35    2.9    ,Exploring High-Definition Filters in Convolutional Neural Networks,"Triangular, overlapping Mel-scaled filters (“f-banks”) are the current standard input for acoustic models that exploit their input's time-frequency geometry, because they provide a psycho-acoustically motivated time-frequency geometry for a speech signal. F-bank coefficients are provably robust to small deformations in the scale. In this paper, we explore two ways in which filter banks can be adjusted for the purposes of speech recognition. First, triangular filters can be replaced with Gabor filters, a compactly supported filter that better localizes events in time. Second, by rearranging the order of operations in computing f-banks, features can be integrated over smaller time scales while
simultaneously providing better frequency resolution. We evaluated the effects of these filters on a modern end-to-end CNN phone recognizer. As a simple drop-in replacement, experimental filter banks produced a lower test PER than f-banks of 17.89%.",1.0,3.4
882,1278,4.15    3.3    3.25    2.9    ,Primacy of Mouth than Eyes to Perceive Audiovisual Mandarin Lexical Tones,"This study investigated Chinese and English speaker’s patterns of eye movement when they were asked to identify audiovisual Mandarin lexical tones. The Chinese and English speakers were presented a clip of Mandarin monosyllable (/ă/, /à/, /ĭ/, /ì/) in audiovisual mode and asked to identify whether the syllable was a dipping tone (/ă/, / ĭ/) or a falling tone (/ à/, /ì/). These audiovisual syllables were presented in clear, noisy and silent conditions. 
An eye-tracker recorded the participants’ eye movement. It was found that the participants gazed more at mouth than eyes and the mouth took more than 40% gaze duration at average. In addition, when the auditory information descended through three acoustic conditions, both Chinese and English speakers increased the gaze duration at mouth rather than eyes. 
The findings suggest, for audiovisual lexical tone, mouth is the primary area, not eyes. The similar eye movement patterns between Chinese and English speakers implies that mouth might play a perceptual cue relevant to articulatory than semantic information.",1.0,3.4
1220,2171,4.15    2.7    3.35    ,Time-Continuous Emotion Recognition Using Spectrogram Based CNN-RNN Modelling,"In area of speech emotion recognition, hand-engineered features are traditionally used as an input. However, it requires an additional step to extract features before the prediction and prior knowledge to select feature set. Thus, recent research has been focused on an approach that predicts emotions directly from speech signal to reduce the required efforts for the feature extraction and increase performance of emotion recognition system. Whereas this approach has been applied for prediction of categorical emotions, the study for prediction of continuous dimensional emotions is still rare. This paper presents a method for time-continuous prediction of emotions from speech using spectrogram. Proposed model comprises convolutional neural network (CNN) and Recurrent Neural Network with Long Short-Term Memory (RNN-LSTM). Hyperparameters of CNN are investigated to improve the performance of the our  model. After finding the optimal hyperparameters, the performance of the system with waveform and spectrogram as input is compared in terms of concordance correlation coefficient (CCC). Proposed method outperforms the end-to-end emotion recognition system based on waveform and provides CCC of 0.7217 predicting arousal on RECOLA database.",1.0,3.4000000000000004
1106,1874,3.5    3.45    3.4    3.3    ,Acoustic and Prosody Adaptation for Emotional Speech Synthesis,"Emotional speech synthesis is very important for conversational TTS in a natural human-computer interaction. Usually, emotion adaptation is used when limited training data are available. This paper focuses on two aspects for emotion adaptation: acoustics and prosody. For acoustic modeling, we extend our previous work on DNN based speaker adaptation to emotion adaptation and try to align it into FF-LSTM based hybrid system. In our new LSTM based emotion adaptation, we set some layers to be emotion dependent and add emotion code for controlling, which is expected to capture the characteristics of target emotion. Other layers are shared between different emotions to describe the common features across emotions. For prosody, we add stress to increase the expressivity of each emotion state. The stress prediction model is adapted to each target emotion. The experimental results show that our methods can improve the overall performance of emotional speech synthesis both objectively and subjectively. The final MOS for happy emotion is improved by about 0.29 after combining all techniques.",1.0,3.4124999999999996
849,1186,3.15    2.8    3.45    4.25    ,Rapid Speaker Adaptation for LSTM Based Acoustic Models,"To reduce the effect of mismatch between training and test data
due to speaker variability, many speaker adaptation techniques
have been proposed for acoustic modeling. In this paper, we
propose a novel approach for speaker adaptation using low footprint
control network (CN) that inserts a frame-wise speaker dependent
bias to 1st layer of the main network. We show that our
approach outperforms learning linear transform based method
when speaker data is limited. For 1.5 minutes of speaker data
from the SpeechOcean dataset, a relative improvement of 7.95%
with CN over the Speaker Independent (SI) model has been obtained
as compared to 4.72% with learning linear transform.
Also, Speaker-aware training (SAT) has been shown to normalize
speaker variability by incorporating speaker information
into the network training process. We argue that speaker adaptation
and SAT complement each other in the way they learn
speaker attributes. Our results show that speaker adaptation
over SAT model trained using multi-task learning with speaker
classification being auxiliary task significantly outperforms the
speaker adaptation on SI model. Using 3 minutes of speaker
data, a relative improvement of 15.9% in word error rate (WER)
is obtained with the proposed cascading scheme.",1.0,3.4124999999999996
958,1493,4.3    3.25    2.65    3.45    ,Dynamic OOV Registration to Language Model for Speech Recognition,"In this study we propose a method of dynamically registering
out-of-vocabulary (OOV) words by editing the pronunciation
of OOV tokens which have been added to an additional, partial copy of our corpus, 
either a) randomly, or b) to part-of-speech (POS) tags in the selected utterances, when training the language model (LM) for speech recognition. 
We also investigate the impact of acoustic complexity and the ``natural'' occurrence frequency of OOV words on registered OOV recognition. The proposed OOV word registration method was evaluated using two modern ASR systems, Julius and Kaldi, using DNN-HMM acoustic models and N-gram language models (plus an additional evaluation using
RNN re-scoring with Kaldi). Results show that with the proposed OOV registration method, modern ASR systems can recognize OOV words without re-training the language model, that the acoustic complexity of OOV words affects OOV recognition, and that differences between the ``natural'' occurrence frequency and assigned frequency of OOV words has little impact on the final recognition results.",1.0,3.4124999999999996
767,81,5.1    4.45    2.55    1.55    ,Intuitive Privacy from Acoustic Reach: a Case for Networked Voice User-Interfaces,"The effect that advances in voice interface technologies have on privacy has not yet received the attention it deserves. Systems in which multiple devices collaborate to provide a unified user-interface amplify those worries about privacy. We discuss ethical implications of voice enabled devices on privacy in typical scenarios at home, office, in a car and in the public. We conclude that people consciously control their speech volume to appropriately scale between different privacy settings. It follows that the reach of voice can be exploited as a feature to intuitively define the extent of privacy. Specifically, exactly those devices which overhear the same signal are eligible to collaborate in processing said signal in a privacy-preserving way. We argue that the proposed approach poses reasonable technological requirements and establishes a natural experience of privacy which confirms intuitive perception. The acoustic reach of speech signals can thus serve as a feature for designing privacy-gentle voice user-interfaces which are intuitive to use.",1.0,3.4125000000000005
109,1128,4.45    3.95    2    3.3    ,Linear Prediction Residual Based Short-term Cepstral Features for Replay Attacks Detection,"Modern automatic speaker verification (ASV) systems are highly vulnerable to spoof attacks, and developing ASV anti-spoofing algorithms to protect ASV systems form these attacks is currently a part of active research. Contrarily to current trends on development of stand-alone spoof detection system, this work aims detection of replay attacks directly on the ASV system. The claim made through replay spoofing trials is rejected as impostors directly by ASV system. The objective here is to model the changes in the excitation signal characteristics caused by playback devices for replay detection. Accordingly, two linear prediction (LP) residual based source features are proposed for rejecting replay spoofing trials namely, RMFCC (residual mel-frequency cepstral coefficients) and LPRHEMFCC (LP residual Hilbert envelope MFCC). A comparative analysis between these two source features has been performed through speaker verification experiments to evaluate their effectiveness for ASV anti-spoofing applications. The comparison between the two has been made in the form of (source feature + MFCC) combination. The experiments are conducted using self-developed IITG-MV replay database. From the experimental results, it has been observed that `LPRHEMFCC+MFCC' combination outperforms `RMFCC+MFCC' combination, under replay attacks. Finally, the experiments are repeated on ASVspoof2017 database to validate the efficacy of proposed work.",0.0,3.425
1000,1596,3.5    1.75    3.6    4.85    ,Computer-aided High Variability Phonetic Training and Its Application to Improve Robustness of Learners’ Listening Comprehension,"When a speaker speaks and a listener listens to that speaker, extra-linguistic 
or environmental factors often degrade the quality of speech. This degradation,
very common in our daily life, is always troublesome to non-native listeners.
To improve learners' robustness in listening, high-variability phonetic training
was introduced, where acoustic variability is manually or technically added to
listening materials. In this paper, advanced speech technologies are used
to prepare new acoustic variability for materials. Listening tests with new variability
are carried out to Japanese learners of English and it is found that they have singular
weakness in listening when radio communication distortion is simulated, although
native listeners can perfectly comprehend those distorted materials. Then, a simple listening
drill for speech samples with different levels of radio communication distortion is prepared
and imposed on those learners. Post-tests show that robustness of listening comprehension is
significantly improved in the case of advanced learners, and that improvement is transferred
to materials with other distortions.",1.0,3.425
1062,1758,3.3    4.2    3.4    2.8    ,A Novel Unified Framework for Speech Enhancement and Bandwidth Extension Based on Jointly Trained Neural Networks,"In this paper, we propose a unified framework for speech enhancement and bandwidth extension. The speech bandwidth extension (BWE) is investigated in noisy environment. Firstly, a Bidirectional Long Short-Term Memory Recurrent Neural Networks (BLSTM-RNN) is trained to map the noisy to clean speech features. Secondly, the BWE is also a BLSTM-RNN model. The feature enhancement neural network serves as a noise normalization module which aimed at explicitly generating the clean features which are easier to BWE by the following neural network. We combined Griffin-Lim algorithm with proposed jointly model to reconstruct wideband speech. To reduce the size of model while maintaining a similar performance, multi-task transfer learning solution is proposed. Experimental results demonstrate that the proposed framework can achieve significant improvements in both objective and subjective measures over the different baseline methods.",1.0,3.425
1327,2594,4.1    2.55    2.75    4.3    ,Production and Perception of English Lexical Stress by Native Speakers of Uyghur,"Previous studies show that language differences cause corresponding influences on second language acquisition. By comparing the production and perception of English stress by native Uyghur speakers with that by native American English speakers, this study examines whether acoustic features, stress positions, and schwa affect stress acquisition, and the relationship between stress production and perception.
Twenty adult subjects (gender balanced) participate in stress production and perception experiments whose stimuli are disyllabic pseudo-words. By analyzing 1,800 recordings from the stress production experiment and 3,600 responses from the perception experiment, this study finds out that: (1) the American group outperforms the Uyghur group in lexical stress production with a higher accuracy rate and wider ranges of acoustic features(F0, intensity, and duration), and in lexical stress perception with a higher accuracy rate and shorter response time; (2) stress positions do affect the realization of lexical stress: by widening the range of acoustic features in initial stress and by narrowing the range in final stress; (3) schwa helps improve accuracy rate of initial stress perception of NSU; (4) a positive correlation between stress perception and production is observed in the Uyghur group.",1.0,3.425
1017,1641,3.5    4.15    2.55    3.5    ,Combining De-noising Auto-encoder and Recurrent Neural Networks in End-to-end Automatic Speech Recognition for Noise Robustness,"In this paper, we propose an end-to-end noise-robust automatic speech recognition system through deep-learning implementation of de-noising auto-encoders and recurrent neural networks. We use batch normalization and a novel design for the front-end de-noising auto-encoder, which mimics a two-stage prediction of a single-frame clean feature vector from multi-frame noisy feature vectors. For the back-end word recognition, we use an end-to-end system based on bidirectional recurrent neural network with long short-term memory cells. The LSTM-BiRNN is trained via connectionist temporal classification criterion. Its performance is compared to a baseline back-end based on hidden Markov models and Gaussian mixture models (HMM-GMM). Our experimental results show that the proposed novel front-end de-noising auto-encoder outperforms the best record we can find for the Aurora 2.0 clean-condition training tasks by an absolute improvement of 1.2% (6.0% vs. 7.2%). In addition, the proposed end-to-end back-end architecture is as good as the traditional HMM-GMM back-end recognizer.",1.0,3.425
819,1118,4.25    2.7    4.35    2.45    ,Feature Combination through a Hybrid CNN/DNN Acoustic Model to Address Tonal Contrasts for a Low-resource Language: YoloxóChitl Mixtec,"This paper focuses on the development of an automatic speech recognition system (ASR) for Yoloxóchitl Mixtec (YM), an Oto-Manguean language spoken by fewer than 5000 speakers in the state of Guerrero, Mexico. Even for a language family characterized by complex tonal inventories, YM has a challenging system with nine basic tones (four level tones, three rising tones, two falling tones) and several other more complex ones. Up to 21 pitch-level contrasts to represent phonemic distinctions have been documented on a single bimoraic/bisyllabic sequence of phones. Apart from phone confusability, a considerable amount of confusion between tone levels exists, leading to a baseline ASR word error rate (WER) of 32%. This paper proposes tone-discriminative features to reduce tone confusions in the ASR system. The tone-discriminative bottleneck features were extracted from a deep neural network with a bottleneck layer trained with gammatone filterbank energies fused with pitch and voicing features as inputs and tone levels as outputs. The tone-discriminative features are fused with speaker-adapted gammatone filterbank features through a hybrid convolutional and deep neural network (CNN-DNN) architecture. The combination of speaker-adapted gammatone filterbank features along with tone-discriminative features provided a relative improvement in WER of 12% over the baseline system.",1.0,3.4375
1217,2164,2.5    5.05    2.7    3.5    ,Glottalization and Diphthongization of Long Vowels in West-Frisian,"This paper presents the results of an acoustic and articulatory study of long vowels in West-Frisian. Although the language is described as having long vowels, the auditory perception of these sounds suggests that they are produced in two phases. Most of the time a modal vowel followed by a creaky vowel [eḛ] or sometimes two vowels separated by a glottal stricture [eʔe] (pressed voiced) quality. These observations are confirmed by EGG data and open quotient measurements. Acoustic measurements (F-pattern) made on the two phases of the long vowels show that they are often realized as diphthongs. In this case they are realized by two vowels of two close vowels qualities ([ɔɑ̰, aɑ̰, eḭ]. Results allow discussing the possible causes of diphthongization in West-Frisian and its socio-phonetic aspects.",1.0,3.4375
1180,2050,2.45    2.75    5.1    3.45    ,Inverse Filtering Based Feature for Analysis of Vowel Nasalization,"Vowel nasalization is present in almost every language and it has significant clinical information as well as speaker information. Researchers have analysed spectral domain characteristics of vowel nasalization and proposed some acoustic parameters for detection of nasalized vowels. In this work we have used an inverse filtering based technique to find out a new feature, which represents the amount of nasalization present in a vowel. The invariability of nasal filter for different nasalized vowels and addition of oral and nasal speech after radiation has been exploited to find out this feature. As the feature gives information about the amount of nasalization, this can be used for detection of vowel nasalization as well as for clinical purposes. Statistical analysis of the feature has been done in this work. The statistical analysis shows that, the feature has good separability for oral vowel and nasalized vowel.",1.0,3.4375
1156,1996,4.2    3.1    3.15    3.3    ,Predicting and Generating Backchannels for a Task-oriented Dialog System Driving a Neuro-psychological Test,"Verbal backchannels performed by conversational agents im-
prove the quality of human-computer interaction. Most of
previous research used statistical models to predict and gen-
erate backchannels from produced and perceived audiovisual
features. This paper presents a backchannel predictor based
on a Long-Short Term Memory (LSTM) recurrent neural net-
work. Training and test data have been collected during inter-
views conducted by a professional neuropsychologist. The pro-
posed system aims at complementing a spoken dialog system
with automatic generation of backchannels during active listen-
ing of the speaker’s interventions: the challenge is to predict
backchannels utterered by the interviewer given parts of speech
uttered by both the subject and the interviewer. F1-measures of
back-channels opportunities are computed to compare the pro-
posed predictor with a baseline model using Conditional Ran-
dom Fields (CRF). Subjective ratings of the effective genera-
tions of verbal backchannels are also performed. The LSTM
model outperforms the state-of-the-art model both in terms of
prediction accuracy, alignment with speech turns and subjective
ratings by third parties.",1.0,3.4375
248,1379,3.9    3.3    4.9    1.65    ,A New Deep Reinforcement Learning Based Coaching Model (DCM) for Slot Filling in Spoken Language Understanding,"In this paper, a deep reinforcement learning(DRL) based multimodal coaching model (DCM) for slot filling in SLU is proposed. This new model functions as a coach to help an RNN based tagger to learn the wrong labeled slots , hence may further improve the performance of an SLU system. Besides, users can also coach the model by correcting its mistakes, and help it progress further. The performance of DCM is evaluated on two datasets: one is the benchmark ATIS corpus dataset, another is our in-house dataset with three different domains. It shows that the new system gives a better performance than the current state-of-the-art results on ATIS by using DCM. Furthermore, we build a demo app to further explain how user's input can also be used as a real-time coach to improve model's performance even more.",0.0,3.4375
1066,1765,3.65    2.6    4.2    3.3    ,Optimizing DPGMM Clustering in Zero Resource Setting Based on Functional Load,"Inspired by the infant language acquisition, unsupervised sub-word discovery of the zero resource language gains attention recently. The problem also serves as the basis of developing a universal speech recognition system for a completely unknown language.  One of the methods to tackle the problem is using an unsupervised clustering algorithm to recover the discrete phone-like units from the speech, such as the Dirichlet Process Gaussian Mixture Model (DPGMM), which currently achieves the top results in the Zero Resource Speech Challenge. However, the DPGMM model is too sensitive to the acoustic variation and often produces too many types of sub-word units. This paper proposes to apply a functional load to reduce the size of sub-word units from DPGMM. The functional load is the measurement of how much information in communication is conveyed by contrasts of these units. Then, the aim is to ignore the contrasts of the sub-word units that contribute little in conveying the information of the speech leading to decrease the number of sub-word classes. We experiment on the official setting of Zerospeech 2015 and measure the performance in terms of the error rate using ABX discrimination test.",1.0,3.4375
979,1540,2.3    2.6    4.9    3.95    ,Multi-dimensional Speaker Information Recognition with Multi-task Neural Network,"This paper proposes a novel approach to simultaneously estimate speaker identity and the other two traits of the speaker, specifically, emotion and gender. This simultaneous process is named as multi-dimensional speaker information recognition. We choose i-vector to represent the utterance with different duration as fixed length vector. In our paper, we first build individual neural networks (NN) for each single recognition task. In this single task, the input is i-vector of one speaker and the output is the corresponding one-hot vector. We then design a multi-task learning (MTL), the difference with the single task is the NN output, which contains three parallel one-hot vectors for MTL. The one-hot vectors denote the identity, emotion and gender information, respectively. The proposed approach is conducted on the KSU-Emotions corpus, which was recorded mainly for emotional recognition. Experimental results show that the best multi-task mechanism recognition accuracy is 5.09% better than the single task.",1.0,3.4375
1034,1681,5.15    2.15    3.3    3.15    ,Projection of the End of Utterance Based on Combinations of Prosodic Features and Phrase-Dependency for Spontaneous Japanese,"This study is aimed at revealing a clue for projecting the end-of-utterance in spontaneous Japanese speech. In casual everyday conversation, participants must predict the ends of utterances of a speaker to perform smooth turn-taking with small gap or overlap. We consider that syntactic and prosodic factors project the end of an utterance of speech and the participants utilize the factors to predict the end-of-utterance. In this paper, we focused on dependency structure among bunsetsu-phrases as a syntactic feature and F0, intensity and mora duration for bunsetsu-phrases as prosodic features. We investigated the relationship between a bunsetsu-phrase position in an utterance and the features. The results showed that the single feature could not be an authoritative clue that determines the position of the bunsetsu-phrases. Next, we constructed a Bayesian hierarchical model to estimate the bunsetsu-phrase position from the syntactic and prosodic features. From results of the model, prosodic features vary in usefulness according to the speakers. These suggest that the different combinations of the syntactic and prosodic features for each speaker are relevant to project the ends of the utterances.",1.0,3.4375000000000004
984,1560,3.35    3.55    4.2    2.65    ,Novel Metric Learning for Alignment Task in Voice Conversion,"The significance of an accurate alignment on a quality of converted voice via Voice Conversion (VC) is not much explored. Unsupervised alignment algorithm, namely, an Iterative combination of a Nearest Neighbor search step and a Conversion step Alignment (INCA) iteratively tries to align the spectral features by minimizing the Euclidean distance metric between the intermediate converted and the target spectral feature vectors. However, the Euclidean distance may not correlate well with the perceptual distance between the two (sound or visual) patterns in a feature space. In this paper, we propose to learn distance metric using Large Margin Nearest Neighbor (LMNN) technique that gives a minimum distance for the same phoneme uttered by the different speakers and more distance for the different set of phonemes. This learned metric is then used for finding the NN pairs in the INCA. Furthermore, we propose to use this learned metric only for the first iteration in the INCA, since the intermediate converted features (which are not the actual acoustic features) may not behave well w.r.t. the learned metric. We obtained on an average 1.93 \% absolute increment in \% Phonetic Accuracy (PA). This is positively reflected in the analysis of subjective and objective evaluations.",1.0,3.4375000000000004
199,1294,3.3    5    4.2    1.4    3.3    ,Speech Enhancement Using the Minimum-probability-of-error Criterion,"We propose a novel speech denoising framework by minimizing
the probability of error (PE), which measures the deviation
probability of the estimate from its true value. To develop the
minimum PE (MPE) criterion, one requires the knowledge of
the noise probability density function (p.d.f.), which may not
be available in a parametric form in speech denoising applications.
Therefore, we adopt two approaches for modeling the
noise p.d.f.: (i) Gaussian modeling based on adaptive variance
estimation; and (ii) a Gaussian mixture model (GMM) in view
of its approximation capabilities. We consider discrete cosine
transform (DCT) domain shrinkage, where the optimum shrinkage
parameter is obtained by minimizing an estimate of the PE.
A performance assessment for real-world noise types shows that
for input signal-to-noise ratios (SNR) greater than 5 dB, the
proposed MPE-based point-wise shrinkage estimators outperform
three benchmark techniques in terms of segmental SNR
and short-time objective intelligibility (STOI) scores.",0.0,3.44
993,1585,2.3    2.8    4.55    4.15    ,The Optimal Level for Extracting a Discriminating Acoustic Features to Improve Speech Emotion Recognition,"Performance of Speech Emotion Recognition (SER) system depends on whether the extracted features are relevant to emotions conveyed in speech or not. Extracting a discriminative acoustic features form speech is still a challenging problem. Traditionally, the most common approach for feature extraction has been to extract a large number of features from all frames of utterance. Then applying several statistics for all frames’ features to obtain utterance-level features. However, emotions have different effects on the properties of each phoneme. Consequently, it has different effect for voiced and unvoiced segments. Therefore, utterance-level features are less effective, due to applying statistics for all segments. To keep the discriminative properties of the extracted features, this study proposes a voiced and unvoiced levels for feature extraction. The performance using features extracted from voiced and unvoiced levels are compared with the utterance-level. For the three levels, 13 MFCC coefficients were extracted for each frame and then 13 statistics were applied for each level separately. It is found that voiced-level have many discriminative features furthermore, its performance is close to utterance-level. To improve the traditional SER system, features extracted from both voiced-level and utterance-level were combined. The results of the combined level outperform the traditional one.",1.0,3.4499999999999997
1209,2143,4.2    2.7    3.55    3.35    ,Conditional Joint Model for Spoken Dialogue System,"Spoken Language Understanding (SLU) and Dialogue Management (DM) are two core components of a spoken dialogue system. Traditional methods model SLU and DM separately. Recently, joint learning has made much progress in dialogue system research via taking full advantage of all supervised signals. In this paper, we propose an extension of joint model to a conditional setting. Our model does not only share knowledge between intent and slot, but also efficiently make use of intent as a condition to predict system action. We conduct experiments on popular benchmark DSTC4, which includes rich dialogues derived from real world. The results show that our model gives excellent performance and outperforms other popular methods significantly, including independent learning methods and joint models. This paper gives a new way for spoken dialogue system research.",1.0,3.4499999999999997
1254,2283,4.35    2.75    3.35    3.35    ,Dual-label Deep LSTM Dereverberation for Speaker Verification,"In this paper, we present a reverberation removal approach for speaker verification, utilizing dual-label deep neural networks (DNNs). The networks perform feature mapping between the spectral features of reverberant and clean speech. Long short term memory recurrent neural networks (LSTMs) are trained to map corrupted Mel filterbank (MFB) features to two sets of labels: i) the clean MFB features, and ii) either estimated pitch tracks or the fast Fourier transform (FFT) spectrogram of clean speech. The performance of reverberation removal is evaluated by equal error rates (EERs) of speaker verification experiments.",1.0,3.4499999999999997
977,1534,3.4    4.15    2.75    3.5    ,Rapid Adaptation of Neural Network Based Filterbank Layer for Automatic Speech Recognition,"Deep neural networks (DNN) have achieved significant success in the field of automatic speech recognition. Previously, we proposed a filterbank-incorporated DNN which takes power spectra as input features. This method has a function of VTLN (Vocal tract length normalization) and fMLLR (feature-space maximum likelihood linear regression). The filterbank layer can be implemented by using a small number of parameters and is optimized under a framework of backpropagation. Therefore, it is advantageous in adaptation under limited available data. In this paper, speaker adaptation is applied to the filterbank-incorporated DNN. By applying speaker adaptation using 15 utterances, the adapted model gave a 7.4% relative improvement in WER over the baseline DNN at a significance level of 0.005 on CSJ task. Adaptation of filterbank layer also showed better performance than the other adaptation methods; singular value decomposition (SVD) based adaptation and learning hidden unit contributions (LHUC).",1.0,3.45
955,1483,4.35    3.15    3.55    4.95    1.25    ,An Exploration of Directly Using Word as Acoustic Modeling Unit for English Conversational Speech Recognition,"Conventional acoustic models for automatic speech recognition (ASR) are usually constructed from sub-word unit (e.g., context-dependent phoneme, grapheme, wordpiece etc.). Recent studies demonstrate that connectionist temporal classification (CTC) based acoustic-to-word (A2W) models are also promising for ASR. Such structures have drawn increasing attention as they can directly target words as output units, which simplify ASR pipeline by avoiding additional pronunciation lexicon, or even language model. In this study, we systematically explore word as acoustic modeling unit for speech recognition. By replacing senone alignment with word alignment in a convolutional bidirectional LSTM architecture and employing a lexicon-free weighted finite-state transducer (WFST) based decoding, we greatly simplify conventional hybrid speech recognition system. On Hub5-2000 Switchboard/CallHome test sets with 300-hour training data, we achieve a WER that is close to the senone based hybrid systems with a WFST based decoding.",1.0,3.45
1289,2428,2.55    2.7    4.4    4.15    ,An Acoustical Analysis of the Stops in Suzhou Wu Produced by Young and Elderly Speakers,"From a representational viewpoint, stressed word-initial stops of Wu can be perceived as “voiced” when signaled by a low tone with some voiced breathiness following. However, intervocalic stops are voiced. We applied a series of spectral and noise measures to understand the phonetic voicing and the breathy voice in Suzhou Wu.
Word-initial stops were likely to lose phonetic voicing with limited prevoiced stops produced in our data. Moreover, H1-A1 and HNR showed a strong correlation between breathy voice and low tone in the onset of the following vowel which lost its salience at the midpoint. However, H1-H2 didn’t report this correlation. The heterogeneous results suggested that it would be more appropriate to use the term “breathy voice” to describe the degree of slight aspiration rather than a phonation type.
Intervocalic stops followed by the vowels with (inherent low) neutralized tone had a higher voicing-ratio and more breathy voice comparing to their counterparts with (inherent high) neutralized tone. However, breathy voice didn’t vary in vowels. 
Cross-generation analysis suggested a more breathy voice for young speakers according to H1-A1 in contrary to HNR. Overall, no correlation between breathy voice and generation was confirmed.
Index Terms: Suzhou Wu, voicing, tone, breathy voice, cross-generation.",1.0,3.45
1018,1642,3.35    4.35    3.5    2.65    ,AISHELL-2: Transforming Mandarin ASR Research into Industrial Scale,"AISHELL-1 is by far the largest open-source speech corpus
available for Mandarin speech recognition research. It was released
with a baseline system containing training and testing
pipelines for Mandarin ASR. In AISHELL-2, 1000 hours of
clean read-speech data is published, which is free for academic
usage. On top of AISHELL-2 corpus, an improved recipe is
developed and released, containing key components for industrial
applications, such as Chinese word segmentation, robust
data augmentation, domain-specific fine-tuning towards acoustic
model and language model, flexible vocabulary expension
and phone set transformation, etc. Pipelines supporting various
state-of-the-art techniques are provided, such as recurrent neural
network acoustic models. Moreover, a real-time ASR demo
is released, to help AISHELL-2 users evaluate and experience
target ASR system via their own laptop microphone. For research
community, we hope that AISHELL-2 corpus can be a
solid resource for topics like transfer learning and robust ASR.
For industry, we hope AISHELL-2 recipe can be a helpful reference
for building meaningful industrial systems and products.",1.0,3.4625
795,1041,3.45    3.9    3.15    3.35    ,Spoken English Fluency Scoring Using Multivariate Fractional Polynomial Linear Regression,"In this paper, we propose a spoken English fluency scoring using multivariate fractional polynomial linear regression. Automatic spoken English fluency scoring is a general regression problem. The model parameters are trained to map input features to corresponding ground truth scores, and then used to predict a score for an input feature. 
Although DNN-based methods are known to outperform other machine learning methods, straightforward method such as linear regression is the most popular for the fluency scoring model. This is because interpretability of scoring model is crucial to provide feedback to improve speaking skill. However, it is difficult to analyse why such score results came out when using DNN based method.

Therefore, we propose a novel fluency scoring model using multivariate fractional polynomial linear regression to improve the prediction performance while maintaining the interpretability. The proposed approach is derived by approximating DNN using Taylor series so that fluency score is predicted by linear combination of multivariate fractional polynomial basis.

The effectiveness of the proposed approach was evaluated using in-house Korean-Spoken English Corpus.",1.0,3.4625
1095,1844,3.95    2.65    3.15    4.1    ,An ERP Study to Evaluate the Quality of Speech Processed by Wiener Filtering,"Speech enhancement algorithms (SEAs) play a crucial role in speech signal processing. Wiener filtering (WF) as one of the most popular SEAs introduces undesired speech distortion to the enhanced signals. It is important to objectively examine the perception of the enhanced speech in different conditions. The purpose of this work was to investigate whether the mismatch negativity (MMN) response based on the event-related potential (ERP) could objectively evaluate the quality of speech processed by WF. There were two deviant stimulus types: the disturbed stimuli corrupted by a steady-state noise at signal-to-noise ratio (SNR) of -5 and +5 dB and their corresponding enhanced stimuli processed by WF. Experimental results showed that the enhanced stimuli at -5 dB SNR evoked a shorter MMN latency than that at +5 dB SNR, accounting for the effect of SNR level on the quality of WF-processed speech.  The results also demonstrated that the MMN latencies elicited by the enhanced stimuli were shorter than those elicited by the disturbed stimuli, reflecting the effect of WF processing on speech quality. The MMN may be potentially used as an objective measure for evaluating the quality of speech processed by WF.
Index Terms: Wiener filtering, speech quality, the mismatch negativity",1.0,3.4625
1212,2153,4.4    2.25    2.95    4.25    ,An Objective Measure to Assess Musical Noise Using Connected Time-Frequency Regions,"In this work, we propose an objective measure to assess the amount of musical noise that results from speech enhancement algorithms. The algorithms can result in non-smooth suppression of background noise which in turn translates to isolated regions of high energy, referred to as musical noise. We propose to identify such regions by combining time-frequency (TF) bins associated through connectivity along with additional properties of these regions such as area, aspect ratio and total energy. The objective measure proposed is based on an average count of such regions. The effectiveness of the proposed measure is studied by correlating it with subjective assessment of listeners using enhanced speech of various algorithms.",1.0,3.4625000000000004
940,1437,3.5    3.9    2.9    3.55    ,Deep Loss: a New Deep Neural Network Based Loss Criterion for Denoising Autoencoder Training with Application to Robust Automatic Speech Recognition,"A new deep neural network based loss criterion for denoising autoencoder (DAE) front-end training is proposed for noise robust acoustic speech recognition (ASR) applications. Traditionally, the mean-square-error (MSE) criterion is used to train DAE in the original feature space where clean features and denoised features belong to. Due to lack of the back-end acoustic model (AM) information, the denoised features may be not optimal in the whole ASR systems. Therefore the well-trained deep neural network (DNN) AM is proposed to design deep loss networks, which transform these features to some space containing more discriminative information for better recognition. There are two specific loss designs: (1) consider the whole AM as the loss module, i.e., to optimize the distance between clean and denoised features in senone space, (2) consider some lower layers of the acoustic model as the loss module, which is in internal feature representation space. Experiments with CHiME-3 single channel ASR task show that DAE training with hidden loss and output loss can reduce WER relatively by about $15\%$ and $18\%$, respectively, compared to MSE loss based results. Hidden loss results is a little worse than output loss ones, with the benefit of reducing one third training time.",1.0,3.4625000000000004
1081,1801,2.7    4.4    4.3    2.45    ,Automatic Speaker Height and Age Estimation from Short Duration Speech,"Many forensic applications of speech demand the extraction of information about the speaker characteristics from as little speech data as possible. In this work, 
we explore the estimation of speaker's height and age from short duration ($1-3$ seconds) of speech. We explore three different feature streams for age and height estimation derived from the speech spectrum at different resolutions namely - short term log-mel spectrogram, pitch and formant features and the harmonic frequency features of the speech. The statistics of these features over the speech recording is used in conjunction with a support vector regression model for speaker height/age estimation. The experiments are performed on the TIMIT dataset where we show that each of the individual feature is able to achieve results that are better than previously published results. Furthermore, the estimation errors from these systems are complimentary which allows the system combination to further improve the results. The combined system achieves a performance of 5.1 cm mean absolute error for height estimation and 5.7 years for age estimation from short audio snippets.",1.0,3.4625000000000004
751,36,4.8    3.2    3.3    2.55    ,Mandarin Tone Identification with F0-Flattening Processed Vowels,"Fundamental frequency (F0) contour carries important information for lexical
tone identification in a tonal language like Mandarin Chinese. To examine the
perceptual contribution of F0 contour, F0-flattening processing has been
commonly used in many studies. However, as temporal envelope co-varies with F0
contour, listeners may still capture the F0-contour information from amplitude
fluctuation. The present work assessed Mandarin tone identification with
F0-flattening processed vowels. The vowel stimuli were processed by the
STRAIGHT algorithm to replace the original F0 contour with a flattened contour.
Listening experiments showed that under the F0-flattening processed condition,
normal-hearing listeners still achieved a more than 50.0% accuracy rate to
identify Mandarin tones, and tone-2 was more misidentified as tone-1 than
tone-3 and tone-4 were. The present work provided evidence that the
F0-flattening processing could not completely remove the F0 contour cue, which
was also preserved in amplitude fluctuation, for Mandarin tone identification.",1.0,3.4625000000000004
1202,2134,4.25    2.2    3.95    ,Evaluating Neural Sentence Planning for Dialogue Generation,"Natural language generation (NLG) is a key component of conversational
agents.  Traditional NLG architectures have an explicit sentence
planning phase, but recent neural NLG models instead aim to jointly
learn sentence planning and surface realization from parallel data
that implicitly provides the mapping from dialogue act meaning
representations to surface forms.  To date, however, previous work has
not systematically tested whether complex sentence planning phenomenon
such as aggregation and discourse structuring can be generated with a
neural model, and whether the learned models can generalise beyond
specific examples and attribute values that were seen in the training
data.  Here we systematically create a large training corpus with
variations in complex sentence planning operations. We then compare a
neural model without explicit latent variables for sentence planning
to one that provides explicit structural sentence planning supervision
during training, and evaluate what the models learn. We show that an
architecture that provides additional sentence planning supervision
allows the model to faithfully reproduce sentence planning operations
and generalise to situations unseen in training.",1.0,3.466666666666667
891,1307,2.8    2.85    4.85    3.4    ,Investigating RNN Based Bottleneck Features with CTC Loss for Improved Speaker Recognition,"This study investigates the use of recurrent neural network (RNN) based bottleneck features for Speaker Identification(SID). We employ a Long Short-Term Memory (LSTM) network for ASR in an end-to-end fashion using the Connectionist Temporal Classification (CTC) objective function. Bottleneck features are extracted using the LSTM network and then used to train a UBM to obtain frame posteriors for training an i-Vector extractor matrix. To minimize the loss of speaker specific information, a hybrid approach is adopted to train the i-Vector extractor matrix using MFCC features with the frame-posteriors extracted from bottleneck features. Our proposed approach is shown to outperform a standard MFCC based gender-independent i-Vector PLDA SID system on the NIST SRE 2010 C5 trials. Further, fusing the scores of the LSTM bottleneck features based hybrid SID system, and the MFCC based SID
system reduces the EER by +22.19% and +14.98% for the gender-independent, and gender-dependent PLDA back-ends relative to the baseline MFCC SID system, demonstrating the
complementary nature of the proposed approach.",1.0,3.475
946,1461,2.75    4.3    4.2    2.7    ,Emotion Recognition Using Intrasegmental Features of Continuous Speech,"This paper proposed a new emotion recognition system using intrasegmental features, extracted from long vowels in continuous speech. 36 vocal tract features and 11 glottal source features were investigated and an optimal subset was selected using Maximum Relevance Minimal Redundancy Backward Wrapping (MRMRBW). Five different classifiers were considered. The feed-forward neural network (FFNN) and support vector machines (SVM) were found to give the best emotion recognition performance. The PAVOQUE and Berlin emotional speech corpora along with a newly constructed JL corpus were used to evaluate the emotion classifiers. By using the optimal classifier, we achieved recognition accuracies of 85.6%, 66.1% and 70.5% independent of speakers and vowel types for different emotion sets of PAVOQUE, Berlin and JL corpora respectively. The significance of these results is also discussed in the paper.",1.0,3.4875
998,1594,3.45    4.85    3.3    2.35    ,Stacked SDC Features for Language Identification,"Language identification(LID) systems, which can model high-level information such as phones, phone frequencies, and phonotactics have exhibited superior performance. State-of-the-art models use sequential models to capture the high-level information, but the utterance level models are sensitive to the length of the utterance and do not equally generalize over variable length utterances. To capture this information, a feature
that can model the long-term temporal context is required. This study aims to capture the long-term temporal context by appending successive shifted delta cepstral(SDC) features. Deep neural networks have been explored for developing LID systems. Experiments have been performed using  AP17-OLR database. LID systems developed by stacking 9 successive SDC features (±4 right and left context) have shown significant improvement compared to the system trained with SDC features. The proposed feature has reduced the equal error rate(EER) from 18.24, 10.56, 4.83 to 15.34, 8.56, 2.60 on the 1-second, 3-seconds and > 3-second test utterances respectively. Use of residual connections in the feed-forward networks has further reduced the corresponding EERs to 14.45, 7.86 and 2.44. The LID system trained using the proposed feature has performed better than the state-of-the-art approaches for > 3-second utterances and comparable performance for 1-second utterances",1.0,3.4875000000000003
827,1141,2.7    4.25    2.55    4.5    ,Cold-Start End-to-End Spoken Language Interface Development for Question Answering,"Question answering (QA) has become a key capability for voice enabled personal assistants to automatically answer various user questions. However, the development of a spoken language interface for QA in a new domain is time consuming and requires a lot of human labors. In this paper, we design an end-to-end system, SliQA, that facilitates developers to easily and quickly build a QA interface with cold start. Over existing knowledge graphs, SliQA outputs a high quality plug-and-play QA engine to support answering factoid questions regarding an entity. SliQA is developed by incorporating a novel iterative humanin- the-loop question generator and a deep learning based QA engine, thereby requiring light human workload. We evaluate SliQA system on three domains and the results show that our approach outperforms the baseline in terms of different aspects, including the quality of generated data, QA engine performance and human workload efficiency.",1.0,3.5
970,1513,3.55    2.55    2.9    5    ,A Discourse-Prosody Interface Study of Chinese Reading Texts within the Framework of Centering Theory,"Discourse-prosody interface studies have been increasingly regarded as an important research area, due to its theoretical significance and application value in linguistics and speech engineering. As a discourse theory on coherence and salience, Centering Theory has been widely applied in discourse analysis and natural language acquisition. The present research aims to put forward a representation system of Chinese reading texts based on Centering Theory, and investigate the influence of discourse information on prosodic features from the perspectives of pause duration and stress degree. Phonetic and statistical analyses are conducted on reading texts selected from the Annotated Speech Corpus of Chinese Discourse (ASCCD). The results show that: both transition states and center’s information status exhibit a significant influence the pause durations between clauses; center’s part of speech and syntactic position exert an impact on stress degrees; but the realization type of centers has no impact on pause durations, which can be considered as an evidence of indirect and direct centers’ homogeneity. These findings well reflect the explanatory power of Centering Theory on prosodic features, and are of a theoretical significance in discourse-prosody interface studies.",1.0,3.5
1134,1946,4.25    3.35    3.25    3.15    ,Convolutional Neural Network with Raw and Heuristic Spectrographic Features for Speech Emotion Recognition,"Conventional speech emotion recognition has a wealth of achievements using heuristic features that refer to acoustic features of speech. In recent years, the convolutional neural network (CNN) has exhibited a great power at mining deep information from raw spectrograms, but the knowledge based on heuristic features was not utilized sufficiently as did in the traditional method. To address this problem, we propose a novel feature fusion strategy to utilize comprehensive spectrographic information and the prior knowledge simultaneously. Firstly, heuristic features are rearranged as heuristic spectrographic features for effective learning by CNN. Next, we fuse raw spectrographic features and heuristic spectrographic features as compositional spectrographic features. Since compositional spectrographic features lack global and dynamic information, statistical features are added to generate rich-compositional spectrographic features. Then, the fused features in the form of two-dimensional (2-D) images are fed to CNN to extract hierarchical features. Finally, the bi-directional long-short term memory (BLSTM) is employed to utilize the context information and recognize emotions. Compared with hierarchical raw spectrographic features, our results show that hierarchical compositional and rich-compositional spectrographic features improve the unweighted accuracy by a relative error reduction of 32.04% and 36.91%, respectively.",1.0,3.5
987,1571,4.2    2.85    4.15    2.8    ,A Database and Initial Results for Indian Folk Songs Classification,"India is famous for its culture of traditional folk music. Folk music represents identity of different Indian regions and festival celebrations of people in those regions. Many popular Bollywood songs are based on folk music. There is a wide scope to apply machine intelligence in this domain. However, to the best of our knowledge, there has been no availability of any audio dataset which represents Indian folk songs from various regions. In this article, we introduce a novel audio dataset of 299 folk songs from five major regions of India, namely, Assamese, Marathi, Kashmiri, Kannada and Uttarakhandi. We further
demonstrate a baseline performance on this dataset using short-term spectral features, and compare the performance over a convolutional neural network(CNN) trained on mel-spectrogram
based audio representations for classifying a given folk song into one of these five regions. Results indicate that CNN based end-to-end classifier outperforms traditional classification performance by achieving around 61% accuracy.",1.0,3.5
1248,2265,4.15    4.3    2.05    3.55    ,Siamese LSTM Based Architecture for Cry Recognition in Early Development,"In this paper, we present a Deep Learning based approach for cry recognition in early development as a part of Interspeech, ComParE 2018 Crying sub-challenge. Our proposed architecture involves extracting features from input speech signal and training a discriminative model using a time distributed siamese long short term memory(LSTM) network as opposed to the baseline architecture which models a generative network for this supervised emotion recognition problem. We make use of pitch, Mel-frequency cepstral coefficients(MFCC), energy and spectral information embedded in the signal as features. These features are then fed to time distributed Siamese LSTM which models the discriminating features between the classes of baby being neutral, fuzzing and crying and provides us with fixed dimensional discriminative features. Using siamese also mitigates the limited dataset problem until some extent. These extracted discriminating features are further fed to a Neural Network(NN) for the final emotion recognition.",1.0,3.5125
1103,1865,4.25    3.6    4.65    1.55    ,"Data-pooling and Multi-task Learning Methods to Improve the Performance of Speech Recognition Systems for Gujarati, Tamil and Telugu","We present two methods to improve the performance of automatic speech recognition (ASR) systems for Gujarati, Tamil and Telugu. In the first method of data-pooling with phone mapping, a deep neural network (DNN) is trained to predict the senones for the target language; then we use the feature vectors and their alignments from source languages to map the phones from the source to the target language. The lexicons of the source languages are then modified using this phone mapping and an ASR system for the target language is trained using both the target and the modified source data. This method gives relative improvements in word error rates of 5.09% (Gujarati), 3.14% (Tamil) and 3.39% (Telugu) over the baseline figures. In the second method of multi-task DNN modeling, we use feature vectors from all the languages and train a DNN with three output layers, each predicting the senones of one of the languages. Objective functions of the output layers are modified such that during training, only those DNN layers responsible for predicting the senones of a language are updated, if the feature vector belongs to that language. This achieves relative improvements of 5.72% (Gujarati), 3.32% (Tamil) and 5.15% (Telugu).",1.0,3.5125
974,1530,1.4    4.3    4.85    ,Effects of Stimuli Duration and Vowel Quality in Tone Perception by English Musicians and Non-musicians,"The link between music and language has been a subject of great interest, and evidence suggesting a connection between musical abilities and prosodic processing skills in language is growing. Acoustic fundamental frequency (F0), perceived as pitch, differentiates notes in music and word meaning in lexical tone languages. This study examines categorical perception of pitch stimuli among 14 English musicians and 15 English non-musicians, both groups having no exposure to tonal languages. The stimuli consist of continua of falling and rising F0 contours produced on high and low vowels with 9 different durations. The results revealed that musicians were more sensitive to variation in stimulus duration than non-musicians were, and music experience enhanced the sharpness of category boundaries. Significant main effects of vowel quality and pitch directions as well as two-way interactions between vowel and pitch direction, vowel and duration, group and duration, and pitch direction and duration on identification rate were also found. Formulae for minimum duration required for English musicians and non-English musicians to perceive rising and falling F0 were derived, revealing that musicians require less time to perceive a pitch fall and rise if the change is less than 12semitones.",1.0,3.516666666666666
1319,2570,2.15    4.25    4.15    ,A Dynamic Multi-band Filter Structure for Speech Intelligibility Improvement and Its Language Dependency,"Intelligibility and quality are the significant perceptual attributes of speech that need to be improved by speech enhancement algorithms. Here, intelligibility refers to the understandability of speech and it depends on the underlying sound units in the speech. As the distribution of sound units is different in different languages, there may exist a language influence on intelligibility. Analysis on various Indian languages, such as Tamil, Telugu, Malayalam, Hindi, and Indian English, with different phonetic distribution, shows that a particular phone class occurs most frequently (dominant) for a particular language. Frequency of occurrence of phonemes belonging to these dominant classes is expected to be high in a given sentence and in turn the noise power of the respective sub-band in a multi-band filter structure. On this basis, a dynamic multi-band filter structure, varying over every sentence, is proposed that splits these sub-bands with highest noise power further based on perceptual characteristics for reduced speech distortion and improved intelligibility. The efficiency of the proposed dynamic filter structure is analyzed in terms of intelligibility for all the above mentioned Indian languages using Coherence Speech Intelligibility Index (CSII) measures and is found to improve the intelligibility with effective signal restoration.",1.0,3.516666666666667
335,1555,2.7    3.75    4.1    ,Effect of TTS Generated Audio on OOV Detection and Word Error Rate in ASR for Low-resource Language,"Out-of-Vocabulary (OOV) detection and recovery is an important aspect of reducing Word Error Rate (WER) in Automatic Speech Recognition (ASR). In this paper, we evaluate the
effect on WER for a low-resource language ASR system using OOV detection and recovery. We use a small seed corpus of continuous speech and improve the vocabulary by incorporating the detected OOV words. We use a syllable-model to detect and learn OOV words and, augment the word-model with these words leading to improved recognition. Our research investigates the effect on OOV detection and recovery after adding missing syllable sounds in the syllable model using a Text-to-Speech (TTS) system. Our experiments are conducted using 5 hours of continuous speech Kannada corpus. We use an already available Festival TTS for Hindi to generate Kannada speech. Our initial experiments report an improvement in OOV detection due to addition of missing syllable sounds using a cross-lingual TTS system.",0.0,3.516666666666667
1120,1901,3.5    4.1    2.95    ,Estimation of the WER without Decoding Based on Noise Analysis,"Many sources of variability can degrade the performance of Automatic Speech Recognition (ASR) system. In this study, the degradations caused by the type and level of noise are explored in order to predict the a priori quality of an ASR, i.e. even before decoding. Our method is based on a separation of speech and noise. Statistics on sub-band spectral features are extracted on speech and on noise. Then, a regression model is used to predict the Word Error Rate (WER). The experiment was carried out on the Wall Street Journal corpus, noised with the NOISEX-92 corpus (17 types of noise) that we apply at 9 levels of Signal-to-Noise Ratio (SNR). The proposed regression method obtains less than 8% of the mean error between the predicted WER and the real WER obtained by ASR system. The statistical feature extracted by our method, compared to classical sub-band SNR extraction, obtained a relative amelioration of 20% of mean error prediction. Thereby, this method can be used to select relevant audio material before decoding and to select the most suitable ASR system, i.e. the one that achieves the best transcription quality.",1.0,3.516666666666667
179,1258,4.15    2.4    4    ,Nebula: F0 Estimation and Voicing Detection by Modeling the Statistical Properties of Feature Extractors,"A F0 and voicing status estimation algorithm for high quality speech analysis/synthesis is proposed. This problem is approached from a different perspective that models the behavior of feature extractors under noise, instead of directly modeling speech signals. Under time-frequency locality assumptions, the joint distribution of extracted features and target F0 can be characterized by training a bank of Gaussian mixture models (GMM) on artificial data generated from Monte-Carlo simulations. The trained GMMs can then be used to generate a set of conditional distributions on the predicted F0, which are then combined and post-processed by Viterbi algorithm to give a final F0 trajectory. Evaluation on CSTR and CMU Arctic speech databases shows that the proposed method, trained on fully synthetic data, achieves lower gross error rates than state-of-the-art methods.",0.0,3.516666666666667
966,1507,3.6    3.55    3.45    3.5    ,Exploring Speech Separation as a Countermeasure for Voice Conversion Spoofing Attack,"Voice conversion is a high-risk spoofing technique for automatic speaker verification systems where the identity of imposter speaker’s speech is transformed to that of genuine speaker’s without altering linguistic content. This work aims to explore speech separation as a countermeasure for voice conversion spoofing. The spoofed speech signal is modeled as a linear mixture of source and target speech. A countermeasure is proposed by combining an unsupervised co-channel speech separation framework based on non-negative matrix factorization with convolutional neural network (CNN) based binary classifier. Binary classifier is trained using Mel-spectrograms of separated target and spoofed speech resultant from speech separation. Further, CNN based automatic speaker verification system is built and tested for Voice Conversion Challenge 2016 dataset
and significant reduction in false alarm rate is observed by incorporating the proposed countermeasure.
Index Terms: voice conversion, speech separation, automatic
speaker verification, convolutional neural network.",1.0,3.5250000000000004
1111,1882,4.85    3.95    1.8    ,PLDA Score - Is It Really Suited for Forensic SID Applications?,"For several years probabilistic linear discriminant analysis (PLDA) scoring has been widely used in state of the art speaker identification (SID) systems. These systems usually extract a fixed length vector, denoted i-vector, in order to represent a variable length speech sequence, using two models, a universal-background-model (UBM) and a total-variability (TV) matrix. PLDA is mainly seen as a scoring process, even if it also allows to take into account slight channel differences. PLDA scoring is seen as a similarity measure and it is well admitted that PLDA is not a metric. In this paper we highlight and explore some other limitations of PLDA-based scoring, less obvious than the previous one. Particularly, we demonstrate that the PLDA-based similarity between a given vector and itself is not necessarily the largest one. This result decreases significantly the interest of the use of PLDA as a similarity measure, particularly for forensic applications: It becomes possible to find an i-vector extracted from some speaker 'Y' with a higher PLDA score than the PLDA-similarity computed between a speech extract and itself. After demonstrating this claim, we give some examples and we analyze both the source of this anomaly and some of its implications.",1.0,3.5333333333333337
1154,1994,2.7    4.35    2.6    4.5    ,Unsupervised Acoustic Sub-word Unit Modeling for Indian Language ASR,"The popular approach to multi-lingual low resource  speech recognition is the use of common/shared acoustic model which has a language specific target layer. The model is either used directly or used for bottleneck feature representations from this model. However, all these approaches assume the presence of labeled data from multiple languages to train the multi-lingual acoustic model. In this paper, we explore a parallel approach for unsupervised multilingual acoustic modeling where an attempt is made for bottleneck feature extraction from raw acoustic data without labels. The unsupervised modeling uses a clustering algorithm based on hidden Markov model (HMM). The mapping from the data to the cluster labels is done with deep neural network that is later used for bottleneck feature extraction. We perform speech recognition experiments using a combination of the unsupervised multilingual bottleneck features  with conventional log-mel features. In the low resource Indian language ASR experiments using the MSR 2018 Challenge dataset, the proposed approach provides promising results.",1.0,3.5375
1215,2160,3.2    2.7    3.95    4.3    ,Conditional Encoder-decoder for Goal-oriented Dialogue,"The most useful applications of dialog systems (such as digital personal assistants) are currently goal-oriented. Recently with the development of deep learning, the dialogue systems built on sequence-to-sequence framework have achieved encouraging success in chit-chat dialogue. However, it is hard to carry over this success to goal-oriented settings, which require knowledge that goes beyond sequence-to-sequence model. To address the problem, in this paper, we propose a novel conditional encoder-decoder model for goal-oriented dialog. We apply an extension of encoder-decoder to a conditional setting to complete a specific task with a clear goal. We conduct experiments on both English and Chinese dataset including several application scenarios. Empirical results show that our method can effectively deal with goal-oriented dialogue, and outperforms traditional encoder-decoder model by a large margin in terms of several popular metrics. This paper gives a new way for sequence-to-sequence framework to tackle with goal-oriented dialogue.",1.0,3.5375000000000005
888,1300,3.15    3.3    4.2    ,Listening to Laryngectomees: a Study of Intelligibility and Self-reported Listening Effort of Spanish Oesophageal Speech,"Oesophageal speakers face a multitude of challenges, such as difficulty in basic everyday communication and inability to interact with digital voice assistants. We aim to quantify the difficulty involved in understanding oesophageal speech by measuring intelligibility and listening effort. We conducted a web-based listening test to collect these metrics. Participants were asked to transcribe and then rate the sentences for listening effort on a 5-point Likert scale. Intelligibility, calculated as Word Error Rate (WER), showed significant correlation with user rated effort. Speaker type (healthy or oesophageal) had a major effect on intelligibility and effort, whereas listener age showed no effect. Additionally, we calculated speaker-wise mean WERs and they were significantly lower when compared to an automatic speech recognition system.",1.0,3.5499999999999994
756,53,2.35    3.5    5.05    3.3    ,Effect of Pitch-Adaptive Spectral Estimation on Automatic Speech Recognition,"Front-end speech parameterization plays a vital role in automatic speech recognition (ASR). The primary objective is to derive a set features that compactly represent the relevant spectral information and discard the redundancies. In the context of speaker-independent ASR systems, it is desirable that the employed acoustic features exhibit lesser sensitivity to the pitch of the speech signals. Our earlier studies have shown that the conventional Mel-frequency cepstral coefficients features get affected by gross variation in the pitch of signals. This is attributed to insufficient smoothing of pitch harmonics, in particular, for higher-pitched speech signals. In this paper, we explore a pitch-adaptive spectral estimation approach, referred to as TANDEM STRAIGHT, in deriving the front-end features. The experimental evaluations reported in this work are performed on speech data collected from a wide range of speakers belonging to different age groups including children. The proposed features are found to be effective for all groups of speakers.",1.0,3.55
835,1163,2.45    3    5.5    3.25    ,Visual Information Affects Auditory Frequency Discrimination with Random Stimulus Sequences: Evidence from ERPs,"A combination of signals across perceptual modalities can facilitate sensory perception. Congruent visual information is generally considered to complement insufficient speech information for hearing-impaired and normal listeners in noisy conditions. In order to develop effective audiovisual strategies for speech perception, it is important to understand the mechanism accounting for the audiovisual integration ability. The present work investigated the effect of simultaneous visual stimulation on detecting auditory frequency changes with random stimulus sequences. Event-related potential responses with normal-hearing listeners showed that simultaneous visual and auditory stimuli elicited larger N1/P2 amplitude and shorter P2 peak latency than auditory-only stimuli did. The present study provided neurophysiological evidence that the cross-modal correspondence between the size of visual stimulus and the level of auditory pitch favors the detection of auditory frequency changes presented in random stimulus sequences.",1.0,3.55
1004,1609,2.7    3.45    3.75    4.3    ,Multitask Learning for DNN-HMM Based Keyword Spotting,"We propose a new DNN-HMM based keyword spotting system utilizing triphone keyword models and monophone filler models. Such a system typically uses two separate DNNs: one for triphones and the other for monophones. In this work, we propose to utilize multitask learning to use a single DNN for both triphone and monophone acoustic models with the aim of reduced computational complexity and increased generalization performance. Experimental results using PBW corpus show that the proposed DNN-HMM based approach has lower error rate than the conventional methods such as a GMM-HMM based approach by 39.7%. It also decreases decoding time by 14.5% compared with the conventional two-DNN approach that does not use multitask learning framework.",1.0,3.55
1064,1762,4.15    4.15    3.35    2.55    ,Detection of Kashmiri Palatalized Consonants,"The acoustic-phonetic attributes of palatalization in the Kashmiri speech is investigated in this study. This is a unique phonetic feature in Kashmiri in the Indian context. An automated approach is proposed to detect this unique phonetic feature in the continuous Kashmiri speech. The i-matra vowel has the impact of palatalizing the consonant connected to it. Therefore, these consonants have been investigated in synchronous with vowel regions, which are spotted using the instantaneous energy computed from the envelope-derivative of the speech signal. The resonating characteristics of the vocal-tract system
framework that reflect the formant dynamics are used to differentiate palatalized consonants from the other consonants. In this regard, the Hilbert envelope of the numerator of the group-delay function that provides good time-frequency resolution has been used to extract formants. The experimentation was carried out in various vowel settings using the acoustic cues in spotting palatalized consonants that produced a promising result with a
detection accuracy of 92.46 %",1.0,3.55
879,1274,4.25    2.65    4.1    3.2    ,Critical Phoneme Subsets for Speaker Identity in Accent Conversion,"Accent conversion (AC) is a technique that seeks to transform non-native utterances to sound as if the speaker had a native accent. AC finds applications in computer-assisted pronunciation training, where it can be used to generate personalized models for each second-language learner. However, AC methods are susceptible to mispronunciations and missing phonemes in speech corpora, which are unavoidable when recording speech from second language learners. To address this problem, we describe a forward-selection procedure to identify a critical subset of phonemes that capture the identity of the learner’s voice. Then, we perform AC using only those phonemes that are within the critical subset from the learner’s corpus, borrowing the remaining phonemes from the source speaker’s corpus. We validate the approach through a series of subjective listening tests of acoustic quality and speaker identity. Our results show that using a reduced number of phonemes from the learner’s corpus improves the acoustic quality of AC speech without sacrificing the similarity between the AC speech and the learner’s voice.",1.0,3.55
670,2384,4.3    2.2    4.15    ,Sampling Strategies in Siamese Networks for Unsupervised Speech Representation Learning,"Recent studies have investigated siamese network architectures for learning invariant speech representations using same-different side information at the word level. Here we investigate systematically an often ignored component of siamese networks: the sampling procedure (how pairs of same vs. different tokens are selected). We show that sampling strategies taking into account Zipf's Law, the distribution of speakers and the proportions of same and different pairs of words significantly impact the performance of the network. In particular, we show that word frequency compression improves learning across a large range of variations in the number of training pairs. This effect does not apply to the same extent to the fully unsupervised setting, where the pairs of same-different words are obtained by spoken term discovery. We apply these results to pairs of words discovered using an unsupervised algorithm and show an improvement on the state-of-the-art in unsupervised representation learning using siamese networks.",0.0,3.5500000000000003
1233,2218,3.4    4.35    2.85    3.65    ,Exploring the Robustness of Features and Enhancement on Speech Recognition Systems in Highly-reverberant Real Environments,"This paper evaluates the robustness of a DNN-HMM-based speech recognition system in highly-reverberant real environments using the HRRE database.  The performance of locally-normalized filter bank (LNFB) and Mel filter bank (MelFB) features in combination with Non-negative Matrix Factorization (NMF), Suppression of Slowly-varying components and the Falling edge (SSF) and Weighted Prediction Error (WPE) enhancement methods are discussed and evaluated. Two training conditions were considered: clean and reverberated (Reverb). With Reverb training the use of WPE and LNFB provides WERs that are 3% and 20% lower in average than SSF and NMF, respectively. WPE and MelFB provides WERs that are 11% and 24% lower in average than SSF and NMF, respectively. With clean training, which represents a significant mismatch between testing and training conditions, LNFB features clearly outperform MelFB features.  The results show that different types of training, parametrization, and enhancement techniques may work better for a specific combination of speaker-microphone distance and reverberation time.  This suggests that there could be some degree of complementarity between systems trained with different enhancement and parametrization methods.",1.0,3.5625
1301,2480,4.05    2.5    5    2.7    ,Acoustic-to-Word Recognition with Sequence-to-Sequence Models,"Recently, there has been increasing interest in direct Acoustic-to-Word (A2W) modeling for Automatic Speech Recognition. The success of sequence-training algorithms such as Connectionist Temporal Classification (CTC) and Attention-based Sequence-to-Sequence (seq2seq) models for end-to-end speech recognition make such modeling feasible. CTC-based models have shown very good results for A2W recognition. In this paper, we present a simple seq2seq model for A2W recognition that gives competitive results with character recognition models without added complexity. We also present a comparative study of different target units for end-to-end speech recognition such as characters, byte-pair encodings and word models with seq2seq models.",1.0,3.5625
899,1329,3.7    4    3.25    3.3    ,Speaker Embeddings for Speaker-Targeted Automatic Speech Recognition,"In this work, we investigated using three types of pre-extracted speaker embeddings as additional features for speaker-targeted speech recognition in cocktail parties. Using previously available speech segments or face images from the target speaker's mobile phones or social media, the speaker embedding model can characterize the target speaker to generate a text-independent speaker embedding before a speech signal is uttered, to enhance transcribing efficiency. The pre-extracted speaker embedding is then concatenated with acoustic features as input to our DNN acoustic model.
We considered both acoustic and visual speaker embeddings. i-vectors and x-vectors both use speech segments as input. i-vectors are extracted by the traditional GMM-UBM. x-vectors are extracted using bottleneck features of a Time Delay NN-based speaker recognizer. Face vectors are extracted by a deep CNN trained on triplet loss using face images. 
Empirical evaluation was performed on the TED-LIUM corpus release 2 by overlapping a target speaker and a background speaker's speech signals. We have shown that using speaker embedding reduces WER significantly, from 65.7 to 29.5. Among the three types of speaker embeddings, x-vectors and face vectors are more robust against environment variations while i-vectors tend to overfit to the exact speaker and environment condition.",1.0,3.5625
816,1109,3.3    2.6    2.55    5.8    ,An Efficient Non-iterative Kalman Filter for Single Channel Speech Enhancement in Non-stationary Noise Environment,"This paper presents a non-iterative Kalman filter (NIT-KF) for single channel speech enhancement in non-stationary noise condition (NNC). To adopt with NNC during framing, a dynamic Kaiser window with overlapping is used with a pre-filter based on a voiced activity detection (VAD) and spectral subtraction (SS), called VAD-SS. Specifically, VAD is used to update the noise spectrum and SS to estimate the clean speech spectrum on a framewise basis from where the noise variance and LPCs are estimated. With these estimated parameters, the state space model (SSM) including the set of recursive equations of NIT-KF are implemented to suppress noise yielding the enhanced speech. Extensive simulation results reveal that the proposed method outperforms other benchmark KF methods.",1.0,3.5625
1192,2105,3.15    4.3    3.25    ,The Observation Likelihood of Silence: Analysis and Prospects for VAD Applications,"This paper shows a research on the behaviour of the observation likelihoods generated by the central state of a silence HMM (Hidden Markov Model) trained for Automatic Speech Recognition (ASR) using cepstral mean and variance normalization (CMVN). We have seen that observation likelihood shows a stable behaviour under different recording conditions, and this can be used to discriminate between speech and silence frames. We present several experiments which prove that the use of a decision threshold is robust to very different recording channels and noise conditions, by testing with very different databases. The results have also been compared with those obtained by two standard VAD systems, showing promising prospects. All in all, observation likelihood scores could be useful as the basis for the development of future VAD systems, with further research and analysis to refine the results.",1.0,3.5666666666666664
1065,1763,3.6    3.5    3.6    ,A Novel Multi-Pitch Estimation Algorithm with Permutation Invariant Training,"Multi-pitch estimation is critical for many applications, including computational auditory scene analysis (CASA), speech enhancement and mixed speech analysis; however, despite many efforts have been done, it remains a challenging problem. In this paper, we propose a novel method for pitch estimation of mixed speech. Proposed method is based on permutation invariant training (PIT) for pitch estimation. We compute the target features over all frames in the whole utterance for each possible output-target assignment, pick the one with the minimum error, and optimize for that assignment. The different neural networks including recurrent neural network (RNN), Long Short-Term Memory (LSTM-RNN) and bidirectional Long Short-Term Memory (BLSTM-RNN) are investigated for PIT based pitch estimation to model these two pitch contours of mixed speech. A multi-task learning solution for pitch estimation was also proposed. The experiment results demonstrate that the proposed method can substantially outperform different baseline methods in multi-pitch estimation of mixed speech and show good generalization ability to new speakers.",1.0,3.5666666666666664
1008,1619,4.2    3.45    3.05    ,Voiceless Consonant Detection and Bandwidth Extension of Narrow Band Speech,"Usually, high frequency components of telephone speech are lost due to bandwidth limitation.
In this paper, we propose a bandwidth extension method using multi phase rectification.
A detection method of voiceless consonants is derived and used for restoring voiceless consonants.
Objective experiments show that the proposed method provides better results in comparison with the conventional methods.",1.0,3.5666666666666664
1086,1815,3.55    4.8    3.1    2.85    ,Angular Softmax Loss for End-to-end Speaker Verification,"End-to-end speaker verification systems have received increasing interests.
The traditional i-vector approach trains a generative model (basically the factor-analysis model) to extract i-vectors as speaker embeddings.
In contrast, the end-to-end approach directly trains a discriminative model (neural networks) to learn discriminative speaker embeddings; a crucial component is the training criterion.
In this paper, we use angular softmax (A-softmax), which is originally proposed for face verification, as the loss function for feature learning in end-to-end speaker verification. 
By introducing margins between classes into softmax loss, A-softmax can learn more discriminative features than softmax loss and triplet loss, and at the same time, is easy and stable for usage.
We make two contributions in this work. 1) We introduce A-softmax loss into end-to-end speaker verification and achieve significant EER reductions. 2) We find that the combination of using A-softmax in training the front-end and using PLDA in the back-end scoring further boosts the performance of end-to-end systems under short utterance condition (short in both enrollment and test).
Experiments are conducted on part of $Fisher$ dataset and demonstrate the improvements of using A-softmax.",1.0,3.5749999999999997
769,85,3.4    3.45    3.3    4.15    ,Improved Epoch Extraction from Speech Signals Using Wavelet Synchrosqueezed Transform,"Synchrosqueezed wavelet transform (WSST) is an effective tool in tracking instantaneous frequency of a given signal. The objective of the present work is to propose a WSST based method for accurate epoch estimation from speech. Epochs in speech represent the instants where the excitation to the vocaltract is maximum and instantaneous F0 contour is derived from epoch locations. The proposed hypothesis in this paper is that the signal reconstructed by discarding higher frequency modes (above the mean F0)in the WSST transformed time frequency domain observed to predominantly represent source characteristics. The presence of the source characteristics in the modified WSST reconstructed signal is validated by the improved identification accuracy obtained for the epochs estimated from clean speech utterances of CMU-Arctic database. To further demonstrate the effectiveness of the WSST in improving the overall epoch estimation performance, a WSST modified zero frequency filtering (ZFF) of speech, which is one of the simple and effective tools for epoch extraction, is proposed. The sharp instantaneous frequency representation by WSST also found to be effective in estimating epochs emotion utterances where rapid pitch variations are present. The improved epoch estimation performance from emotive utterances is confirmed by validating on the German emotion speech corpus(EmoDb).",1.0,3.5749999999999997
915,1382,4.9    2.9    3.5    3    ,"From Speech Signals to Semantics -- Tagging Performance at Acoustic, Phonetic and Word Levels","Spoken language understanding (SLU) is to decode the semantic information embedded in speech input. SLU decoding can be significantly degraded by mismatched acoustic/language models between training and testing of a decoder. In this paper we investigate the semantic tagging performance of bi-directional LSTM RNN (BLSTM-RNN) with input at acoustic, phonetic and word levels. It is tested on a crowd sourcing, spoken dialog speech corpus spoken by non-native speakers in a job interview task. The tagging performance is shown to be improved successively from low-level, acoustic MFCC, mid-level, stochastic senone posteriorgram, to high-level, ASR recognized word string, with the corresponding tagging accuracies at 70.6%, 82.1% and 85.1%, respectively. With a score fusion of the three individual RNNs together, the accuracy can be further improved to 87.0%.",1.0,3.575
991,1578,5    2.5    4.9    1.9    ,Spectral-change Enhancement with Prior SNR for the Hearing Impaired,"A previous signal processing algorithm that aimed to enhance spectral changes over time showed some benefit for recognizing speech in background noise for hearing-impaired listeners. A genetic algorithm was applied to select the “best” parameter values for each listener due to large individual differences. In this work, the spectral-change-enhancement (SCE) was manipulated only for segments that were dominated by the target speech, rather than treating all frames equally. Prior instantaneous signal-to-noise ratios (iSNRs) were calculated to determine the frames which should be processed with SCE.  Speech intelligibility and clarity preference were measured for three types of speech: unprocessed, processed by the previous SCE, and processed by the current algorithm (SCE-iSNR), across 12 hearing-impaired listeners and with steady speech-spectrum noise (SSN) and six-talk speech (STS) masking. The results showed that the modified algorithm improved speech intelligibility significantly for both SSN and STS masking at high signal-to-masker ratios (SMRs) and only for STS masking at low SMRs, while the effect of processing on speech quality was small.",1.0,3.575
1330,2601,4.45    3.95    3.4    2.5    ,Diagnosis of Obstructive Sleep Apnea from Speech Signals Using a Convolutional Neural Network,"Obstructive sleep apnea (OSA) is a sleep-related breathing
disorder in which pharyngeal collapse during sleep causes
complete or partial airway obstruction. Although OSA is
common and can have severe impacts, it often remains
unrecognized because of its complicated diagnostic procedure.
It was shown that OSA is associated with anatomical and
functional abnormalities of the upper airway, which affect the
acoustic parameters of a speech signal. In this study, we
propose a new method that enables detection of OSA subjects
while they are awake, only by analysis of their speech signals.
The proposed algorithm uses the convolutional neural network
to obtain a simple and effective system that can estimate the
severity of OSA. Based on sustained vowels analysis alone (356
subjects; 285 for training, 71 for validation), our system yields
encouraging results (accuracy of 76%) showing the potential of
speech analysis to detect OSA. Analysis of the system shows
superior performance as opposed to the regression-tree
ensemble method, based on the same type of dataset.",1.0,3.575
1171,2026,3.5    3.35    4.45    3    ,Improved MVDR Beamforming Using LSTM Speech Models to Clean Spatial Clustering Masks,"Spatial clustering techniques can achieve significant multi-channel noise reduction across relatively arbitrary microphone configurations, but have difficulty incorporating a detailed speech/noise model. In contrast, LSTM neural networks have successfully been trained to recognize speech from noise on single-channel inputs, but have difficulty taking full advantage of the information in multi-channel recordings. This paper integrates these two approaches, training LSTM speech models to clean the masks generated by the Model-based EM Source Separation and Localization (MESSL) spatial clustering method. By doing so, it attains both the spatial separation performance and generality of multi-channel spatial clustering and the signal mdeling performance of multiple parallel single-channel LSTM speech enhancers. Our experiments show that when our system is applied to the CHiME-3 dataset of noisy tablet recordings, it increases speech quality as measured by the Perceptual Evaluation of Speech Quality (PESQ) algorithm and reduces the word error rate of the baseline CHiME-3 speech recognizer, as compared to the default BeamformIt beamformer.",1.0,3.575
1322,2579,3.25    4.7    5.05    1.35    ,Inter-frame and Intra-frame Predictive Split Vector Quantization of LSF Parameters Using Deep Neural Network,"The vocal tract of human is usually modelled by a liner predictive analysis filter whose coefficients are converted into the line spectral frequency (LSF) parameters for quantization. In this paper, the inter-frame and intra-frame predictive split vector quantization (SVQ) of LSF parameters based on the deep neural network (DNN) is proposed. The introduced nonlinear DNN predictor, which exploits both the inter-frame and intra-frame correlation of LSF parameters, produces much higher prediction gain than that of the inter-frame predictor based on the linear auto-regressive (AR) model. Compared with the AR inter-frame predictive SVQ, the proposed DNN predictive SVQ gains 2 bits in terms of average spectral distortion and 3 bits in terms of the number of outlier frames.",1.0,3.5875
784,1005,2.75    3.45    3.9    4.25    ,Unit-Selection Singing Voice Conversion with a Multi-Target Cost Function,"This paper presents a unit-selection algorithm for non-parallel singing voice conversion, which integrates the specific musical factors of the singing voice during voice conversion. Unit- based algorithms present important advantages for voice conversion: unit-selection voice conversion allows the preservation of the original characteristics of the target voice by using real unit; prior segmentation of speech into linguistic units such as phonemes gives the possibility to learn the conversion from on- the-fly source and target voices databases. The main contribution of this paper is to extend unit-based conversion algorithms to singing voice conversion, by integrating the musical factors of the singing voice (pitch, intensity, duration) into the target cost function used for the selection of spectral target units. The main objective is to encourage the selection of spectral envelopes of the target singer sharing a similar musical context with the ones of the source singer. A perceptual experiment has been conducted to compare the proposed multi-objective optimization with existing unit-selection algorithms: the proposed unit-selection algorithm is perceived as significantly more similar to the target voice that the standard unit-selection algorithm.",1.0,3.5875
773,90,4.2    3.05    3.65    3.45    ,Multitask with Residual Learning for CTC-based Acoustic Modeling,"This study improves the use of Connectionist Temporal Classification (CTC)-based neural network modeling in Japanese speech recognition. There are a lot of Ideograms named Kanji in Japanese, which are treated as modeling units in CTC systems. Kanji are always polyphones. This polyphone problem produces high error rates in grapheme-based CTC systems. Since a Japanese pronunciation lexicon is available, integrating the phoneme information into the grapheme-based CTC system can partially solve the multiple pronunciations of Kanji and make the acoustic model more robust. We use a multitask learning strategy to realize this framework. Furthermore, we add the residual learning to our architecture to avoid the performance deterioration in the situation of deep networks. Experiments are carried out on the King-ASR-117 Japanese corpus, and the experimental results demonstrate the effectiveness of the proposed methods.",1.0,3.5875000000000004
1021,1648,4.65    4    4.25    2.6    2.45    ,A Proposal of Vibrato Feature Reflecting Magnitude of Fluctuation of Fundamental Frequency to Evaluate Reproducibility of Individuality in Singing Voice,"This paper describes a new vibrate feature which reflects the magnitude of fluctuation of fundamental frequency(fo) of singing voice with vibrato.
This vibrato feature is an improved version of our conventional vibrato feature.
This feature enables us to measure how vibrato of an impressionist is close to that of the target singer.
By using this feature, we calculated the vibrato-feature distortions in order to verify if the impressionist does deformation, which corresponds to emphasis of impressive feature of the target singer.
The smallest distortions were observed in the case where the magnitude of fo fluctuation of the impressionists is larger than that of the target singer.
This observation indicates that the impressionist does deformation.
Next, we conducted a subjective experiment in order to investigate how the deformation affects vibrato perception. 
In this experiment, the highest score was obtained in the case where the magnitude of fo fluctuation of the synthetic sound is larger than that of the target singer.
This result implies that listeners feel deformation increase individuality.",1.0,3.59
1165,2009,3.25    2.1    4.95    3.45    4.2    ,Neutral-to-Emotional Voice Conversion with Latent Representations of F0 Using Generative Adversarial Networks,"In this paper, we propose a novel neutral-to-emotional voice conversion (VC) model that can effectively learn a mapping from neutral to emotional speech with limited emotional voice data. Although conventional VC techniques have achieved tremendous success in spectral conversion, the lack of representations in fundamental frequency (F0), which explicitly represents prosody information, is still a major limiting factor for emotional VC. To overcome this limitation, in our proposed model, a continuous wavelet transform (CWT) is applied for decomposing F0 into different temporal levels representations. Moreover, to better measure similarities between the converted and real CWT-F0 features, we combine a variational autoencoder (VAE) with a generative adversarial network (GAN), which is named VA-GAN. In the VA-GAN, VAE learns the latent representations of CWT-F0 features, while the discriminator of the GAN can use the learned feature representations as a basis for the VAE reconstruction objective. Experiments show that our proposed method achieves good and consistent performance, in both objective and subjective evaluations.",1.0,3.59
1164,2008,3.05    2.95    4.2    4.2    ,JSUT Corpus: Free Large-Scale Japanese Speech Corpus toward End-to-End Speech Synthesis,"Thanks to improvements in machine learning techniques including deep learning, a free large-scale speech corpus that can be shared between academic institutions and commercial companies has an important role. However, such a corpus for Japanese speech synthesis does not exist. In this paper, we designed a novel Japanese speech corpus, named the JSUT corpus, that is aimed at achieving end-to-end speech synthesis. The corpus consists of 10 hours of reading-style speech data and its transcription and covers all of the main pronunciations of daily-use Japanese characters. In this paper, we describe how we designed and analyzed the corpus. The corpus is freely available online.",1.0,3.5999999999999996
1036,1683,3.2    3.65    4    3.55    ,Automatic Acoustic Evaluation of Communication Skill in Face-to-Face Interviews in the Wild,"Face-to-face interviews are a crucial part of any candidate selection
process. Automatic assessment of communication skill
in real face-to-face interviews is a key issue in Social Computing.
Interactive online platforms have enabled conducting these
interviews online without the need of collocation of both
interviewer and interviewee. In this work, we automatically
predict the communication skill score of job applicants giving
real online face-to-face interviews. We attempt this problem by
extracting non-verbal audio and verbal cues from the
interviewee and perform inference on the
communication skill score. To this end, we have collected 45 audio-visual
recordings of real face-to-face interviews conducted using
an online platform of a HR training company. The Skype-like online platform allows
recording and streaming the audio-visual content of both interviewer
and interviewee. We then performed automatic speaker
diarization to extract spoken segments and use an Automatic
Speech Recognition (ASR) system to extract the spoken content
of the interviewee. Previously trained Machine Learning models (in controlled
simulated environment) were used to automatically infer
the communication skill score. Results obtained indicate
that models trained on data collected in controlled environment
generalize for the face-to-face interviews conducted in real uncontrolled
environment, with diversity in people and audio recording hardware. ",1.0,3.5999999999999996
901,1332,4.15    2.5    3.5    4.25    ,Assessment of Human Perception on Emphasis with Audio and Text Clues,"Emphasis is an important factor of human speech that helps to
convey emotion and essential information of utterances. Many
studies have been conducted on emphasis analyses or modeling
but with only either audio or text clues. It has been well known
that emphasis can be expressed at various levels in both text
and audio forms. However, the question whether humans can
clearly distinguish the difference between emphasis levels and
perceive the same emphasis level in both text and audio forms
are still yet to answer. The answers to these question are necessary to construct speech translation systems considering emphasis. In this paper, we conducted analyses on human perception
of emphasis with both audio and text clues. We first collect a
corpus contains pre-defined levels of emphasis presented in text
and audio form. Then, we run crowd-sourced evaluation where
participants decide whether they perceive the same emphasis
levels in both text and audio. We discovered that there are not
only many disambiguations between certain emphasis levels in
audio, but it is also hard for participants to distinguish some
certain emphasis levels when they are presented both audio and
text together.",1.0,3.6
578,2118,2.5    2.9    4.85    4.15    ,Improved Acoustic Modelling for Automatic Literacy Assessment of Children,"Automatic literacy assessment of children is a complex task that normally requires carefully annotated data. This paper focuses on a system for the assessment of reading skills, aiming to de- tection of a range of fluency and pronunciation errors. Natu- rally, reading is a prompted task, and thereby the acquisition of training data for acoustic modelling should be straightfor- ward. However, given the prominence of errors in the training set and the importance of labelling them in the transcription, a lightly supervised approach to acoustic modelling has better chances of success. A method based on weighted finite state transducers is proposed, to model specific prompt corrections, such as repetitions, substitutions, and deletions, as observed in real recordings. Iterative cycles of lightly-supervised training are performed in which decoding improves the transcriptions and the derived models. Improvements are due to increasing accuracy in phone-to-sound alignment and in the training data selection. The effectiveness of the proposed methods for rela- belling and acoustic modelling is assessed through experiemnts on the CHOREC corpus, in terms of sequence error rate and alignment accuracy. Improvements over the baseline of up to 60% and 23.3% respectively are observed.",0.0,3.6
1315,2556,2.45    4.05    3.75    3.5    4.25    ,Progressive Learning Based Combined Framework for Speech Enhancement and Recognition Front-end,"Recently, progressive learning method has created many success in speech enhancement and speaker diarization tasks. We extend the application scope of progressive learning to not only speech enhancement,  but alos speech recognition front-end, aiming to build a framework that produces some outputs for high-performance speech enhancement and some other outputs for accurate speech recognition. So we investigate the LSTM-based densely connected multi-target progressive learning (LSTM-PDML) model, and observe that LSTM-PDML regression output speech behaves the best in speech enhancement metrics, while  LSTM-PDML middle target 1 output speech has the greatest performance in speech recognitoin task.",1.0,3.6
953,1480,2.85    4.15    4.25    3.15    ,A Multi-model Soft Targets Approach for Pronunciation Erroneous Tendency Detection,"Detecting pronunciation erroneous tendency (PET) can provide detailed instructive feedback for second language learners in computer aided pronunciation training (CAPT). In this paper, we utilize soft targets with knowledge from various models for improving the detection performance of PET. First, we examined the effectiveness of soft targets in three single systems by replacing hard targets with soft targets directly for mispronunciation detection. Then, two kinds of methods using multi-model soft targets are proposed in this paper: 1) explicit combination, which uses multi-model soft targets as the final targets by weighted linear combination; 2) implicit combination, which employs the multi-task framework to combine soft targets. Experimental results showed that the detection performance of PET can be improved by using both single soft targets and multi-model soft targets. Moreover, using multi-model soft targets within multi-task framework achieve the best results in pronunciation error detection task, and it is more efficient than conventional ensemble methods which require multiple decoding runs or forward passes.",1.0,3.6
925,1408,3.95    4.15    1.75    4.55    ,Liquids Share the F2 and F3 Perceptual Space: Evidence from Mandarin,"Liquids are a diverse set of sounds, composed /r/ and /l/ sounds. Articulatory properties that unite liquids have been difficult to discover, leading some researchers to suggest the connection between them is acoustic/perceptual. This study examines the correlation between F2 and F3 and liquid perception.

16 native speakers of Mandarin participated in two identification tasks where they had to identify the stimuli as either /r j l w/ in either CV or VCV environments.  Formants from a Cr and VrV sequence were manipulated at 250 Hz steps to make 35 and 30 tokens, respectively.

The results indicated that across both conditions, liquid identification was concentrated from an F2 range of ~1,100 – 1,900 Hz. F3 divided the space between rhotics and laterals. ~2,300 Hz and above was more densely populated by laterals. However, there was significant overlap between the identification of rhotics and laterals along the F3 continua.

The results suggest that liquids share the same F2 range in Mandarin listeners’ perceptual space. The F3 dimension contributes to identification of /r/ and /l/, but was not strongly predictive of either liquid’s identification, suggesting liquids occupy the same perceptual space. Overall, the data offers perceptual motivation for the superclass of liquids.",1.0,3.6000000000000005
1050,1720,3.6    2.45    3.55    4.8    3.6    ,Speech/Music Classification Based on Selection of Low Level Descriptors and Functionals,"State-of-the-art speech-music discrimination techniques typically use dynamic frame-level acoustic features such as the timbral texture features, the rhythmic content features and the pitch content features. In this paper, we investigate the approach of selecting
low level descriptors along with the functionals, which is commonly used in paralinguistic tasks,for speech-music discrimination. Specifically, from a pool of 6373 utterance level features, we select the most indicative features for speech-music discrimination
task by using linear regression and least square analysis. Experimental studies on two publicly available databases, namely, GTzan and MUSAN show that the proposed approach
performs comparable to Convolutional Neural Network (CNN)-based baseline system and achieves an almost perfect speech/music discrimination, even with a simple classifier. On the two databases, we further found that the end classifier models information related to auditory spectrum, MFCC and Fourier magnitude spectrum characteristics.",1.0,3.6000000000000005
1318,2565,2.65    4.9    4.45    2.45    ,Sound Event Detection Using Recurrent Neural Networks,"A new method for the detection of a desired sound event with weak labels is proposed. In the method, each sound clip of prescribed duration is simply labelled as positive if it contains the sound event and negative otherwise. Using a long short-term memory (LSTM) network, we train by unrolling for the duration of the sound clip and then taking the maximum of the unrolled-step scores as the score for that clip. Furthermore, the resulting output from the LSTM model can directly be used to predict the presence or absence of the sound event without the requirement of an additional post processing stage. Experimental results show the effectiveness of the proposed approach.",1.0,3.6125
1188,2095,1.95    3.4    4.25    4.85    ,Unsupervised Clustering of Vowel Sounds: Revisiting the Performance of MFCC Features Associated with Fundamental Frequency and Spectral Bandwidth,"Increasing fundamental frequency (fo) of vowel sounds reduces the number of harmonics and causing undersampling of vocal-tract characteristics. The harmonic undersampling impacts the overall spectral shape which influences the acoustic features such as MFCCs. In this paper, we studied the effects of fo in isolated steady-state vowel sounds in terms of their MFCCs using unsupervised k-means clustering. We used 8 standard-German vowels produced by 4 professional female stage actresses at 14 different fo levels (between 196-698 Hz) with three vocal efforts. The experiment was carried out under three different settings, increasing the range of fo, decreasing the range of fo and different levels of fo. In every experimental setting, the MFCC parameters – such as the number of coefficients and spectral bandwidth (SB) – were varied. The performance of the clustering was analyzed using a new measure counting the number of clusters that did not receive a relative vowel majority (No-relative-vowel-majority count, NRVM). The results indicate that lower number of MFCC coefficients well representing the vowel characteristics through the entire range of fo as well as increasing SB supports the vowel representation. The results of this study have direct implications on speech applications dealing with a wide range of fo.",1.0,3.6125
826,1137,4.65    3.25    2.35    4.2    ,Keyword Spotting Based on CTC and RNN for Mandarin Chinese Speech,"The state-of-the-art acoustic modeling for Keyword Spotting (KWS) systems is mainly based on the hybrid model of Hidden Markov Model (HMM) and Neural Network (NN). However, it is challenging to efficiently train such a hybrid system, since the dependence of the intermediate phonetic representation. Motivated by the end-to-end speech recognition systems, we propose a Mandarin KWS system using the end-to-end method, which directly predict the posterior of phonetic units.The system is based on Connectionist Temporal Classifier (CTC) and Recurrent Neural Network (RNN). The main difference between our system and other CTC-based KWS system is the output alphabet and its corresponded keyword searching mechanism. We adopt Mandarin syllables as the output labels,
rather than the phonemes or characters. Extensive experiments are conducted on the Mandarin Chinese speech dataset. Experimental results indicate that: 1) Compared with HMM-based KWS system, the end-to-end KWS system achieves a significant improvement, without any increase of computational cost. 2) Our syllable-based end-to-end KWS system obtains better performance than the state-of-the-art ones based on Chinese context independent (CI) phonemes or Chinese characters.",1.0,3.6125
859,1201,3.9    4.35    2.65    3.55    ,DNN Based Speech Enhancement for Unseen Noises Using Monte Carlo Dropout,"In this work, we propose the use of dropouts as a Bayesian estimator for increasing the generalizability of a deep neural network (DNN) for speech enhancement. By using Monte Carlo (MC) dropout, we show that the DNN performs better enhancement in unseen noise and SNR conditions. The DNN is trained on speech corrupted with Factory2, M109, Babble, Leopard and Volvo noises at SNRs of 0, 5 and 10 dB and tested on speech with white, pink and factory1 noises. Speech samples are obtained from the TIMIT database and noises from NOISEX-92. In another experiment, we train five DNN models separately on speech corrupted with Factory2, M109, Babble, Leopard and Volvo noises, at 0, 5 and 10 dB SNRs. The model precision (estimated using MC dropout) is used as a proxy for squared error to dynamically select the best of the DNN models based on their performance on each frame of test data.",1.0,3.6125
781,1001,5    2.4    3.85    3.2    ,Tutorial for the Spoken CALL Shared Task,"This paper presents a tutorial and tool-box for 
working with the shared task for spoken Computer Aided Language Learning (CALL) for English as a Second Language (ESL). This is a computer-based dialogue system with Swiss school children learning English that should be able to judge the correctness of language and meaning in their responses.  
In order to work with this task, a number of files and baseline systems are provided to the researchers. 
The number of files, the syntax of ID's and the scripts as well as their usage in a pipe-line architecture can be confusing to a novice researcher in the field. In order to facilitate work with this task, this paper proposes a tutorial set-up and an architecture.  
The resulting tool-kit is an open source project on Github
that walks the user through the corpus, setting up the 
system for training and comparing the results against human 
judgment. The contribution of this paper is the description 
of the tool-box in order to enable new research 
groups to find their way around the problem statement more easily, avoiding many of the typical start-up pit-falls.",1.0,3.6125
957,1492,2.55    5.1    4.15    2.65    ,Design and Development of a Multilingual Speech Corpora(TaMaR-EmoDB)for Emotion Analysis,"This paper presents the design, the development and the evaluation of a new multilingual emotional speech corpus, TaMaR-EmoDB (Tamil Malayalam Ravula - Emotion Data Base) in Indian languages. The corpus consists of utterances from three languages of South India,
namely, Malayalam, Tamil and a tribal language, Ravula. The data base contains short speech utterances in four emotions - anger, anxiety, happy and sad, along with neutral utterances. In order to give naturalness to the emotions, the speakers with language proficiency have been chosen for the data preparation. The corpus is first evaluated using a perception test, in order to understand how well the emotional state in emotional speech is identified by humans. Later, a baseline MFCC - GMM framework is used to analyze how well the emotional information in speech is detected in human-computer communication.
The baseline GMM based classifier reports an overall classification accuracy of 54.20%, 58.80% and 59.20% for Malayalam, Tamil and Ravula language respectively. This database represents a new linguistic resource that will enable future research in speech emotion detection, speech modification, corpus based prosody analysis and speech synthesis",1.0,3.6125000000000003
989,1573,3.1    2.2    3.5    5.65    ,Speech Intelligibility Enhancement by Equalization for In-car Applications,"In this paper, we propose a new speech intelligibility enhancement method for typical in-car applications in noisy environment. While traditional speech enhancement algorithms aim at increasing the Signal to Noise Ratio (SNR), the goal here is to increase intelligibility by applying dedicated voice transformation techniques without changing the original SNR. The proposed method consists in an adaptive equalizer which reallocates the energy of frequency bands in a way that maximizes the Speech Intelligibility Index (SII), a widely used objective measure of intelligibility. The validation of the algorithm was carried out  by means of a perceptual test derived from the  Hearing in Noise Test (HINT) using four typical in-car noises at different speeds and with or without rain. The results obtained demonstrate the merit of the algorithm for all noise types, especially for the car noises without perturbation which is mostly a low-frequency noise.",1.0,3.6125000000000003
786,1009,3.35    2.5    3.45    5.15    ,Speaking-Rate Adaptation of Automatic Speech Recognition System through Fuzzy Classification Based Time-Scale Modification,"In this paper, we study the role of speaking-rate adaptation (SRA) of automatic speech recognition (ASR) systems. The performance of an ASR system is reported to degrade when the speaking-rate is either too fast or too slow. In order to simulate such a situation,  an ASR system was trained on adults' speech and used for transcribing speech data from adult as well as child speakers. Earlier studies have shown that, speaking-rate is significantly lower in the case children when compared to adults. Consequently, the recognition performance for children's speech was noted to be very poor in contrast to adults' speech. To improve the recognition performance with respect to children's speech, speaking-rate was explicitly changed using time-scale modification (TSM). A recently proposed TSM approach based on fuzzy classification of spectral bins has been explored in this regard. The fuzzy-classification-based TSM technique is reported to be superior to state-of-the-art approaches. Effectiveness of the said TSM technique has not been studied yet in the context of ASR. The experimental studies presented in this paper show that SRA based on fuzzy classification results in a relative improvement of 30% over the baseline.",1.0,3.6125000000000003
139,1188,4.2    4.2    2.45    2.3    4.95    ,Transcription Correction for Indian Languages Using Acoustic Signatures,"Accurate phonetic transcription of the speech corpus has a significant impact on the performance of speech processing applications especially for low resource languages.  Mismatches between the transcriptions and their utterances occur often at phoneme level due to insertion/deletion/substitution errors. This is very common in Indian languages owing to schwa deletion in the context of vowels, and agglutination in the context of consonants.

An attempt is made in this paper to use acoustic cues at the syllable level to remove vowels from the transcription when they are poorly articulated or absent. Hidden Markov model (HMM) based forced Viterbi alignment (FVA) and group delay (GD) based signal processing are employed in tandem to achieve this task. Disagreement between FVA (which produces vowel boundaries based on transcription) and GD boundaries (which uses signal processing  cues for syllables) are used to correct the transcription.  An increase in likelihood of 0.3% is observed across 3 Indian languages, namely, Gujarati, Telugu and Tamil.",0.0,3.62
1053,1729,3.05    3.9    2.45    5.1    ,Convolutional Neural Network Based Sound Source Separation Using a Non-uniform Linear Microphone Array,"Since MPEG-H supports not only channel-based but also object-based audio contents, there is a need for a sound source separation technique that converts channel-based to object-based audio. Among various sound source separation techniques, multi-channel non-uniform linear microphone array based sound source separation has been proposed in converting channel-based audio to object-based audio. Unfortunately, this technique has difficulties in setting the optimal azimuth and width. In this paper, we propose a method to determine the optimal azimuth and width based on a convolutional neural network (CNN) classifier. First, depending on numerous azimuths and widths, different sets of audio signals are separated. After that, each audio set is categorized into a specific audio class by using the CNN classifier. Then, in order to separate a desired audio signal, the azimuth and width with the highest similarity for a given class are selected. The performance of the CNN classifier is evaluated in terms of separation accuracy and objective measures such as signal-to-distortion ratio (SDR), signal-to-interference ratio (SIR), and signal-to-artifacts ratio (SAR). Consequently, the proposed method provides higher SDR, SAR, SIR, and separation accuracy than a minimum variance distortionless response (MVDR) beamformer as well as the method that only uses azimuth-frequency analysis.",1.0,3.6249999999999996
743,2578,4.15    3.1    ,Multi-frame Coding of LSF Parameters Using Block-Constrained Trellis Coded Vector Quantization,"In this paper, the predictive block-constrained trellis coded vector quantization (BC-TCVQ) schemes are developed for quantization of multiple frames line spectral frequency (LSF) parameters. The consecutive LSF frames are interleaved to subvectors for trellis modeling. The predictive BC-TCVQ systems are then designed to encode multi-frame LSF parameters. The performance evaluation of proposed schemes is compared with the single-frame LSF encoding methods using multi-stage vector quantization (MSVQ) and predictive block-constrained trellis coded quantization (BC-TCQ), demonstrating significant reduction of bit rate for transparent coding. The developed multi-frame LSF quantization schemes show satisfactory performance even at very low encoding rate, and thus can be efficiently applied to the speech coders with moderate delay.",0.0,3.625
910,1361,4.35    3.45    4.05    2.65    ,Retrieving Emotional Behaviors from Target Subjects: Exploring the Intersection between Speaker Verification and Emotion Recognition,"Many scenarios require the use of speaker verification systems using audio with high emotional content (e.g., calls from 911, forensic analysis of threatening recordings). For these cases, it is important to explore the intersection between speaker and emotion recognition tasks. This paper explores the problem of automatic retrieval of  specific emotional behaviors from target speakers (e.g., detecting highly aroused recordings from ""Joe""). A key challenge to address this problem is the lack of resources, since current emotional databases are commonly limited in size and number of speakers. We have collected a pool of sentences from multiple speakers (132,930 segments), where some of these speaking turns belong to 146 speakers in the MSP-Podcast database. Our framework trains speaking verification models, which are used to retrieve candidate speaking turns from the  pool of sentences. The emotional content in these sentences are detected using state-of-the-art emotion recognition algorithms. The experimental evaluation provides promising results, where most of the retrieved sentences belong to the target speakers and has the target emotional content.",1.0,3.625
1031,1673,3.3    4.25    2.75    4.2    ,Phone Aware Nearest Neighbor Technique Using Spectral Transition Measure for Non-Parallel Voice Conversion,"The Nearest Neighbor (NN)-based alignment techniques are popular in the area of non-parallel VC. The performance of NN-based alignment improves with the information of phone boundary. However, estimating the exact phone boundaries are challenging. If the text corresponding to the non-parallel utterances is available, then the Hidden Markov Model (HMM) can be used to identify the correct phone boundaries. However, it requires a large amount of training data that is difficult to collect in realistic scenarios. Hence, we propose to exploit a novel Spectral Transition Measure (STM)-based alignment technique that does not require any apriori training data in the context of VC. The key idea behind STM is that neurons respond very much to the transitional stimuli compared to the steady-state stimuli in the auditory or visual cortex. The phone boundaries estimated using the STM algorithm are then applied to the NN technique to obtain the aligned source and target speakers’ spectral features. Proposed STM+NN alignment technique is giving on an average 2.80% more phonetic accuracy (PA) compared to the NN-based alignment technique. The improvement in % PA after alignment has positively reflected in the better performance in terms of speech quality and speaker similarity of the converted voice.",1.0,3.625
1239,2236,4.35    3.45    2.3    4.4    ,Speaker Adaptation with Minimal Data for DNN-based Speech Synthesis Using a Postfiltering Network,"DNN-based speech synthesis systems with various inner structures have been widely adopted as they offer significant improvements in quality and intelligibility compared to the statistical synthesis methods. However, they require substantially larger amounts of training data and their rapid adaptation to new speakers have not been explored as much as the statistical systems. Here, we propose a shallow neural network model for postfiltering the cepstral parameters that are generated with a DNN-based system. Since our postfilter model contains relatively few parameters, it can be adapted rapidly to target speakers with only a few utterances. Additionally, we incorporate an adversarial training and a cluster-based initialization technique into our model to further boost the performance. Our subjective results suggest significant improvements in similarity to the original speaker when compared to the conventional adaptation methods without a loss in perceptual quality.",1.0,3.625
906,1344,3.3    3.4    4.2    ,CNN Parameter Selection and Context Windows for Robust Phoneme Recognition,"A common belief in the community is that deep learning requires large datasets to be effective. We show that with careful parameter selection, deep feature extraction can be applied even to small datasets. Another problem is that state-of-the-art results are rarely reproducible because they use proprietary datasets, pretrained networks and/or weight initializations from other larger networks. We present a two-fold novelty for this situation where an intelligently defined CNN architecture, together with context windows and a knowledge-driven classifier achieves nearly state-of-the-art phoneme recognition results with absolutely no pretraining or external weight initialization. More importantly, we are able to achieve transparent, reproducible frame-level accuracy, comparable to the state of the art and, additionally, perform a convergence analysis to show the generalization capacity of the model. Furthermore, we show how algorithms with strong learning guarantees can not only benefit from raw data extraction but contribute with more robust results.",1.0,3.633333333333333
799,1061,3.5    3.7    3.7    ,Speech Dereverberation Using Complex Cepstrum and Multivariate Empirical Mode Decomposition,Reverberation dominantly causes troubles in speech signal processing. This paper proposes a speech dereverberation method by using complex cepstrum (CCEP) and multivariate empirical mode decomposition (MEMD). Our method estimates CCEP of room impulse response (RIR) from ensemble averages of CCEP of reverberant speech signals by using MEMD. An ensemble average of CCEP of reverberant speech signals is decomposed by using MEMD into two groups of intrinsic mode functions (IMFs) associated with CCEP of clean speech and RIR. Automatic detection of IMF of RIR is achieved by using similarity measurement between IMFs. The estimated CCEP of RIR is then used to enhance reverberant speech signals. Both objective and subjective evaluations were carried out to evaluate speech quality and intelligibility. The results showed that the proposed method could significantly decrease reverberation and improve speech quality or intelligibility.,1.0,3.6333333333333333
1182,2060,3.95    3.95    4.05    2.6    ,Adaptation of LVCSR Language Models for Medical Tests Using Machine Translated Domain Data,"Neurodegenerative diseases such as dementia affect a person's speech and language. A person's cognitive state can be assessed by tasks that are based on spontaneous speech, e.g. describing the Cookie Theft picture. The domain of this task is fixed. However, dementia's pathology causes the person to use unforeseen language which degrades speech recognition performance. Still, the constrained domain allows the usage of data of that domain to adapt a large vocabulary continuous speech recognizer's (LVCSR) Language Model (LM). Native English transliterations of that domain from DementiaBank's Pitt corpus are converted to German using state of the art machine translation software. We evaluate this adaptation on a dataset of 45 German recordings of Cookie Theft picture descriptions. By adapting a LVCSR's LM, an average absolute improvement in word accuracy (WAC) of 9.4% up to 71.1% is achieved.",1.0,3.6374999999999997
1101,1855,2.25    4.85    3.1    4.35    ,Amazon’S Statistical Speech Waveform Synthesis System: AmaWave,"Statistical TTS systems that directly predict the speech waveform have recently reported improvements in synthesis quality. This investigation introduces AmaWave, Amazon's statistical speech waveform synthesis (SSWS) system. An in-depth evaluation of AmaWave is conducted across a number of domains to better understand the consistency in SSWS quality. The results of this evaluation are validated by repeating the procedure on a separate group of testers. Finally, an analysis of the nature of speech errors of AmaWave compared to hybrid unit selection synthesis is conducted to identify the strengths and weaknesses of SSWS. Having a deeper insight into SSWS allows us to better define the focus of future work to improve this new technology.",1.0,3.6374999999999997
942,1444,2.75    3.35    5    3.45    ,Hypothesis Correction Based on Semi-character Recurrent Neural Network for End-to-end Speech Recognition,"End-to-end automatic speech recognition (ASR) has become popular because of its simple modeling, but it encounters out-of-vocabulary words more frequently than the conventional hybrid approaches that jointly use acoustic and language models. In particular, word-based end-to-end systems cannot output any words unseen in training data. To address this problem, character-based end-to-end systems have been proposed; however, they are susceptible to noise, and their output words are not necessarily correct in terms of language. This is because language constraints, such as lexicons and language models, are lacking in the decoding process. Thus, errors like misspellings occur frequently. In the field of natural language processing, to correct spelling errors, a semi-character recurrent neural network (scRNN) was proposed whose inputs are the counts of characters in a word and outputs are word ids. To apply scRNN to ASR, extensions are needed because scRNN focuses only on substitution errors. Here, to consider insertion and deletion errors, we introduce blank word symbols, similar to blank symbols in connectionist temporal classification, and word concatenation. Two different ASR tasks, a noisy ASR task and an ASR task with a large vocabulary, showed that scRNN with the proposed extension improved the word error rate.",1.0,3.6375
902,1334,3.4    4.15    2.95    4.05    ,Identification of [R] Sound Disorder for Arabic Children Speakers Using ASR,"Automatic identification of speech disorders in children’s speech is very important for the diagnosis and monitoring of speech therapy. In this work, the [r]-sound disorder diagnosis Arabic speech database was used for building an automatic system which identifies [r] articulation disorder type (five classes) using 40-dimentional feature vectors extracted from the acoustic signal corresponding to the [r] phone. The boundaries of [r] sound within a word have been identified by applying forced alignment with an Arabic speech recognizer. The well-known GMM-UBM and I-vector techniques were used for this task. The [r] sound has been selected as it is the most common pronunciation problem that Arabic speaking children suffer from. The impact of [r] location in a word on the speech disorders has been investigated by considering words with [r] in the beginning, middle and end. The best performance of our systems, is 84.1% accuracy with our I-vector system and 79.7% for our GMM-UBM system.",1.0,3.6375
1015,1639,3.95    3.4    3.75    3.45    ,Attentive Multi-Scale CNN-MLP for Speech Emotion Recognition,"Deep learning system, such as Convolutional Neural Networks (CNNs), can infer a hierarchical representation of input data that facilitates categorization. In this paper, we propose an Attentive Multi-Scale CNN-MLP (AMS-CNN-MLP) model for speech emotion recognition. By means of the attention mechanism and the multi-scale convolution module, our model learns to focus on the salient parts of the input for which the model’s output is most sensitive to and promotes competition among a set of multi-scale convolutional filters. In the multi-scale convolution module, we use maxout activation unit to select the maximum value among the multi-scale convolutional filters and reduce the dimension. The proposed method is evaluated on the FAU-Aibo emotion corpus as defined in the Interspeech 2009 Emotion Challenge and measured by unweighted average recall (UA recall) rate. The UA recall rate of official SVM baseline system for static modeling framework is 38.2%. The proposed AMS-CNN-MLP model achieves 46.8% UA recall rate, which is one of the best UA recall rates achieved on FAU-Aibo tasks within the static modeling framework.",1.0,3.6375
806,1073,3.35    5.15    3.25    2.8    ,Coherence-based Processing for Robust Recognition of Reverberant and Noisy Speech,"This paper introduces a combination of techniques for improving speech recognition accuracy using two microphones in reverberant and noisy environments. These techniques include both monaural and binaural processing.  The first step is  monaural precedence-based processing that enhances the onsets of the incoming speech signal, and hence suppresses later components that are more affected by reverberation.  Onset enhancement has been shown to be useful to the human auditory system in separating the direct field from the reverberant field in reverberant environments.  The second step applies enhancement and suppression based on an estimation of the inter-microphone coherence of the incoming speech signal.  Specifically, coherence-based weighting is applied that suppresses portions of the speech signal that are less coherent, which serves to reduce the impact of components that are dominated by diffuse noise or high degrees of reverberation in the speech signal.  This is followed by a third technique that emphasizes regions of coherence in frequency.  A combination of these techniques is shown to lead to very significant improvements in terms of speech recognition accuracy.",1.0,3.6375
1281,2390,4.35    4.1    2.7    3.45    ,Language Redundancy and Prosodic Boundary Strength,"It has long been recognized that spoken language does not necessarily provide reliable cues to indicate word boundaries in all contexts. Turk (2010) hypothesized that prosodic boundary structure is planned in order to achieve smooth signal redundancy (Aylett 2000, Aylett and Turk 2004). On this view, speakers manipulate the strength of boundaries between words in order to make the recognition of each word in an utterance equally likely. Prosodic boundary strength is assumed to inversely relate to language redundancy, i.e., the likelihood of recognition on the basis of non-acoustic information, including the likelihood of the syntactic structure, the likelihood based on the semantic and pragmatic context, lexical word frequency and word bigram frequency. This paper explores the impact of syntactic, word-bigram and word frequency measures (assessed via the ICE-GB, WebCelex, and Google) on the placement and acoustic salience of prosodic word and phrase boundaries in English.",1.0,3.6499999999999995
1286,2419,3.4    3.55    2.75    4.9    ,Do Sentiment Analysis Scores Correlate with Acoustic Features of Emotional Speech?,"Abundant literature has shown that emotional speech is characterized by various acoustic cues. However, most studies focused on sentences produced by actors, disregarding naturally read speech due to the difficulty in finding suitable emotional data. In this contribution we perform an analysis on read speech from audiobooks using the LibriSpeech corpus. 
We computed sentiment analysis scores using tools and lexicons provided by the Python-NLTK library (i.e. Vader and SentiWordnet) at word and sentence levels, and extracted several prosodic parameters (pitch mean, pitch range, pitch stdev, articulation rate, pause-speech ratio) and spectral parameters (shimmer, jitter, HNR, Hammarberg Index, Do1000 and Pe1000). In order to explore the relationship between acoustic and textual metrics, we built mixed-effects models for predicting acoustic metrics from sentiment analysis scores. The effects tend to be small and marginally significant, and the power of the models is usually low. A neural network classifier predicting sentiment (positive, neutral or negative) on the basis of the extracted acoustic cues hardly obtained +0.13 accuracy above chance level.
These preliminary results seem to suggest that the different components used to express emotions (acoustic vs lexical and syntactic) tend to be partially complementary rather than simply additive, at least in read speech.",1.0,3.65
1168,2016,2.5    4.95    2.25    4.9    ,EyeSeg -- Multimodal Speech Segmentation Using Gaze Data and Spectrogram Image Features,"Nearly all automatic speech segmentation approaches rely solely on acoustic features, which differs from the way humans segment speech using phonetic annotation software.

In order to get closer to human-level performance in speech segmentation, we adopt a multimodal approach, which includes image-based segmentation (ISeg) features, and extend it with gaze data obtained from human experts performing a manual segmentation task.
We analyze this database of segmentation behavior, collected using an eye tracker, and introduce gaze as an additional modality in the automatic segmentation process.

Experiments were conducted for automatic speech segmentation, comparing the image-only, ISeg technique, as well as ISeg combined with hidden Markov model (HMM) based acoustic segmentation, with respective segmentation approaches conditioned on the gaze data.
The results show that enhancing the image-based segmentation with gaze information improves the accuracy in both conditions.",1.0,3.65
1293,2442,2.7    4.05    3.6    4.25    ,Causal Analysis of Casual Speech,"While automatic speech recognition (ASR) technology continues to improve with the use of enormous datasets and better methods for modeling acoustic and lexical probabilities, word error rates remain high in many applications, such as far-field ASR. Moreover, the causes of such errors are not well understood. Our ultimate goal is to explain the sources of ASR errors when decoding meeting-based speech and ultimately improve ASR transcription of meetings. This paper performs an error analysis on an ASR transcription of the AMI Meeting Corpus through a causal discovery of the effects of word-based features on ASR transcription. It identifies 16 relevant features related to disfluency, prosody, and pronunciation.",1.0,3.65
1310,2519,5.1    5    2.75    1.75    ,Exploiting Deep Learning for Lip-Reading Driven Speech Enhancement,"In the literature, state-of-the-art audio-only speech enhancement methods, such as Wiener filtering and spectral subtraction, have proven to be ineffective in unpredictable, non-stationary real-world noisy environments. This is typically attributed to their dependence on the noise model, with poor noise estimation leading to residual noise, often resulting in highly distracting perceptual artifacts, known as musical noise. This paper proposes a novel lip-reading driven deep learning framework for speech enhancement that leverages the inherent noise independent property of visual lip-reading cues. A major bottleneck to a successful implementation of such multi-modal speech enhancement method, is lack of accurate and automated lip-reading capability, particularly in speaker independent testing scenarios. For effective lip reading, we developed a novel deep learning based regression model to approximate clean audio features using temporal visual features (i.e. lip reading) only. Comparative simulation results under dynamic real-world noise at different SNR levels (ranging from low to high SNRs) demonstrate the potential of our proposed lip-reading driven speech enhancement framework to produce more intelligible, re-constructed speech as compared to the state-of-the-art audio-only speech enhancement algorithms.",1.0,3.65
1122,1903,3.85    3.35    3.75    ,Multi-language Assessment of Hypernasality Levels in Children with Cleft Lip and Palate,"Cleft Lip and Palate (CLP) patients may experience different speech impairments such as soft voice, omission or substitution of sounds, and hypernasality. The aim of this work is to assess the nasality level of children with CLP based on a nasal sound detector system. Several acoustic features are extracted from nasal speech sounds detected with a pre-trained model based on connectionist temporal classification and recurrent neural networks. Speech recordings of CLP children from Colombia (86 speakers) and Germany (83 speakers) are considered. All of the children were assessed by experts according to a clinical score. A multi-class system is trained with features extracted from the nasal sounds for the automatic evaluation of CLP considering different levels of speech impairments. According to the results it is possible to improve the detection of different hypernasality levels, extracting features from the detected nasal speech sounds.",1.0,3.65
912,1365,5.15    4.4    2.45    2.6    ,Adversarial Multi-label Prediction for Spoken and Visual Signal Tagging,"We introduce an adversarial multi-label classification (ADMLC)  framework to improve the robustness and performance of existing  algorithms on multi-domain signals.  The core contribution of our ADMLC is the innovation of an  `adversarial module' that serves as a critic to provide augmenting information to improve supervised learning in multi label classification (MLC) tasks. Our approach is not intended to be regarded as an emerging competitor for many well-established algorithms in the field. In fact, many existing deep and shallow architectures can all be adopted as building blocks integrated in the ADMLC framework. We show the performance and generalization ability of ADMLC on diverse tasks including documents, audio and image tagging.",1.0,3.65
1275,2374,2.25    4.1    4.25    4    ,What Lies within: Mining Speech-To-Text Customer Transcripts for Automated Problem Remediation,"Technical support services get several thousand voice calls every year that vary across a range of technical  issues for a suite of products. On receiving the call, a support agent creates a service request that contains her interpretation of the customer problem, which is then used in the problem remediation process. However, it has been empirically observed that the actual complaint voiced by the customer is often different from the recorded interpretation in the service request and  runs the risk of missing key information elements present in the customer voice records. In this paper, we build a framework that taps into the speech-to-text transcripts and uses various learning methods to enrich the service requests with additional information. We show that using this enriched data  
for automated problem resolution improves the quality of response from an information retrieval system.",1.0,3.65
761,66,3.8    4.25    4    2.55    ,Prosodic and Voice Quality Analyses of Offensive Speech,"In this study, differences in the acoustic-prosodic features are analyzed in the low-moral or offensive speech. Discrimination of the offensive speech is thought to be important for human-robot interactions. The same contents were spoken by multiple speakers with different speaking styles, including reading out, aggressive speech, extremely aggressive (frenzy), and joking styles. Acoustic-prosodic analyses indicated that different speakers use different speaking styles for expressing offensive speech. Clear changes in voice quality, such as tense and harsh voices, were observed for high levels of expressivity of aggressiveness and threatening.",1.0,3.6500000000000004
1176,2046,5.2    4.15    1.75    3.5    ,Exploiting Gender-specific Gaussian Posteriorgram for Query-by-Example Spoken Term Detection,"This work presents the use of gender-specific knowledge in Gaussian posteriorgram for Query-by-Example Spoken Term Detection (QbE-STD) task. The Gaussian posteriorgram based representation is widely used in QbE-STD system. The Gaussian posteriorgrams are obtained from the wide range of speakers and thus, expected to be speaker-independent. However, during audio matching, due to spectral variation among two speakers, the acoustic features exhibit different characteristics and hence, Gaussian posteriorgram may also vary. This may result in poor template matching even in the case of same lexical information in the audio. To overcome this, we proposed to use Gaussian posteriorgram obtained using two separate Gaussian Mixture Models (GMMs), characterized by the two different spectral scaling. To estimate the spectral scaling, we use Vocal Tract Length Normalization (VTLN) approach and create two different GMMs representing each gender based on the VTLN warping factor. During audio matching, we select the GMM based on the VTLN warping factor. It can be observed that proposed gender-identification based framework improves the performance by around 3 % for MAP and p@N in TIMIT and also. The experiments conducted on Spoken Web Search (SWS) 2013 task also reveal the importance of spectral information normalization via VTLN.",1.0,3.6500000000000004
1088,1818,4.1    4.95    3.15    2.4    ,"Exploring Acoustic Measures of Vowels (VSA, FCR3, VAI4, VFR) in Children with Hearing Impairment","Acoustic analysis of speech contributes in understanding the articulatory dynamics. Hence, it can be utilized for severity assessment of speech disorders and also for monitoring the efficacy of treatment. Purpose of this study was to evaluate the sensitivity of four derived acoustic measures of speech in Hindi speaking children. The study was conducted in two groups of participants, one with hearing impairment (Group I, n=16) and  another with normal hearing (Group II, n=30). The derived measures were vowel space area (VSA), formant centralization ratio (FCR3), four vowel articulation index (VAI4) and the vocalic anatomical functional ratio (VFR). The measures were derived from formant frequencies (F1 and F2) of vowels /a/, /i/, /u/, and /ae/. Parametric statistical analysis was performed to compare these measures across the groups. Results indicated a statistically significant difference in VSA, FCR3, VAI4, and VFR between the groups. VSA was found to be the most sensitive measure. The study indicated that derived acoustic measures were affected in children with hearing impairment. Understanding of these key acoustic features can aid in developing automatic speech assessment tools and in evaluating the efficacy of therapy techniques for children with hearing impairment.

Index Terms: vowel space area, formant frequencies, hearing impairment.",1.0,3.6500000000000004
858,1200,4.9    2.35    4.9    2.45    ,Siamese Hierarchical Attention Networks for Extractive Summarization,"In this paper, we present an approach to document summarization
based on Siamese Neural Networks. Specifically, we propose
to use Hierarchical Attention Networks to select the most
relevant sentences of a text to make its summary. We train
Siamese Neural Networks using document-summary pairs to
determine whether the summary is appropriated for the document
or not. By means of the sentence-level attention mechanisms
the most relevant sentences in the document can be identified.
Hence, once the network is trained, it can be used to
generate extractive summaries. The experimentation carried out
using the CNN/DailyMail summarization corpus shows the adequacy
of the proposal. Our approach obtains results in-line
with the state-of-the-art and it is generally much faster to train
than other models.",1.0,3.6500000000000004
1245,2252,2.65    4.3    4.2    3.5    ,Energy Separation Algorithm Based Spectrum Estimation for Very Short Duration of Speech,"In this paper, we propose a novel method of estimation of short-time spectrum for analysis of speech signals in the closed phase regions of glottal activity. This method uses Teager Energy Operator (TEO) and a related Energy Separation Algorithm (ESA) iteratively, along with the design of resonator to estimate formants from a very short duration of the speech. The spectrum of the cascade of these four resonators is referred to as our proposed ESA-based spectrum. The novelty of the proposed approach lies in using very short duration of analysis speech frame synchronized with glottal closure instant (i.e., about 1-2 ms) to estimate the proposed spectrum in order to ensure that the vocal tract system characteristics do not change much within this interval and to alleviate erroneous estimation of formants due to nonlinear interaction of excitation source with the vocal tract system. To demonstrate the effectiveness of proposed algorithm for formant estimation on speech data, we have used 1.5 ms speech signal corresponding to closed phase glottal cycles derived from a male speaker of CMU-ARCTIC database.",1.0,3.6624999999999996
755,49,4    2.45    3.8    4.4    ,A Study on Landmark Verification of Mandarin Alveolar-palatal Consonants,"Due to trait of the Mandarin syllable structure, mandarin alveolar-palatal consonants ʨ(j), ʨʰ(q), ɕ(x) can only be followed by vowels starting with i(i) or y(ü), it probably means that the Mandarin acoustic landmarks of these phones may be different from the English acoustic landmarks. According to this hypothesis, this paper proposes methods to verify landmarks of Mandarin alveolar-palatal consonants by perceptual experiments and phones classification experiments. In the meantime, it is found that mandarin alveolar-palatal consonants are common pronunciation errors of L2 learners. Based on the result of the landmark verification experiments, we apply the result to pronunciation error detection to validate it. The detection performance becomes satisfactory if the   proposed landmark positions are applied.  Eventually, the experimental results illustrate that the acoustic landmarks of Mandarin alveolar-palatal consonants ʨ(j), ʨʰ(q), ɕ(x) are located at both midpoint and end of the phones, which is not consistent with the English acoustic landmarks.",1.0,3.6625
1265,2337,1.8    5.75    4.5    2.6    ,Background Scream Detection in Speech from the Entropy of Cepstral Coefficients,"This paper proposes an unsupervised method for the detection of background scream in the midst of speech. This temporal analysis of the entropy of Cepstral coefficients is executed to this effect. Entropy is a measure of the chaos or uncertainty involved with a data distribution. The Cepstral coefficients emanating from speech frames tends towards a more uniform distribution for the scream duration leading to higher chaos in that interval. The entropy of each of the 39 Mel Frequency Cepstral Coefficients emanating from every frame in the speech segment are added up, and the peaks in the summation entropy graph along the time-axis indicate the presence of scream if a threshold is crossed. The non-extensive entropy is used for the summation to be on the safe side, since the sum of non-extensive entropies excludes background noise. The experiments performed by superimposing scream samples at fixed intervals in the benchmark IITKGP-SEHS Hindi language speech corpus confirm the utility of our approach.",1.0,3.6625
759,59,5    4.05    2.3    3.3    ,Measuring Prosodic Transfer in Vector Space by Weighted Tonal Events,"Prosodic transfer plays a significant role in forming stereotyped foreign accent. To polish foreign accent native-like, in addition to appropriate drills, prosodic transfer quantification is in need for learners to locate their positions and make corresponding adjustments. We advance a method to automatically calculate prosodic distance in vector space by the weights of each prosodic event abstraction, that is, the similarity of listed ToBI annotation. This is an attempt to measure prosodic transfer in such a high generalized level rather than on raw or processed fundamental frequency. The experiment exhibits the separation in high dimensional vector space of NINGBO and US speakers with different aggregation degrees in categorized non-native subjects, and thus proves a success in calculating prosodic transfer with discrete representations of intonation. Hopefully, the approach would be of help for language learners to take control of their learning cycles.",1.0,3.6625000000000005
1242,2244,4.95    2.8    3.05    4.3    3.25    ,Improved Detection of Spoofed Speech Using Efficient Scoring with Frame Selection,"In this paper, we introduce a frame selection strategy for improved detection of spoofed speech in the presence of voice conversion (VC) and speech synthesis (SS) attacks. A state-of-the-art counter-measure (CM) system uses Gaussian mixture model (GMM) based classifier for computing the log-likelihood scores. Here the average log-likelihood ratio for all speech frames of the test utterances is calculated as the score for the decision making process. As opposed to this conventional approach, we propose to use selected speech frames of the test utterance in the scoring process. We present two simple, computationally efficient frame selection strategy based on the log-likelihood ratio of the individual frames. The performance is evaluated with constant Q cepstral coefficients (CQCCs) as front-end and two-class GMM as back-end. We conduct the experiments using ASVspoof 2015 corpus that includes spoofed voice created with 10 different methods. The results show that the CM systems based on the proposed scoring technique substantially outperform CM systems based on the conventional scoring technique for both known and unknown attacks. With one of our proposed method, we have obtained 0% EER for all attacks in the evaluation set of ASVspoof 2015 corpus.",1.0,3.6700000000000004
790,1028,3.35    4.8    4    2.55    ,Vocal Attractiveness in Bilingual Charismatic Political Speakers,"Charismatic leaders use voice quality manipulation to share their beliefs and persuade listeners to vote for them. This manipulation is adapted to the context of communication, i.e. the environment in which the persuasive communication takes place. The speakers' vocal patterns convey information about themselves such as their psychological and emotional states to produce a particular effect on listeners. These extralinguistic aspects of communication are intentionally controlled by the speaker to achieve the pragmatic aspects of communication and influence listeners' choice of leadership. Speech data were collected from two bilingual politicians, Jeb Bush and Marco Rubio, during television interviews in American English and American Spanish. The goal was to assess leaders' vocal attractiveness in these two languages and to evaluate the role of their voice quality manipulations to convey personality traits and to alter listeners' emotional states and affect their voting preference significantly.",1.0,3.675
1098,1850,3.5    3.3    2.8    5.1    ,Likability Estimation for Call-center Agents Based on Part-wise Acoustic and Lexical Features,"This paper proposes a new likability estimation technique for long speech segments of call-center agents.
Since most conventional methods for speaker likability focus on short speeches and employed only acoustic features, they fail to well identify the factors determining likability.
To tackle long speech segments in call-centers, we analyze acoustic and lexical features of the parts of long speech samples that are identified as either `likable' or `non-likable'.
We focus on the fact that likable agents maintain good likability from start to finish, while non-likable agents cannot.
As most agents use very similar typical opening utterances in the first half of each call, we divide the speech samples in half, and extract part-wise acoustic features from the divided parts.
As part-wise lexical features, we use also the frequency of emotional connection words, such as appreciation, apology and excuse.
Our proposal yields better accuracy, 73.7%, than the previous method, which attains 68.9%.",1.0,3.675
1311,2526,2.5    3.4    5.8    3    ,Cross Corpus Detection of Dysarthria Presence,"Developing models for automated detection of dysarthria presence has important implications for clinical practice as well as the modification of speech systems for improved usability by persons with disordered speech. A robust model requires a large and diverse corpus of dysarthric speech.  The limited size of individual clinical datasets motivates their combination into cross-corpus study.  This study progresses work on cross-corpus training for dysarthric speech and includes the Universal Access Research (UA-Speech), TORGO, and Atlanta Motor Speech Disorders Corpus (AMSDC) corpora.  Cross corpus training requires normalization of each corpus to alleviate differences in collection methods. However, the AMSDC has only dysarthric speech making it unsuitable for corpus level normalization prior to combination with other corpora that have speakers with dysarthria and controls.  Global normalization of AMSDC with another database was examined but is not an ideal solution and produced low accuracy overall (45.75%).  A corpus task normalization procedure is proposed that yielded significant improvement over global normalization while allowing for AMSDC to still be normalized on its own data before combination in cross training.  A leave-one-out test of AMSDC cross trained with UA-Speech and TORGO resulted in an improved overall recognition accuracy of 84.08%.",1.0,3.675
1290,2435,3.25    4.05    3.55    3.85    ,Learning Deep Speaker Embeddings for Speaker Identification and Verification,"Learning effective feature representations is critical for speaker recognition. Such effective features should have low intra-speaker but high inter-speaker variance. Feature learning based on Deep Convolutional Neural Networks has become the state-of-the-art. However, networks with standard softmax-loss only focus on achieving good inter-speaker separation without considering intra-speaker variance. In this paper, we explore different network loss functions, such as the triplet-loss and center-loss, in order to consider both inter and intra-speaker variance jointly during speaker embeddings training. We also apply L2-constraint on the regular softmax-loss, which leads to increase of inter-class variance and decrease of intra-class variance simultaneously. Our experiments on the LibriSpeech database show that the proposed speaker embeddings learning strategy, which aims to optimize both inter and intra-speaker vairance jointly, significantly outperforms the baseline system with regular softmax-loss on both Accuracy (ACC) and Equal Error Rate (EER) measurements. Compared to the regular softmax-loss baseline system, we achieve 1.6\% and 15.9\% relative improvement on the ACC and EER respectively under the 3s short-duration test and 1.3\% and 13.8\% on the ACC and EER respectively under the full-duration test. These results also show that our methods are more robustness and effectiveness under the short-duration conditions.",1.0,3.675
889,1303,5.2    3.95    3.4    2.95    2.9    ,Deep-learning Approach for Spoofing Attacks Detection in Telephone Channel,"Automatic Speaker Verification Spoofing and Countermeasures (ASVspoof) initiative has significantly pushed forward the development of spoofing detection methods for voice verification systems. ASVspoof Challenges in 2015 and 2017 confirmed the impressive perspective in detection of unforeseen spoofing trials in microphone channel. However, telephone channel, used for different ASV applications, presents much more challenging
conditions for spoofing detection, due to limited bandwidth, various coding standards and channel effects. The main purpose of this research was to investigate the applicability of a deep learning approach to obtain a single system for the detection of
various spoofing attacks on telephone channel, and to assess it on more realistic data. The experiments for data in real telephone channel showed insufficient performance of the systems trained on data from emulated telephone channel. The database used here was collected to exemplify the most threatening types of real spoofing attacks in telephone channel. This paper describes both this database and the proposed CNN-based system
for spoofing detection in telephone channel.",1.0,3.6799999999999997
1274,2369,3.45    3.3    4.3    ,The Intelligent Voice System for the DIHARD Speaker Diarization Challenge,"This paper describes the Intelligent Voice (IV) speaker diarization system for the first DIHARD challenge. The aim of this challenge is to provide an evaluation protocol to assess speaker diarization on more challenging domains with the speech across a wide array of challenging acoustic and environmental conditions. We developed a new frame-level speaker diarization build on the success of deep neural network based speaker embeddings, known as d-vectors, in speaker verification systems. In contrary to acoustic features such as MFCCs, frame-level speaker embeddings are much better at discerning speaker identities. We perform spectral clustering on our proposed LSTM-based frame-level speaker embeddings to generate speaker log likelihood for each frame. An HMM is then used to refine the speaker posterior probabilities through limiting the probability of switching between speakers when changing frames.",1.0,3.6833333333333336
731,2529,5.15    2.4    3.5    ,"Indian languages ASR: A multilingual phone recognition framework with IPA based common phone-set, predicted articulatory features and feature fusion","In this study, a multilingual phone recognition system for four Indian languages - Kannada, Telugu, Bengali, and Odia - is described. International phonetic alphabets are used to derive the transcription. Multilingual Phone Recognition System (MPRS) is developed using the state-of-the-art DNNs. The performance of MPRS is improved using the Articulatory Features (AFs). DNNs are used to predict the AFs for place, manner, roundness, frontness, and height AF groups. Further, the MPRS is also developed using oracle AFs and their performance is compared with that of predicted AFs. Oracle AFs are used to set the best performance realizable by AFs predicted from MFCC features by DNNs. In addition to the AFs, we have also explored the use of phone posteriors to further boost the performance of MPRS.We show that oracle AFs by feature fusion with MFCCs offer a remarkably low target of PER of 10.4%, which is 24.7% absolute reduction compared to baseline MPRS with MFCCs alone. The best performing system using predicted AFs has shown 2.8% reduction in absolute PER (8% reduction in relative PER) compared to baseline MPRS.",0.0,3.6833333333333336
1084,1813,4.2    3.6    2.75    4.2    ,CNN-based Segment-Level Pyramid Match Kernel for the Classification of Varying Length Patterns of Speech,"Convolutional Neural Networks (CNNs) and the features learnt from CNNs have achieved state-of-the-art performance when used for different tasks like large vocabulary continuous speech recognition, spoken language identification, speaker verification, speech emotion recognition etc. Conventionally CNNs are built by considering fixed size context segments as input. In this work, we attempt to consider varying length speech as input to CNN. We propose to use spatial pyramid pooling (SPP) layer to remove the fixed-length constraint and to allow the original varying length speech features as input to train CNN. This new
architecture results in set of varying size feature maps. We propose a novel CNN-based segment-level pyramid match kernel (CNN-SLPMK) between a pair of set of varying size feature maps corresponding to a pair of speech signals. The CNN-SLPMK is used for the classification of varying length patterns of long duration speech represented as varying size feature maps using support vector machines (SVMs). This kernel is designed by partitioning the feature map into increasingly finer segments and matching the corresponding segments. We demonstrate the effectiveness of proposed approaches by achieving state-of-the-art results for speech emotion recognition task.",1.0,3.6875
1069,1774,4.9    2.75    1.85    5.25    ,Task-Oriented Dialog System with Rich Context Awareness,"Intelligent ""task-oriented"" dialog systems (TDSs) have revolutionized the way in which users interact with mobile devices by providing easy accessibility and management of schedules, activities, transactions/reservations, and social interactions. However, most existing TDSs are constrained in scope due to their command-driven nature, weak modeling of conversation context, and huge data requirement. To address this, we propose a collaborative, context-aware, natural language agent that utilizes inference and execution models to interact with the user. Due to the modular nature of the system, it is capable of leveraging advanced NLU techniques and the performance is not restricted by data availability. We demonstrate our approach in the domain of a calendar management system, where the system achieves ~68% accuracy in complex real-world settings. Qualitative comparison with several popular conversational agents indicate the rich context awareness of our technique.",1.0,3.6875
822,1127,5    2.25    4.9    2.6    ,Acoustic Scene Classification Based on Dense Convolutional Networks Incorporating Multi-channel Features,"Motivated by the state-of-the-art performance of Dense Convolutional Networks (DenseNet) on highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN and ImageNet), this work presents improvements to the Acoustic Scene Classification (ASC) task of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE 2017) based on an optimized DenseNet model. Multi-channel Convolutional Neural Network (CNN) is also explored to extract features from different audio channels in an end-to-end manner. In the experiments, the proposed model is compared with the challenge baseline model of DCASE 2017 and several other state-of-the-art CNN architectures subject to the classification accuracy on the same open-source DCASE datasets. The results show that the proposed DenseNet-based architecture can achieve a superior performance in classification accuracy but with a lower model complexity in contrast with other models.",1.0,3.6875
936,1429,4.25    4.65    3.05    2.8    ,Two-stage Strategy for Small-footprint Wake-up-word Speech Recognition System,"In this paper, we propose a small-footprint wake-up-word speech recognition (WUWSR) system with two stages to recognize a two-syllable wake-up word. In the first stage, convolution neural network (CNN) is trained to predict the posterior probability of context-dependent state. Thus a wake-up-word is detected according to the confidence score obtained by dynamic programming. In the second stage, we cascade bidirectional LSTM, convolutional modules and deep feed-forward network (BLCDNN) successively to verify the detection. In addition, without the intervention of any decoding modules, the proposed system can guarantee low latency. The experimental results show that our system, named CNN-BLCDNN, reaches high accuracy and maintains low false alarm rate.",1.0,3.6875
1216,2163,4.4    3.35    3.6    3.4    ,A Multi-pass SLU System Enriched by Semantic Metalabels,"This paper presents a method to help disambiguate spoken language understanding (SLU) outputs through a multi-pass approach. The proposed method is based on the generalization of the semantic labels targeted by the SLU system and their integration as prior knowledge in the next passes. The proposed generalization reduces the number of labels by merging some of them, especially the most confusing ones, into the same metalabel. Hence, reducing the task complexity, we expect the first pass of the SLU system to be more efficient at a higher concept level. As awaited, SLU outputs labeled in a first pass by metalabels contain less errors. These metalabels are then used to enrich inputs of next passes. Several techniques are explored to benefit from this information like word features or multi-system voting. Experiments were carried on the French MEDIA corpus focusing on hotel reservation and show a significant improvement of the SLU system performances with a relative reduction of 2.4% of concept/value error rate and 3.9% of concept error rate (CER) in comparison to our baseline neural SLU system.",1.0,3.6875
986,1567,4.4    2.55    4.25    3.55    ,Dialogue Strategies for Open Domain Dialogue Systems: Evaluating an Alexa Prize Bot,"The greatest challenges in building sophisticated open-domain
conversational agents arise directly from the potential for ongoing
mixed-initiative multi-turn dialogues, which do not follow a
particular plan or pursue a particular fixed information need. In
order to make coherent conversational contributions in this context, a
conversational agent must be able to track the types and attributes of
the entities under discussion in the conversation and know how they
are related.  In some cases, the agent can rely on structured
information sources to help identify the relevant semantic relations
and produce a turn, but in other cases, the only content available
comes from search, and it may be unclear which semantic relations hold
between the search results and the discourse context. A further
constraint is that the system must produce its contribution to the
ongoing conversation in real-time.  This paper describes the dialogue
manager for SlugBot for the 2017 Alexa Prize. We discuss SlugBot's
strategies for managing open domain dialogue, and present an
evaluation of how different strategies performed in the 2017
challenge, where SlugBot was able to carry on conversations whose
length over the semi-finals user evaluation period averaged 8:17
minutes.",1.0,3.6875
502,1928,2.75    4.2    3.55    4.25    ,Enhancement of Noisy Speech Signal by Non-Local Means Estimation of Variational Mode Functions,"In this paper, a speech enhancement approach exploiting the efficacy of non-local means (NLM) estimation and variational mode decomposition (VMD) is proposed.  The NLM estimation is effective in removing noises whenever non-local similarities are present among the samples of the signal under consideration. However, it suffers from the issue of under-averaging in those regions where amplitude and frequency variations are abrupt. Since speech is a non-stationary signal, the magnitude and frequency vary over the time. Consequently, NLM is not that effective in removing the noise components from the speech signal as observed in the case of image enhancement. To address this issue, the noisy speech signal is first decomposed into variational mode functions (VMFs) using VMD. Each of the VMFs represents a small portion of the overall frequency components of the signal. The VMFs are then combined into different groups depending on their similarities to reduce computational cost. Next, the non-local similarity present in each group of VMFs is exploited for an effective speech enhancement through NLM estimation. The enhancement performance of the proposed method is compared with two existing speech enhancement techniques. The experimental results presented in this study show that, the proposed method provides better speech enhancement performance.",0.0,3.6875
1225,2182,2.2    4.25    5.05    3.25    ,Multi-category Audio Classification Using BoAW,"Audio classification is a necessary prerequisite for most analysis applications. This work proposes a novel approach for classifying speech, music, environmental sounds and their pair-wise combinations. Representative audio words are separately estimated from frame level temporal and spectral features of speech, music and environmental sounds. These are used to construct Bag of Audio Words BoAW histograms for audio intervals. Ensemble of classifiers trained on these BoAW representations are used for multi-category audio classification. Another contribution of this work involves the construction of manually annotated audio dataset obtained from 5 hours of Indian movies. Our proposal is validated on this dataset leading to an overall F-score of 82%.",1.0,3.6875
896,1323,2.7    4.3    2.55    5.2    ,LSTM-Based Pitch Range Estimation from Spectral Information of Brief Speech Input,"In human speech communication, pitch can be normalized automatically by the listener through a subjective estimation of the speaker’s overall pitch range, even from a very brief speech input. In speech technologies, pitch range estimation used to be done on the basis of the maximum and minimum F0 values detected from a lengthy speech input, but estimation from a brief speech input has yet to be solved. In this study, we proposed a novel method of estimating pitch range from the spectral structure of a very brief speech input, using machine learning techniques to simulate the perceptual mechanisms of human beings. Our experiment on a 56-hour speech corpus recorded from 166 speakers (158 for training and 8 for testing) showed that the estimation converged when the speech input was as short as 300-500ms. The mean absolute percentage errors (MAPE) for pitch level and pitch span were 2.3% and 12%, respectively, verifying the validity of the proposed method.",1.0,3.6875
1197,2116,3.4    4.1    3.45    3.85    ,Densely Connected Progressive Multitask Learning for LSTM-based Speaker-independent Child Speech Separation,"Extracting the child speech from realistic audio with interfering adult speech is quite an important step for many child applications, e.g., child language acquisition. However, the related research is quite limited. In this study, we propose a speaker-independent model for child speech separation via a novel long short-term memory (LSTM) architecture. First, by measuring the dissimilarities between children and adults using i-vectors, we demonstrate that the distances between children and adults are large enough to guarantee a possible separation. Accordingly, we present a LSTM architecture with the design of hidden layers via densely connected progressive learning and output layer via multiple-target learning for speech separation between child group and adult group. Experimental results on the simulation data demonstrate that the proposed approach can yield consistent and significant gains of PESQ, STOI, and SSNR measures over LSTM baseline for child speech separation. Furthermore, our preliminary results on SeedLing corpus with realistic recordings for child language acquisition show that our approach can achieve better overall separation performance than LSTM baseline based on the spectrograms, implying the potential for the child-involved diarization.",1.0,3.6999999999999997
1266,2340,4.1    2.9    5.05    2.75    ,Characterizing Sub-Types of Infant Cries Due to Different Cry-Causes,"Automatic identification of infant cry-causes can help developing multiple assistive technologies for infant healthcare. Hence, it is important to first characterise and understand the differences amongst different cry-causes and their phonation sub-types. In this paper, an attempt is made to first categorize the infant cries as per different cry-cause factors, and then characterise the sub-types of infant cries. Infant cries are first categorized broadly into severe and non-severe categories. In each category, the cry-sub types, namely, growl, shrill, squeal, moan and strained etc. are characterised. Initial observations are made from the spectrograms. Then differences are examined in the production characteristics using signal energy and spectral sub-band energy features. Peculiar characteristics of the cry-causes categories are then analysed by examining changes in the vocal tract filter characteristics, the five Formants (F1 to F5). Effectiveness of these production features in characterising the cry-cause categories and cry sub-types is validated by classifying these, using variants of K-Nearest Neighbors method. Encouraging results are obtained. The insights gained in this study may be helpful in understanding the dynamic nature of changes in the infant cries, during their production. This in-turn can help towards developing related assistive technologies and systems for infant healthcare.",1.0,3.7
983,1557,3.45    3.9    4.2    3.25    ,Improving Mandarin-English Code-switching Speech Recognition with a Data-Driven Lexicon Optimization Approach,"A data-driven lexicon optimization approach is proposed in this paper for Mandarin-English code-switching speech recognition tasks. The goal is to map words of embedded language to phone set of matrix language, and two steps are included. Firstly the English (embedded language) lexicon of Mandarin (matrix language) phone set is generated by multiple strategies to obtain the initial lexicon. The the filtering-and-expanding optimization is conducted to improve the quality of initial lexicon, and finally frequency-based filtering are achieved to keep the lexicon in an acceptable size. By the proposed approach, the English words are well mapped to the Mandarin phones to form a high quality lexicon. This approach can also be carried out on monolingual recognition tasks to improve the performance of traditional speech recognition. Experiments on Mandarin-English code-switching speech recognition tasks show that, with the proposed approach, a 24.28% relative reduction on word error rate (WER) of English words is achieved, where the mix error rate (MER) relative reduction of total testset is 7.54%.",1.0,3.7
1058,1738,3.45    3.05    3.4    4.9    ,Broad Phoneme Class Specific Deep Neural Network Based Speech Enhancement,"In this work, we present a speech enhancement algorithm using broad phoneme class specific deep neural networks (DNNs).We build a classifier network to obtain the probabilities of each class in every test frame. After training the individual DNNs separately, we jointly train the whole system with the final output as the linear combination of the outputs from these class specific DNNs with combination weights as the probabilities predicted by the classifier network. We experiment with two (vowel and non-vowel) and five (vowel, stop, fricative, nasal and silence) broad phoneme classes. We also experiment with two target outputs viz clean speech log spectrum and speech presence probability (SPP). Experiments are performed using speech data from TIMIT corpus, nine noise types (four seen and five unseen), and four SNR conditions. Experimental results show that the proposed class specific DNN approach outperforms the single DNN based speech enhancement scheme for both seen and unseen noise types in all SNR conditions considered.",1.0,3.7
1143,1969,2.95    4.05    3.5    4.35    ,Automatic Recognition of Autism Spectrum Disorders from Speech,"Autism spectrum disorder (ASD) is a neurodevelopmental disorder characterised by impaired social communication and restricted and repetitive behaviours. Diagnosis of ASD is based on clinical expertise and can be  costly and time consuming. Methods that could help streamline this process, including screening for children at high risk of the disorder, are   needed. In this paper we propose a method to compute highly discriminative features from speech signals which could be developed to identify ASD. This allows for non-invasive identification from speech alone. Our method achieves a classification accuracy as high as 98.96\% for a dataset of 49 speech recordings. We also report on the effect of the speech segmentation parameters (in particular the tolerance of silence) towards the classification accuracy. Finally, using two-sample Kolmogorov-Smirnov test, we show the high discriminative power of the selected features. We have evolved a general method which works not only well on our dataset but is transferable to other such two-label classification tasks.",1.0,3.7125
1005,1611,5    3.25    2.35    4.25    ,Compensating Utterance Information in Fixed Phrase Speaker Verification,"This work investigates on explicitly utilizing utterance information for fixed phrase speaker verification (SV). In this scenario, the same phrase is spoken by the speakers during the training and testing sessions. In other words, the speaker model possesses both speaker as well as utterance information. Therefore, there is a potential to improve the speaker characterization by compensating the utterance information. In this work, we propose a framework to compensate the utterance information, which is used to normalize the lexical content. A hidden Markov model (HMM) based triphone model is considered as a universal background model (UBM). It is used for adapting the speaker-utterance model and the background utterance model in the proposed utterance compensation framework. Given a test utterance and a claimed speaker-utterance model, the UBM as well as background utterance model is utilized for compensating the average speaker information and the lexical content information, respectively. The studies are conducted on RSR2015 database, which reveal the importance of the proposed utterance compensation framework as compared to the framework without utterance compensation.",1.0,3.7125
959,1496,3.7    2.4    4.25    4.5    ,An Effective Discriminative Learning Approach for Emotion-Specific Features Using Deep Neural Networks,"Speech contains rich yet entangled information ranging from phonetic to emotional components. These different components are always mixed together to hinder certain tasks from achieving better performance. Therefore, automatically learning a good representation that disentangles these components is non-trivial. In this paper, we use deep neural networks (DNNs) to extract emotion salient features from frame-based low-level features such as MFCCs for speech emotion recognition. Most of the available DNNs for such recognition tasks use softmax loss to supervise the system to learn inter-class separable features. However, softmax loss does not facilitate the intra-class compactness of the learned features. Inspired by recent progress in face recognition, we introduce centre loss as a complementary supervision signal to enhance the discriminative power of the deep learning features. With the joint supervision of these two loss functions, we can train the DNNs to obtain separable and discriminative emotion-specific features. Experiments on CASIA Chinese Emotional Corpus show that our approach substantially boosts the performance of systems that are trained with softmax loss alone.",1.0,3.7125
1288,2424,2.5    4.35    2.9    5.1    ,"BandNet: a Neural Network-based, Multi-Instrument Beatles-Style MIDI Music Composition Machine","In this paper, we propose a recurrent neural network (RNN)-based MIDI music composition machine that is able to learn musical knowledge from existing Beatles' songs and generate music in the style of the Beatles with little human intervention. In the learning stage, a sequence of stylistically uniform, multiple-channel music samples was modeled by a RNN. In the composition stage, a short clip of randomly-generated music was used as a seed for the RNN to start music score prediction. To form structured music, segments of generated music from different seeds were concatenated together. To improve the quality and structure of the generated music, we integrated music theory knowledge into the model, such as controlling the spacing of gaps in the vocal melody, normalizing the timing of chord changes, and requiring notes to be related to the song's key (C major, for example). This integration improved the quality of the generated music as verified by a professional composer. We also conducted a subjective listening test that showed our generated music was close to original music by the Beatles in terms of style similarity, professional quality, and interestingness. Generated music samples are at 
\href{https://goo.gl/uaLXoB}{https://goo.gl/uaLXoB}",1.0,3.7125
1039,1691,3.1    4.55    3.55    3.65    ,Speech Dereverberation under Mismatched Condition Using Frequency-Domain Cycle-Consistent Adversarial Networks,"To train a speech dereverberation model, current deep learning-based models require datasets where clean and reverberant speech exist in pairs. However, this is problematic in real-world applications where paired clean speech is often not available. Motivated by recent advances in generative adversarial network (GAN) techniques, we propose a novel speech dereverberation model that can be used in cases where paired speech data cannot be easily obtained.  We use cycle-consistent adversarial networks (CycleGAN) for the translation between reverberant and clean speech. Furthermore, we add an auxiliary detail regressor to accommodate the perceptual details in a time-frequency representation. We evaluate the proposed method on mismatched conditions where the training and the test data hold different characteristics (e.g., different languages, different acoustic environments). Experimental results show that the proposed method outperforms the paired training methods in the mismatched conditions, and suggest the possibility of applying our method to real-world applications.",1.0,3.7125
586,2133,4.05    2.45    5    3.35    ,Development of Large Vocabulary Speech Recognition System with Keyword Search for Manipuri,"Research in Automatic Speech Recognition (ASR) has witnessed a steep improvement in the past decade (especially for English language) where the variety and amount of training data available is huge. In this work, we develop an ASR and Keyword Search (KWS) system for Manipuri, a low-resource Indian Language. Manipuri (also known as Meitei), is a Tibeto-Burman language spoken predominantly in Manipur (a northeastern state of India). We collect and transcribe telephonic read speech data of 90+ hours from 300+ speakers for the ASR task. Both state-of-the-art Gaussian Mixture-Hidden Markov Model (GMM-HMM) and Deep Neural Network-Hidden Markov Model (DNN-HMM) based architectures are developed as a baseline. Using the collected data, we achieve better performance using DNN-HMM systems, i.e., 13.57% WER for ASR and 7.64% EER for KWS. The KALDI speech recognition tool-kit is used for developing the systems. The Manipuri ASR system along with KWS is integrated as a visual interface for demonstration purpose. Future systems will be improved with more amount of training data and advanced forms of acoustic models and language models.",0.0,3.7125
893,1314,4.4    4.15    2.5    3.8    ,Using Deep Neural Network to Evaluate Child-Speech Feature Detection and Classification for Clinical Applications,"In this paper we report the results of a phoneme classification system modeled on the current clinical comprehension of phonological development in children. We propose Deep Neural Networks are applied at segmental and sub-segmental levels to gain better performance. The interrelation between a test word-pair, the corresponding speech pattern, and clinical knowledge of typical phonological errors is being modelled first time on child-speech. An appropriate threshold based on a distribution of pronunciation errors for each phoneme along with the likelihood of the network to identify a phoneme is integrated to achieve the phoneme identification. Improvement in error rate because of the segmental feature segregation are confirmed in the classification and error detection experiments. Goodness of Pronunciation scores based on log likelihood ratios were tested along with phonetic error rate on a corpus built from recording clinically used word-pair from 138 children aged 3-11 years. On comparing with non-DNN based models such as GMM-HMM, an improvement of 13.6% is observed. The results of this study indicate a positive feasibility for integration into clinical applications to aid children's speech therapy",1.0,3.7125000000000004
1317,2564,3.55    3.35    4.25    ,Study of Voicebars and Nasals for the Detection of Hypernasality,"In this work, acoustic similarity of voice bars with that of nasals in hypernasal speech is exploited  to propose a  method for hypernasality detection. The acoustic-phonetic features namely strength of excitation (SoE), location of first formant ($F_1$), and ratio between the strengths of first ($A_1$) and second ($A_2$) dominant resonances ($A_1-A_2$ ratio in dB) are computed using epoch synchronous approach. These three features namely SoE, $F_1$, and $A_1-A_2$ ratio in dB are indicators of abruptness of glottal closure, vocal tract constriction, and introduction of high frequency resonances, respectively which are distinct in nature for voice bars and nasals.   It is observed that effect of hypernasality shifts the distributions of these features for the voice bars towards that of  nasals. A voice bar based hypernasality detection system using these features is demonstrated with an accuracy of $78.84\%$.",1.0,3.716666666666667
1284,2404,3.95    4.05    2.7    4.2    ,Speech Markers in Abstinent and Current Cocaine Users,"One of the main foci of addiction research is the delineation of markers that track the propensity of relapse. Speech analysis can provide an unbiased assessment that can be deployed outside the lab, enabling objective measurements and relapse susceptibility tracking. This work is the first attempt to study unscripted speech markers in cocaine users. We analyzed 23 subjects performing two tasks: describing the positive consequences (PC) of abstinence and the negative consequences (NC) of using cocaine. Clinical variables such as the Cocaine Selective Severity Assessment (CSSA) that assesses withdrawal signs and symptoms, Beck Depression Inventory (BDI), and days of abstinence from cocaine (DoA) were obtained. We computed acoustic features (AF) using OpenSmile and Praat toolboxes, and semantic features using GloVe. Significant correlations were found with AF and DoA for PC (R2=0.51), and AF and CCSA for NC (R2=0.48). For BDI, the most significant correlation was found for semantic features for NC (R2=0.42). We also performed binary classification to identify the previous 30-day abstinence period using only AF, obtaining accuracies of 0.81(NC) and 0.89 (PC). Our results show that free speech may be used as a surrogate of clinical variables and as a predictor to identify abstinence in cocaine users.",1.0,3.7249999999999996
783,1004,4.1    5.05    2.45    3.3    ,Robust Sonorant/Non-sonorant Segmentation Using Sonority Feature,"Sonorant segments present in a speech signal are of crucial importance as these are perceptually important regions and robust to detect in adverse conditions. Therefore, segmentation of sonorant regions is of great importance for the applications like automatic speech recognition systems, audio search systems and for automatic segmentation of speech corpora. The primary issue in sonorant segmentation is the confusion between weakly sonorant and non-sonorant sounds. The sonority feature proposed in our previous study incorporates three important aspects of sonority that are the less vocal-tract constriction, higher strength of excitation and periodicity, which has the ability to represent the sonority hierarchy. In this work, the sonority feature is exploited to differentiate between weakly sonorant and non-sonorant sounds in comparison to Mel-frequency cepstral coefficients (MFCCs) and existing acoustic features. The importance of each aspect of the sonority feature along with the context information is analyzed for sonorant segmentation. Both empirical (threshold based) and statistical (using support vector machines) sonorant/non-sonorant classifiers are developed and tested in presence various types/levels of noise, which provide better performance compared to MFCCs.",1.0,3.7249999999999996
943,1445,2.85    4.1    2.9    5.05    ,An Efficient End-to-End Method by Attending to One Speaker Exclusively for Two-Talker Speech Separation,"Most of speech separation systems usually enhance the magnitude spectrum of the mixture. The phase spectrum is left unchanged, which is inherent in the short-time Fourier transform (STFT) coefficients of the input signal. However, recent studies have suggested that phase was important for perceptual quality. In order to simultaneously make full use of magnitude and phase, this work develop a novel end-to-end method for two-talker speech separation, based on an encoder-decoder fully-convolutional structure. Different from traditional speech separation systems, in this paper, deep neural network outputs one speaker's signals exclusively. We evaluate the proposed model on the TIMIT dataset. The experimental results show that the proposed method significantly outperforms the permutation invariant training (PIT) baseline method, with a relative improvement of 16.06\% in signal-to-distortion ratio (SDR).",1.0,3.7249999999999996
55,1034,2.85    3.35    5    ,Scalable Factorized Hierarchical Variational Autoencoder Training,"Deep generative models have achieved great success in unsupervised learning with the ability to capture complex nonlinear relationships between latent generating factors and observations. Among them, a factorized hierarchical variational autoencoder (FHVAE) is a variational inference-based model that formulates a hierarchical generative process for sequential data. Specifically, an FHVAE model can learn disentangled and interpretable representations, which have been proven useful for numerous speech applications, such as speaker verification, robust speech recognition, and voice conversion. However, as we will elaborate in this paper, the training algorithm proposed in the original paper is not scalable to datasets of thousands of hours, which makes this model less applicable on a larger scale. After identifying limitations in terms of runtime, memory, and hyperparameter optimization, we propose a hierarchical sampling training algorithm to address all three issues. Our proposed method is evaluated comprehensively on a wide variety of datasets, ranging from 3 to 1,000 hours and involving different types of generating factors, such as recording conditions and noise types. In addition, we also present a new visualization method for qualitatively evaluating the performance with respect to the interpretability and disentanglement. Models trained with our proposed algorithm demonstrate the desired characteristics on all the datasets.",0.0,3.733333333333333
788,1014,4.85    3.3    2.75    4.05    ,Non-intrusive Speech Quality Assessment Using Deep Belief Network and Backpropagation Neural Network,"In this paper, we present a new speech quality assessment method to estimate the quality of degraded speech without the reference speech. The traditional non-intrusive assessment methods cannot meet the requirement of high consistency with subjective results owing to the lack of original reference signals. To solve these issues, deep belief network is trained to produce artificial reference speech signal of degraded speech. Then mel-frequency cepstrum coefficients of artificial reference speech and degraded speech are extracted to calculate feature differences. The feature differences are mapped to speech quality score using backpropagation neural network. Experiments are conducted in a dataset containing various degraded speech signals and subjective listening scores. When compared with the standardization ITU-T P.563, Gaussian Mixture Model method and the autoencoder-based method, the proposed method brings about a higher correlation coefficient between predicted scores and subjective scores.",1.0,3.7375
1306,2506,3.3    4.2    2.5    4.95    ,Emotion Detection Using Excitation Source Features of Speech,"In this paper, we address the issue of speaker specific emotion detection (neutral vs emotion) from speech signals without using any training data. Since the emotional speech is produced when a speaker is in a non-normal (not neutral) state, there are significant changes associated with the speech production mechanism. The main objective of the present study is to explore the deviations in emotional speech with reference to neutral speech of a speaker to develop a system for automatic detection of emotions. From this study, we also show the deviations are mostly in the excitation component of the voiced speech. The excitation source features used in this study are instantaneous fundamental frequency, strength of excitation and energy of excitation extracted from the speech signal. Kullback-Leibler (KL) distance is computed to measure the similarity between the neutral speech features and test (emotional) speech features to detect whether the speech is neutral or emotion. The advantages of the proposed automatic emotion detection system are, it is independent of language, lexical content, environment and importantly there is no need of training data. The proposed emotion detection system is evaluated with two types of databases and the performance observed to be better than existing methods.",1.0,3.7375
1121,1902,4.6    4.35    4.05    1.95    ,Detection of Vowel-Like Speech Using Variance of Sample Magnitudes,"Vowel, semi vowel and  diphthong sound units are collectively referred to as vowel-like speech (VLS). VLS are dominant voiced regions in a given speech signal. Consequently, within a short-analysis frame the variance of sample magnitudes (VSM) is significantly higher for VLS when compared with other speech regions. In this work, a signal processing approach is proposed to robustly extract the VSM within an analysis frame.  The VSM at each time instant is then nonlinearly mapped (NLM) using negative exponential function to suppress the fluctuations.  The NLM-VSM values are nearly constant and significantly less in magnitude for VLS than other speech, silence and noise regions.  The NLM-VSM  is used as a front-end  feature for detecting the VLS in a given speech signal. The experimental results presented in this paper show that, for  clean as well as noisy speech signals, the proposed feature outperforms some of the earlier reported features for the task of detecting VLS and corresponding onset and offset points.",1.0,3.7375
551,2045,3.5    2.45    4.9    4.1    ,Music Genre Recognition Using Deep Neural Networks and Transfer Learning,Music genre recognition is a very interesting area of research in the broad scope of music information retrieval and audio signal processing. In this work we propose a novel approach for music genre recognition using an ensemble of convolutional long short term memory based neural networks (CNN LSTM) and a transfer learning model. The neural network models are trained on a diverse set of spectral and rhythmic features whereas the transfer learning model was originally trained on the task of music tagging. We compare our system with a number of recently published works and show that our model outperforms them and achieves new state of the art results.,0.0,3.7375000000000003
764,75,4.2    3.5    5.05    2.25    ,Optimizing Class Priors to Improve Social Signal Detection from Audio Data,"To detect social signals such as laughter and filler events in an audio recording, probably the most straightforward way is to utilize a Hidden Markov Model -- or these days a Hidden Markov Model/Deep Neural Network (HMM/DNN) hybrid. HMM/DNNs, however, perform best if the DNN outputs are scaled by dividing them by the a priori class probabilities first, before applying a dynamic or Viterbi beam search. These class a priori probability values (or {\it priors} for short) are usually estimated by counting the frame occurrences of each class in the training set and then dividing these totals by the total number of frames. In this study we show that these estimates are suboptimal, and more reliable scaling factors can be obtained by optimization. With this strategy, we managed to achieve a 6-9% relative error reduction both at the frame level and the segment level, using a public database containing spontaneous English mobile phone conversations.",1.0,3.75
766,80,5.15    3.9    2.5    3.45    ,Factorised Hidden Layer Based Domain Adaptation for Recurrent Neural Network Language Models,"Language models which are used in various tasks like speech recognition or sentence completion are usually used on texts covering various domains. Therefore, domain adaptation has been a long-ongoing challenge in language model research. Conventional methods mainly work by adding a domain dependent bias. In this paper, we propose a novel way to adapt neural network based language models. Our proposed approach relies on a linear combination of factorised hidden layers which are learnt by the network. For domain adaptation we use topic features from latent Dirichlet allocation. These features are input into an auxiliary network and the output of this network is used to calculate the hidden layer weights. Both, the auxiliary network, as well as, the main network can be trained jointly by error backpropagation. This makes our proposed approach completely unsupervised. In order to evaluate our method, we show results for the well-known Penn Treebank and the TED-LIUM dataset.",1.0,3.75
965,1505,4.5    3.45    3.3    ,Discovery of Steady-state and Transient Units in Speech Using Syllable-like Units and Its Applications to Keyword Spotting,"The paper proposes an alternative idea to discover acoustic units (AU) using the transient and steady-state regions in a speech signal.  A syllable consists of an onset, attack, and decay. The properties of each of these units depend upon the phonemes that make up a syllable. The waveform is first segmented into constituent syllable-like units by using an unsupervised approach. The syllable is further segmented into an onset, attack, and decay (OAD). These are then clustered to form a finite vocabulary of 40 acoustic units representing a set of steady-state and transient signatures. Manual inspection of these signature confirms that the units do indeed have the assumed properties.  The approach is also validated on a keyword spotting application. A given keyword is transcribed in terms of these units. Similarly, search audio is also transcribed in terms of these units. Keyword spotting is performed using the transcribed query and the transcribed audio. The detection performance is evaluated on the TIMIT database, and the performance is comparable to that of the supervised approach.",1.0,3.75
845,1179,4    4.1    3.7    3.2    ,CSS10: a Collection of Single Speaker Speech Datasets for 10 Languages,"We describe our development of CSS10, a collection of single
speaker speech datasets for ten languages. It is composed of
short audio clips from LibriVox audiobooks and their aligned
texts. To validate its quality we train two neural text-to-speech
models on each dataset. Subsequently, we conduct Mean Opin-
ion Score tests on the synthesized speech samples. We make our
datasets, pretrained models, and test resources publicly avail-
able. We hope they will be used for future speech tasks.",1.0,3.75
842,1172,2.55    4.2    3.25    5    ,Emphasis Detection for Voice Dialogue Applications Using Multi-channel Convolutional Bidirectional Long Short-Term Memory Network,"Emphasis detection is important for user intention understanding in human-computer interaction scenario. Techniques have been developed to detect the emphatic words in speech, but challenges still exist in Voice Dialogue Applications (VDAs): the tremendous non-specific speakers and their various expressions. In this work, we present a novel approach to automatically detect emphasis in VDAs by using multi-channel convolutional bi-directional long short-term memory neural networks (MC-BLSTM), which can learn various expressions of large amounts of speakers and long span temporal dependencies across speech trajectories. In particular, we employ a multi-channel convolutional component in the proposed approach to extract high-level representation of input acoustic features. The experimental results on a 3400 real-world dataset collected from Sogou Voice Assistant outperform current state-of-the-art baseline systems (+6.2% in terms of F1-measure on average).",1.0,3.75
1295,2451,3.55    3.35    4.35    ,On Excitation Based Features for Automatic Speech Recognition Using Convolutional Neural Networks,"In this paper we have explored the use of complementary acoustic information extracted from speech to improve the robustness of Automatic Speech Recognition (ASR) systems based on deep Convolutional Neural Network (CNN) acoustic models. State-of-the-art acoustic features were augmented (either concatenated in a single stream or presented in parallel streams to the acoustic model) with data that characterize the excitation signal and/or with speaker-adaptive data. The ASR experiments conducted on CHiME-3 and CHiME-4 have revealed that the excitation based features have a significant effect in reducing the word error rate in both cases (5% and 8% relative reduction, respectively).",1.0,3.75
999,1595,3.55    3.35    4.35    ,A Deep Neural Network Framework for Speech Enhancement,"Speech enhancement plays an important role in speech and audio signal processing fields. Recently, speech enhancement using deep neural network has been expected to be a new solution to provide a high performance for speech corrupted by high-level noise. In this paper, we propose a new effective framework of a deep neural network for speech enhancement. The loss function is designed so that the statistical property of the error follows the normal distribution. The structure of deep neural network has a multiplicative skip connection, which relaxes a complexity of solving a regression problem. Through several experiments for speech enhancement, we reveal that the proposed framework can provide a high noise reduction performance compared with the conventional frameworks.",1.0,3.75
1035,1682,4.05    4.35    3.3    3.35    ,Novel Demodulation-Based Features Using Classifier-level Fusion of GMM and CNN for Replay Detection,"In this study, we explore the use of Convolutional Neural Networks
(CNN) for replay spoof detection in Automatic Speaker
Verification (ASV). The Amplitude and Frequency Modulation
(AM-FM) feature sets obtained from the Hilbert transform (HT)
and Energy Separation Algorithm (ESA) are used as the front
end. We have observed the effect of max-pooling and fully
connected (FC) layers, when replaced with the convolutional
layers in CNN. The results are compared with Gaussian Mixture
Model (GMM) classifier, furthermore to obtain the possible
complementary information of both the GMM and CNN
classifiers, we have explored classifier-level fusion. In addition,
we compared our results with Constant-Q Cepstral Coefficients
(CQCC) and Mel Frequency Cepstral Coefficients
(MFCC) feature sets. The architecture with max-pooling when
replaced with the convolutional layer along with FC layers
had performed relatively better on most of the AM-FM feature
sets compared to other CNNs. The ESA-based AM features
(i.e., Instantaneous Amplitude Cosine Coefficients (ESAIACC))
performed better as AM do not have more fluctuation
as FM have during models training. The lower EER is obtained
with classifier-level fusion of ESA-IACC feature set resulting in
2.54 % EER on development set and 6.04 % on evaluation set
of ASV Spoof 2017 Challenge database.",1.0,3.7624999999999997
1256,2287,3.15    3.3    5.1    3.5    ,Leveraging Adaptive Learning Techniques in Convolutional Neural Networks for Classifying Infant Vocalizations,"Different types of infant vocalizations serve different communicative functions and depict distinct spectro-temporal patterns, that can be recovered and learned through end-to-end machine learning systems. A common problem in such systems is the limited availability of data. In order to address this, we propose a multi-task multi-domain (MTMD) framework that takes advantage of additional resources, which are relevant to the problem of interest without necessarily having identical outcomes. The MTMD approach is based on a 5-layer convolutional neural network (CNN), whose input consists of time-frequency patches of the speech spectrogram and output is the union of the classes included in all considered datasets. Our target data come from the Cry Recognition In Early Development (CRIED), provided in the INTERSPEECH 2018 Crying Sub-Challenge, while the source data that come from three publicly available resources: the Oxford Vocal (OxVoc) Sounds database, the Google AudioSet, and the Freesound repository. Our results indicate that the proposed MTMD CNN outperforms regular CNNs trained solely on the target data, as well as neural networks pre-trained on large-scale image datasets and adapted to the target data (e.g., VGG16), suggesting the effectiveness of adaptation techniques in overcoming the problem of small datasets in human-related applications.",1.0,3.7624999999999997
785,1006,4.3    4.3    2.8    3.65    ,Speech Emotion Recognition Using Convolutional Neural Network with Audio Word-based Embedding,"A complete emotional expression typically contains a complex temporal course in a natural conversation. Related research on utterance-level, segment-level and multi-level processing lacks understanding of the underlying relation of emotional speech. In this work, a convolutional neural network with audio word-based embedding is proposed for emotion modeling. In this study, vector quantization is first applied to convert the low level features of each speech frame into audio words using k-means algorithm. Word2vec is adopted to construct to convert an input speech utterance into the corresponding audio word vector sequence. Finally, the audio word vector sequences of the training emotional speech data with emotion annotation are used to construct the CNN-based emotion model. The NCKU-ES database, containing seven emotion categories: happiness, boredom, anger, anxiety, sadness, surprise and disgust, was collected and five-fold cross validation was used to evaluate the performance of the proposed CNN-based method for speech emotion recognition. Experimental results show that the proposed method achieved an emotion recognition accuracy of 82.34%, improving by 8.7% compared to the Long Short Term Memory (LSTM)-based method, which faced the challenging issue of long input sequence. Comparing with raw features, the audio word-based embedding achieved an improvement of 3.4% for speech emotion recognition.",1.0,3.7624999999999997
746,2590,3.25    4.1    3.5    4.2    ,"A New Frequency Coverage Metric and a New Subband Encoding Model, with an Application in Pitch Estimation","The auditory filterbank has been a well-accepted and important tool for speech feature extraction. It decomposes the speech signal into subbands usually on an equivalent rectangular bandwidth frequency scale before further subband analysis and processing, such as auto-correlation and cross-correlation. However, the choice of the number of subbands and  subband center frequencies for a given frequency range has been essentially empirical in the literature. Moreover, correlation of subband signals may not produce distinct peaks for feature extraction. This paper proposes a novel frequency coverage metric to calculate the required number of subbands. It also presents a new subband encoding model for correlation processing, inspired by psychoacoustic studies and statistical analysis. The proposed frequency coverage metric and the subband encoding model are applied to a pitch estimation method as an example of their possible implementations in the speech feature extraction. Compared with state-of-the-art methods, evaluation results demonstrate the benefits of the proposed methods.",0.0,3.7625
1201,2132,3.55    4.35    2.85    4.3    ,Automatic Assessment of Motivational Interviews with Diabetes Patients,"The cost of diabetes to the UK in 2016 is estimated to be £14 billion. The impact on the health service can be reduced if patients take ownership of day-to-day monitoring and medication. Motivational Interviewing (MI) is a kind of goal-driven clinical conversion  that seeks to achieve this objective. This paper presents initial results on automatic quality assessment of MIs. This is challenging because the speech is conversational, medical terminology is used and recordings are made with a distant microphone in natural environments. We describe the development of automatic speech recognition (ASR), speaker diarization and topic analysis for MIs. We explore approaches to DNN-HMM training and adaptation for ASR using out-of-domain AMI data plus a limited amount of in-domain MI speech. Diarization, using DNN i-vectors and GMM-based clustering, is used to separate clinician and patient speech, and enables speaker-specific fMLLR training. On a test set of over 45 minutes of MI data, our best ASR and diarization systems achieve 43.59% word error rate and an F-measure of 0.765, respectively. Finally, we explore the impact of ASR error rate on LDA topic modeling.",1.0,3.7625
1149,1981,2.65    3.15    4.25    5    ,Simultaneously Addressing Data-imbalance and Feature Dimensionality,"The INTERSPEECH ComParE 2018 Self-Assessed Affect Sub-Challenge requires the classification of speech utterances into low, mid and high valence. The nature of the database is that the low valence samples are under-represented by a factor of four. The Random Forest (RF) classifier, which gave good results on both the Emo-DB database and a database of our own, is adversely affected by the data imbalance. We propose a simple, scalable solution that addresses the imbalance: four data-groups are created by repeating the low-valence samples each, but the medium- and high-valence samples in the groups are disjoint. A separate model using the RF classifier is trained for each group. Due to the reduction in the number of training samples per group, it was found that reducing the feature dimensionality helped in increasing the accuracy. The final step is to fuse the scores of the models using different feature subsets. As of 23 Mar 2018, the best unweighted average recall (UAR) was 58.47% on the development partition and 60.84% on the test partition.",1.0,3.7625
982,1554,2.35    3.4    5.15    4.15    ,Data Augmentation Using Conditional Generative Adversarial Networks for Robust Speech Recognition,"For noise robust speech recognition, the data mismatch between training and testing is a significant challenge. To reduce this mismatch, the traditional approach of data augmentation usually adds noise to the original waveform directly. In our previous work we utilized generative adversarial network (GAN) to generate data for speech recognition at the first time. In this work, we extend the previous basic GAN and explore the conditional generative adversarial network (CGAN) for data augmentation to further improve speech recognition under noise conditions. Two different conditions are explored, including the acoustic state for each speech frame and the original paired clean speech for each speech frame. Different from using basic GAN, these newly designed CGANs incorporate the specific conditions into data generation and provide true labels directly. The proposed CGAN-based data augmentation approach is evaluated on both Aurora4 and AMI-SDM, which have noise types such as additive noise, channel distortion and reverberation. Experimental results show that the CGAN-based method consistently outperforms our previous GAN-based one under all noisy conditions, and a relative 6% to 10% WER reduction can be obtained upon an advanced acoustic model.",1.0,3.7625
1118,1895,4.2    4.25    2.85    ,Detection of Vowels in Speech Signals Degraded by Speech-Like Noise,"Detecting vowels in a noisy speech signal is a very challenging task. The problem is further aggravated when the noise  exhibits speech-like  characteristics, e.g., babble noise. In this work, a novel front-end feature extraction technique exploiting variational mode decomposition (VMD)  is proposed  to improve the detection of vowels in speech data degraded by speech-like noise. Each short-time analysis frame of speech is first decomposed into a set of variational mode functions (VMFs) using VMD. The logarithmic energy present in each of the VMFs is then used as the front-end features for detecting vowels.  A three-class classifier (vowel, non-vowel and silence) with acoustic modeling based on long short-term memory (LSTM) architecture is developed on the TIMIT database  using the proposed features as well as mel-frequency cepstral coefficients (MFCC). Using the three-class classifier, frame-level time-alignments for a given speech utterance are obtained to detect the vowel regions. The proposed features result in significantly improved performance under noisy test conditions than the MFCC features. Further, the vowel regions detected using the proposed features are also quite different from those obtained through the MFCC.  Exploiting the aforementioned differences, the evidences are combined to further improve the detection accuracy.",1.0,3.766666666666666
980,1548,3.95    5.05    3.35    2.75    ,"Comparison of Formant Frequencies of Sounds Produced by a Man, a Woman and a Child with Varying Fundamental Frequency and Vocal Effort","Formant frequency patterns of vowel sounds are assumed to relate to age and gender of the speakers because of differences in their average vocal tract length. However, concerning voiced sounds, empirical evidence is only given for sounds produced with medium vocal effort on fundamental frequencies (fo) of relaxed speech. In the present study, 888 sounds of the long Standard German Vowels /u–o–a–ɛ–ø–e–y–i/ produced by a man, a woman and a child with varying fo (C major scale, fo range = 131 and 165–523 Hz for the man, 165–523 Hz for the woman, 220–523 Hz for the child) and – on each fo level – varying vocal effort (medium, low, high) were investigated. A listening test involving five phonetic expert listeners was conducted, and average F1, F2 and F3 of the sound nuclei were estimated. As major results, systematic F1 overlapping were found for the sounds for all three speakers. The same holds true for F2 of sounds of /u–o–y/. Pronounced overlapping F1–F2 and F1–F2–F3 patterns were found for the sounds of the adults, but only few overlapping patterns were found comparing adults and child.",1.0,3.775
1172,2036,5.1    1.8    4.85    3.35    ,Modeling Vowel Sequence Production of the Vocal Tract with Moving Boundaries,"To obtain continuous speech in a natural way, the vocal tract needs to be modeled as a high-dimensional tube with moving complex boundaries, which is hardly achieved either physically or numerically. In this study, a numerical model is developed for vowel sequence production governed by the two-dimensional wave equation with moving boundary conditions, in which the governing equations are solved by the finite-difference time-domain (FDTD) method and the boundary conditions are specified at the moving boundaries by the immersed boundary method (IBM). To represent a free space with a finite computational domain, the PML condition is used to absorb the outgoing waves at the open end of the tube. This model is firstly tested by a sound wave propagation problem in a two-segment uniform tube with time-varying radiuses. Physically meaningful results are obtained and carefully examined against theoretical analysis. The processes of sound wave propagation in the vocal tract pronouncing the mandarin vowel sequences /i/ to /a/ and its reverse are simulated. The obtained formant frequencies are compared with those of real speech sounds.",1.0,3.775
1127,1918,2.9    4.5    3.55    4.15    ,Reward Only Training of Encoder-Decoder Digit Recognition Systems Based on Policy Gradient Methods,"Zero resource speech recognition is getting attention for engineering as well as scientific purposes. Based on the existing unsupervised learning frameworks, however, it is impossible to associate automatically found linguistic units with spellings and concepts. In this paper, we propose an approach that uses a scalar reward that is assumed to be given for each decoding result of an utterance. While the approach is straightforward using reinforcement learning,
the difficulty is to obtain a convergence without the help of supervised learning. Focusing on encoder-decoder based speech recognition, we explore several neural network architectures, optimization methods, and reward definitions, seeking a suitable configuration for policy gradient reinforcement learning. Experiments were performed using connected digit utterances from the TIDIGITS corpus as training and evaluation sets. While the learning is challenging with a randomly initialized network, we show that it is possible by combining a proper network structure and a reward definition successfully learning a connected digit recognition system.",1.0,3.775
1068,1771,3.7    4.2    4.35    2.85    ,Inferring Dementia with Features of Topic Repetition in Single Conversation: Evaluation against Data of Regular Monitoring Service,"Detecting signs of dementia in everyday situations has become more and more important. Previous studies used the acoustic, prosodic, and linguistic features of those affected by dementia to characterize their language dysfunctions, and they succeeded in differentiating seniors with dementia from healthy seniors. However, they mainly investigated conversational data during neuropsychological tests, so inferring dementia from daily conversations remains largely unexplored. We analyzed conversational data collected from a regular monitoring service for seniors that includes dementia participants and we applied a biterm topic model to investigate how topic patterns in a single conversation differed between seniors with dementia and healthy seniors. In dementia participants, we found significant increases in the features of topic repetition, including topic similarities and the frequency of the same topics in a single conversation. We built a classification model to infer dementia by combining these features of topic patterns with the linguistic features typically used in previous studies and it achieved 98.0% accuracy in 10-fold cross-validation. The results suggest that topic repetition within a single conversation could be useful for inferring dementia in everyday situations.",1.0,3.775
737,2551,3.55    2.95    4.25    4.35    ,Using Prosodic and Lexical Information for Learning Utterance-level Behaviors in Psychotherapy,"In this paper, we present an approach for predicting utterance level behaviors in psychotherapy sessions using both speech and lexical features. We train long short term memory (LSTM) networks with an attention mechanism using words, both manually and automatically transcribed, and prosodic features, at the word level, to predict the annotated behaviors. We demonstrate that prosodic features provide discriminative information relevant to the behavior task and show that they improve prediction when fused with automatically derived lexical features.  Additionally, we investigate the weights of the attention mechanism to determine words and prosodic patterns which are of importance to the behavior prediction task.",0.0,3.775
752,37,4.85    2.95    2.45    4.85    ,Assessing the Effect of Spectral Amplitude Elevation for Vowel Identification in Simulated Electric-Acoustic Hearing,"Though the benefits of combined electric-acoustic stimulation (EAS) have been
widely accepted, its underlying mechanisms responsible are not well understood
yet. This study assessed the effect of unbalanced spectral amplitudes of
acoustic and electric portions for vowel identification in simulated EAS. Four
vowels (i.e., /iy/, /eh/, /oo/ and /ah/) were synthesized by using pure-tone
harmonic components, and the spectral amplitudes of the synthesized vowels were
elevated for either acoustic or electric portion from 5 to 30 dB in 5-dB step.
The four synthesized vowel stimuli were processed by the signal processing
simulating combined electric-acoustic stimulation for identification by
normal-hearing listeners. Results showed declined vowel identification scores
as a function of (acoustic or electric) spectral amplitude elevation for vowels
/oo/ and /ah/. The specific loudness pattern computed from Moore et al.’s
model was found to effectively account for the variance of vowel identification
(i.e., correlation coefficient r=0.9). The results in this work suggest the
importance to maintain the balance between the spectral amplitudes of acoustic
and electric portions for vowel identification in EAS.",1.0,3.775
838,1167,4.05    3.65    4.15    3.25    ,Non-Linear Time Series Analysis of Emotional Speech Signal,"Identification of the physical process involved in the production
of speech signal is essential to characterize the dynamics
of various emotions. The conventional linear approach may not
be sufficient for such an analysis since speech is produced from
an underlying non-linear process. Therefore, the present work
proposes a novel perspective in the analysis of emotional speech
signal using a set of non-linear methods. The proposed nonlinear
analysis tries to identify the physics of emotive speech
production. Initially, we confirm the non-linear nature of the
emotive speech signal through surrogate analysis. We also identified
that the emotive speech signal is a multi-fractal signal,
indicating its scale free nature and difference in local fluctuations.
Further, we explore the dynamic behavior of the emotive
speech using recurrence plot obtained from the reconstructed
phase space. The extracted non-linear features are used to identify
the dynamical states of various emotive speech utterances.
Experimental results on speech signals uttered in five emotions,
taken from German emotional database (EMO-DB), show that
the ensemble of non-linear features provide cues for the characterization
of emotive speech.",1.0,3.775
1234,2219,5.7    5.1    2.4    1.9    ,Vowel Duration as an Acoustic Cue to Contrastive Focus in Bangla,"Bangla, an Indo-Aryan language, spoken near Kolkata, West Bengal, is known to have no vowel length distinction phonemically. Even though the impact of information structure and various prosodic positions determine the phonetic duration of the vowel (Kawasaki and Shattuck-Hufnagel 1988, Chatter- jee 1921, Anderson 1962, Ghosh 1995). This paper presents an acoustic study of the effect of Contrastive focus on duration of high vowels in Bangla. Six monosyllabic and six disyllabic words with the target vowels, altogether 24 words, embedded in simple declarative sentences, have been recorded in both Con- trastive focus and Neutral focus contexts. The sentences were recorded as response to yes-no question to get contrastive or corrective focus, while for Neutral focus context simple declar- ative sentences were recorded from six speakers of Standard Kolkata Bangla between age group of 20 to 28. Focus in Bangla is known to be prosodically realized by low rising pitch pat- tern followed by a high boundary tone (Hayes and Lahiri 1991, Khan 2008). The statistical analysis of the data recorded for the study shows significant difference in vowel duration aug- mentation in terms of focus condition and also the nature of the vowel.",1.0,3.7750000000000004
1162,2006,5.25    2.3    2.5    5.05    ,Emotion Classification by Regression on Long-term Speech Features,"Recognizing emotion from speech is an important, non-
intrusive and practical solution for enterprises. In the litera-
ture, both regression and classification techniques have been
used to recognize emotions from speech. These classifiers op-
erate on usually low-level features or aggregate features derived
from them. Another option is to concatenate low-level features
so that their information is not lost in aggregate statistics. In
this paper, we propose a regression model on features concate-
nated for 20 - 200 milliseconds. When followed by three-level
quantization into negative, neutral and positive valence, its per-
formance of 80.24% equals that of the current state of the art
on our non-acted, spontaneous database for speaker-dependent
partitions (every speaker appears in the training and test data).
Although the same technique performs under-par for the stan-
dard Emo-DB database, it equals state-of-the-art performance
in cross-corpus testing. Further, for speaker independent parti-
tions on our database, the accuracy of 56.7% is an improvement
over the current state of the art.",1.0,3.7750000000000004
948,1466,4.2    4.9    2.75    3.25    ,Bandwidth Extension for Deep Neural Network Acoustic Modeling,"This paper investigates new techniques for bandwidth extension (BWE) of narrowband signals to allow accurate recognition by wideband-trained speech recognition models. The mapping from the narrowband to wideband features is captured by a deep neural network in the logmel space. A variety of network architectures, including deep feedforward and convolutional neural networks, are investigated for both the mapping functions and the associated acoustic models. Both generative and discriminative training criteria are evaluated for the estimation of the mapping functions.  Experiments are carried out on the 50-hour Broadcast News (BN50) dataset. It is shown that discriminatively trained mapping based on convolutional neural networks allow a wideband model to accurately recognize narrowband data. Furthermore, the accuracy on the BWE-mapped narrowband data is better than recognizing the narrowband speech from a model trained on parallel narrowband data alone.",1.0,3.7750000000000004
776,96,5.75    4.8    1.8    2.75    ,"Comparison of Cross-cultural Perceptions of Japanese Cake Seller Voices among Japanese, Chinese, American English and Indian Listeners","Within each culture/community there are traditional “voices”, that are used for social communication thatconveyspecific meaning. The study of cultural influences on  voice is referred here to as ethnophonetics. This study examines the ethnophonetic  qualities of Japanese cake sellers’ voice , and how it  is perceived by listeners of different language/cultural groups.  Specifically, we asked Japanese, Chinese, American and Indian listeners to judge samples of Japanese cake-selling street voices, and to rate which voice in their opinion was the “best” cake-seller voice. All cultural groups identified different voices as the best cake-seller voice. In other words, the voice that Japanese speakers perceived as the best cake-seller voice was incompatible with the perception of the other cultural groups.. Japanese listeners, preferred a cake-seller voice with a slight twang with a breathy quality. Chinese listeners preferred acake- selling voice with a higher F0, one that sounds like the moe anime voice; American listeners preferred the voice with a more dynamic range of F0; and Indian listeners preferred the normal speaking voice of the supermarket worker with limited twang.",1.0,3.7750000000000004
829,1144,3.1    3.25    5    ,Gated Recurrent Per-Channel Energy Normalization for Robust and Far-Field Speech Recognition,"Far-field speech recognition is an important technique to free users' hands in man-machine communication. Even though recent studies have led to significant advances in far-field speech recognition, there is still a large performance gap with close-talk speech recognition espically in complex environments. Energy variations caused by different distances, microphone gains and various loudness levels across speakers is one of the major challanges that harm the recognition accuracy. Recently, a novel frontend called trainable per-channel energy normalization (T-PCEN) was introduced to improve the robustness of energy variations. It was a trainable frontend and could be optimized jointly with acoustic model. In this paper, an improved topoloy of PCEN layer named Gated Recurrent PCEN (GR-PCEN) is introduced. The parameters of GR-PCEN are generalized to be time-frequency dependent which introduce more temporal contextual information than T-PCEN. Experimental results on a 50-hour far-field Mandarin Chinese speech recognition task have shown that further character error rate (CER) improvements could be obtained compared to T-PCEN method.",1.0,3.783333333333333
1294,2445,2.8    4.6    3.95    ,Incompatibility of Voicing and Turbulence: Supra-laryngeal Implementation of Voicing in Russian Sibilants,"Voicing in fricatives needs more delicate gestural coordination compared with that in stops. This is because voiced fricatives, particularly voiced sibilants, must cope with two conflicting aerodynamic requirements; low supraglottal pressure is required to maintain vocal fold vibration, while high pressure is crucial to bring about supralaryngeal frication. Despite this fact, supralaryngeal gestural control for fricatives has received less attention in comparison with stops. The aim of the current study is to advance our understanding of the effect of voicing on tongue root position during the production of Russian sibilants in utterance-initial position. Native speakers of Russian (N=3) produced nonsense words containing alveolar sibilants (/s/ and /z/) as well as stops (/t/ and /d/) in utterance-initial position. Ultrasound imaging was used to capture tongue root configuration. The results showed that Russian speakers did not advance the tongue root neither in stops or sibilants, exhibiting a different pattern from other language.",1.0,3.783333333333333
180,1259,3.35    4.25    3.3    4.25    ,A Novel Approach for Effective Recognition of the Code-Switched Data on Monolingual Language Model,"Code-switching refers to the phenomena of mixing of words or phrases from foreign languages while communicating in a native language by the multilingual speakers.  Code-switching is a global phenomenon and is widely accepted in multilingual communities. However, for training the language model (LM) for such tasks, a very limited code-switched textual resources are available as yet. In this work, we present an approach to reduce the perplexity (PPL) of Hindi-English code-switched data when tested over the LM trained on purely native Hindi data. For this purpose, we propose a novel textual feature which allows the LM to predict the code-switching instances. The proposed feature is referred to as code-switching factor (CS-factor). Also, we developed a tagger that facilitates the automatic tagging of the code-switching instances. This tagger is trained on a development data and assigns an equivalent class of foreign (English) words to each of the potential native (Hindi) words. For this study, the textual resource has been created by crawling the blogs from a couple of websites educating about the usage of the Internet. In the context of recognition of the code-switching data, the proposed technique is found to yield a substantial improvement in terms of PPL.",0.0,3.7874999999999996
1255,2285,4.1    2.95    5.15    2.95    ,Computational Modelling of Human Spoken Word Recognition: the Effects of Pre-lexical Representation Quality on Fine-Tracker’S Modelling Performance,"Fine-Tracker is a speech-based model of human speech recognition that is able to model the use of durational cues for the disambiguation of temporarily ambiguous speech. While previous work has shown that Fine-Tracker is successful at modelling the role of durational cues in human spoken-word recognition, its speech recognition performance is not comparable to that of human performance. The original Fine-Tracker model uses MLPs to convert the speech signal into articulatory features (AFs) which are used as the intermediate representation (i.e., prelexical representations) of the speech signal in the model.
This study investigates the effect of improved AF representations as prelexical representations on Fine-Tracker’s simulation and recognition performance. These improved AFs were obtained using deep convolutional neural networks. The simulation results showed that an improvement in the quality of the prelexical representations resulted in improved speech recognition for Fine-Tracker; however, surprisingly, it did not lead to an improvement in Fine-Tracker’s simulation power regarding the use of durational information for the disambiguation of temporarily ambiguous speech in human speech processing.",1.0,3.7874999999999996
1153,1993,4.35    3.45    4.05    3.3    ,Varying Length Segment Initialization in Information Bottleneck Based Speaker Diarization System,"Information bottleneck (IB) based speaker diarization is a bottom-up clustering approach where the size of the initial segments to be clustered are assumed to be of fixed duration. Information in a segment is dependent upon the number of phonemes that make up a segment. Instead of a fixed duration segment, phoneme rate is used as a measure to obtain varying length segments. The duration is chosen such that each initial segment consists of approximately same number of phonemes. Varying length segments are then clustered using IB based diarization system. This is followed by Kullback-Leibler hidden Markov model (KL-HMM) based realignment. The effectiveness of the proposed technique is evaluated on the standard NIST and AMI datasets. A best-case absolute improvement of 7% (relative 27.2%) in terms of speaker error rate was obtained on the evaluation dataset.",1.0,3.7874999999999996
758,55,4.15    4.3    2.7    4    ,"High-quality Waveform Generator from Fundamental Oscillation, Spectral Envelope, and Band Aperiodicity","This paper introduces a waveform generation algorithm from three speech parameters (fundamental oscillation f_o, spectral envelope, and band aperiodicity).
The conventional speech analysis/synthesis system based on vocoder mainly has a waveform generator based on pitch synchronous overlap and add (PSOLA).
Since it has generally used the fast Fourier transform (FFT) to generate the excitation signal, the processing speed is proportional to the f_o.
The algorithm also uses the spectral representation of the aperiodicity, whereas the band aperiodicity is mainly used in speech synthesis applications such as statistical parametric speech synthesis.
We propose a waveform generation algorithm to reduce the computational cost and memory usage without degrading the synthesized speech.
The algorithm uses an excitation signal generation by directly using the band aperiodicity.
Since the excitation signal is filtered and processed by the simple overlap-add (OLA) algorithm, the computational cost in a certain period is fixed.
We carried out two evaluations for the processing speed and sound quality by using the re-synthesized speech.
The result showed that the proposed algorithm can reduce the computational cost and memory usage as expected.
The sound quality of the synthesized speech was almost the same as that of the conventional algorithm.",1.0,3.7874999999999996
1246,2253,4.2    4.25    4.9    1.8    ,Investigating Text-independent Speaker Verification from Practically Realizable System Perspective,"This work projects an attempt to explore the prospects of text-independent speaker verification (SV) for practical realizable systems. Although the advancements in SV systems have gained attention towards deployable systems, the performance seems to degrade under uncontrolled conditions. A protocol for data collection is designed for the text-independent SV with student attendance as an application to create a database in a real-world scenario. The i-vector based speaker modeling is used for evaluating the performance that depicts major deviation of results from that obtained on standard database. This portrays the significance of having real-world scenario based databases for robust SV studies. Further, studies are performed related to speaker categorization, speaker confidence and model update that showcase their significance towards systems in practice. The database created in this work is available as a part of multi-style speaker recognition database.",1.0,3.7875
580,2122,2.75    5.15    2.4    4.85    ,"Automatic Speech Recognition with Articulatory Information and a Unified Dictionary for Hindi, Marathi, Bengali, and Oriya","Despite the continuous progress of Automatic Speech recognition (ASR) technologies, these systems for Indian languages are still in infancy stage due to a multitude of challenges involved, including resource deficiency. This paper addressed this challenge with four Indian languages, Hindi, Marathi, Bengali, and Oriya by integrating articulatory information into acoustic features, thereby compensating the low resource property of these languages for improved performance. Articulatory movements were recorded during speech production using an electromagnetic articulograph and trained together with acoustic features to build automatic speech recognizers for these languages. Both speaker-dependent and -independent recognition experiments were conducted by adopting three ASR models: Gaussian Mixture Model (GMM)-Hidden Markov Model (HMM), Deep Neural Network (DNN)-HMM, and Long Short Term Memory recurrent neural network (LSTM)-HMM. A cross-language similarity was discerned in both acoustic and articulatory domains in the pairs of Oriya-Bengali and Hindi-Marathi. Based on these observations, a multi-lingual, multi-modal speech recognizer was built by constructing a unified dictionary consisting of common and unique phonemes of all the four languages, which reduced the phoneme error rates.",0.0,3.7875
805,1072,3.05    3.5    5    3.6    ,Pitch Synchronized Phase Information for Noise-robust Speaker Recognition,"In conventional speaker identification methods based on mel-frequency cepstral coefficients (MFCCs), phase information is ignored. Recent studies have shown that phase information contains speaker dependent characteristics, and it is effective for speaker recognition. In this paper, we propose a pitch synchronized relative phase information for speaker identification in noisy environments. To mitigate the affect of noise on pseudo pitch synchronized relative phase information extraction, a peak error detection using an auto-correlation based algorithm was proposed. Experiments were conducted using the JNAS (Japanese Newspaper Article Sentence) database. The pitch synchronized relative phase information with peak error detection based method achieved a relative speaker identification error reduction rate of 23.9% compared to the conventional phase information (that is pitch non-synchronized relative phase). By combining the proposed method with MFCC, the speaker identification rate was improved from 55.0% (MFCC) to 76.9%.",1.0,3.7875
1173,2037,4.2    4.2    3.4    3.35    ,Deep Learning and I-vector for Mandarin Accent Identification towards Accent Robust ASR,"In this paper, we present an in-depth study on the classification of regional accents in Mandarin speech, and the relation to the robustness of automatic speech recognition (ASR).
First, by evaluating ASR on accented Mandarin speech data systematically collected from 15 different geographical regions in China for broad coverage, we demonstrate that the character error rate (CER) strongly varies by accent, even if the variance can be reduced through speaker adaptation.
This motivates for the introduction of automatic accent classification to contribute to ASR robustness, e.g., by identifying `difficult' speech data with high CER.
To this end, we investigate accent classification approaches incorporating bidirectional LSTM (BLSTM) networks and/or i-vector to model longer-term acoustic context.
Starting from the classification of the collected accent data into the 15 regions of recording, we derive a three-class grouping via non-metric dimensional scaling, for which 66.4% average recall can be obtained. 
Finally, we show that the CER of our ASR systems is significantly different among the  accent groups predicted by our classifiers, thus highlighting the importance and potential usefulness of automatic accent classification in robust ASR.",1.0,3.7875
928,1411,4    3.2    4.75    3.25    ,On Using Spoken Word Posterior Features in Neural Machine Translation,"Spoken language translation (ST) system consists of at least two modules: an automatic speech recognition (ASR) system and a machine translation (MT) system. In most cases, MT is only trained and optimized using error-free text data. Consequently, If ASR made errors, the translation accuracy will be greatly reduced.  Previous researchers have shown that training MT system with ASR parameters or word lattices can improve the translation quality. However, they require a large change to standard MT system resulting in a complicated model that is hard to train. In this paper, a neural sequence-to-sequence ASR is used as a 'feature processing' that is trained to produce word posterior features given spoken utterances. The resulting probabilistic features are used to train the back-end neural MT (NMT) with only a slight modification. Experimental results reveal that the proposed method could improve up to 5.8 BLEU scores with synthesized speech or 4.3 BLEU scores with the natural speech in comparison with conventional cascaded-based ST system that translates from 1-best of recognition candidates.",1.0,3.8
1001,1601,4.15    3.15    2.85    5.05    ,Characterization of Spoken English Vowels Using Tree Structures,"In this work, we adopted the concept of the relational tree proposed by Ehrich to process and characterize speech waveforms. This type of tree representation was initially used by
researchers to understand characteristics of waveforms and forming symbolic representation of signals. It was used for seismic classification and Electro Cardiogram (ECG) classification as well. We further analyzed them by comparing the structures of trees for different vowel sounds using the algorithm proposed by Zhang and Shasha. The sounds used in
this work are spoken English vowels. Experimental details are presented and the results are encouraging. Further studies are needed to use this method for implementation in resource-constrained devices.",1.0,3.8
374,1646,4.4    5    4.35    1.45    ,Processing Transition Regions of Glottal Stop Substituted /S/ for Intelligibility Enhancement of Cleft Palate Speech,"In this study, intelligibility enhancement of cleft palate (CP)
speech is attempted. The individuals with CP exhibit differ-
ent types of mal-adaptive articulation error caused by a vari-
ety of craniofacial issues. In this work only glottal stop distor-
tion is considered. The silence like characteristics caused by
the glottal stop is transformed by inserting sustained synthetic
fricative /s/. It is observed that apart from the distortion in sus-
tained fricative region, the transition regions are also affected
due to co-articulation. Thus, for intelligibility, besides the sus-
tained region correction using insertion method, transition re-
gions modification is also performed. For the transformation of
transition regions, we explore 2D − DCT based joint spectro-
temporal modeling. The 2D − DCT coefficients of CP speech
are modified by projecting it onto the singular vectors of normal
speech. The singular vectors are derived by performing singular
value decomposition (SVD) of the 2D − DCT coefficients of
transition region components of normal speech. Further, the in-
telligibility of the modified speech is compared with the original
speech using objective and subjective assessment.",0.0,3.8
996,1591,2.6    3.7    4.1    4.85    ,Complementary Auto-encoder Bottleneck Features for ASR Acoustic Modeling,"Performance of automatic speech recognition (ASR) systems degrade in highly diverse acoustic conditions characterized by variabilities found in environment noises and speakers. One commonly adopted approach to handle this problem for deep neural network (DNN) based acoustic models is to use additional auxiliary input features to encode these variabilities during both training and test time. A key issue associated with this approach is the suitable form of auxiliary features.
In the paper, a novel semi-supervised complementary auto-encoder (CAE) based feature extraction method is proposed to automatically learn the acoustic variability encoding. The CAE consists of an auxiliary target encoder trained using a supervised manner, an unsupervised reconstruction decoder, and a complementary feature encoder shared between the two. CAE bottleneck features were extracted in an unsupervised fashion and used in both DNN system training and test time.
On the Aurora 4 task, the best DNN system using the proposed CAE features outperformed the baseline DNN system by over
10% relative reduction in word error rate.  In addition, the conventional DNN bottleneck features and proposed CAE features were also found complementary to improve DNN system performance on both the Switchboard and UASpeech databases.",1.0,3.8125
187,1270,2.55    4.5    4.05    4.15    ,Paired Phone-Posteriors Approach to ESL Pronunciation Quality Assessment,"This work proposes to incorporate paired phone-posteriors as input features into a neural net (NN) model for assessing ESL learner’s pronunciation quality. In this work, posteriors of forty phones, instead of several thousand sub-phonemic senones, are used to circumvent the sparsity issues in NN training. Phone posteriors are assembled with their corresponding senone posteriors estimated via a speaker-independent, DNN-based acoustic model, trained with standard American English speech data (i.e., Wall Street Journal database). Phone posteriors of both reference(standard American English speaker) and test speaker are paired together as augmented input feature vectors to train an NN based, 2-class, i.e., native vs nonnative speaker, classiﬁer. The Goodness of Pronunciation (GOP), a proven effective measure, is used as the baseline for comparison. The binary NN classiﬁer trained with such features achieves a high classification accuracy of 89.6% on native and non-native speakers’ data. The classiﬁer also shows a better equal error rate (EER) than the GOP-based baseline classiﬁer in either phone or word level pronunciation, i.e., at phone level from 18.3% to 6.2%, and at word level from 12.98% to 2.54%.",0.0,3.8125
1285,2408,4.15    2.95    4.85    3.3    ,A Bag of Multilayer Deep Representations for Heart Sound Classification,"Heart disease is one of the major global health concerns. Phonocardiography provides a method to record sounds and murmurs for heart disease diagnosis, and the results can be visualized as a scalogram. Classification of heart sounds, especially with ``deep'' methods, is challenging, because of the small amount of data that is typically available. 

In this paper, we propose a novel method to explore deep representations: we extract Bag-of-Word features at multiple levels of a pre-trained deep model, on which we train individual support vector machine classifiers. The final decision is given by majority voting. Scalograms are ``visual'' features, so we can use convolutional neural networks trained to perform object detection to extract Bag-of-Word features, and the fusion of multiple classifiers can compensate for the domain mismatch to some extent.

The approach has been applied to the Heart Beats Sub-Challenge of the Interspeech 2018 Computational Paralinguistics Challenge. The proposed approach can achieve 47.9\,\% unweighted average recall, which outperforms two alternative CNN-based approaches and single model baselines.",1.0,3.8125
917,1385,2.7    4.1    4.2    4.25    ,Advancing Multi-Accented LSTM-CTC Speech Recognition Using a Domain Specific Student-Teacher Learning Paradigm,"In this study, we consider a recurrent neural network (RNN) with connectionist temporal classification (CTC) cost function trained on multi-accent English data including US (Native), Indian and Hispanic accents. We exploit dark knowledge from a model trained with the multi-accent data to train student models under the guidance of both a teacher model and CTC cost of target transcription.  We show that transferring the knowledge from a single RNN-CTC trained model to a student model, yields better performance than the stand-alone teacher model. Since the outputs of different trained CTC models are not necessarily aligned, we cannot simply use an ensemble of CTC teacher models. To address this problem, we train accent specific models under the guidance of single multi-accent teacher which results in having multiple aligned trained CTC models. Furthermore, we train a student model under the supervision of the domain-specific teachers, resulting in an even more complementary model, which achieves 20.1% relative Character Error Rate (CER) reduction compared to the baseline trained without any teacher.  Using the accent specific model's outputs to regularize the adapting process (i.e., a knowledge distillation version of Kullback–Leibler(KL) divergence) results in superior performance compared to the conventional approach using general teacher models.",1.0,3.8125
1244,2248,4.15    2.7    3.15    5.25    ,Comparison between 2D and 3D Models for Articulatory Speech Synthesis of French Vowels,"In the present work, we study the production and the synthesis of five vowels of the French language. The idea behind this work, is to compare 2-dimensional models with 3-dimensional models for speech synthesis, and examine whether the 2-dimensional models can describe the acoustics of the vocal tract. For the purpose of our experiments, we used 3-dimensional MRI data to acquire the shape of the vocal tract. We then use it in order to simulate how the pressure wave produced from the vocal folds propagates until it reaches the mouth. We make aerodynamic simulations for both the 3-dimensional shape as well as of the 2-dimensional (midsagital) shape of the vocal tract. We also make 2-dimensional electrical simulation using again the midsagital plane. We then compare the results we get from the simulations with the audio recordings, as well as with the theoretical values. Our experimental results show that, although vocal tract has a complex 3-dimensional shape, 2-dimensional models can adequately describe its acoustic properties.",1.0,3.8125
1303,2487,4.75    3.85    3.15    3.5    ,Characterizing Variation in Crowd-sourced Data for Training Neural Generators to Produce Stylistically Varied Outputs,"Dialogue systems have become one of the key applications in speech processing but there are still many ways in which these systems can be improved. One obvious possible improvement is in the system’s language generation to make it more natural and more varied. Here we take a large corpus of 50K crowd-sourced utterances in the restaurant domain and develop text analysis methods that systematically characterize types of sentences in the training data. We then automatically label the training data to allow us to conduct two kinds of experiments with a neural generator. First, we test the effect of training the system with different stylistic partitions and quantify the effect of smaller, but more stylistically controlled training data. Second, we try a method of labeling the style variants during training, and show that we can modify the style of the output to some extent using our stylistic labels. We contrast and compare these methods that can be used with any existing large corpus, showing how they vary in terms of semantic quality and stylistic control.",1.0,3.8125
1157,1997,3.45    5    3.35    3.45    ,Prosodic Cues and Discursive Changes in Audiobooks,"It can be assumed that changes in discursive perspective and character are encoded at the prosodic level when one reads aloud audiobooks. Understanding which prosodic cues are used to this end is relevant since they could be implemented to enhance speech quality and expressiveness in text-to-speech synthesis systems.
 This paper is presenting a preliminary study aiming at highlighting robust prosodic patterns able to encode discourse changes. Extracts from a ten hours single speaker corpus of French audiobooks have been annotated in term of discourse changes, and automatically analyzed in order to evaluate whether articulation rate and pitch range at breath group level do mark changes in discourse perspective (from narration to direct discourse, and vice versa). 
The results show that, among the two prosodic cues investigated, pitch range plays a crucial role: the pitch range associated with the first breath group of a direct speech paragraph is more compressed than the one observed in the previous breath group belonging to a distinct discourse type (p-value <0.01). Note however that these changes in pitch range are done locally. By contrast, articulation rate does not seem to come into play.",1.0,3.8125
944,1448,3.45    4.35    2.65    4.8    ,Conv-codes: Audio Hashing for Bird Species Classification,"In this work, we propose a supervised, convex representation
based hashing framework for bird species classification. The
proposed framework utilizes archetypal analysis (AA) to obtain
convex sparse representations of a bird vocalization. These convex
representations are used to generate a compact hash code
called a conv-code. A hash table is populated using these convcodes
as keys while hash values/slots are pointers to the corresponding
bird vocalization recordings or species identification
information. During testing, convex representations and
conv-codes are generated for a test example. The hash table
is searched to find the species information corresponding to a
conv-code that provides maximum similarity with the test convcode.
Hence, the proposed framework classifies a bird vocalization
in conv-code space and requires no explicit classifier or reconstruction
error calculations. The classification performance
of the proposed framework is compared with other bird species
classification frameworks such as dynamic kernels based support
vector machines (SVM) and deep neural network (DNN)
on the audio recordings of 50 different bird species. Based on
min-hash and direct addressing, we also propose a variant of
the proposed hashing framework that provides rapid classification/testing
with a small drop in classification accuracy.",1.0,3.8125
960,1497,3.55    2.55    4.35    4.8    ,Revisiting UBM-GMM Based Classification with Trial-specific top-C Scoring,"Universal Background Model-Gaussian Mixture Model (UBM-GMM) based classifier uses fast top-C scoring during testing. The value of C is empirically fixed and remains constant for all
trials. Owing to the mismatch between train and test conditions, it is possible that the number of C components need not be constant even for trials across different sessions of the same speaker. In this paper, we revisit the UBM-GMM classifier using a novel trial-specific top-C scoring approach. The top-C is chosen such that for every trial, the number of feature vectors used in decision making is proportionate to the number
of feature vectors available for test. The effectiveness of the proposed trial-specific top-C is tested on three different applications namely, replay attack detection, speaker identification, and speaker verification. In all the three applications, variable top-C scoring outperforms the fixed top-C scoring systems.",1.0,3.8125
1196,2111,3.4    4.25    4.15    3.45    ,Speech and Monophonic Singing Segmentation Using Pitch Parameters,"In this paper we present a novel method for automatic segmentation of speech and monophonic singing voice based only on two parameters derived from pitch: proportion of voiced segments and percentage of pitch labelled as a musical note. First, voice is located in audio files using a GMM-HMM based VAD and pitch is calculated. Using the pitch curve, automatic musical note labelling is made applying stable value sequence search. Then pitch features extracted from each voice island are classified with Support Vector Machines. Our corpus consists in recordings of live sung poetry sessions where audio files contain both singing and speech voices. The proposed system has been compared with other speech/singing discrimination systems with good results.",1.0,3.8125
162,1237,3.55    3.5    4.15    4.1    ,Biophysically-inspired Features Improve the Generalizability of Neural Network-based Speech Enhancement Systems,"Recent advances in neural network (NN)-based speech enhancement schemes are shown to outperform most conventional techniques. However, the performance of such systems in adverse listening conditions such as negative signal-to-noise ratios and unseen noises is still far from that of humans. Motivated by the remarkable performance of humans under these challenging conditions, this paper investigates whether biophysically-inspired features can mitigate the poor generalization capabilities of NN-based speech enhancement systems. We make use of features derived from several human auditory periphery models for training a speech enhancement system that employs long short-term memory (LSTM), and evaluate them on a variety of mismatched testing conditions. The results reveal that biophysically-inspired auditory models such as nonlinear transmission line models improve the generalizability of LSTM-based noise suppression systems in terms of various objective quality measures, suggesting that such features lead to robust speech representations that are less sensitive to the noise type.",0.0,3.8249999999999997
1145,1975,5.2    3.5    1.75    4.85    ,Charisma Perception from Voice: Narendra Modi vs. Rahul Gandhi,"Speech signal intrinsically have amplitude modulations in the range of 1-9 Hz. The study investigated the role of amplitude modulation, temporal and spectral features in Indian political arena by comparing speech produced by Narendra Modi and Rahul Gandhi across three different platforms i.e. an interview, a political rally and a parliamentary debate.

For both the speakers, study of modulation spectra after matching the overall intensities showed greater power in frequency range 1-15 Hz in political rallies as compared to interviews and parliamentary debates. For rallies and parliamentary debates, greater power was found in Gandhi’s modulation spectra (as compared to Modi’s). Dominant peaks were observed around 4Hz and 6 Hz for Gandhi’s utterances whereas it was around 7 Hz and 13 Hz for Modi’s speech, which indicate a preference of modulation around these values. For interviews, greater power is found in Modi’s modulation spectra against Rahul’s for the range of 1-9 Hz and vice-versa for the range of 9-15 Hz. Analysis on pauses, rate of speaking, pitch, use of fillers and formants for vowel pronunciation were also made. Results were interpreted for a listener perception of speaker’s charismatic personality.",1.0,3.8249999999999997
923,1398,4.4    3.5    4.15    3.25    ,"Physical VOCODER with EGG Signals, Ultrasonic Images, and Hand Gestures","A vocoder transforms a speech signal into a different form and then back into a speech signal. Speech-related bio-signals can also be used to do this. In the present study, electroglottograph (EGG) signals, ultrasonic images, and hand gestures were used as speech-related bio-signals. These bio-signals were acquired from an EGG device, an ultrasound device, and a camera or a motion capture device, respectively. We used two methods to synthesize speech. The first was a PC-controlled version of Umeda-Teranishi's model. Multiple bars were inserted into two sides of the oral, pharyngeal, and nasal cavities of the model. The positions of the bars in the oral and pharyngeal cavities were controlled by a PC, frame-by-frame. EGG signals were used as input. The other bio-signals controlled the position of the bars to change the configuration of the vocal-tract dynamically. The second speech-synthesis method was based on digital pattern playback (DPP). Formant movements derived from a bio-signal were used to draw a spectrographic representation. The spectrogram was then converted back into a speech signal using the DPP technique. This system is useful for both research and pedagogy in speech science.",1.0,3.825
869,1228,3.8    4.15    2.6    4.75    ,Cross-Gender Voice Conversion with Constant F0-Ratio and Average Background Conversion Model,"This paper presents the method for spectral voice conversion using parallel training data. The proposed solution was submitted to the 2018 Voice Conversion Challenge. The method focuses on the preparation of the generative model for cross-gender voice conversion in differential-filtering framework. To improve the quality of the Gaussian mixture conversion model we introduced the usage of the averaged speaker background model pre-training step. Constant F0 ratio transformation of source speech using WORLD vocoder was also proposed to improve cross-gender conversion quality. The evaluation results show that the proposed solution outperforms most of the concurrent systems submitted to 2018 Voice Conversion Challenge, both in terms of speech quality and similarity. The system achieved 75% similarity score and 3.31 mean opinion score in cross-gender conversion task.",1.0,3.825
1020,1647,3.35    3.35    4.2    4.4    ,Significance of Teager Energy Operator Phase for Replay Spoof Detection,"The increased use of voice biometrics for various security applications,
motivated authors to investigate different countermeasures
for the hazard of spoofing attacks, where the attacker tries
to imitate the genuine speaker. The replay is the most accessible
spoofing attack. Past studies have ignored phase information
for various speech processing applications. In this paper, we
explore the excitation source-like feature set, namely, Teager
Energy Operator (TEO) phase and its significance in the replay
spoof detection task. This feature set is further fused at scorelevel
with magnitude spectrum-based features, such as Constant
Q Cepstral Coefficients (CQCC), Mel Frequency Cepstral Coefficients
(MFCC), and Linear Frequency Cepstral Coefficients
(LFCC). The improvement in the results show that the TEO
phase feature set contains the complementary information to the
magnitude spectrum-based features. The experiments are performed
on the ASV Spoof 2017 Challenge database. The systems
are implemented with Gaussian Mixture Model (GMM)
as a classifier. Our best system using TEO phase achieves the
Equal Error Rate (EER) of 6.57 % and 15.39 % on the development
and evaluation set, respectively.",1.0,3.825
413,1730,2.75    5.1    4.9    2.6    ,Speech Enhancement Using Deep Mixture of Experts Based on Hard Expectation Maximization,"We consider the problem of deep mixture of experts based speech enhancement. The deep mixture of experts, where experts are considered as deep neural network (DNN), is difficult to train due to the network structure. In this work, we propose a pre-training method for individual DNN in deep mixture of experts. We use hard expectation maximization (EM) to pre-train the individual DNNs. After pre-training, we take a weighted combination of outputs of individual DNN experts and jointly train the whole system. We compare the proposed method with single DNN based speech enhancement scheme. Speech enhancement experiments, in four SNR conditions, show the superiority of proposed method over the baseline scheme. The average improvements obtained for four seen noise cases over single DNN scheme are 0.08, 0.59 dB and 0.015 in terms of objective measures viz perceptual evaluation of speech quality (PESQ), segmental signal to noise ratio (seg SNR) and short time objective intelligibility (STOI) respectively.",0.0,3.8375
1061,1752,4.2    3.55    4    3.6    ,Automatic Detection of the Alzheimer's Dementia Using the Modulation Spectrum of Speech of the Revised Hasegawa's Dementia Scale,"This paper presents an automatic detection method of Alzheimer's dementia using a novel speech features called the modulation spectrum. Many developed countries are facing problems of an aging society, e.g., the increasing number of patients with Alzheimer's dementia. Therefore, an automated detection system of this disease will be helpful for time- and cost-efficient screening and the use of speech features derived from recorded speech is a promising approach to deploy the system. To tackle this issue, we introduce the modulation spectrum of short-term speech features. The modulation spectrum is a long-term speech feature that can capture fluctuations of short-term speech features and is known as a useful feature to artificially synthesize natural-sounding (healthy-control-like) speech in statistical parametric speech synthesis. Using the modulation spectrum of speech data of the revised Hasegawa's dementia scale, we build classifiers to classify healthy control, mild cognitive impairment, and Alzheimer's dementia. The experimental results demonstrate that 1) the modulation spectrum improves the classification accuracy compared to that of conventional short-term speech features, and 2) the classifier achieves 83% sensitivity and 78% specificity.",1.0,3.8375
818,1117,4.85    3.55    5.15    1.8    ,Exploring Conditional Independence in End-To-End Models,"End-to-end encoder-decoder speech recognition models, which fold the acoustic, pronunciation and language model components into a single neural network, have shown competitive performance to conventional ASR systems which utilize separate, independently-trained components. However, the performance of these models degrades (i.e., words are misspelled) when recognizing words seen rarely during training. One hypothesis for this degradation is that it is the result of the tight coupling between the encoder (analogous to the acoustic model) and decoder networks (analogous to the pronunciation and language models) comprising the end-to-end  model. In this paper, we investigate this hypothesis by introducing partial conditional independence into the end-to-end model and examine how this coupling affects performance when decoding rare words. We find that the coupling is actually beneficial for the end-to-end model, and that removing it degrades performance. This suggests that alternative techniques are required in order to improve the recognition of rare words in the end-to-end approach.",1.0,3.8375
890,1304,4.35    2.65    4.85    3.5    ,A Reranker Scheme for Integrating Large Scale Nlu Models,"Large-scale Natural Language Understanding (NLU) systems are typically trained on large quantities of data, requiring a fast and scalable training strategy. A typical design for NLU systems consists of domain-level NLU modules, which can be trained in parallel. At runtime, outputs from these parallel modules are combined by a downstream module. Specifically, the combination strategy integrates outputs from domain-level recognizers (e.g. named entity recognition and intent classification models), returning a scored list of cross-domain hypotheses. The ideal combination strategy will exhibit the following two properties: (a) it should prefer the most relevant hypothesis for the given input, and (b) any interpretation score produced by the module should be calibrated so that the score can be associated with a meaningful semantic and be comparable across domains. We propose a “re-ranked” based combination strategy that addresses these aspects while retaining the system modularity. Furthermore, we extend it for joint optimization over multiple contexts, termed as a “multiplexed re-ranker”.",1.0,3.8375
994,1586,4.45    3.6    2.05    5.25    ,A Simple Vowel-Consonant Detection Feature for Rhythm-Based Language Identification and Complexity Comparison,"In this paper, we expand upon an approach to periodicity-based speech rhythm feature extraction for automatic language identification (LID). In order to extract periodicities of relevant speech elements, we use a feature combination which predicts the presence of a vowel or consonant, based on a comparison between the values with a fixed threshold. Those onset positions are validated and afterwards and applied in two ways: By using them as a basis for rhythm features in an automatic LID task for two multilingual corpora with different qualities. Second, by calculating the entropy of the phone onset functions, which is a measure for their temporal complexity. An analysis of variance is used to verify whether the complexity within a language is significantly different than the one across languages. The features provide improved classification results compared to previous attempts using similar features, indicating that the use of higher level speech elements as a basis of periodicity feature extraction bears relevant information. The onset functions display significantly greater complexity across than within languages, but results vary between datasets. LID performance does not compete with the state-of-the-art approaches, but results are informative concerning speech rhythm and its modeling for automatic LID.",1.0,3.8375000000000004
1136,1951,3.55    2.65    4.95    4.25    ,Categorical Classification of Objective Speech Quality Using a Convolutional Neural Network,"Objective metrics such as the perceptual evaluation of speech quality (PESQ) have become standard measures for evaluating speech. These metrics enable efficient and costless evaluations, where ratings are often computed by comparing a degraded speech signal to its underlying clean reference signal. Reference-based metrics, however, cannot be used to evaluate real-world signals that have inaccessible references. Reference-less based objective metrics exist, but their assumptions are often invalid, so they do not perform well. This project develops a data-driven framework to improve reference-less objective evaluation of speech. We develop a novel approach that uses a convolutional neural network (CNN) to predict the objective categorical quality ranking of noisy speech signals. The original objective quality scores are grouped into categories based on pre-defined ranges. This approach is evaluated using the TIMIT speech corpus and a wide range of signal-to-noise ratios. The results show that the proposed system achieves good prediction performance and that categorical quality prediction using a CNN is promising.",1.0,3.8499999999999996
1128,1919,2.75    4.25    4.35    4.05    ,Novel Unsupervised Sorted GMM Posteriorgram for DNN and GAN-based Voice Conversion Framework,"Voice Conversion (VC) requires an alignment of the spectral features before learning the mapping function, due to the speaking rate variations across the source and target speakers. To address this issue, the idea of training two parallel networks with the use of speaker-independent representation was proposed. In this paper, we explore the unsupervised Gaussian Mixture Model (GMM) posteriorgram as a speaker-independent representation. However, in the GMM posteriorgram, the same phonetic information gets spread across more than one component due to the speaking style variations across the speakers. In particular, this spread is limited to a group of neighboring components for a given phone. We propose to share the posterior probability of each component with the limited number of neighboring components that are sorted based on the Kullback-Leibler (KL) divergence. We propose to employ a Deep Neural Network (DNN) and a Generative Adversarial Network (GAN)-based framework to measure the effectiveness of the proposed sorted GMM (SGMM) posteriorgram on the Voice Conversion Challenge (VCC) 2016 database. The relative improvement of 13.73 %, and 5.25 % is obtained with the proposed SGMM posteriorgram w.r.t. the GMM posteriorgram for the speech quality and the speaker similarity of the converted voices, respectively.",1.0,3.8499999999999996
861,1207,4.1    3.85    4.15    3.3    ,Disfluency Insertion for Spontaneous TTS: Formalization and Proof of Concept,"This paper presents an exploratory work to automatically insert disfluencies in text-to-speech (TTS) systems. The objective is to make TTS more spontaneous and expressive. To achieve this, we propose to focus on the linguistic level of speech through the insertion of pauses, repetitions and revisions. We formalize the problem as a theoretical process, where transformations are iteratively composed. This is a novel contribution since most of the previous work either focus on the detection or cleaning of linguistic disfluencies in speech transcripts, or solely concentrate on acoustic phenomena in TTS, especially pauses. We present a first implementation of the proposed process using conditional random fields and language models. The objective and perceptual evalation conducted on an English corpus of spontaneous speech show that our proposition is effective to generate disfluencies, and highlights perspectives for future improvements.",1.0,3.8499999999999996
796,1051,2.35    4.35    4.85    3.85    ,Towards Computation Efficient Speech Recognition by Combining Lower Frame Rate with Advanced Multiframe Prediction,"Neural networks have become the state of art acoustic models for automatic speech recognition. From DNN to LSTM, more and more powerful but complex models are used for acoustic modeling, resulting in dramatic performance gain. On the other hand, it is essential to take the complexity, latency and realtime into account when apply speech recognition systems to real-world applications. In this work, we come up with an \emph{AMFP-LFR-DFSMN} acoustic modeling framework to handle these problems. We propose to combine the lower frame rate (LFR) with multi-frame prediction (MFP) and also present an advanced MFP (AMFP) architecture by adding some small ReLU layers before each independent output layer. These additional ReLU layers help to enhance the model capacity, especially when we jointly predict multiple LFR-based frames. We adopt the DFSMN as main network and then evaluate the performance of AMFP-LFR-DFSMN acoustic model on a 20000 hours large vocabulary Mandarin speech recognition task. Experimental results show that it can jointly predict 6 consecutive LFR-based frames without any performance degradation. Compared to LFR models, the proposed method can reduce the total number of floating point multiplication during training and decoding by 66\% and speedup the real time factor (RTF) from 0.25 to 0.09.",1.0,3.8499999999999996
777,98,5.9    3.45    2.2    ,Analysis of Speech and Singing Signals for Temporal Alignment,"Accurate alignment between singing signal and its spoken lyrics
at frame-level is imperative to several applications in singing
signal processing. As the acoustic characteristics of speech and
singing signals differ significantly, finding the temporal alignment
between them is not easy. In this paper, we study the characteristics
of speech and singing signals to identify their common
properties to facilitate temporal alignment. We observe
that: (i) the characteristics of excitation source in human voice
production mechanism largely vary with speaking and singing
and, (ii) for the same linguistic content, speaking and singing
signals present very different formant patterns. Based on these
observations, we formulate a set of tandem features that represent
only those characteristics consistent between speech and
singing signals. Such tandem features are used in dynamic time
warping for temporal alignment, and in a speech-to-singing
conversion experiment. In both objective and subjective evaluations,
we show that the proposed tandem features are significantly
superior to the baseline features in temporal alignment.",1.0,3.85
1260,2311,2.55    3.6    5.25    5.1    2.75    ,Detecting Overlapping Speech Segments in Real Conversations Using Deep Learning,"Segmenting natural occurrences of overlapping speech in real conversations has proven to be an extremely difficult challenge, demanding heavy feature engineering, or controlled studies using artificial data. We demonstrate training a Deep Convolutional Neural Network (DCNN) for this task, affording us to use relatively low-level log-scaled Mel-spectrograms from only mono-aural audio, one of the most difficult scenarios for this task. We propose using the real conversational data from the Fisher English Corpus to properly train the DCNN while maintaining and testing its generalizability to real conversational scenarios. To alleviate the then imposed challenge of severe class-imbalance, we report the improvements achieved by removing silence from the training objective and uniformly randomly under-sampling the majority class during training. Simultaneously, we perform temporal smoothing using the Viterbi algorithm to enhance the final segmentation. With more than 60% precision and more than 29% recall over more than 91 hours of conversations, our results demonstrate the applicability of deep learning for this task and, using our modular and open-sourced RENNET framework, straightforward avenues for future works in this direction.",1.0,3.85
512,1947,4.2    3.1    5.15    2.95    ,Analysis of Variational Mode Functions for Robust Detection of Vowels,"In this work, initially the speech signal is decomposed into variational mode 
functions (VMFs) with the aid of variational mode decomposition (VMD). Each 
decomposed VMF represents different frequency band of the input speech signal. 
An approximate speech signal is then reconstructed by using a set of selected 
VMFs  whose center frequency  predominantly corresponds to the frequency range 
of the vowels. In the reconstructed speech signal, energy due to  the high 
frequency unvoiced sound units and  noises is suppressed.  Consequently, over an 
analysis frame,  the mean of the  square magnitude (MSM) of the sample points  
is significantly higher  for the vowels than other sound units. Further, the MSM 
at each time instant  is  non-linearly mapped (NLM) using a negative exponential 
functions to enhance the transitions at the onset and the offset points of  
vowels, and suppress small fluctuations. The NLM-MSM is used as a front-end 
feature for discriminating  vowels in a given speech signal. The experiments 
conducted on TIMIT database show that, the proposed approach outperforms the 
existing methods for the task of detecting vowels in a given speech signal under 
clean and noisy test scenarios.",0.0,3.8500000000000005
789,1017,3.3    4.15    5.05    2.95    ,A Quantitative Comparison of Epoch Extraction Algorithms for Emotional Speech,"Developments in speech technology such as expressive speech synthesis, emotion conversion, prosody modification, voice transformation etc., facilitate the modification of voice characteristics. The potential use of these technologies depends on the speech analysis algorithms that can handle different acoustic variations such as emotion/expressive voices often found in conversational speech. Detection of epochs is typically required in the analysis stages and thus the importance of more reliable epoch detection algorithms is evident. Most of the existing epoch extraction algorithms provide excellent results on neutral/modal speech. There are no studies on the effectiveness of these algorithms for analysis of emotional speech. In this paper, performance of the existing state-of-art epoch extraction algorithms on emotional speech data is studied quantitatively. We used the Berlin EMO-DB emotional speech database which contains simultaneous EGG recordings as ground truth. The state-of-art epoch extraction algorithms considered in this study are: ZFF, YAGA, DYPSA, SEDREAMS, SE-VQ and MMF. The algorithms are evaluated in terms of both reliability and accuracy measures over seven different emotion categories.",1.0,3.8625
1278,2380,2.75    5.75    2.65    4.3    ,An End-to-End Deep Neural Network for Acoustic Addressee Detection in Adult-Child Conversations,"Addressee detection is an interesting paralinguistic and dialogue challenge which arises in real applications, e.g., in automatic spoken dialogue systems dealing with multiparty conversations that may involve several humans talking to each other while addressing a system (human-human-machine conversations). In the present paper, we distinguish addressees in conversations between several adults and children. Talking to children, adults tend to change their normal manner of speech, making it more rhythmical, louder and generally more understandable. Adults do this intentionally in order to attract children’s attention. Many users act in a similar way while addressing a system since they do not perceive it as an adequate conversational agent. We propose an end-to-end neural network architecture solving the acoustic addressee detection problem and examine it on the HomeBank Child-Adult Addressee Corpus introduced at the Addressee Sub-Challenge of the Interspeech 2017 Computational Paralinguistics Challenge. The proposed model significantly outperforms the baseline standalone end-to-end approach. We also explore relations between the human-machine and the adult-child addressee detection problem and make an attempt to leverage the Smart Video Corpus containing human-human-machine conversations for solving the adult-child addressee detection problem.",1.0,3.8625
1080,1796,2.5    3.45    4.25    5.25    ,Enhanced Denoising Auto-Encoder for Robust Speech Recognition in Unseen Noise Conditions,"We present a robust front-end processing method for speech recognition in unseen noise conditions. Towards this end, we have investigated the efficacy of a Time Delay Neural Network (TDNN) based Denoising Auto-Encoder (DAE) in seen and unseen noise conditions. While the DAE succeeds in improving the performance of the speech recognition by a large margin in seen noise conditions (noise encountered during decoding was used during training of the DAE), it fails to improve the performance in unseen noise conditions (noise encountered during decoding was not used during training of the DAE). To address this, we propose to pre-process the training input to the DAE using an enhancement technique. In essence, the DAE is being trained to address the residual noise left behind by the enhancement technique. For this task, we compare the performance of two enhancement techniques, namely Vector Taylor Series with Acoustic Masking (VTS-AM) and Spectral Subtraction (SS). We show that both these enhancement techniques improve the efficacy of the DAE significantly in unseen noise conditions and that the VTS-AM outperforms the SS.",1.0,3.8625
384,1664,5    2.5    4.35    3.6    ,All-Neural Multi-Channel Speech Enhancement,"This study proposes a novel all-neural approach for multi-channel speech enhancement, where robust speaker localization, acoustic beamforming, post-filtering and spatial filtering are all done using deep learning based time-frequency (T-F) masking. Our system first performs monaural speech enhancement on each microphone signal to obtain the estimated ideal ratio masks for beamforming and robust time delay of arrival (TDOA) estimation. Then with the estimated TDOA, directional features indicating whether each T-F unit is dominated by the signal coming from the estimated target direction are computed. Next, the directional features are combined with the spectral features extracted from the beamformed signal to achieve further enhancement. Experiments on a two-microphone setup in reverberant environments with strong diffuse babble noise demonstrate the effectiveness of the proposed approach for multi-channel speech enhancement.",0.0,3.8625
1108,1876,3.25    5.2    4.3    2.7    ,Mixture of Expert Based Feature Transformation for Improved ASR Performance,"In this paper Mixture of Experts (MoE) is proposed in the context
of Automatic Speech Recognition (ASR) and we show that
MoE helps to reduce the overlap in distributions between different
broad acoustic classes in the feature domain, which leads
to significant improvement in the ASR accuracy. Experiments
conducted in a large scale setting using 2500 hours of training
data and with LSTM models show that MoE based features
outperforms the baseline system that is trained in a conventional
framework. Two cases of MoE are considered, where
the classes are defined either using phonetic knowledge or in
an unsupervised manner. It is observed that the 3-class MoE
system under the phonetic knowledge approach provides a significant
improvement in the ASR accuracy. Detailed analysis
conducted by means of scatter diagram and linear discriminant
analysis (LDA) show that MoE indeed improves the separation
between classes, especially between non-speech and speech
classes, which corroborate the results.",1.0,3.8625
1181,2056,2.8    3.8    4.35    4.5    ,Evaluation of CRFs-based Grapheme-to-Phoneme Conversion Using Length-restricted Graphonemes for Non-linear Grapheme-Phoneme Alignment in Asian Languages,"Conventional G2P require aligned grapheme-phoneme data as joint units for training a G2P model. In most languages, grapheme sequences can be directly and linearly aligned to phoneme sequences as joint grapheme-phoneme sequences, i.e. Graphonemes that can be applied automatic schemes for alignment and prediction. Due to complex grapheme-phoneme alignments in Asian languages; for instance, inter-syllabic and intra-syllabic grapheme-phoneme cross-alignment and implicit tone assignment from orthographies, conventional contiguous character-based graphonemes might not be applicable. Concerning the complexness of the alignments, choices of unit lengths also should be analyzed. Thus, this paper proposes to use a Conditional Random Fields (CRFs) based G2P using GP with various length restrictions to evaluate their G2P performance on Asian languages including Thai, Lao, Myanmar and Mongolian under comparable conditions. This paper also analyze appropriate length restrictions of joint-multigrams for the experimental Asian languages by varying the lengths of graphonemes ranging from character level to supra-syllable level. Furthermore, the results show that the maximum length constraints of two graphemes/phonemes provide the best accuracies for the experimental languages.",1.0,3.8625
874,1255,2.45    4.7    4    4.3    ,Analysis of Communicative Phrase Prosody Based on Linguistic Modalities of Constituent Words,"In this paper, phrase prosody is analyzed based on linguistic modalities of constituent words for communicative speech synthesis. Although there are strong demands of communicative speech output, there are little works on its characterization and generation scheme. In this study, we analyzed phrase prosody differences from linguistic modality differences to generalize and understand communicative prosody control possibility we have been studying for more than a decade. Since Japanese final particles and auxiliaries play crucial roles to indicate speaker’s intention and attitudes as modality differences, Japanese phrase sets showing different degree of judgment were employed. Communicative/reading speech data were compared over 5 kinds of modality of epistemic judgment (uncertainty of what speaker said) and 8 kinds of modality of evaluative judgment (ideal of speaker). These modality differences were quantified in 6-point SD (Semantic Differential) scales. The corresponding phrase communicative/reading prosody differences were measured by the F0 rising in the phrase final mora. Statistical analysis showed negative correlation between F0 rising in the phrase final mora and SD about judgment only in communicative prosody but not in reading prosody. These results support the communicative prosody control possibilities from the modality information embedded in constituent words.",1.0,3.8625
813,1101,3.35    3.45    4.3    4.35    ,Investigating Data Augmentation Techniques for Children Spontaneous Speech Recognition,"The ability to assess conversational interaction between children and adults is critical in determine language and cognitive proficiency for typically developing and at-risk children. To date, limited research exists in employing effective speech/language technologies to measure child-adult speech interaction in the classroom. Automatic speech recognition for young child conversational speech is a more challenging than for adults, specifically because of the developing language planning and motor skills of young speakers at age 2.5 to 5. It is common to experience WERs in the 60-90% range for such ASR conditions. In this study, we explore data augmentation techniques to improve both language and acoustic models. Two text augmentation approaches are investigated to increase diversity in word/phrase production: using Web data and via texts generated by recurrent neural networks. We also compare several acoustic augmentation techniques for children speech: tempo perturbation, speed perturbation, adult data, as well as semi-supervised training. Finally, we comment on expanding speech and conversational interaction analysis strategies that do not rely exclusively on flawless ASR performance. Specifically, we discuss factors that assess children speech development: children word count rates, heat map of adult word count when engaging children in learning spaces.",1.0,3.8625000000000003
1283,2399,3.5    5.8    2.3    ,Domain Robust Feature Extraction for Rapid Low Resource ASR Development,"Developing a practical speech recognizer for a low resource language is challenging, not only because of the (potentially unknown) properties of the language, but also because test data may not be from the same domain as available training data. In previous work, we have shown that sequence models can be trained successfully on a small amount of target language data, if a matching corpus is available in a single well-resourced ""source"" language, thereby avoiding multi-lingual training.

In this paper, we show how a pre-trained robust English recognizer can be used to extract features on Turkish Broadcast News and Babel (telephony) data, and improve recognition in particular for the cross-domain case. It thus becomes possible to rapidly develop a speech recognizer for a new language and ""any"" domain, which is the main challenge for speech processing in the DARPA Lorelei project.

We achieve relative improvements in phoneme error rate of around 25% when testing in various cross-domain scenarios, with improvements being close to 50% for some domains.",1.0,3.866666666666667
1038,1689,4.95    3.45    4.25    4.1    2.6    ,Novel Amplitude Weighted Frequency Modulation Features for Replay Spoof Detection,"In this paper, we propose the use of Amplitude Modulation and
Frequency Modulation (AM-FM) features for replay detection
task. In AM-FM signal, AM component is known to be severely
affected by noise (in this case, due to replay mechanism) which
is exploited in proposed feature extraction. In particular, we explore
this damage in AM component to corresponding instantaneous
frequency. Thus, the novelty of proposed Amplitude
Weighted Frequency Cosine Coefficients (AWFCC) feature set
is the use of frequency components along with weighted amplitude
components that are degraded due to replay noise. The
AWFCC features have the information of AM-FM and hence,
gave discriminatory information in the spectral characteristics.
On ASV Spoof 2017 Challenge database the experiments were
performed using AWFCC feature set that gave an Equal Error
Rate (EER) of 6.37% (dev) and 11.72% (eval) set. We
have compared our results with CQCC, LFCC and MFCC using
Gaussian Mixture Model (GMM) classifier and found that
AWFCC feature set performed better than the other feature sets
on both dev and eval data sets. In addition, we have used score-level
fusion of CQCC and AWFCC to obtain a lower EER of
3.60% and 11.22% on dev and eval set.",1.0,3.87
997,1592,3.3    3.4    4.7    4.1    ,A Deep Neural Network Approach for Large-Scale Text-Independent Multitarget Speaker Detection,"As speaker recognition technology becomes more commonly used as a means for identity verification in industry, the need for a robust large-scale multitarget speaker detection system is increasing. However, most speaker recognition studies to date have focused only on single-target speaker verification tasks. A large-scale multitarget speaker detection task has not been thoroughly studied. In this paper we explore several approaches for text-independent multitarget speaker detection, where the target set contains over 3,000 speakers. Specifically, we discover that introducing a deep neural network (DNN) classifier allows us to significantly improve the performance of our multitarget speaker detection system over a baseline i-vector framework. As a discriminative model, the DNN-based classifier complements i-vector representations when encountering nontarget speakers. In addition, we extend the t-normalization technique to handle the multitarget task, which further improves the performance of our multitarget speaker detector.",1.0,3.8749999999999996
1119,1900,2.65    3.8    3.95    5.1    ,Depression Score Prediction from Speech Using Logistic Regression with Partial Proportional Odds,"Speech has recently gained significant research attention as an objective biomarker for depression. Depression scores are ordinal in nature, and hence neither classification nor regression is the ideal approach for learning a mapping from speech to depression scores. Logistic regression, a tool for ordinal regression, provides an appealing framework, and one with multiple alternative model structures. This paper investigates and compares three alternatives: logistic regression with and without the proportional odds assumption and the partial proportional odds model, which has not been investigated in speech classification contexts. Evaluations on the AVEC 2017 dataset indicate that the proposed partial proportional odds model provides depression prediction performance better than the AVEC 2017 baseline and approaching that of the Gaussian Staircase Regression model used in state-of-the-art system designs. Interestingly, this paper also demonstrates that partial proportional odds models can elegantly trade-off between modelling complexity and avoiding overfitting.",1.0,3.8749999999999996
950,1474,5    4.15    1.3    5.05    ,Toward Domain Invariant Speech Recognition: an Empirical Study,"Current state-of-the-art automatic speech recognition (ASR) systems are trained to work in specific `domains', defined based on factors like application, sampling rate, and codec. When such recognizers are used in conditions that do not match the training domain, performance significantly drops. In this paper, we explore the idea of building a single model that can work well for multiple domains. To that effect, we do an empirical study to understand how well data augmentation -- real, simulated, and both -- works. We show that a single system trained with very large training sets can learn to work as well as  domain-specific models merely through data augmentation. Data augmentation makes the model robust to multiple application domains, and other variations like codecs and noise. Furthermore, such models also generalize better to unseen conditions and address the data sparsity issue for low-resource domains. We also highlight some of the limitations of data augmentation and areas that need addressing in future work.",1.0,3.875
490,1897,3.55    3.6    3.5    4.85    ,Empirical Evaluation of Speaker Adaptation on DNN Based Acoustic Model,"Speaker adaptation aims to estimate a speaker specific acoustic model from a speaker independent one to minimize the mismatch between the training and testing conditions arisen from speaker variabilities.  A variety of neural network adaptation methods have been proposed since deep learning models have become the main stream.
But there still lacks an experimental comparison between different methods, especially when DNN-based acoustic models have been advanced greatly. In this paper, we aim to close this gap by providing an empirical evaluation of three typical speaker adaptation methods: LIN, LHUC and KLD. Adaptation experiments, with different size of adaptation data, are conducted on a strong TDNN-LSTM acoustic model. More challengingly, here, the source and target we are concerned with are standard Mandarin speaker model and accented Mandarin speaker model. We compare the performances of different methods and their combinations. Speaker adaptation performance is also examined by speaker's accent degree.",0.0,3.875
609,2212,4.05    2.05    5.25    4.15    ,"A French-Spanish Multimodal Speech Communication Corpus Incorporating Acoustic Data, Facial, Hands and Arms Gestures Information","A Bilingual Multimodal Speech Communication Corpus incorporating acoustic data as well as visual data related to face, hands and arms gestures during speech, is presented in this paper. This corpus comprises different speaking modalities, including scripted text speech, natural conversation, and free speech. The corpus has been compiled in two different languages, viz., French and Spanish. The experimental setups for the recording of the corpus, the acquisition protocols, and the employed equipment are described. Statistics regarding the number and gender of the speakers, number of words, number of sentences, and duration of the recording sessions, are also provided. Preliminary results from the analysis of the correlation among speech, head and hand movements during spontaneous speech are also presented in this paper, showing that acoustic prosodic features are related with head and hand gestures.",0.0,3.875
836,1164,4.95    3.55    3.55    3.45    ,A Dynamic Deep Learning Approach for F0 Modeling,"Parametrical speech synthesis is nowadays the most used TTS methodology for research due to its flexibility.
Prosody modeling remains a big challenge for the community.

In this paper, we are proposing to model the fundamental frequency (F0) using a dedicated Neural Network (NN).
To do so, we are representing the interpolated F0 by its dynamic evolution and a start position.
The start position is computed based on the average F0 of the corpus
The dynamic is parameterized using a sign value, giving the direction, and a quantized magnitude value.
The neural network is trained to produce this two categorical values.

We have evaluated the proposed methodology in comparison to a state of the art parametrical TTS system.
To do so, we have developed a segmental synthesizer to neutralize the effect of the spectrum.
This synthesizer use the F0 and linguistic features to predict the spectrum, the aperiodicity and the voicing informations.
Our system performed as good as the reference system and we observed a trend in the native speakers to prefer our methodology.",1.0,3.875
1298,2470,3.75    4.05    3.55    4.15    ,The Role of Fundamental Frequency in Speech Accommodation of Korean-English and English Speakers,"Speech accommodation refers to changes in speech production induced by perceiving speech. Among the parameters in which accommodation has been observed, fundamental frequency (f0) is of interest because it is not linguistically contrastive in English, yet it has been observed to covary with contrastive speech parameters. For this project, 10 participants were selected from an existing dataset (Tobin2015), based on 40 [k]-initial monosyllabic English words. A female native speaker of American English recorded the words, and participants repeated them in a shadowing task. In the present analysis, we evaluate accommodation in f0 among Korean–English bilinguals and English monolinguals with exposure to this model speaker. Overall, we see patterns of convergence towards the model speaker. Mismatch of L1 between participant and model talker did not inhibit convergence. Patterns were largely consistent within gender x language groups. The only group that did not converge was the female English monolingual group, whose mean f0 was similar to that of the model speaker, suggesting that the difference between participants’ baseline f0 and the model speaker’s f0 may have an impact on convergence. Such results demonstrate that patterns of accommodation are mediated by linguistic-phonetic and social factors, insofar as these are reflected in speech patterns.",1.0,3.875
1160,2002,4.3    3.3    5.1    2.8    ,Non-native Italian: Accent Recognition Experiments,"This paper focuses on human identification and automatic classification of read non-native Italian speech. We are interested in discriminating among the oral productions of 84 speakers whose mother-tongues are either Russian, English, German, French, Romanian, or Spanish, also weighed against a control group of 16 native speakers of Italian. 
Two approaches were adopted for the classification task, evaluated in a leave-one-speaker-out manner. First of all, we tested a standard GMM-SVM system, (13-dimensional MFCC features, ∆ and ∆∆; 256 diag. Gaussians), that yields an overall recognition rate of 0.43 (class-wise averaged: 0.42). Secondly, we evaluated a SVM system using a large set of prosodic features, yielding 0.36 recognition rate (class-wise averaged: 0.36). While the results seem similar, we discuss differences for individual classes. 
A human perception experiment with 288 Italian native speakers yielded 0.47 overall accuracy, significantly out-performing the previous classification results.",1.0,3.875
608,2211,3.55    3.35    4.25    4.35    ,Task Specific Sentence Embeddings for ASR Error Detection,"This paper presents a study on the modeling of automatic speech recognition errors at the sentence level. We aim in this study  to compensate certain phenomena highlighted by the analysis of the outputs generated by the ASR error detection system we previously proposed. We investigated three different approaches, that are based respectively on the use of sentence embeddings dedicated to ASR error detection task, a probabilistic contextual model  and a bidirectional long short term memory (BLSTM) architecture. An approach to build task-specific sentence embeddings is  proposed and compared to the Doc2vec approach. Experiments are performed on transcriptions generated by the LIUM ASR system applied to the ETAPE corpus. They show that the proposed sentence embeddings dedicated to ASR error detection achieve better results than generic sentence embeddings, and that the integration of task-specific sentence embeddings in our system achieves better results than the probabilistic contextual model and BLSTM models.",0.0,3.875
1148,1978,4.25    3.1    4.35    3.85    ,Prosody and Discourse Interpretation: Prosodic and Pragmatic Values of Discourse Particles in French,"This paper analyses prosodic properties of three discourse particles (DP) in French: ‘alors’ (‘so’), ‘bon’ (‘well’) and ‘donc’ (‘thus’), according to their different pragmatic functions. Occurrences of these words have been randomly extracted from a large speech database and they have been annotated manually, as DP or non-DP, and with pragmatic labels when DP. For each DP, prosodic properties are analysed for different pragmatic functions. Prosodic characteristics of occurrences of each pragmatic function (conclusive, introductive, etc.) are automatically extracted and for each DP and each pragmatic function, the most frequent F0 forms are retained as the representative form(s). Results show that a pragmatic function, common to several discourse particles, gives rise to a uniform prosodic marking. In these cases, DPs under question are most of the time commutable. Indeed, the similar prosodic marking would bring about an equivalency of the lexical content, highlighting thus the predominant role of the prosody when the marking of the pragmatic function is concerned.",1.0,3.8874999999999997
846,1180,4.8    2.75    2.9    5.1    ,Between-speaker Variability of Oral Voiceless Fricatives in Persian,"As well as varying in their acoustic structures from one language to another, fricatives have displayed great potentiality to vary substantially from individual to individual. In the present study, we explored speaker-specific acoustic parameters of oral voiceless fricatives in Persian with a particular focus on forensic-phonetic applications. The acoustic parameters investigated are the first two spectral moments (centre of gravity and standard deviation), intensity and duration. Our aim was to test whether oral voiceless fricatives and selected acoustic parameters are able to discriminate between Persian speakers and which parameters of which oral voiceless fricatives could explain the variability across speakers the best. Furthermore, we analyzed within-speaker variability within and between recording sessions. 12 male Persian speakers were recorded non-contemporaneously, on two occasions separated by one to two weeks. The results of the study indicated that intensity and centre of gravity are the most highly discriminant acoustic parameters across Persian speakers while fricative duration did not show an effect of speaker. The results also demonstrated that fricative /s/ carries the most speaker-specific information and thus might be most relevant in forensic phonetic caseworks.",1.0,3.8874999999999997
1200,2126,4.35    2.4    5.2    3.6    ,Real-Time Voice Activity Detection for Distant Speech,"Voice activity detection (VAD) is one of the first steps prior to doing many downstream speech processing tasks. However, performance degradation of VAD models under far-field conditions has not been studied extensively. In this paper, we investigate several VAD models for processing distant speech, ranging from simple linear classifiers, to those based on convolutional and recurrent neural networks. To evaluate a realistic online setting, we record several data sets collected in different distance and environmental conditions, with one property of the speech modified at a time, allowing us to investigate the mismatch between close-talking speech and distant speech in a controlled setting. We are able to characterize the performance of the models under different conditions, such as separating the degradation of reverberation and energy decay. We also compare real-time factors for all models. The results of our experiments show that a convolutional/recurrent neural model can achieve excellent VAD performance on a wide range of recording scenarios.",1.0,3.8874999999999997
973,1525,2.6    4.4    4.35    4.2    ,Data Augmentation for Robust Keyword Spotting under Playback Interference,"Accurate on-device keyword spotting (KWS) with low false accept and false reject rate is crucial to customer experience for far-field voice control of conversational agents. It is particularly challenging to maintain low false reject rate in real world conditions where there is (a) ambient noise from external sources such as TV, household appliances, or other speech that is not directed at the device (b) imperfect cancellation of the audio playback from the device, resulting in residual echo, after being processed by the Acoustic Echo Cancellation (AEC) system. In this paper, we propose a data augmentation strategy to improve keyword spotting performance under these challenging conditions. The training set audio is artificially corrupted by mixing in music and TV/movie audio, at different signal to interference ratios. Our results show that we get around 30-45% relative reduction in false reject rates, at a range of false alarm rates, under audio playback from such devices.",1.0,3.8875
975,1532,3.8    3.2    4.35    4.2    ,Measuring Conversational Productivity in Child Forensic Interviews,"Child Forensic Interviewing (FI) presents a challenge for effective information retrieval and decision making. The high stakes associated with the process demand that expert legal interviewers are able to effectively establish a channel of communication and elicit substantive knowledge from the child-client while minimizing potential for experiencing trauma. As a first step toward computationally modeling and producing quality spoken interviewing strategies and a generalized understanding of interview dynamics, we propose a novel methodology to computationally model effectiveness criteria, by applying summarization and topic modeling techniques to objectively measure and rank the responsiveness and conversational productivity of a child during FI.We score information retrieval by constructing an agenda to represent general topics of interest and measuring alignment with a given response and leveraging lexical entrainment for responsiveness. For comparison, we present our methods along with traditional metrics of evaluation and discuss the use of prior information for generating situational awareness",1.0,3.8875
441,1780,4.05    4.2    3.45    ,Investigating Generative Adversarial Networks Based Speech Dereverberation for Robust Speech Recognition,"We investigate the use of generative adversarial networks (GANs) in speech dereverberation for robust speech recognition. GANs have been recently studied for speech enhancement to remove additive noises, but there still lacks of a work to examine their ability in speech dereverberation and the advantages of using GANs have not been fully established. In this paper, we provide deep investigations in the use of GAN-based dereverberation front-end in ASR. First, we study the effectiveness of different dereverberation networks (the generator in GAN) and find that LSTM leads a significant improvement as compared with feed-forward DNN and CNN in our dataset. Second, further adding residual connections in the deep LSTMs can boost the performance as well. Finally, we find that, for the success of GAN, it is important to update the generator and the discriminator using the same mini-batch data during training. Moreover, using reverberant spectrogram as a condition to discriminator, as suggested in previous studies, may degrade the performance. In summary, our GAN-based dereverberation front-end achieves 14%--19% relative CER reduction as compared to the baseline DNN dereverberation network when tested on a strong multi-condition training acoustic model.",0.0,3.9
1299,2472,3.1    5    3.85    3.65    ,Bag of Audio Words Based on Auto-Encoder Codebook for Continuous Emotion Prediction,"In this paper we present a new way of extracting the well-known Bag-of-Words (BoW) representation based on a Neural Network codebook (BoW-NN). The conventional BoW model is based on a dictionary (codebook), built from elementary representations, selected randomly or by using a clustering algorithm on a training dataset. A metric is then used to assign unseen elementary representations to the closest dictionary entries in order to produce a histogram. In the proposed BoW-NN model, an autoencoder (AE) encompasses the role of both the dictionary and the assignment metric. The dimension of the AE hidden layer represents the size of the dictionary and the values of its neurons represents the assignment metric. Experiments on continuous emotion prediction task using AVEC 2017 audio test set have shown an improvement of the Concordance Correlation Coefficient (CCC) from 0.225 to 0.322 for Arousal and from 0.244 to 0.368 for Valence dimensions relative to the conventional BoW version implemented in baseline system.",1.0,3.9
898,1325,4.15    3.9    2.45    5.1    ,A Speech Corpus for the Albanian Language,"Speech data availability for high-resource languages has led to breakthrough performance in speech recognition and to new insights into the properties and evolution of these languages. Scaling these tools and insights to understudied languages requires the development of new corpora for the scientific community. In this paper, we introduce the first-ever corpus for the Albanian language. We first introduce a large text corpus of 3.16 million unique sentences generated by mining 255 Albanian news media sources. The text corpus is tokenized at the sentence level and a set of prompts are selected from these sentences to record a new speech database. The speech corpus consists of 2000 unique sentences, spoken by 49 speakers (21 female and 28 male). We use the text and speech corpora to train and compare several speech recognition models with the best performing model yielding a word error rate of  19.03% on out-of-sample speakers.",1.0,3.9
33,991,5.05    2.6    2.9    5.05    ,Prediction of Aesthetic Elements in Karnatic Music: a Machine Learning Approach,"Gamakas, the embellishments and ornamentations used to enhance musical experience, are defining features of Karnatic Music (KM). The appropriateness of using gamaka is determined by aesthetics and is often developed by musicians with experience. Therefore, understanding and modeling gamaka is a significant bottleneck in applications like music synthesis, automatic accompaniment, etc. in the context of KM. To this end, we propose to learn both the presence and the type of gamaka in a data-driven manner using annotated symbolic music. In particular, we explore the efficacy of three classes of features – note-based, phonetic and structural – and train a Random Forest
Classifier to predict the existence and the type of gamaka. The observed accuracy is ∼70% for gamaka detection and ∼60% for gamaka classification. Finally, we present an analysis of the features and find that frequency and duration of the neighbouring notes prove to be the most important features.

Index Terms: gamaka, Karnatic Music, Symbolic Music, Random Forest Classifier.",0.0,3.9000000000000004
867,1220,3.5    3.7    3.45    4.95    ,A Federated Speech Enhancement Algorithm Based on Microphone Array,"This paper proposes a federated speech enhancement algorithm combining the coherence-based algorithm and the differential beamforming. The coherence-based algorithm is applied to the broadside dual-microphone, which filters the side noise better than the previous method benefiting from the more accurate signal-to-noise ratio (SNR) estimation. However, the broadside array cannot eliminate the rear noise. Therefore, we apply the differential beamforming to the endfire dual-microphone to form a null at 180° in the beambattern. The improvement in speech quality evaluation reveals that, in condition of various noise scenarios and different input SNR from -10dB to 10dB, the proposed algorithm is more effective than the previous coherence-based algorithm and the ordinary differential beamformer with the triple-microphone.",1.0,3.9000000000000004
680,2413,2.85    4.2    4.35    4.2    ,"An Ensemble of Transfer, Semi-supervised and Supervised Learning Methods for Pathological Heart Sound Classification","In this work, we propose an ensemble of classifiers to distinguish between various degrees of abnormalities of the heart using Phonocardiogram (PCG) signals acquired using digital stethoscopes in a clinical setting, for the INTERSPEECH 2018 Computational Paralinguistics (ComParE) Heart Beats Sub-Challenge. Our primary classification framework constitutes a convolutional neural network with 1D-CNN time-convolution (tConv) layers, which uses features transferred from a model trained on the 2016 Physionet Heart Sound Database. We also employ a Representation Learning (RL) approach to generate features in an unsupervised manner using Deep Recurrent Autoencoders and use Support Vector Machine (SVM) and Linear Discriminant Analysis (LDA) classifiers. Finally, we utilize an SVM classifier on a high-dimensional segment-level feature extracted using various functionals on short-term acoustic features, i.e., Low-Level Descriptors (LLD). An ensemble of the three different approaches provides a relative improvement of 11.13\% compared to our best single sub-system in terms of the Unweighted Average Recall (UAR) performance metric on the evaluation dataset.",0.0,3.9000000000000004
907,1355,4.2    4.3    3.45    3.7    ,Two-Stage Enhancement of Noisy and Reverberant Microphone Array Speech for Automatic Speech Recognition Systems Trained with Only Clean Speech,"We propose a two-stage approach to enhancement of far-field microphone array speech collected in reverberant conditions corrupted by interfering speakers and noises. We intend to produce top-quality enhanced speech to be used by a black-box automatic speech recognition (ASR) system already trained with clean speech. We explore different deep neural network (DNN) architectures and the best configuration comprises two stages. First, in pre-enhancement, we utilize features in temporal context in a subset of microphones to perform enhancement. Second, in integration, we concatenate the enhanced and noisy features from all microphones to estimate anechoic speech of a reference channel as the overall output. Tested on eight speakers, each with 5 minutes of speech for DNN training, from the Wall Street Journal corpus, at a signal-to-interference-plus-noise-ratio level of 5-15dB, at a distance of 1-5m and a reverberation time of 0.2-0.3s, our best 8-channel, speaker-dependent enhancement DNN attains a perceptual evaluation of speech quality score of 2.95, up from 2.43 for our single-channel system. Followed by speaker-independent ASR for a 230K-word recognition task, we achieve a word error rate of 7.16%, down from 17.89% for enhanced speech of the single-channel system, and from 49.51% for unprocessed noisy speech of the reference channel.",1.0,3.9124999999999996
1272,2363,2.7    4.2    4.85    ,Lexical Tones in Deori,"Deori, a Tibeto-Burman (henceforth TB) language belonging to the Bodo-Koch language group can be unambiguously called an endangered language. This paper studies the current use of lexical tone distinctions in Deori and describes the language change attested in the speech of the younger generation. Most recently, some remnant lexical tonal contrast in the speech of the older generation speakers has been reported in Deori with acoustic evidence [13]. Following the minimal tonal contrast prevalent among the older generation [13] a production experiment was conducted to determine the tonal distinction in the speech of the younger generation. From the analysis it is apparent that tone distinction in the speech of the younger generation is not clear. The results show that there is considerable phonetic variation across different speakers. It shows a trend of underlying tone reversal H>L; L>H in monosyllables and a considerable F0 variation in the disyllabic stems. This study investigates the initial stages of tonoexodus in progress in Deori in the speech of younger generation speakers and a shift towards a metrical prominence stage which supports the idea conjectured in a previous study [13].",1.0,3.9166666666666665
862,1208,3.6    3.65    4.1    4.35    ,Towards Unsupervised Automatic Speech Recognition Trained by Unaligned Speech and Text Only,"Automatic speech recognition (ASR) has been widely researched with supervised approaches, while many low-resourced languages lack audio-text aligned data, and supervised methods cannot be applied on them. 
  In this work, we propose a framework to achieve unsupervised ASR on a read English speech dataset, where audio and text are unaligned. 
In the first stage, each word-level audio segment in the utterances is represented by a vector representation extracted by a sequence-of-sequence autoencoder, in which phonetic information and speaker information are disentangled.
 Secondly, semantic embeddings of audio segments are trained from the vector representations using a skip-gram model. 
Last but not the least, an unsupervised method is utilized to transform semantic embeddings of audio segments to text embedding space, and finally the transformed embeddings are mapped to words.
  With the above framework, we are towards unsupervised ASR trained by unaligned text and speech only.",1.0,3.925
825,1136,4.85    4.25    2.65    3.95    ,A Comparable Study of Modeling Units for End-to-end Mandarin Speech Recognition,"End-To-End speech recognition have become increasingly popular in mandarin speech recognition and achieved delightful performance.Mandarin is a tonal language which is different from English and requires special treatment for the acoustic modeling units. There have been several different kinds of modeling units for mandarin such as phoneme, syllable and Chinese character.In this work, we explore two major end-to-end models: connectionist temporal classification (CTC)  model and attention based encoder-decoder model for mandarin speech recognition. We compare the performance of three different scaled modeling units: context dependent phoneme(CDP), syllable with tone and Chinese character. We find that all types of modeling units can achieve approximate character error rate (CER) in CTC model and the performance of Chinese character attention model is better than syllable attention model. Furthermore, we find that Chinese character is a reasonable unit for mandarin speech recognition. On DidiCallcenter task, Chinese character attention model achieves a CER of 5.68% and CTC model gets a CER of 7.29%,  on the other DidiReading task, CER are 4.89% and 5.79%, respectively. Moreover, attention model achieves a better performance than CTC model on both datasets.",1.0,3.925
66,1054,5    2.55    3.3    4.85    ,Towards a Better Characterization of Parkinsonian Speech: a Multidimensional Acoustic Study,"This paper reports on a first attempt at adopting a new perspective in characterizing speech disorders in Parkinson's Disease (PD) based on individual patient profiles. Acoustic data were collected on 13 Belgian French speakers with PD, 6 male, 7 female, aged 45-81, and 50 healthy controls (HC) using the ""MonPaGe"" protocol (Fougeron et al., LREC2018). In this protocol, various kinds of linguistic material are recorded in different speech conditions, in order to assess multiple speech dimensions for each speaker. First, we compared a variety of voice and speech parameters across groups (HC vs. PD patients). Second, we examined individual profiles of PD patients. Results showed that as a group PD participants most systematically differed from HC in terms of speech tempo and rythm. Moreover, the analysis of individual profiles revealed that other parameters, related to pneumophonatory control and linguistic prosody, were valuable to describe the speech specificities of several PD patients.",0.0,3.925
843,1175,3.45    5.15    3.45    3.65    ,Text-Independent Speaker Verification Based on Deep Neural Networks and Segmental Dynamic Time Warping,"We present a method for text-independent speaker verification that combines segmental dynamic time warping (SDTW) and the d-vector approach. The d-vectors, generated from a feed forward deep neural network trained to distinguish between speakers, are used as features to perform alignment and hence calculate the overall distance between the enrolment and test utterances. Generalization to other network architectures could be easily achieved.
We present results on the NIST 2008 data set for speaker verification where the proposed method outperforms the conventional d-vector approach with local distances based on cosine and probabilistic linear discriminant analysis (PLDA) scores. Also score combination with the i-vector/PLDA baseline leads to significant gains.",1.0,3.9250000000000003
903,1337,5.4    3.2    3.2    ,Unsupervised Representation Learning of Speech for Dialect Identification,"In this paper, we explore the use of a Factorized Hierarchical Variational Autoencoder (FHVAE) model to learn an unsupervised latent representation for the task of Dialect Identification (DID). An FHVAE can learn a latent space that is able to separately model the more static characteristics (e.g., speaker, channel) of a time sequence representation from the more temporally dynamic characteristics (e.g., phonetic).  Useful factors for dialect identification such as phonetic or linguistic content are encoded by a segmental latent variable, while content such as channel or speaker information, which is relatively consistent within a sequence, is encoded by a sequential latent variable. The FHVAE process makes the segmental latent variable less susceptible to channel and speaker variation and thus reduces degradation from channel domain mismatch.  In our reported DID experiments, the FHVAE model achieved the best performance compared to i-vector and end-to-end dialect identification systems trained on conventional acoustic features. We also observed that the proposed approach significantly improved the DID system on the resource-poor condition when in-domain dataset labels are not available.",1.0,3.9333333333333336
1078,1793,3.45    5    5    2.3    ,Dialect Identification Using Tonal and Spectral Features in Two Dialects of Ao,"Ao is an under-studied and under-resourced Tibeto-Burman language spoken in Nagaland, India. It is a tone language and has three lexical tones, namely, high, mid and low. There are three distinct dialects of the language namely, Chungli, Mongsen and Changki, where the tone assignment on lexical words among the dialects may be different. The goal of this study is to ascertain if dialect specific tone assignment in Ao can be utilized to automatically identify two Ao dialects, namely, Changki and Mongsen. Considering that, a perception study was conducted on the native speakers of Ao to confirm that they can indeed identify the two dialects based on dialect specific tone information. Secondly, to confirm that tone is the primary cue in dialect identification in the two varieties, speech stimuli were neutralized for tone by flattening the F0 contours to static F0 values. This set of stimuli was subjected to a GMM based dialect identification system that yielded low accuracy rates of identification, confirming that pitch did affect the recognition of the two Ao dialects.  Finally, the raw speech stimuli were subjected to a GMM based dialect identification system with tonal and spectral features, yielding better dialect recognition accuracy.",1.0,3.9375
1070,1775,4.15    5.05    2.6    3.95    ,Data Augmentation Based on Voice Conversion for Automatic Speech Recognition,"In this work we explore the possibility of data augmentation using voice conversion for automatic speech recognition (ASR). Voice conversion for real-time application scenario involves operating on non-parallel data, i.e., source and target speakers do not repeat the same texts. A recent Wasserstein generative adversarial network was used for performing non-parallel voice conversion. Data generated from the proposed voice conversion system were used to augment the training set of Aurora-4. The ASR experiments were performed with fully-connected and convolutional neural networks based acoustic models. The results have shown that the proposed data augmentation based on voice conversion improves the word error rate by 4% relative for both acoustic models using standard log-Mel filterbank features.",1.0,3.9375
929,1412,3.4    4.25    4.2    3.65    4.2    ,Attention-based Deep Recurrent Neural Network for Speech Enhancement,"Integration of contextual information is a common idea to improve the performance of speech enhancement. One of the widely used methods is to splice multiple adjacent frames from both sides of the current input. The addition of more context frames usually leads to a better speech quality. However, adding a large amount of frame also degrade the performance because it introduces more irrelevant information. In this paper, we propose an attention-based deep recurrent neural network model for speech enhancement (ARMSE) to help better utilize the context by encoding relevant frames into a context vector. The context vector provides additional cues for noisy speech structures and is expected to have better performance on mismatch conditions by using dynamic context information, similar to the noise-aware training (NAT) technique. Also, to reduce the computational complexity, attention mechanism is modified to focus on a limited scope around current frame instead of the whole speech. We compare the performance of our model with related works on benchmark datasets under different signal-to-noise (SNR) conditions. Experimental results show that it outperforms the state-of-the-art approaches in all the tests, and it performs especially well in unseen noise condition.",1.0,3.9400000000000004
563,2079,2.6    4.4    4.45    4.35    ,CRIM's System for the MGB-3 English Multi-Genre Broadcast Media Transcription,"The second English Multi-Genre Broadcast Challenge (MGB-3) is a controlled evaluation of speech recognition and lightly supervised alignment using BBC TV recordings.  CRIM is participating in the speech recognition part of the challenge. This paper presents CRIM's contributions to the MGB-3 transcription task.  This task is inherently more difficult than the first task as the training audio has been reduced from 1200 hours to 500 hours. CRIM's main contributions are experimentation with bidirectional LSTM models and lattice-free MMI (LF-MMI) trained TDNN models for acoustic modeling, LSTM and DNN models for speech/non-speech detection for input to speaker diarization, and LSTM language models for rescoring lattices. We also show that adding senone posteriors to the input of LSTM and DNN models for speech/non-speech detection (VAD) reduces error rate. CRIM's best single decoding WER for the MGB-3 dev17 dev set (with reference segmentation) went down from 27.6% (with our MGB-1 challenge system) to 24.1% for this task using the LF-MMI trained TDNN models.  The final WER on dev17 set (after VAD) is 20.9%, and on the new dev18 development set is 20.8%.",0.0,3.9499999999999997
817,1116,4.1    4.1    3.35    4.25    ,End-to-end Language Identification Using NetFV and NetVLAD,"In this paper, we apply the NetFV and NetVLAD layers  for the end-to-end language identification task. NetFV and NetVLAD layers are the differentiable implementations of the standard Fisher Vector
and Vector of Locally Aggregated Descriptors methods, respectively. Both of them can encode  a sequence of feature vectors into a fixed dimensional vector which is very important to process those variable-length utterances. 
We first present the relevances and differences between the traditional framework (Gaussian Mixture Model + i-vector) and the aforementioned encoding schemes. 
Then, we construct a flexible end-to-end framework including a convolutional neural network (CNN) architecture and an encoding layer (NetFV or NetVLAD) for the language identification task.
Experimental results on the NIST LRE 2007 close-set task show that the proposed system achieves significant EER reductions against the conventional i-vector baseline and the CNN temporal average pooling system, respectively.",1.0,3.9499999999999997
1130,1930,5    2.65    4.95    3.2    ,Using Recurrent Neural Network Point Process Model to Predict Next Turn-Taking Event: a Case Study in Autism Diagnostic Interviews,"Studies show that humans have developed a unique mechanism in carrying out smooth turn-taking in conversations, specifically leveraging verbal/non-verbal cues (prosody, syntax, and gaze directions, etc) to signify and anticipate occurrence of a turn-taking event. Researches have also investigated patterns on the sequence of turn taking events in order to understand the conversational dynamics across contexts. In this work, we propose to use recurrent neural network point process model (RNNPP) to jointly model both the history of turn-taking events and the local turn-related behavior signals in order to predict the next turn-taking event. We evaluate our method on a spontaneous interaction data collected during the Autism Diagnostic Observation Schedule (ADOS). Our result shows that by using a 5 seconds window of memory in capturing  past sequence of events along with the current-turn local prosodic cues help obtain a prediction result of 0.59 in a four turn-taking event type prediction with an average timing prediction error to be 2.22 second. Our analysis reveal that the prediction accuracy is inversely correlated with the ADOS behavior codes of ``Report of Events"".",1.0,3.95
1240,2237,4.2    3.75    5.2    2.65    ,Acoustic Modeling Data Restorers: Generative Models of Acoustic Modeling Data with Neural Language Modeling and Neural Speech Synthesis,"This paper proposes a novel generative model that can model generative process of acoustic modeling data. In automatic speech recognition (ASR) fields, speech log data collected during practical services involve customers' personal privacy information, so the log data have to often be preserved in segregated storage. Our motivation is to permanently and flexibly utilize the log data for acoustic modeling without moving them from the segregated storage. Our key idea is to construct a portable model that can artificially restore the acoustic modeling data. The proposed model called acoustic modeling data restorer can randomly sample a triplet of a senone sequence, an acoustic feature sequence, and utterance attribute information without using the original data. In order to precisely model generative process of the acoustic modeling data, we introduce neural language modeling for generating the senone sequences, and neural speech synthesis for generating the acoustic feature sequences. In experiments, we show that an acoustic model constructed from the restored data can yield encouraging ASR performance, although the quality of the restored data still does not match that of the original data. In addition, we demonstrate the restored data can improve ASR performance by leveraging them for data augmentation.",1.0,3.95
462,1828,3.5    3.75    5.15    3.4    ,Non-Uniform Spectral Smoothing for Robust Children's Speech Recognition,"Insufficient spectral smoothing during front-end speech parametrization results in pitch-induced distortions in the short-time magnitude spectra. This, in turn, degrades the performance of an automatic speech recognition (ASR) system for high-pitched speakers. Motivated by this fact, a non-uniform spectral smoothing algorithm is proposed in this paper in order to mitigate the acoustic mismatch resulting from pitch differences. In the proposed technique, the speech utterance is first segmented into vowel and non-vowel regions. The short-time magnitude spectrum obtained by discrete Fourier transform is then processed through a single-pole low-pass filter with different pole values for vowel and non-vowel regions. Sufficiently smoothed spectra is obtained by keeping higher values for the pole in the case of vowels while lower values are chosen for non-vowel regions. The Mel-frequency cepstral coefficients computed using the derived smoothed spectra are observed to be less affected by pitch variations. In order to validate this claim, an ASR system is developed on speech from adult speakers and evaluated on a test set which consists of children's speech to simulate large pitch differences. The experimental evaluations as well as signal domain analyses presented in this paper support the claim.",0.0,3.95
108,1126,1.45    5    4.45    4.9    ,Formant Measures of Vowels Adjacent to Alveolar and Retroflex Consonants in Arrernte: Stressed and Unstressed Position,"This study presents formant data for six speakers of Arrernte,
a language of central Australia. The focus of the study is the
(marginal) phonemic contrast between two sets of apical
consonants: alveolar and retroflex. The apical contrast is
studied for the stop, nasal and lateral manners of articulation:
/t ʈ/, /n ɳ/ and /l ɭ/. The apical consonants are examined both in
strong prosodic context (preceding a stressed vowel) and in
weak prosodic context (preceding an unstressed vowel).
Formant data are sampled 10 ms before the onset of the
consonant, and 10 ms after the offset of the consonant. Results
show no differences in F2 or F4 in the various conditions
studied, and results for F1 show differences between
obstruents and sonorants. F3 is lower at consonant onset than
consonant offset for retroflex stops in the weak prosodic
context, and to a lesser extent for retroflex stops in the strong
prosodic context; it is also lower for laterals in the weak
prosodic context. Other effects on F3 suggest that the apical
contrast is most clearly realized for the stop manner of
articulation.",0.0,3.95
1056,1735,4.9    5.1    2    3.8    ,Building an Articulatory Speech Synthesizer from Static Coarticulation-Aware Articulatory Targets,"This work describes a pipeline of an articulatory speech synthesizer. As an input, it takes the transcription of a speech sound, syllable or a phrase to be generated. It processes the input to predict the coarticulatory effects that will take place and then determines the necessary articulatory targets. The targets can come either from our dataset, which consists of 97 static magnetic resonance images capturing the articulation of French vowels and blocked consonant-vowel syllables, or from its extension which provides for all possible sound combinations, even those that have not been seen in the initial dataset. The targets are interpreted within a principal-component-analysis (PCA)-based articulatory model. The algorithm creates transitions between the chosen articulatory targets in synchrony with operating the source (glottal opening), which results in a sequence of area functions along with other control parameters. Finally, all the generated data is given to an acoustic simulation system that produces speech. The system was tested on vowels, fricatives, stops and their combinations. The evaluation criteria were the naturalness of the generated articulatory movements, the acoustics of the obtained sound and its perceptual qualities.",1.0,3.95
840,1169,4.2    4.15    4.85    2.6    ,Mixed-Bandwidth Acoustic Model Training Using Multi-Task Learning Architectures,"Deep neural network (DNN) for acoustic modeling relies on the availability of large amounts of training data to be successfully applied in the target application.  In this paper, we investigate DNN acoustic modeling techniques to address the bandwidth as well as the domain mismatch problem for recognition of narrowband speech, when only a small amount of in-domain is available for training the deep models. 
Our aim is to determine the optimal way addressing the domain and bandwidth mismatch problems by leveraging large corpora of wideband data, which have become available in recent years through advances in the data acquisition electronics. 
Our mixed bandwidth adaptation is achieved through multi-task learning and a hierarchical DNN structure, called multi-level adaptive networks (MLAN) whereby the network jointly predicts the wideband and the narrowband targets so that a single model can be used to recognize both narrow and wideband speech without the need for retraining. 
Our findings show that wideband training data can be exploited to improve the recognition performance of narrowband speech of air traffic controllers through proposed mixed bandwidth adaptation techniques and that a single model can be used to recognise both narrowband and wideband speech without significant degradation in the recognition rate.",1.0,3.95
474,1857,5.05    4.25    2.55    ,Exemplar-based Speech Waveform Generation,"This paper presents a simple but effective method for generating speech waveforms by selecting small units of stored speech to match a low-dimensional target representation. The method is designed as a drop-in replacement for the vocoder in a deep neural network-based text-to-speech system. Most previous work on hybrid unit selection waveform generation relies on phonetic annotation for determining unit boundaries, or for specifying target cost, or for candidate preselection. In contrast, our waveform generator requires no phonetic information, annotation, or alignment. Unit boundaries are determined by epochs, and spectral analysis provides representations which are compared directly with target features at runtime. As in unit selection, we minimise a combination of target cost and join cost, but find that greedy left-to-right nearest-neighbour search gives similar results to dynamic programming. The method is fast and can generate the waveform incrementally. We use publicly available data and provide a permissively-licensed open source toolkit for reproducing our results.",0.0,3.9500000000000006
41,1013,3.35    4.35    4.95    3.2    ,Capsule Networks for Low Resource Spoken Language Understanding,"Designing a spoken language understanding system for command-and-control applications can be challenging because of a wide variety of domains and users or because of a lack of training data. In this paper we discuss a system that learns from scratch from user demonstrations. This method has the advantage that the same system can be used for many domains and users without modifications and that no training data is required prior to deployment. The user is required to train the system, so for a user friendly experience it is crucial to minimize the required amount of data. In this paper we investigate whether a capsule network can make efficient use of the limited amount of available training data. We compare the proposed model to an approach based on Non-negative Matrix Factorisation which is the state-of-the-art in this setting and another deep learning approach that was recently introduced for end-to-end spoken language understanding. We show that the proposed model outperforms the baseline models for three command-and-control applications: controlling a small robot, a vocally guided card game and a home automation task.",0.0,3.9624999999999995
878,1273,2.6    4.5    3.65    5.1    ,Automatic Speech Expression Evaluation in Computer-assisted Language Learning: a Deep Learning Approach,"Computer assisted language learning (CALL) has attracted increasing
interest in language teaching and learning. In the
computer-supported learning environment, both pronunciation
correction and expression modulation are certified to be essential
for contemporary learners. However, while mispronunciation
detection and diagnosis (MDD) technologies have achieved
significant successes, speech expression evaluation is still relied
on expensive and resources consuming manual assessment.
In this paper, we proposed a novel multi-modal neural network
based approach for automatic speech expression evaluation
in CALL. In particular, a multi-modal sparse auto encoder
(MSAE) is firstly employed to make full use of both lexical
and acoustic features. A recurrent auto encoder (RAE) is further
employed to down-sample the features with less compression
distortion and reduce the computing complexity of the following
bi-directional long-short term memory (BLSTM) model
which is employed to score the speech expression. Experimental
results using data collected from realistic airline broadcast
evaluation demonstrate the effectiveness of the proposed approach,
achieving a human-level predictive ability with acceptable
rate 70.4%.",1.0,3.9625
15,60,5.1    2.25    5.1    3.4    ,The Trajectory of Voice Onset Time with Vocal Aging,"Vocal aging, a universal process of human aging, can largely affect one's language use, possibly including some subtle acoustic features of one's utterances like Voice Onset Time. To figure out the time effects, Queen Elizabeth's Christmas speeches are documented and analyzed in the long-term trend. We build statistical models of time dependence in Voice Onset Time, controlling a wide range of other fixed factors, to present annual variations and the simulated trajectory. It is revealed that the variation range of Voice Onset Time has been narrowing over fifty years with a slight reduction in the mean value, which, possibly, is an effect of diminishing exertion, resulting from subdued muscle contraction, transcending other non-linguistic factors in forming Voice Onset Time patterns over a long time.",0.0,3.9625
949,1469,3.6    3.35    4.75    4.15    ,Single-Channel Speech Enhancement Using Noise Estimation with Adaptive Dynamic Quantile Tracking,"A single-channel speech enhancement technique using adaptive dynamic quantile tracking for estimating the noise spectrum is presented. It involves estimating a histogram for each spectral sample by dynamically tracking eight quantiles. Each quantile is updated recursively, without involving storage and sorting of past spectral samples, using an increment determined by the dynamically estimated range and a convergence factor dependent on the speech-presence probability. The histogram peak is used as the adaptive quantile for estimating the noise at each spectral sample. The performance of the proposed noise estimation technique in combination with spectral subtraction based on the geometric approach is evaluated using the GRID database. Enhancement of speech corrupted with different types of additive stationary and non-stationary noise showed improvements of 0.10 − 0.49 in the PESQ score.",1.0,3.9625
933,1418,4.6    2.75    3.6    4.9    ,Semi-supervised Robust Feature Selection with Lq-norm Graph for Multiclass Classification,"Flexible manifold embedding (FME) is a semi-supervised dimension reduction framework. It has been extended into feature selection by using different loss functions and sparse regularization methods. However, this kind of methods used the quadratic form of graph embedding, thus the results are sensitive to noise and outliers. In this paper, we propose a general semi-supervised feature selection model that optimizes an Lq-norm of FME to decrease the noise sensitivity. Compare to the fixed parameter model, the Lq-norm graph brings flexibility to balance the manifold smoothness and the sensitivity to noise by tuning its parameter. We present an efficient iterative algorithm to solve the proposed Lq-norm graph embedding based semi-supervised feature selection problem, and offer a rigorous convergence analysis. Experiments performed on typical speech emotion datasets demonstrate that our method is effective to the multiclass classification task, and outperforms the related state-of-the-art methods.",1.0,3.9625
388,1675,3.25    5.1    4.1    3.4    ,Effectiveness of Speech Demodulation-Based Features for Replay Detection,"Replay attack presents a great threat to Automatic Speaker Verification
(ASV) system. The speech can be modeled as amplitude
and frequency modulated (AM-FM) signals. In this
paper, we explore speech demodulation-based features using
Hilbert transform (HT) and Teager Energy Operator (TEO) for
replay detection. In particular, we propose features, namely,
HT-based Instantaneous Amplitude (IA) and Instantaneous Frequency
(IF) Cosine Coefficients (i.e., HT-IACC and HT-IFCC)
and Energy Separation Algorithm (ESA)-based features (i.e.,
ESA-IACC and ESA-IFCC). For adapting instantaneous energy
w.r.t given sampling frequency, ESA requires 3 samples
whereas HT requires relatively large number of samples and
thus, ESA gives high time resolution.The experiments were
performed on ASV spoof 2017 Challenge database for replay
spoof speech detection (SSD).The experimental results shows
that ESA-based features gave lower EER. In addition, linearly-spaced
Gabor filterbank gave lower EER than Butterworth filterbank.
To explore possible complementary information using
amplitude and frequency, we have used score-level fusion of IA
and IF. With HT-based feature set, the score-level fusion gave
EER of 5.24 % (dev) and 10.03 % (eval), whereas ESA-based
feature set reduced the EER to 2.01 % (dev) and 9.64 % (eval).",0.0,3.9625
177,1254,4.1    3.45    3.9    4.4    ,A First Investigation of the Timing of Turn-taking in Ruuli,"Turn-taking behavior in conversation is reported to be universal among cultures, although the language-specific means used to accomplish smooth turn-taking are likely to differ. Previous studies investigating turn-taking have primarily focused on languages which are already heavily-studied. The current work investigates the timing of turn-taking in question-response sequences in naturalistic conversations in Ruuli, an under-studied Bantu language spoken in Uganda. We extracted sequences involving wh-questions and polar questions, and measured the duration of the gap or overlap between questions and their following responses, additionally differentiating between different response types such as affirmative (i.e. type-conforming) or negative (i.e. non-type-conforming) responses to polar questions. We find that the timing of responses to various question types in Ruuli is consistent with timings that have been reported for a variety of other languages, with a mean gap duration between questions and responses of around 259 ms. Our findings thus emphasize the universal nature of turn-taking behavior in human interaction, despite Ruuli's substantial structural differences from languages in which turn-taking has been previously studied.",0.0,3.9625
1007,1617,4.95    3.8    2.3    4.8    ,Topically Driven Conversational Models,"Most Sequence-to-sequence neural conversation models have an ubiquitous problem that they tend to generate trivial or safe responses, such as “I don’t know” or “I’m OK”. In this paper, we study the response generation problem of open-domain chatbots to achieve diversity and scalability. We propose a topically driven conversational model(TDCM) via incorporating topic information into the sequence-to-sequence framework. Our model has two components: the topic generation model and the topically driven seq2seq model. We train them jointly by treating each component as a sub-task in a multi-task learning setting. Base on observing that people often subconsciously extract the theme of the input message and then generate a follow-up response for the topic. We predict the possible topic of the responses by generating the input message topic, thus increasing the diversity of responses and reducing the probability of high-frequency generic responses. Therefore, our model not only extracts the message topic, also predicts the response topic. Evaluation results on large scale dataset indicate that our model can significantly outperform state-of-the-art methods on response generation of the conversation system.",1.0,3.9625000000000004
851,1189,3.4    3.5    5.2    3.75    ,Speaker Verification Based on Deep Neural Network for Text-Constrained Short Utterances,"Speaker verification under the condition of short utterances has
been a tough task. However, deep neural networks have shown
great performance in feature extraction, so we have investigated
how to exploit it for short utterances. In this paper, we propose
an effective way to train a model and extract good features for
speaker verification with various cases. Our proposed approach
achieves 5.89% equal error rate on word scale utterances
shorter than 1 second, and with linear discriminative analysis, it
decreases to 3.43%.",1.0,3.9625000000000004
780,998,5.2    2.35    4.35    ,Visually Grounded Cross-Lingual Keyword Spotting in Speech,"There is growing interest in models that learn from images paired with speech. Visual context can be used to ground speech in the absence of transcriptions, e.g. for unwritten languages. We ask whether visual grounding can be used for cross-lingual keyword spotting. Given a text keyword in one language, the task is to retrieve spoken utterances containing translations of that keyword in another language (the search language). This could enable searching through speech data in a (potentially) low-resource language using text queries in a high-resource language. We train models solely on images of scenes paired with untranscribed spoken descriptions (in the search language). As a proof-of-concept, we use English speech with German queries: we use an external German visual tagger to add keyword labels to each training image, and then train a convolutional neural model to directly map English speech to German keywords. Without seeing any parallel speech-transcriptions or translations, the model achieves a precision at ten of 58%. In an exhaustive error analysis, we find that most errors result from descriptions that do not contain the query exactly, but equivalent or semantically relevant keywords. Excluding these cases would improve P@10 to 91%.",1.0,3.966666666666667
962,1501,3.3    4.25    4.4    3.95    ,Acoustic Analysis of Ewe Vowels with a Multi-Modal Ewe-Mandarin Speech Corpus,"Multi-modal speech database is important to understand the physiological mechanism of the speech production and provides effective information for speech processing services. In this work, a multi-modal Ewe-Mandarin speech dataset was collected from Ewe native speakers using a recording system composed of a synchronized electromagnetic articulography, ultrasound, and audio system. The formants frequencies were analyzed, and the vowel and articulatory spaces were described. The results showed that in Ewe vowels system, /i/ has the widest closing, foremost tongue position, /u/ the widest closing, rearmost tongue position and /a/ the widest opening, rearmost tongue position. Compared to males, Ewe females have the highest mouth opening degree with the tongue position more posterior and the greater lip rounding degree, except for /a/ where the mouth opening degree is smaller compared to males, and /o/, /ɔ/ where the vowel articulation is the same with males. A comparison of Mandarin and Ewe vowel spaces produced by Ewe speakers showed a similarity between the two vowel spaces; the Mandarin vowel spaces produced by Ewe males and females are affected by their native language",1.0,3.9749999999999996
38,1000,1.9    4.7    5    4.3    ,The CSU-K Rule-Based System for the 2Nd Edition Spoken CALL Shared Task,"This paper presents the set-up and results of the rule-based
Cooperative State University Karlsruhe (CSU-K) system 
for the 2nd edition of the shared spoken CALL ESL task.
The data was collected from Swiss teenage students 
using a speech-enabled online tool for English conversation 
practice. The tool should eventually be able to judge student 
input with respect to syntactic and semantic correctness.
The tasks consisted of training data of a German 
text prompt with the associated audio file  
containing an English language response by the students.
In the second edition of the task, 6.698 utterances were 
provided in addition to the 2017 task. 
The contribution of this paper is a further look at 
how rule-based systems can be employed for these sorts of tasks. 
Meaning and grammar are treated separately in order to 
classify the language as correct. 
A number of experts were constructed to deal separately with 
different POS such as nouns, adjectives, verb usage and pronouns or determiners. 
Distance measurements derived from Doc2Vec where then 
employed between utterance and prompt responses.
A D-value of 10.08 is reported on the final 2nd Edition evaluation test files.",0.0,3.9749999999999996
476,1860,4.3    4.35    2.25    5    ,A Study of Objective Measurement of Comprehensibility through Native Speakers' Shadowing of Learners' Utterances,"While learners desire to acquire so comprehensible pronunciations as to make themselves
understood smoothly, acquisition often becomes difficult because, outside of classrooms,
it is not rare that learners can hardly find chances to talk in the target language.
Even when they talk to native speakers, they may receive only lenient or superficial suggestions from native speakers. How can learners know native speakers' honest perception on their
utterances? In this paper, shadowing is introduced not to learners but to native listeners,
who are asked to shadow learners' utterances. Since shadowing is as simultaneous repetition
as possible, it is expected that native listeners' perceived comprehensibility can be
measured objectively as smoothness of natives' shadowings. Experiments show that
1) shadowers' subjective assessment of learners' speech and that of their shadowings
are highly correlated and that 2) the former is more correlated with 
the GOP scores of natives' shadowings than that of learners' speech.
These results indicate that it is valid to regard comprehensible pronunciation
as pronunciation.",0.0,3.9749999999999996
744,2581,3.6    4.1    3.95    4.25    ,An End-to-End Deep Learning Framework for Speech Emotion Recognition of Atypical Individuals,"The goal of the ongoing ComParE 2018 Atypical Affect sub-challenge is to recognize the emotional states of atypical individuals. In this work, we present three modeling methods under the end-to-end learning framework, namely CNN combined with extended features, CNN+RNN and ResNet, respectively. Furthermore, we investigate multiple data augmentation, balancing and sampling methods to further enhance the system performance. The experimental results show that data balancing and augmentation increase the unweighted accuracy (UAR) by 10\% absolutely. After score level fusion, our proposed system achieves 48.8\% UAR on the develop dataset.",0.0,3.9749999999999996
1237,2233,3.45    5.1    2.25    5.1    ,End-to-end Named Entity Extraction from Speech,"Named entity recognition (NER) is among SLU tasks that usually extract  semantic information from textual documents. 
Until now, NER from speech is made through a pipeline process that consists in processing first an automatic speech recognition (ASR) on the audio and then processing a NER on the ASR outputs.
Such approach has some disadvantages (error propagation, metric to tune ASR systems sub-optimal in regards to the final task, reduced space search at the ASR output level,...) and it is known that more integrated approaches outperform sequential ones, when they can be applied.
In this paper, we present a first study of end-to-end approach that directly extract named entities from speech, though a unique neural architecture. On a such way, a joint optimization is able for both ASR and NER. Experiments are carried on French data easily accessible, composed of data distributed in several evaluation campaign.
Experimental results show that this end-to-end approach provides better results (F-measure=0.66 on test data) than a classical pipeline approach to detect named entity categories (F-measure=0.64).",1.0,3.975
549,2042,4.1    5    3.4    3.4    ,Effects of Dimensional Input on Paralinguistic Information Perceived from Synthesized Dialogue Speech with Neural Network,"A novel method of controlling paralinguistic information in neural network-based dialogue speech synthesis is proposed. Controlling paralinguistic information was achieved by feeding emotion dimensions in continuous values into the input layer of the neural networks. Compared to the method using the multiple regression HMM, the naturalness of synthesized speech was improved. The controllability of paralinguistic information was evaluated by examining the shift of the distribution of synthesized parameters. A subjective evaluation test revealed that the correlation between given and perceived paralinguistic in- formation was moderate, though less apparent compared to the multiple regression HMM-based method.",0.0,3.975
1146,1976,5.1    4.2    3.75    2.85    ,Improving Performance of Query-by-Example Spoken Term Detection Using SMOTE Algorithm,"Phonetic classes of natural speech data are generally imbalanced. The difference in number of occurrences and durations of the phones in natural speech accounts for this imbalance, which is independent of the language spoken, and size of database. This non-uniformity in the distribution of phones results in improper detection of phonetic classes, which indirectly affects the performance of a spoken term detection system. Hence, we propose Synthetically Minority Oversampling TEchnique, referred to as SMOTE, which elevates the concentration of minority classes, along with reducing the strength of majority class entities, to improve the uniformity in distribution of classes in speech data. We are introducing two new approaches in SMOTE, namely semisupervised and unsupervised SMOTE methods suitable for Query-by-Example Spoken Term Detection (QbE-STD) tasks. Performance comparison revealed semisupervised SMOTE model with  relative improvement of 13.94 \% and unsupervised SMOTE model with relative improvement of 8.69 \%  in MAP, w.r.t. non-SMOTE (i.e., without SMOTE) conventional model  at best scenarios. Semisupervised SMOTE model outperformed unsupervised SMOTE model in terms of reducing the imbalance across phonetic classes.",1.0,3.975
1100,1854,4.2    2.85    3.85    5    ,Decoding of Language Discriminative Features in EEG Signals,"This paper presents an experimental study to probe the language discriminative cues in neural responses to speech stimuli presented from two languages. In this study using electroencephalography (EEG) recordings, one of the first ones in this direction, we explore the differences in human perception while listening to a familiar and an unfamiliar language.  The main objective of this study is to analyze the changes in brain responses when a human subject is listening under familiar and foreign conditions in a linguistic sense. The stimuli-set for this study contains  words from English (familiar language) and  from Japanese (unfamiliar language). The language discriminative representations are probed by designing an off-line language classifier  on the EEG signals. These experiments reveal that the time-frequency representations along with the phase information of EEG signal carry significant language discriminative information. We obtain an average accuracy of $69.9$\% among 12 subjects with a support vector machine based classifier, which is significantly above chance. We also identify the brain regions (based on EEG channels) that contain the most language discriminative cues as the temporal lobe and the frontal lobe.",1.0,3.975
1159,2000,4.25    3.4    3.9    4.35    ,A Study on FMLLR Based Speaker Adaptation in Deep Convolutional Neural Networks for ASR,"The paper revisits the problem of performing speaker adaptation using feature space maximum likelihood linear regression (FMLLR) in convolutional neural networks (CNN). Previous studies have shown that it is more efficient to perform FMLLR adaptation using the joint CNN/DNN framework (JCD), rather than directly using FMLLR transformed FBANK features as input for CNNs. In this paper, we explore two approaches to derive FMLLR transformed FBANK features as  inputs for CNNs and show that the proposed approaches perform better than JCD. One approach (referred as M1) directly estimates the FMLLR transform on FBANK features, while the other approach (referred as M2) applies an inverse discrete cosine transform (IDCT) on FMLLR transformed Mel frequency cepstral coefficients (MFCC). On the single channel CHiME4 real noise evaluation set, M1 provides a relative gain of 3.6% WER and M2 provides a relative gain of 9.4% WER respectively when compared with the performance of JCD. On the multi- channel CHiME4 real noise evaluation set, both M1 and M2 perform similar and provide a relative gain of 4.4% WER when compared with the performance of JCD. The results show that applying FMLLR adaptation on the FBANK features for use an input with CNNs is better.",1.0,3.975
579,2119,5.1    4.2    4    2.6    ,Speech Intelligibility Enhancement Based on a Non-causal Wavenet-like Model,"Low speech intelligibility in noisy listening conditions makes more difficult our communication with others. Various strategies have been suggested to modify a speech signal before it is presented in a noisy listening environment with the goal to increase its intelligibility. A state-of-the art approach, referred to as Spectral Shaping and Dynamic Range Compression (SSDRC),  relies on modifying spectral and temporal structure of the clean speech and has been shown to considerably improve the intelligibility of speech in noisy listening conditions. In this paper, we present a non-causal Wavenet-like model for mapping clean speech samples to samples generated by SSDRC. A successful non-linear mapping function has the potential to be used a) in improving the intelligibility of noisy speech and b) in the Wavenet-based speech synthesizers as a model based  intelligibility improvement layer. Objective and subjective results show that the Wavenet-based mapping function is able to reproduce the intelligibility gains of SSDRC, while by far it improves the quality of the modified signal compared to the quality obtained by SSDRC.",0.0,3.975
132,1171,4.05    3.95    4.1    4.65    3.15    ,Homophone Identification and Merging for Code-switched Speech Recognition,"Code-switching or mixing is the use of multiple languages in a single utterance or conversation. Borrowing occurs when a word from a foreign language becomes part of the vocabulary of a language. In multilingual societies, switching/mixing and borrowing are not always clearly distinguishable. Due to this, transcription of code-switched and borrowed words is often not standardized and leads to the presence of homophones in the training data. In this work, we automatically identify and disambiguate homophones in code-switched data to improve recognition of code-switched speech. We use a WX-based common pronunciation scheme for both languages being mixed and unify the homophones during training, which results in a lower word error rate for systems built using this data. We also extend this framework to propose a metric for code-switched speech recognition that takes into account homophones in both languages while calculating WER, which can help provide a more accurate picture of errors the ASR system makes on code-switched speech.",0.0,3.9799999999999995
919,1390,3.2    4.75    4    ,A Teacher-student Learning Approach for Domain Adaptation of Sequence-trained ASR Models,"Teacher-student learning is a transfer learning approach, where a teacher network is used to teach a student network to make the same predictions as the teacher. Originally formulated for model compression, this approach has also been used for domain adaptation, and is particularly effective when parallel data is available in source and target domains.  The standard approach uses a frame-level objective of minimizing the KL divergence between the frame-level posteriors of the teacher and student networks. However, this might not be applicable to sequence-trained models for speech recognition.  In this case, it is more appropriate to train the student to mimic the sequence-level posterior of the teacher network.  In this work, we propose to use a sequence-level KL divergence objective in the lattice-free training framework. We compare the proposed approach to using semi-supervised lattice-free MMI training to do domain adaptation. We evaluate the proposed approach in multiple scenarios including adapting from clean speech to noisy speech, bandwidth mismatch and channel mismatch.",1.0,3.983333333333333
978,1537,3.95    3.25    4.75    ,"Exploring Deep Neural Network Architectures for Speech, Music and Noise Classification and Segmentation","Speech, Music and Noise classification/segmentation is an important preprocessing step for audio processing/indexing. To this end, we investigated various Deep Neural Network architectures including 1D convolutional (CNN), recurrent (RNN) and 2D convolutional architectures on the MUSAN corpus - an openly available comprehensive collection of noise, music and speech samples, suitable for deep learning. We identified three best performing architectures, namely WaveNet, Gated Recurrent Unit (GRU) and MobileNet within these three classes of networks respectively. A novel modified WaveNet architecture is proposed which can achieve high accuracy on MFCC features while also being lightweight and fast. Among the investigated RNNs, GRU with convolutional preprocessing performed best while providing a good performance to speed trade-off. Among 2D CNNs, the networks pre-trained on ImageNet performed extremely well on MFB features without any modification of architecture. MobileNet outperformed other CNNs in terms of speed and accuracy. All proposed networks achieved high overall accuracy in clip (length of 0.5-2s) classification (>97% accuracy) and frame-wise segmentation (>92% accuracy) tasks. MobileNet achieved near perfection (>99.7% accuracy) in speech/non-speech discrimination task. To verify the robustness of the networks trained on MUSAN, we evaluated them independently (""In-the-Wild"") on a different corpus – GTZAN and found good accuracy.",1.0,3.983333333333333
864,1216,3.75    3.25    4.95    ,Speaker Adapted Beamforming for Multi-Channel Automatic Speech Recognition,"This paper presents, in the context of multi-channel ASR, a
method to adapt a mask based, statistically optimal beamforming
approach to a speaker of interest. The beamforming vector
of the statistically optimal beamformer is computed by utilizing
speech and noise masks, which are estimated by a neural network.
The proposed adaptation approach is based on the integration
of the beamformer, which includes the mask estimation
network, and the acoustic model of the ASR system. This allows
for the propagation of the training error, from the acoustic
modeling cost function, all the way through the beamforming
operation and through the mask estimation network. By using
the results of a first pass recognition and by keeping all other
parameters fixed, the mask estimation network can therefore be
fine tuned by retraining. Utterances of a speaker of interest can
thus be used in a two pass approach, to optimize the beamforming
for the speech characteristics of that specific speaker. It
is shown that this approach improves the ASR performance of
a state-of-the-art multi-channel ASR system on the CHiME-4
data. Furthermore the effect of the adaptation on the estimated
speech masks is discussed.",1.0,3.983333333333333
937,1433,5.05    4.1    4.85    1.95    ,Semi-supervised Learning with Multimodal Semantic Confidence Measures for Speech Recognition in Air Traffic Control,"Automatic Speech Recognition (ASR) systems have recently shown to improve operational efficiency of Air Traffic Control (ATC) systems and reduce the workload of air traffic controllers. While building domain specific ASR systems for each ATC operational area is resource intensive, continuous operation of ATC systems provides increasing amounts of untranscribed speech and radar data allowing for the use of semi-supervised learning methods. This paper presents novel data selection methods for semi-supervised learning based on confidence measures from speech and radar modalities that exploit semantic representations of ATC commands. Starting with baseline ASR models that utilize out-of-domain speech data, we apply radar derived command plausibility measures and combine them with ASR derived word confidences to build domain specific semi-supervised ASR models. Experiments on real data from operational environments show relative improvements of 28% and 24% in word and command error rates respectively, compared to a baseline that does not utilize untranscribed data.",1.0,3.9874999999999994
963,1502,3.4    4.35    4.25    3.95    ,Replay Attacks Detection Using High Spectro-Temporal Resolution Features,"The easy implementation of replay attacks by a fraudster poses a serious threat to automatic speaker verification technology than other spoofing attacks like speech synthesis and voice conversion. Channel pattern characteristics are the main cues to differentiate genuine and replay recordings. In order to capture these characteristics, one need to extract features from the spectrum, which should have high spectral and temporal resolutions. Zero time windowing (ZTW) analysis of speech is one such
time-frequency analysis technique, which results in high spectral and temporal resolution spectrum at each sampling instant. In this study, new features are proposed by applying cepstral analysis to zero time windowing spectrum. These cepstral features are modeled using Gaussian mixture model. Experiments are performed on two publicly available replay attack databases namely BTAS 2016 and ASVspoof 2017. The proposed system results an half total error rate of 0.75 % and an equal error rate of 14.75 % on BTAS 2016 and ASVspoof 2017 evaluation sets respectively. The score level fusion of zero time windowing cepstral coefficients and single frequency filtering cepstral coefficients based subsystems out performed the previously reported best results on these two datasets.",1.0,3.9875
865,1217,3.45    3.7    5.25    3.55    ,Evaluation of the Pain Level from Speech: Introducing a Novel Pain Database and Benchmarks,"In many clinical settings, the evaluation of pain is achieved through manual and time intensive diagnosis based on the expressions and descriptions of patients. Furthermore, such manual diagnosis is costly, liable to subjective biases, and often inaccurate. The automatic evaluation of pain based on speech enables an objective method for improving the accuracy of diagnosis thereby benefiting the treatment of patients. This paper describes a new pain audiovisual database, the Duesseldorf Acute Pain Corpus, in which 844 recordings were collected from 80 subjects whose speech was collected while they undertook a cold pressure pain induction paradigm.  The database is split into speaker independent training/development/test sets for a three-class level of pain classification task and we provide a comprehensive set of experimental results as the benchmark on this dataset. The feature representations tested include functionals and bag-of-audio-words from three classes of feature sets: the Computational Paralinguistics Challenge (ComParE) features, mel-frequency ceptral coefficients, and deep spectrum representations. We use support vector machines and long short-term memory recurrent neural networks (LSTM-RNN) as the classifiers. The best result of 42.1% unweighted average recall on the test set is obtained by LSTM-RNN working on the deep spectrum representations.",1.0,3.9875
102,1115,5.25    3.25    2.45    5    ,A Comparison of Input Types to a Deep Neural Network-based Forced Aligner,"The present paper investigates the effect of different inputs on the accuracy of a forced alignment tool built using deep neural networks. Both raw audio samples and Mel-frequency cepstral coefficients were compared as network inputs. A set of experiments were performed using the TIMIT speech corpus as training data and its accompanying test data set. The networks consisted of a series of convolutional layers followed by a series of bidirectional long short-term memory (LSTM) layers. The convolutional layers were trained first to act as feature detectors, after which their weights were frozen. Then, the LSTM layers were trained to learn the temporal relations in the data. The current results indicate that networks using raw audio perform better than those using Mel-frequency cepstral coefficients and an off-the-shelf forced aligner. Possible explanations for why the raw audio networks perform better are discussed. We then lay out potential ways to improve the results of the networks and conclude with a comparison of human cognition to network architecture.",0.0,3.9875
1013,1633,5.1    3.6    3.7    3.55    ,Query-by-Example Spoken Term Detection Using Generative Adversarial Network,"Several neural network-based representation techniques have already been proposed in Query-by-Example Spoken Term Detection (QbE-STD) task. The recent advancement in Generative Adversarial Network (GAN) for several speech applications, motivated us to explore the GAN in QbE-STD. In this work, we propose to exploit GAN with the regularized cross-entropy loss and develop a framework featuring GAN, trained using Gaussian Mixture Model (GMM)-based labels. The proposed GAN maps the speech-specific features to the unsupervised labels. This mapping represents the speech through an unsupervised GAN posteriorgram (uGAN-PG), for matching the query (keyword) with the utterances in the document. The QbE-STD, using proposed posteriorgram is performed on the TIMIT database. We compare the performance of the proposed uGAN-PG with the unsupervised DNN posteriorgram (uDNN-PG) and obtained the relative performance improvement of 10.32 \% Mean Average Precision (MAP) and 5.6 \% precision@N over uDNN-PG.",1.0,3.9875
1161,2004,3.4    3.45    4.1    5    ,Experiments for Point Anomaly Detection in Expressive Reading with the EMO&LY Corpus.,"Anomaly detection is widely used in various domains like camera/video surveillance, network, software or house/building intrusion.
This paper focuses on an evaluation of Point Anomaly detection approaches used to detect unexpected reactions in expressive discourse.
Then, we present detection scores achieved by different types of algorithms (GMM, iForest, k-means) on different modalities (face, audio, and both of them), and different levels of analysis (considering either samples or sub-sequences of samples).
These strategies are evaluated on the EMO&LY (EMotion & anomaLY) corpus, composed of three readings (with audio and video recordings) of a tale by 41 subjects, including reading with reactions resulting from unexpected stimuli.
As a consequence, this corpus contains expressive speech and videos with annotated anomalies.
Our results show significantly better detection scores when using multimodal data (audio + video) compared to monomodal data. For instance, the overall detection score achieved  by  the  k-means  model on multimodal data is 0.84+/-0.03, while the detection scores on audio alone is 0.74+/-0.03 and on video alone, 0.80+/-0.04.",1.0,3.9875
397,1693,2.7    5.1    5.05    3.1    ,Feature with Complementarity of Statistics and Principal Information for Spoofing Detection,"Constant-Q transform (CQT) has demonstrated its effectiveness in anti-spoofing feature analysis for automatic speaker verification. This paper introduces a statistics-plus-principal information feature where a short-term spectral statistics information (STSSI), octave-band principal information (OPI) and full-band principal information (FPI) are proposed on the basis of CQT. Firstly, in contrast to conventional utterance-level long-term statistic information, STSSI reveals the spectral statistics at frame-level, moreover it provides a feasibility condition for model training while only small training database is available. Secondly, OPI emphasizes the principal information for octave-bands, STSSI and OPI creates a strong complementarity to enhance the anti-spoofing feature. Thirdly, FPI is also of complementary effect with OPI. With the statistical property over CQT spectral domain and the principal information through discrete cosine transform (DCT), the proposed statistics-plus-principal feature shows reasonable advantage of the complementary trait for spoofing detection. In this paper, we setup deep neural network (DNN) classifiers for evaluation of the features. Experiments show the effectiveness of the proposed feature as compared to many conventional features on ASVspoof 2017 and ASVspoof 2015 corpus.",0.0,3.9875
191,1281,4.1    4.1    3.4    4.35    ,Detection of Glottal Closure Instants from Speech Signals: a Convolutional Neural Network Based Method,"Most conventional methods to detect glottal closure instants (GCI) are based on signal processing technologies and different GCI candidate selection methods.  This paper proposes a classification method to detect glottal closure instants from speech waveforms using convolutional neural network (CNN).  The procedure is divided into two successive steps.  Firstly, a low-pass filtered signal is computed, whose negative peaks are taken as candidates for GCI placement.  Secondly, a CNN-based classification model determines for each peak whether it corresponds to a GCI or not.  The method is compared with three existing GCI detection algorithms on two publicly available databases.  For the proposed method, the detection accuracy in terms of F1-score is 98.23%.  Additional experiment indicates that the model can perform better after trained with the speech data from the speakers who are the same as those in the test set.",0.0,3.9875
291,1463,4.1    2.8    4.2    4.85    ,Assessing Speaker Engagement in 2-Person Debates: Overlap Detection in United States Presidential Debates,"Co-channel speech contain significant amounts of overlap in which the intelligibility and quality of the desired speech can be degraded. Convolutive Non-negative Matrix Factorization (CNMF) has been shown to be a successful approach in detecting overlap by extracting specific acoustic basis dimensions for each speaker from an audio stream. While the results of CNMF have been successful, it requires isolated single speech recordings for each speaker to derive their corresponding bases functions/dimensions. In our previous work, Teager-Kaiser Energy Operator (TEO)-based Pyknogram has been introduced. In this study, Pyknogram and CNMF based solutions for overlap detection within audio streams have been examined using the GRID dataset. TEO-based Pyknogram is shown to achieve a relative 8-10% lower Equal Error Rate (EER) compared to CNMF features. In addition, a secondary evaluation was also performed based on naturalistic audio streams with overlap. Specifically, we collected a  real-world audio database of US Presidential debates stemming from the last 12 years that are very challenging due to various forms of overlaps, changing Signal to Interference Ratio (SIR) and, environmental noise among others.  Our experiments indicate that TEO-based Pyknogram is well suited  for detecting overlap in challenging real world scenarios such as the US presidential debates.",0.0,3.9875
1022,1653,3.4    5.1    3.3    4.15    ,Modification of Dental Stop Backing Errors in Cleft Lip and Palate Speech,"Speakers with cleft lip and palate (CLP) exhibit compensatory
articulation errors such as palatal, velar and glottal stop sub-
stitution during the production of dental stops. In this work,
nonnegative matrix factorization (NMF) based spectral conver-
sion of dental stop backing errors is carried out. The activation
matrix derived from dental stop of normal speakers is used to
perform the conversion of the backing errors produced by CLP
speakers for target dental stop. The modified signal is found to
contain spectro-temporal characteristics as that of normal den-
tal stop. The effectiveness of modified backing errors are eval-
uated using objective and subjective measures. The evaluation
results reveal that the modified speech signals are significantly
improved, when compared to the original speech signal.",1.0,3.9875000000000003
716,2490,3.65    3.4    4.75    4.15    ,Audiovisual Speech Activity Detection with Advanced Long Short-Term Memory,"Speech activity detection (SAD) is a key pre-processing step for a speech-based  system. The performance of conventional audio-only SAD (A-SAD) systems is impaired by acoustic noise when they are used in practical applications. An alternative approach to address this problem is to include visual information, creating audiovisual speech activity detection (AV-SAD) solutions. In our previous work, we proposed to build an AV-SAD system using bimodal recurrent neural network (BRNN). This framework was able to capture the task-related characteristics in the audio and visual inputs, and model the temporal information within and across modalities. The approach relied on long short-term memory (LSTM). Although LSTM can model longer temporal dependencies with the cells, the effective memory of the units is limited to a few frames, since the recurrent connection only considers the previous frame. For SAD systems, it is important to model longer temporal dependencies to capture the semi-periodic nature of speech conveyed in acoustic and orofacial features. This study proposes to implement a BRNN-based AV-SAD system with Advanced LSTM (A-LSTM), which overcomes this limitation by including multiple connections to frames in the past. The results show that the proposed framework can significantly outperform the BRNN system trained with the original LSTM.",0.0,3.9875000000000003
575,2114,3.55    4.2    3.2    5.05    ,Effects of Homophone Density on Spoken Word Recognition in Mandarin Chinese,"Homophones, words that sound same, influence spoken word recognition. Whether the effects of homophone density (i.e., number of same-sounding words) on spoken word recognition are facilitatory or inhibitory or complex is a matter of ongoing debate. In addition, there are limited studies investigating the effects of homophone density, probably due to paucity of homophones in the examined languages (e.g., English). In comparison, languages such as Mandarin Chinese have abundant homophony that makes it a suitable tool to investigate the effects of homophone density. In the current study, an auditory naming task was conducted using Mandarin Chinese to investigate the effects of homophone density on spoken word recognition. Using mixed modeling, a significant inhibitory effect of homophone density (β = 0.0098, t = 2.10) on reaction time was found. Participants were slower in naming words with high homophone density, possibly due to competition posed by more number of homophones, as compared to the words with low homophone density. Further, an interaction between homophone density and syllable frequency was found i.e., for high syllable frequency, homophone density effects were inhibitory but for low syllable frequency, the inhibitory effect was reduced. Taken together, the effects of homophone density are not straightforward but complex.",0.0,4.0
0,27,3.15    4.2    4.65    ,Binaural Speech Intelligibility Estimation Using Deep Neural Networks,"We attempted to estimate the speech intelligibility of binaural speech signal
with additive noise. The assumption here was that both the target speech signal
and the noise source are directional sources. In this case, when the speech and
noise sources are located away from each other, the intelligibility generally
improves since the human auditory system can potentially segregate these two
sources. However since intelligibility tests are commonly conducted using
monaurally recorded signals, the intelligibility is often under-estimated
compared to live human listeners since this segregation capability is
neglected. We have previously proposed to use binaurally recorded signals to
estimate the speech intelligibility and compared the estimation accuracy of
several machine learning methods on this signal. We showed that random forests
(RF) combined with the better ear model and Mel filter banks gives the highest
accuracy compared to other methods, such as the support vector machines or
logistic regression. In this paper, we attempt to introduce deep neural
networks (DNN) to this task. Initial evaluation results show that the use of
DNN can provide a modest improvement over RF.",0.0,4.0
316,1521,2.65    5.1    5.15    3.1    ,Acoustic Features Associated with Sustained Vowel and Continuous Speech Productions by Chinese Children with Functional Articulation Disorders,"Functional articulation disorder (FAD) is a speech disorder commonly found in preschoolers, negatively affecting their day-to-day communication and in the long run their psychological development. Current FAD research mainly focused on the perceptual aspects, but not other means such as acoustic and physiological analyses. The present study aimed to evaluate the different acoustic features associated with sustained vowels and continuous speech produced by children with FAD and their age-matched controls. Speech samples produced by 67 children with FAD and 30 typically developing children. Articulatory-acoustic vowel space features, including formant centralization ratio (FCR3), F1 range ratio (F1RR), F2 range ratio (F2RR), and triangular vowel space area (TVSA), were calculated using the first two formant frequencies from vowels /a/, /i/, /u/. Voice onset time (VOT) values associated with the stop consonants were also obtained. Results indicated that children with FAD exhibited articulatory undershooting with reduced range of articulatory movements, as well as poorer control over the release of oral occlusion when producing aspirated or unaspirated stops, when compared with normal counterparts. The findings support the notion that these acoustic features can be used to differentiate misarticulated speech from healthy speech, and could be used to objectively classify and evaluate FAD speech.",0.0,4.0
536,2014,4.1    5.25    4.95    2.3    3.4    ,Detection of Glottal Excitation Epochs in Speech Signal Using Hilbert Envelope,"A technique, suitable for real-time processing, is presented for detection of glottal excitation epochs in voiced speech. It uses Hilbert envelope to enhance saliency of the glottal excitation epochs and to reduce the ripples due to the vocal tract filter. The processing comprises the steps of dynamic range compression, calculation of the Hilbert envelope, and epoch marking. The first step reduces amplitude variation by applying A-law on the signal envelope. The second step calculates the Hilbert envelope using the output of an FIR filter-based Hilbert transformer and the delay-compensated signal. The third step uses a dynamic peak detector with fast rise and slow fall and nonlinear smoothing using a two-step median-mean filter to further enhance the saliency of the epochs, followed by a differentiator to mark them. The technique is tested using the CMU-ARCTIC database with simultaneously recorded speech and EGG signals. The results showed a good match in the performance of the proposed technique with those of the state-of-the-art techniques and its robustness against highpass filtering. It may be useful for diagnosis of voice disorders and high-quality voice conversion.",0.0,4.0
487,1891,2.75    5    4.15    4.1    ,Multilingual Deep Neural Network Training Using Cyclical Learning Rate,"Deep Neural Network (DNN) acoustic models are an essential component in automatic speech recognition (ASR). The main sources of accuracy improvements in ASR involve training DNN models that require large amounts of supervised data and computational resources. While the availability of sufficient monolingual data is a challenge for low-resource languages, the computational requirements for resource rich languages increases significantly with the availability of large data sets.
In this work, we provide novel solutions for these two challenges in the context of training a feed-forward DNN acoustic model (AM) for mobile voice search. To address the data-sparsity challenge, we bootstrap our multilingual AM using data from languages in the same language family to train a multilingual Gaussian Mixture Model (GMM) AM. To reduce training time, we use cyclical learning rate (CLR) which has demon- strated fast convergence with equal or better performance when training neural networks on tasks related to text and images.
We reduce training time for our Mandarin Chinese AM at 81.4% character accuracy from 34 to 17 hours and increase the word accuracy on four romance languages by 2-5% with multi- lingual AMs compared to monolingual DNN baselines.",0.0,4.0
607,2209,5.1    4.05    4.35    2.55    ,Speaker Adaptive Training and Mixup Regularization for Neural Network Acoustic Models in Automatic Speech Recognition,"This work investigates speaker adaptation and regularization techniques for deep neural network acoustic models (AMs) in automatic speech recognition (ASR) systems. In previous works, GMM-derived (GMMD) features have been shown to be an efficient technique for neural network AM adaptation. In this paper, we propose and investigate a novel way to improve speaker adaptive training (SAT) for neural network AMs using GMMD features. The idea is based on using inaccurate transcriptions from ASR for adaptation during neural network training, while keeping the exact transcriptions for targets of neural networks. In addition, we apply a mixup technique, recently proposed for classification tasks, to acoustic models for ASR and investigate the impact of this technique on speaker adapted acoustic models. Experimental results on the TED-LIUM corpus show that the proposed approaches provide an additional gain in speech recognition performance in comparison with the speaker adapted AMs.

Index Terms: speech recognition, acoustic models, data augmentation, mixup, deep neural networks, GMMD features, speaker adaptation, speaker adaptive training, MAP",0.0,4.012499999999999
1267,2342,5.1    3.3    4.45    3.2    ,End-to-End Speech Recognition with Limited Transcribed Speech,"End-to-end recurrent neural network models have become state-of-the-art for large-scale automatic speech recognition (ASR) systems. When trained on smaller datasets, however, these models still consistently underperform traditional ASR models. Unfortunately, many languages for which only relatively small speech corpora are available also do not have the resources necessary to train a well-performing conventional acoustic model. We develop an end-to-end speech recognition model designed for a common low-resource scenario: no pronunciation dictionary or phonemic transcripts, very limited transcribed speech, and much larger non-parallel text and speech corpora. Our semi-supervised model is built on top of a sequence-to-sequence ASR model and takes advantage of non-parallel speech and text corpora in several ways: a text autoencoder that shares parameters with the ASR decoder, a speech autoencoder that shares parameters with the ASR encoder, and adversarial training that encourages the speech and text encoders to use the same embedding space. In prior work, we demonstrated that a preliminary model with this architecture significantly outperforms the baseline in this low-resource condition. Here, we update our earlier model with a modified speech autoencoder and show significant additional improvements.",1.0,4.012499999999999
629,2286,4.35    6    2.55    3.15    ,Audio-visual Voice Conversion Using Deep Canonical Correlation Analysis for Deep Bottleneck Features,"This paper proposes Audio-Visual Voice Conversion (AVVC) methods using Deep BottleNeck Features (DBNF) and Deep Canonical Correlation Analysis (DCCA).
DBNF has been adopted in several speech applications to obtain better feature representations.
DCCA can generate much correlated features in two views, and enhance features in one modality based on another view. 
In addition, DCCA can make projections from different views ideally to the same vector space.
Firstly, in this work, we enhance our conventional AVVC scheme by employing the DBNF technique in the visual modality.
Secondly, we apply the DCCA technology to DBNFs for new effective visual features.
Thirdly, we build a cross-modal voice conversion model available for both audio and visual DCCA features.
In order to clarify effectiveness of these frameworks, we carried out subjective and objective evaluations and compared them with conventional methods.
Experimental results show that our DBNF- and DCCA-based AVVC can successfully improve the quality of converted speech waveforms.",0.0,4.012499999999999
524,1974,4.3    3.1    3.85    4.8    ,Study of Semi-supervised Approaches to Improving English-Mandarin Code-Switching Speech Recognition,"In this paper, we present our overall efforts to improve the performance of a code-switching speech recognition system using semi-supervised training methods from lexicon learning to acoustic modeling, on the South East Asian Mandarin-English (SEAME) data. We first investigate semi-supervised lexicon learning approach to adapt the canonical lexicon, which is meant to alleviate the heavily accented pronunciation issue within the code-switching conversation of the local area. As a result, the learned lexicon yields improved performance. Furthermore, we attempt to use semi-supervised training to deal with those transcriptions that are highly mismatched between human transcribers and ASR system. Specifically, we conduct semi-supervised training assuming those poorly transcribed data as unsupervised data. We found the semi-supervised acoustic modeling can lead to improved results. Finally, to make up for the limitation of the conventional n-gram language models due to data sparsity issue, we perform lattice rescoring using neural network language models, and significant WER reduction is obtained.",0.0,4.0125
905,1341,2.8    2.3    6    4.95    ,Continuation Rises as a Cue for the Identification of Real and Imitated French Accent in Spanish Oral Productions,"This paper aims to investigate the role of continuation rises as a cue for the identification of real and imitated French foreign accent in Spanish utterances. The acoustic realization of F0 contours of continuation rises is analysed in a corpus of native and French-accented Spanish, collected as a part of the Emulando project. Two acoustic parameters related to the realization of continuation rises are analysed: the location of the starting point of the rise and the range of the rise. The results show that there are some differences in the location of the start of the rising movement that may be useful to distinguish between native and real French-accented Spanish, on the one hand, and real and imitated French accent, on the other: native speakers of French tend to locate the beginning of the rising movement in the last syllable, whereas Spanish speakers, either in their native productions or when imitating French accent, tend to place the beginning of the rise in the stressed syllable. The analysis of rise range revealed also some differences between native and imitated French accent, and possibly between imitated and real accent, although the results are not conclusive in this aspect.",1.0,4.0125
1302,2483,4    4    4.05    4    ,Improved Regularization Techniques for End-to-End Speech Recognition,"Regularization is important for end-to-end speech models, since the models are highly flexible and easy to overfit. Data augmentation and dropout has been important for improving end-to-end models in other domains. However, they are relatively under explored for end-to-end speech models. Therefore, we investigate the effectiveness of both methods for end-to-end trainable, deep speech recognition models. 
We augment audio data through random perturbations of tempo, pitch, volume, temporal alignment, and adding random noise. We further investigate the effect of dropout when applied to the inputs of all layers of the network. 
We show that the combination of data augmentation and dropout give a relative performance improvement on both Wall Street Journal (WSJ) and LibriSpeech dataset of over $20\%$ as compared to the baseline. Our model performance is also competitive with other end-to-end speech models on both datasets.",1.0,4.0125
171,1246,3.2    4.45    4.25    4.15    ,Co-whitening of I-vectors for Short and Long Duration Speaker Verification,"An i-vector is a fixed-length and low-rank representation of a speech utterance. It has been used extensively in text-independent speaker verification. Ideally, speech utterances from the same speaker would map to an unique i-vector. However, this is not the case due to some intrinsic and extrinsic factors like physical condition of  the speaker, channel difference, noise and notably the duration of speech utterances. In particular, we found that i-vectors extracted from short utterances exhibit larger variance than that of long utterances. To address the problem, we propose a co-whitening approach, taking into account the duration, while maximizing the correlation between the i-vectors of short and long duration. The proposed co-whitening method was derived based on canonical correlation analysis (CCA). Experimental results on NIST SRE 2010 show that co-whitening method is effective in compensating the duration mismatch, leading to a reduction of up to 13.07% in equal error rate (EER).",0.0,4.0125
376,1650,4.2    3.4    3.4    5.05    ,Using Shifted Real Spectrum Mask as Training Target for Supervised Speech Separation,"Deep learning-based speech separation has been widely studied in recent years. Most of these kind approaches focus on recovering the magnitude spectrum of the target speech, but ignore the phase  estimation. Recently, a method called shifted real spectrum (SRS)  is proposed. Unlike the short-time Fourier transform (STFT), the SRS contains only real components which encode the phase information. In this paper, we propose several SRS-based masks and use them as the training target of deep neural networks. Experimental results show that the proposed target outperforms the commonly used masks computed on STFT in general.",0.0,4.0125
814,1104,4.4    3.35    4.3    ,Ranking Approach to Inverse Text Normalization for Personal Digital Assistants,Personal digital assistants need to display the output from the speech recognizer in a compact and readable representation.  The process of transforming sequences from spoken words to written text is called inverse text normalization (ITN).  Here we present a ranking based approach to ITN that incorporates predicative information from various statistical models to select the best written text to display.  Our approach allows us to bootstrap ITN models using simple handcrafted rules encoded as finite state transducers (FSTs).  The written text candidates derived using our rule set are ranked using a gradient boosted decision tree ensemble (GBDT).  The ranker achieves a 18.48% relative reduction in word error rate over a rule-based system.  Our two-stage approach allows us to decouple speech recognition from ITN and gives us greater flexibility in model deployment.,1.0,4.016666666666667
913,1367,2.6    5.15    4.9    3.45    ,An Empirical Study of Knowledge Exploration for Attention-based Speech Recognition,"Acoustic modeling and language modeling have been two major components in conventional speech recognition systems and have long been trained independently.  Recently,  it has been a trend to optimize them simultaneously  in a unified end-to-end framework. Even though such efforts have shown competitive performance when large training corpus are available, it is still unclear whether such early combination of acoustic and language information is an efficient way for speech recognition tasks, especially with limited training corpus.  In this paper, we investigate the problem by exploring various ways of combining multiple loss functions in the attention-based sequence to sequence framework using Switchboard 300h corpus.  We present a new architecture and training strategy for multi-task training with CTC loss. Our experimental results showed about 13% relative improvement over the baseline.  We also discovered that joint decoding with an independently trained language model consistently contributed 5\% to 10\% relative gains.   Such improvement could be achieved by joint training with the minimum word error rate (mWER) criterion as well.",1.0,4.025
850,1187,4.35    4.85    2.9    4    ,Parameter Generation Algorithms for Text-to-Speech Synthesis with Recurrent Neural Networks,"Recurrent Neural Networks (RNN) have recently proved to be effective in acoustic modeling for TTS. Various techniques such as the Maximum Likelihood Parameter Generation (MLPG) algorithm have been naturally inherited from the HMM-based speech synthesis framework. This paper investigates in which situations parameter generation and variance restoration approaches help for RNN-based TTS. Our experiments focus on F0, but the proposed methods can be applied to other speech parameters. We explore how their performance is affected by various factors such as the choice of the loss function, the application of regularization methods and the amount of training data. We propose an efficient way to calculate MLPG using a convolutional kernel. MLPG is found to be optional when using the L1 loss and proper regularization, but necessary otherwise. We did not observe perceptual improvements when embedding MLPG into the acoustic model and minimizing the error computed after parameter generation. Finally, we show that variance restoration approaches, despite enhancing the global variance, only yield minor perceptual gains.",1.0,4.025
23,70,4.65    2.35    4.95    4.15    ,Gated Convolutional Neural Network for Sentence Matching,"The recurrent neural networks (RNN) have shown promising
results in sentence matching tasks, such as paraphrase identification
(PI), natural language inference (NLI) and answer selection
(AS). However, the recurrent architecture prevents parallel
computation within a sequence and is highly time-consuming.
To overcome this limitation, we propose a gated convolutional
neural network (GCNN) for sentence matching tasks. In this
model, the stacked convolutions encode hierarchical contextaware
representations of a sentence, where the gating mechanism
optionally controls and stores the convolutional contextual
information. Furthermore, the attention mechanism is utilized
to obtain interactive matching information between sentences.
We evaluate our model on PI and NLI tasks, and the experiments
demonstrate the advantages of the proposed approach in
terms of both speed and accuracy performance.",0.0,4.025
146,1205,5.8    2.95    3.35    ,Deep Extractor Network for Target Speaker Recovery from Single Channel Speech Mixtures,"Speaker-aware source separation methods are promising workarounds for major difficulties such as arbitrary source permutation and unknown number of sources. However, it remains challenging to achieve satisfying performance provided a very short available target speaker utterance (anchor). Here we present a novel ""deep extractor network"" which creates an extractor point for the target speaker in a canonical high dimensional embedding space, and pulls together the time-frequency bins corresponding to the target speaker. The proposed model is different from prior works that the carnonical embedding space encodes knowledges of both the anchor and the mixture during training phase: first, embeddings for the anchor and mixture speech are separately constructed in a primary embedding space, and then combined as an input to feed-forward layers to transform to a carnonical embedding space which we discover more stable than the primary one. Experimental results show that given a very short utterance, the proposed model can efficiently recover high quality target speech from a mixture, which outperforms various baseline models, with 5.2% and 6.6% relative improvements in SDR and PESQ respectively compared with a baseline oracle deep attracor model. Meanwhile, we show it can be generalized well to more than one interfering speaker.",0.0,4.033333333333333
778,992,4.1    3.55    4.3    4.2    ,Prosody-aware Subword Embedding Considering Japanese Intonation Systems and Its Application to DNN-based Multi-dialect Speech Synthesis,"This paper presents prosody-aware subword embedding considering Japanese intonation systems and its application to DNN (deep neural network)-based multi-dialect speech synthesis. In accordance with recent improvements of speech synthesis in rich-resourced languages, the research trend is shifting to more challenging languages such as Japanese dialects that still have undefined prosodic contexts. Conventional prosody-aware word embedding can unsupervisedly extract the contexts in a data-driven manner using words and F0 sequences. However, accurate contexts for unknown words are difficult to generate. To solve this problem, we propose prosody-aware subword embedding considering Japanese intonation systems. The unsupervised subword model, which is trained considering language and acoustic characteristics, can tokenize an unknown word into known subwords suitable for prosody-aware embedding. We also propose a modulation filtering method considering intra-subword moras to improve the embedding accuracies. We apply the methods to not only Japanese but also Japanese multi-dialect speech synthesis. In the multi-dialect case, we propose subword models shared among dialects and embedding models conditioned by dialect information. The experimental evaluation demonstrates that the proposed multi-dialect methods can improve speech quality in some Japanese dialects.",1.0,4.0375
909,1360,5.05    2.95    5.15    3.05    ,Privacy-aware Personalization of Siri Speech Recognition,"No matter how specific a phrase spoken to a virtual assistant might be to a single user, the expectation is that an assistant will understand it perfectly. Typical solutions to this problem require user profiles that inform about all personal content a user might speak to the assistant. Such a distillation of private data obviously challenges user privacy. In this paper, we describe how Siri’s automatic speech recognition (ASR) service relies on user profiles that are derived, maintained and utilized on-device only, to help protect user privacy. Key concepts discussed include semi-supervised distributed on-device machine learning to successfully personalize embedded ASR language models and server-side ASR system combination which ensures high transcription accuracy for what the user actually said to the server-based service.",1.0,4.05
774,93,3.55    5.1    3.35    4.2    ,Neural End-to-End Speech Synthesis with Discrete Representation,"End-to-end speech synthesis aims at producing natural speech directly from text. The challenge is how to model the relationship between text in a discrete space and speech in the continuous feature space. The attention mechanism is used in current end-to-end methods to model such relationship. As speech features take different forms for the same text symbol, text-to-speech conversion is apparently a one-to-many mapping. Thus the attention alignment is not easily achieved. Besides, the attention mechanism maximizes the likelihood of each frame with the previous frames of natural speech during training, which is replaced by the predicted previous frame at run-time. Such mismatch leads to accumulated errors along speech generation, that is called exposure bias. To overcome the above two problems, we propose a vector quantization modeling technique, which projects Mel-scale spectrogram of speech into a latent discrete space. The experiments show that end-to-end system with vector quantization learns the attention alignments faster and produces better speech than the traditional end-to-end system. Furthermore, it also works well with less training data.",1.0,4.05
430,1759,3.45    4.2    4.25    4.3    ,On Convolutional LSTM Modeling for Joint Wake-Word Detection and Text Dependent Speaker Verification,"The task of personalized keyword detection system which also performs text dependent speaker verification (TDSV) has received substantial interest recently. Conventional approaches to this task involve the development of the TDSV and  wake up word detection systems separately. In this paper, we show that TDSV and keyword spotting (KWS) can be jointly modeled using the convolutional long short term memory (CLSTM) model architecture, where an initial convolutional feature map is further processed by a LSTM recurrent network.  A single end-to-end multi-task learning (MTL) model, jointly trained for TDSV and KWS is also discussed and compared. Given a small amount of training data for developing the CLSTM system, we show that model provides accurate detection of the presence of the keyword in spoken utterance. For the TDSV task, the MTL model can be well regularized using the CLSTM training examples for personalized  wake up task. In these experiments with multiple keywords, we illustrate that the proposed approach of MTL  significantly improves the performance of previously proposed neural network based text dependent SV systems. We also experimentally illustrate that the CLSTM model provides significant improvements over previously proposed key-word detection systems as well (average relative improvements of 30% over previous approaches).",0.0,4.05
1116,1892,4.15    4.45    4.35    4.85    2.45    ,An Analysis of an End-to-end Replay Spoofing Detection System,"Playing recorded speech samples of an enrolled speaker — “replay attack” — is a simple approach to bypass an automatic speaker verification (ASV) system. The vulnerability of ASV systems to such attacks has been acknowledged and studied, but there has been no research into what spoofing detection systems are actually learning to discriminate. In this paper, we analyse the local behaviour of an end-to-end replay spoofing detection system trained on the ASVspoof 2017 database (version 2.0). We generate temporal and spectral explanations for predictions of the model using the SLIME algorithm. Our findings suggest that the model is identifying an audio file as genuine using information above 7 kHz. To identify spoof the model is looking at energy between 3-4 kHz in the initial frames of the audio signal. Knowledge of the characteristics that spoofing detection systems are exploiting can help build less vulnerable ASV systems, other spoofing systems, as well as better evaluation databases.",1.0,4.05
1187,2085,5    2.75    3.55    4.9    ,A Proposal of Glottal Gesture Coordination in Cleft-palate Speakers,"This paper aims to discuss the compensatory productions of cleft palate population in the light of the notion of articulatory gesture applied particularly to the glottal stop. Considering the speech disorders presented by people with cleft palate, the present study proposes a tripartite contrast for the glottal gesture, namely: open (voiceless), closed-altered (laryngeal and glottal stop) and critical (fricative). In order to understand how the coordination of the laryngeal gestures studied could be modified in the case of cleft palate speech, we grounded our proposal in the supervision notion introduced by cognitive phonetics.
Index Terms: articulatory phonology, cognitive phonetics, speech production, speech disorders, clef-palate",1.0,4.050000000000001
897,1324,4.75    5.9    2.95    2.6    ,Optimizing Neural Network Embeddings Using Pair-Wise Loss for Text-Independent Speaker Matching,"Speaker matching is the task of deciding whether two given speech recordings come from the same speaker or not. The key to solving this task is extracting speaker-discriminative features which could directly be compared to make a decision based on a similarity threshold. The state-of-the-art in such features are i-vectors which represent the distribution of spectral features in recordings. Recently, neural networks have been used to extract features from utterances for speaker matching. These features, extracted from the last layer of a neural network optimized for the task have been shown to be speaker discriminative. Previous work widely uses cross-entropy loss to train networks for speaker matching. In this paper, we study how neural networks could be trained using a different optimization criterion. Instead of minimizing the classification error, we minimize the area under the Detection Error Trade-off (DET) curve. The area can be naturally minimized by maximizing the distance between the distributions of similarity scores of pairs of utterances from matched and mismatched speakers. Results show that this optimization allows the network to learn better speaker discriminative features. In addition, theses features are complementary to i-vectors and their combination can further improve the i-vector baseline.",1.0,4.050000000000001
1268,2349,2.65    4.95    5.1    3.55    ,Expressing Politeness in Polish and Canadian Dialogues,"The aim of this paper is to introduce a novel corpus of speech POLAC - Politeness across cultures and to present some preliminary results on acoustic correlates of intended politeness. It was created for cross-cultural comparative studies on Polish and Canadians. It consists of 180 three-minutes long conversations in an arranged situation where the examined person was interviewing a non-native patient with a broken arm. 
In two experimental conditions, suggestions were made: to be polite, to be unemotional and in the control group there were no suggestions. Recordings were analyzed with voice quality and prosody parameterization methods as well as MFCC-GMM emotion recognition algorithm. 
The following observations for polite speech were made: in spite of similar F0, Canadians tend to modulate speech significantly more than Polish. The intonation was more flat and monotonous for Polish. No significant differences were found in voice timbre. For both, Polish and Canadians, polite speech took more time than in the no-suggestion condition, and the latter was also longer than unemotional condition. From the point of view of emotions in voice, higher proportion of sadness was detected in Polish than for Canadian group in all conditions, but the tendency is the strongest for polite condition.",1.0,4.0625
921,1394,3.15    5.05    3.75    4.3    ,Declination-adjusted Normalisation of Tonal Fundamental Frequency,"An experiment is described to investigate whether the normalisation of citation tone fundamental frequency (F0) can be improved by taking into account its occasion-specific decay. An optimum baseline z-score normalization of the F0 of ten Cantonese speakers’ unstopped citation tones without such declination adjustment gave about a twenty-fold reduction in the between-speaker tonal variance (normalisation index = 21.3). It is shown that adjusting for F0 declination by simply using the linear slope of the phonologically level tones can improve on this baseline normalization a little, by up to about 15% (NI = 24.8). The result is also a representation closer to the tonal pitch.",1.0,4.0625
249,1381,5    5.05    2.3    3.9    ,Sequence-to-sequence Neural Network Model with 2D Attention for Learning Japanese Pitch Accents,"Many Japanese text-to-speech (TTS) systems use word-level pitch accents as one of the prosodic features. Combination of a pronunciation dictionary including lexical pitch accents and a statistical model representing the word accent sandhi is often used to predict pitch accents from a text. However, using human transcribers to build the dictionary and training data for the model is tedious and expensive. This paper proposes a neural pitch accent recognition model. This model combines the information from audio and its transcription (word sequence in hiragana characters) via two-dimensional attention and outputs word-level pitch accents. Experimental results show a reduction in the word pitch accent prediction error rate over that with text only. It lowers the load of human annotators when building a pronunciation dictionary. As the approach is general, it can be used to do pronunciation learning in other languages.",0.0,4.0625
1321,2575,5.1    4.2    3.7    3.25    ,Gender Effects on Perception of Emotional Speech- and Visual-prosody in a Second Language: Emotion Recognition in English-speaking Films,"Speakers use both speech prosody and visual prosody to express emotions. Receivers register and recognise emotion via both types of prosodic cues. Gender differences have been reported for emotion recognition in one’s native language. For example, women outperform men in emotion recognition through visual prosody and emotional contagion. In the present study, we examined gender differences in both recognition of type of emotion (e.g. anger vs. joy) and perceived emotionality (e.g. the degree of anger) expressed via speech prosody and visual prosody in a second language (L2). In a perception experiment using film scenes, proficient Dutch learners of English rated the emotionality of each protagonist and identified the specific type of emotion expressed by each protagonist in each scene in both the visual-only and audio-only modality. We have found no evidence for gender-related differences in perceived emotionality. However, the female L2 learners were more accurate in recognising type of emotion from both speech prosody and visual prosody. These findings suggest that there is transfer of learners’ ability in recognising type of emotion in the native language to L2 and that female L2 learners may be better at learning cues in speech prosody to type of emotion in L2.",1.0,4.0625
992,1582,2.85    5.3    4.85    3.25    ,Masking-based Speech Enhancement Using Convolutional Neural Network-based Generative Adversarial Network,"Speech Enhancement (SE) system deals with improving the perceptual quality and preserving the speech intelligibility of the noisy mixture. The Time-Frequency (T-F) masking-based SE using the supervised learning algorithm, such as a Deep Neural Network (DNN), has outperformed the traditional SE techniques. However, the notable difference observed between the oracle mask and the predicted mask, motivates us to explore different deep learning architectures. In this paper, we propose to use a Convolutional Neural Network (CNN)-based Generative Adversarial Network (GAN) for inherent mask estimation. GAN takes an advantage of the adversarial optimization, an alternative to the other Maximum Likelihood (ML) optimization-based architectures. We also show the need for supervised T-F mask estimation for effective noise suppression. Experimental results demonstrate that the proposed T-F mask-based SE significantly outperforms the recently proposed end-to-end SEGAN and a GAN-based Pix2Pix architecture. The performance evaluation in terms of both the predicted mask and the objective measures, dictates the improvement in the speech quality, while simultaneously reducing the speech distortion observed in the noisy mixture.",1.0,4.0625
1204,2137,3.35    4.45    3.45    5    ,Speech Dereverberation with Subband Deep Neural Networks,"Speech dereverberation with Deep Neural Networks (DNN) has been proved to be superior than those traditional statistical signal processing based algorithms. In this paper, a new network architecture named subband DNN is proposed to improve speech dereverberation performance. Instead of doing dereverberation on full spectral band, subband DNN trains different networks on different subbands and concatenates different network outputs as prediction. Strategies of network parameters sharing among different subbands and non-uniform subband cost are designed to achieve further improvement. Implemented as bidirectional Recurrent Neural Networks with Long Short-Term Memory units, subband DNNs improve the non-subband DNNs a lot on speech dereverberation task using REVERB Challenge 2014 dataset with both objective enhancement measurements and automatic speech recognition measurements. Specifically, it shows that subband DNNs provide 20\% relative improvement over the best systems of that challenge in automatic speech recognition tasks.",1.0,4.0625
834,1161,3.55    4.35    4.3    ,Fast Lip Feature Extraction Using Psychologically Motivated Gabor Features,"The extraction of relevant lip features is of continuing interest in the speech domain.  We present a new, lightweight feature extraction approach, motivated by glimpse based psychological research into facial barcodes.  This allows for 3D geometric features to be produced using Gabor based image patches.  This new approach can successfully extract lip features with a minimum of processing, with parameters that can be quickly adapted and used for detailed analysis, and with preliminary results showing successful feature extraction from a range of different speakers.",1.0,4.066666666666666
520,1967,2.4    4.95    4.85    ,Classification of Disorders in Vocal Folds Using Electroglottographic Signal,"The main objective of this paper is to accurately classify the pathological voice based on the disorders in vocal folds. For this purpose, we have explored the phase of Electroglottographic (EGG) signal which carries significant information related to characteristics of vocal folds. Four important parameters, namely, close quotient, open quotient, average pitch period and jitter computed from the phase of the EGG signal have been explored for discriminating the patients based on the disorder in their vocal folds. These parameters have been used for classification of three types of vocal folds disorders: vocal nodules, vocal polyps and laryngitis. In this study, we have used the EGG signals of seventy-nine patients having disorders in vocal folds, collected from hospital. The database contains the simultaneous recording of speech and EGG signals of four vowel (‘a’,‘e’,‘o’,‘u’) utterances. The experimental result shows that the proposed features extracted from phase, performed well in classification of patients according to the disorder in their vocal folds.",0.0,4.066666666666666
84,1087,3.4    3.05    5    4.85    ,A Deep Learning Approach to Assessing Non-native Pronunciation of English Using Phone Distances,"The way a non-native speaker pronounces the phones of a language is an important predictor of their proficiency. In grading spontaneous speech, the pairwise distances between generative statistical models trained on each phone have been shown to be powerful features. This paper presents a deep learning alternative to model-based phone distances in the form of a tunable Siamese network feature extractor to extract distance metrics directly from the audio frame sequence. Features are extracted at the phone instance level and combined to phone-level representations using an attention mechanism. Pair-wise distances between phone features are then projected through a feed-forward layer to predict score. The extraction stage is initialised on either a binary phone instance-pair classification task, or to mimic the model-based features, then the whole system is fine-tuned end-to-end, optimising the learning of the distance metric to the score prediction task. This method is therefore more adaptable and more sensitive to phone instance level phenomena. Its performance is compared against a DNN trained on Gaussian phone model distance features.",0.0,4.074999999999999
622,2259,3.4    3.65    4.25    5    ,Online Incremental Learning for Speaker-Adaptive Language Models,"Voice control is a prominent interaction method on personal computing devices. While automatic speech recognition (ASR) systems are readily applicable for large audiences, there is room for further adaptation at the edge, ie. locally on devices, targeted for individual users. In this work, we explore improving ASR systems over time through a user's own interactions. Our online learning approach for speaker-adaptive language modeling leverages a user's most recent utterances to enhance the speaker dependent features and traits. We experiment with the Large-Vocabulary Continuous Speech Recognition corpus Tedlium v2, and demonstrate an average reduction in perplexity (PPL) of 19.18% and  average relative reduction in word error rate (WER) of 2.80% compared to a state-of-the-art baseline on Tedlium v2.",0.0,4.075
711,2475,3.35    5.1    5.15    2.7    ,Automatic Detection of Orofacial Impairment in Stroke,"Stroke is a devastating condition that affects the ability of people to communicate through speech, leading to social isolation and poor quality of life. The quantitative evaluation of speech and orofacial movements is essential for assessing the impairment and identifying treatment targets. However, to our knowledge, a tool for the automatic orofacial assessment, which considers multiple aspects of orofacial impairment (e.g., range of motion in addition to asymmetry), has not been developed for this clinical population. In this work, we tested a video-based approach for the automatic orofacial assessment in stroke survivors, combining low-cost depth sensor and face alignment algorithms for extracting facial features. Twelve patients post-stroke and 11 control subjects were evaluated during speech and non-speech tasks. By using a small feature-set representing range of motion and asymmetry of face movements, it was possible to discriminate patients post-stroke from control subjects with high accuracy (87%). Further insights on the choice of the task and face alignment algorithm are provided, demonstrating that a non-parametric approach such as SDM can provide better results. Through this work we demonstrated the feasibility of an objective tool to support clinicians in the assessment of speech and orofacial impairment post-stroke.",0.0,4.075
1297,2460,4    4.95    3.3    ,Emotion Recognition in Speech with a Semi-supervised GAN,"With the advances in the research area of deep learning in recent years, recognizing emotion characteristics in speech became a feasible task. Instead of relying on pre-defined, and most of the times domain-specific, feature extractors, solutions based on deep neural networks can learn auditory representations which are general enough for recognizing emotional characteristics on inter-cultural speech signals.  However, one of the most inherent problems of deep neural models is the necessity of a large amount of labeled data, as these models usually are trained as supervised classifiers. This paper proposes the use of Generative Adversarial Networks (GANs), which are trained in an unsupervised manner, to create a general speech representation to be used in a semi-supervised task of emotional speech classification. We evaluate the learned representations on the categorical emotion recognition task on SAVEE dataset and our experiments show that the proposed model has a better performance when compared to state-of-the-art solutions which are fully supervised, while not relying on strong labeled data.",1.0,4.083333333333333
646,2324,4.4    4.5    4.05    3.4    ,The ACLEW DiViMe: an Easy-to-use Diarization Tool,"We present ``DiViMe'', an open-source virtual machine aimed at packaging speech technology for real-life data, and developed in the context of the ``Analyzing Children's Language Environments across the World'' Project. This first release focuses on Speech Activity Detection, Speaker Diarization, and their evaluation. The present paper introduces the set of included tools and the current workflow, which is focused on making minimal assumptions regarding users' technical skills. Additionally, we show how the current DiViMe tools fare against three sets of challenging data. In a first experiment, we look at performance with samples extracted from daylong recordings gathered using the LENA\textsuperscript{TM}\, system from English-learning children. We find that the performance of the tools currently in DiViMe is not far from  that achieved by the \lena proprietary software. In a second experiment, we generalize to other samples of child-centered daylong files, gathered with non-LENA\textsuperscript{TM}\, hardware from non-English-learning children, showing that performance does not degrade in this condition. Finally, we report on performance in the DiHARD 2018 Challenge Test Data. Originally conceived in the ``Speech Recognition Virtual Kitchen'', DiViMe is a promising platform for packaging speech technology tools for widespread re-use, with potential impact on both fundamental and applied speech and language research.",0.0,4.0874999999999995
1102,1856,4.35    3.25    4.9    3.85    ,Subjective and Objective Studies on the Influence of Speaker's Gender on the Unvoiced Segments,"Subjective and objective experiments are conducted to understand the extent to which a speaker's gender influences the acoustics of unvoiced (U) sounds. U segments in an utterance are replaced by the corresponding segments of a speaker of opposite gender to prepare a modified utterance. Subjects are asked to judge if the modified utterance is spoken by a single speaker or two different speakers. The result shows that subjects are unable to distinguish the modified from the original. This implies that listeners are able to identify the U segments irrespective of the gender, which may be based on some speaker-independent invariant acoustic cues. To test if this finding is purely a perceptual phenomenon, objective experiments are also conducted. Gender specific HMM based phoneme recognition systems are trained using the TIMIT training set and tested on (a) utterances spoken by the same gender (b) utterances spoken by opposite gender and (c) the modified utterances of the test set. As expected the performance is highest for case (a) and lowest for case (b). The performance degrades only slightly for case (c). This result shows that the speaker's gender does not as strongly influence the acoustics of U sounds as of voiced sounds.",1.0,4.0875
945,1458,4.25    3.45    5.1    3.55    ,Unusable Spoken Response Detection with BLSTM Neural Networks,"Voice biometrics has been applied to enhance the security of spoken language proficiency tests and ensure valid test scores by detecting fraudulent activity. These methods can, however, be triggered by certain distortions, including background noise and adjacent test-takers, resulting in false positive alarms. In this paper, a two-layer bi-directional LSTM RNN model is employed to detect these distorted (unusable) responses and a sub-sampling method is applied to reduce the difficulties of model training caused by very long input sequence and imbalanced training data. The system is evaluated on a corpus that was collected from an assessment of English language proficiency around the world. Results show that our approach significantly outperforms two baselines: a Gaussian mixture model (GMM) classifying frame-level features and an AdaBoost classifier operating on i-vectors. Our system's F-score in unusable response detection is 0.60 compared to 0.43 and 0.49 for the two baseline systems.",1.0,4.0875
573,2104,5.05    2.7    4.25    4.35    ,Experience-dependent Influence of Music and Language on Lexical Pitch Learning Is Not Additive,"Research studies provide evidence for the facilitative effects of musical and linguistic experience on lexical pitch learning. However, the effect of interaction of linguistic and musical pitch experience on lexical pitch processing is a matter of ongoing research. In the current study, we sought to examine the effect of combination of musical and linguistic pitch experience on learning of novel lexical pitch. Using a 10-session pseudoword-picture association training paradigm, we compared the learning performance of musicians and non-musicians who either spoke a non-tone language, spoke one tone language, or spoke two tone languages. Among the non-tone language speakers, we found that musicians showed enhanced learning of novel lexical pitch as compared to non-musicians. In comparison, among the tone-language speakers, we found no significant difference in the learning performance of musicians and non-musicians no matter they spoke one or more tone languages. We conclude that though musical experience facilitates linguistic pitch learning, the effects of combination of musical and linguistic pitch experience are not additive i.e. possessing both types of pitch experience is no better than possessing either one of them and knowing two tone languages does not facilitate the learning of a new tone language beyond the knowledge of one.",0.0,4.0875
1167,2013,4.3    4.15    2.6    5.35    ,Highly-Reverberant Real Environment Database: HRRE,Speech recognition in highly-reverberant real environments remains a major challenge. An evaluation dataset for this task is needed. This report describes the generation of the Highly-Reverberant Real Environment database (HRRE). This database contains 13.4 hours of data recorded in real reverberant environments and consists of 20 different testing conditions which consider a wide range of reverberation times and speaker-to-microphone distances. These evaluation sets were generated by re-recording the clean test set of the Aurora-4 database which corresponds to five loudspeaker-microphone distances in four reverberant conditions.,1.0,4.1
894,1317,3.95    4.1    4    4.35    ,Task-Independent EEG Based Subject Identification Using Auditory Stimulus,"Recent studies have shown that task-specific electroencephalography  (EEG) can be used as a reliable biometric. This paper extends this study to task-independent EEG with auditory stimuli. Data collected from 40 subjects in response to various types of audio stimuli, using a 128 channel EEG system is presented to different classifiers, namely,  k-nearest neighbor (k-NN),  artificial neural network (ANN) and universal background model - Gaussian mixture model (UBM-GMM).  It is observed that k-NN and ANN perform well when testing is performed intrasession, while UBM-GMM framework is more robust when testing is performed intersession.  This can be attributed to the fact that the correspondence of the sensor locations across sessions is only approximate. It is also observed that EEG from parietal and temporal regions contain more subject information although the performance using all the 128 channel data is marginally better.",1.0,4.1
1323,2583,2.95    4.35    4    5.1    ,Multi-modal Emotion Speech Recognition Using Audio and Text,"Emotion speech recognition is a challenging task which has been extensively relied on the models that used audio feature in building a well-performing classifier. In this paper, we propose a novel deep dual recurrent encoder model that utilize not only audio signal but also text data simultaneously for better understanding of the speech data. As emotional dialogue is composed of sound and spoken contents, our model encodes each information from audio and text sequence using dual RNNs,then combine them with a feed-forward neural model to predict the emotion class. With this architecture, speech data is analyzed from signal-level to language-level, thus utilizing information within the data more comprehensively compared to that of audio-feature focused model. Extensive experiments are conducted to investigate efficacy and properties of the proposed model. Our proposed model outperforms previous state-of-the-art methods in classifying among four emotion category, (i.e., angry, happy, sad and neutral), from 68.8% to 71.8% using the IEMOCAP dataset. In particular, previous audio-feature focused model suffer from that the class is frequently confused with ""neutral"" class, however, our model show that; it classifies the emotion class correctly; and it reduces the ""misclassified to neutral class"" bias.",1.0,4.1
797,1053,2.05    5.1    5.05    4.2    ,Voice Conversion with Diverse Intonation Using Conditional Variational Auto-Encoder,"Voice conversion is a task of synthesizing an utterance with target speaker's voice while maintaining linguistic information of the source utterance. While a speaker can produce varying utterances from a single script with different intonations, conventional voice conversion models were limited to producing only one result per source input. To overcome this limitation, we propose a novel approach for voice conversion with diverse intonations using conditional variational autoencoder (CVAE).

Experiments have shown that the speaker's style feature can be mapped into a latent space with Gaussian distribution. We have also been able to convert voices with more diverse intonation by making the posterior of the latent space more complex with inverse autoregressive flow (IAF). As a result, the converted voice not only has a diversity of intonations, but also has better sound quality than the model without CVAE.",1.0,4.1
1249,2266,4.35    3.1    4.85    ,A Common Spatial Pattern Approach for Classification of Mental Counting and Motor Execution EEG,"A Brain Computer Interface (BCI) as a medium of communication is convenient for people with severe motor disabilities. Although there are a number of different BCIs, the focus of this paper is on Electroencephalography (EEG) as a means of human computer interaction. 
  Motor imagery and mental arithmetic are most popular techniques used to modulate brain waves that can be used to control devices.  We show that it is possible to define different mental states using real fist rotation and imagined reverse counting. While people have already investigated left fist rotation and right fist rotation for dual state BCI, we intend to define a new state using mental reverse counting. We use Common Spatial Pattern (CSP) approach for feature extraction to distinguish between these states. CSP has been prominently used in context of motor imagery task, we define its applicability for distinction between motor execution and mental counting. CSP features are evaluated using classifiers like GMM, SVM and GMM-UBM. GMM-UBM using data filtered through the beta band (13-30Hz) gives the best performance.",1.0,4.1
101,1114,3.55    3.65    4.1    5.1    ,A Deep Neural Network Based Harmonic Noise Model for Speech Enhancement,"In this paper, we present a novel deep neural network (DNN) based speech enhancement method that uses a harmonic noise model (HNM) to estimate the clean speech. By utilizing HNM to model the clean speech in the short-time Fourier transform domain and extracting some time-frequency features of noisy speech for the DNN training, the new method predicts the harmonic and residual amplitudes of clean speech from a set of noisy speech features. In order to emphasize the importance of the harmonic component and reduce the effect caused by the residual, a scaling factor is also introduced and applied to the residual amplitude. The enhanced speech is reconstructed with the estimated clean speech amplitude and the noisy phase of HNM. Experimental results demonstrate that our proposed HNM-DNN method outperforms two existing DNN based speech enhancement methods in terms of both speech quality and intelligibility.",0.0,4.1
1125,1916,3.8    4.95    3.55    4.1    ,Time-frequency Spectral Error for Analysis of High Arousal Speech,"High arousal speech is produced by speakers when they raise their loudness levels. There are deviations from neutral speech, especially in the excitation component of the speech
production mechanism in the high arousal mode. In this study, a parameter, called the time-frequency spectral error (Tfe) is derived using the single frequency filtering (SFF) spectrogram. It is used to characterize the high arousal regions in speech signals. The proposed parameter captures the fine temporal and spectral variations due to changes in the excitation source.",1.0,4.1
988,1572,5.05    3.05    2.8    5.5    ,Automatic Intonation Classification for British English Speech Using Temporal Patterns in Utterance-level Pitch Contour and Perceptually Motivated Pitch Transformation,"Typically, in spoken British English (BE), meaning of an utterance is conveyed through four intonation classes -- Glide-up, Glide-down, Dive and Take-off. We, in this work, consider the problem of automatically classifying the intonation of BE utterances which could be useful for providing feedback to the second language learners. We hypothesize that the intonation classes could be discriminated using temporal variations in the utterance-level pitch contour. Further, we show that the temporal variations in pitch contour, when transformed based on the perceptual properties of the ear, are more representative of the intonation classes. For this, we consider three different pitch transformations. In order to represent the temporal variations, we propose a 3-dim feature sequence and model them using hidden Markov models. Experiments are conducted on the speech data collected from a spoken English training material for teaching intonation of BE. We obtain higher unweighted average recall (UAR) using the proposed scheme with and without transformation compared to a baseline scheme that does not exploit temporal structures in utterance-level pitch contour. Among different transformations considered, the highest absolute improvements in the UAR are found to be 36.19% and 13.56% over the baseline scheme and a scheme without using transformation respectively.",1.0,4.1
1138,1960,3.3    5.2    5.25    2.65    ,End-to-End Multi-Lingual Multi-Speaker Speech Recognition,"The expressive power of end-to-end automatic speech recognition (ASR) systems enables direct estimation of the character or word label sequence from  a sequence of acoustic features. Direct optimization of the whole system is advantageous because it not only eliminates the internal linkage necessary for hybrid systems, but also extends the scope of potential application use cases by training the model for multiple objectives. In our previous work, we proposed an multi-lingual ASR system based on a monolithic neural network architecture without language-dependent modules, and showed that a modeling of multiple languages is no exception for the framework of end-to-end system. Recently, there is a great interest on multi-speaker speech recognition which enables generation of multiple label sequences from single-channel mixed speech. In another work, we proposed a multi-speaker end-to-end ASR system, and showed that the system can directly model one-to-many mapping without additional auxiliary clues. In this paper, we propose an all-in-one end-to-end multi-lingual multi-speaker ASR system by integrating both systems. The proposed model is evaluated using mixtures of two speakers generated by using 10 languages, including mixed-language utterances.",1.0,4.1
202,1297,4.15    4.2    3.95    ,Exploration of Compressed ILPR Features for Replay Attack Detection,"This paper deals with the problem of detecting replay attacks
on speaker verification systems. In literature, apart from the
acoustic features, source features have also been successfully
used for this task. In existing source features, only the informa-
tion around glottal closure instants (GCIs) have been utilized.
We hypothesize that the feature derived by capturing the tem-
poral dynamics between two GCIs would be more discrimina-
tive for such task. Motivated by that, in this work we explore
the use of discrete cosine transform compressed integrated lin-
ear prediction residual (ILPR) features for discriminating be-
tween genuine and replayed signals. A spoof detection system
is built using the compressed ILPR feature and a Gaussian mix-
ture model (GMM) classifier. A baseline system is also built
using constant-Q cepstral coefficient feature with GMM back-
end. These systems are tested on the ASVSpoof 2017 Version
2.0 database. On fusing the systems developed using acoustic
and proposed source features an equal error rate of 9.41% is
achieved on the evaluation set.",0.0,4.1000000000000005
765,76,3.25    5.05    4.05    4.05    ,A Multi-Stream Framework to Unify Acoustic-Based and G2P Conversion-Based Pronunciation Extraction,"In this paper, we propose a multi-stream formulation for pronunciation generation to unify the orthography-based approach for pronunciation extraction, i.e., grapheme-to-phoneme (G2P) conversion approach, and the acoustic exemplar-based approach for pronunciation extraction.  The approach involves (1) estimation of phone posterior probabilities given the observation (acoustic or orthographic representation of a word), (2) combination of the posterior probabilities using probability combination rules, and (3)  inference of pronunciations by decoding the probabilities obtained after combination. We validate the proposed framework on two challenging English corpora, namely PhoneBook and NameDat, which contain uncommon words and proper names. We show that the multi-stream formulation leads to lexicons that yield significantly better speech recognition systems, including when compared against the alternative approach of incorporating acoustics along with G2P conversion for pronunciation variant selection.",1.0,4.1000000000000005
324,1535,4.15    4.2    3.95    ,A Multi-Discriminator CycleGAN for Unsupervised Non-Parallel Speech Domain Adaptation,"Domain adaptation plays an important role for speech recognition models, in particular, for domains that have low resources. We propose a novel generative model based on cyclic-consistent generative adversarial network (CycleGAN) for unsupervised non-parallel speech domain adaptation. The proposed model employs multiple independent discriminator on the power spectrogram, each in charge of different frequency bands. As a result we have 1) better discriminators that focuses on fine-grained details of the frequency features, and 2) a generator that is capable of generating more realistic domain adapted spectrogram. We demonstrate the effectiveness of our method on speech recognition with gender adaptation, where the model only have access to supervised data from one gender during training, but is evaluated on the other at testing time. Our model is able to achieve an average of 7.41% on phoneme error rate, and 11.10% word error rate relative performance improvement as compared to the baseline on TIMIT and WSJ dataset, respectively. Qualitatively, our model also generate more realistic sounding speech synthesis when conditioned on data from the other domain.",0.0,4.1000000000000005
182,1264,4.3    4.3    5.1    2.75    ,Perceptual and Automatic Evaluations of the Intelligibility of Speech Degraded by Noise Induced Hearing Loss Simulation,"This study aims at comparing perceptual and automatic intelligibility measures on degraded speech.
It follows a previous study that designed a novel approach for the automatic prediction of Age-Related Hearing Loss (ARHL) effects on speech intelligibility.
In this work, we adapted this approach to a different type of hearing disorder: the Noise Induced Hearing Loss (NIHL), i.e., hearing loss caused by noise exposure at work.
Thus, we created a speech corpus made of both isolated words and short sentences pronounced by three speakers (male, female and child) and we simulated different levels of NIHL. A repetition task has been carried out with 60 participants to collect perceptual intelligibility scores. Then, an Automatic Speech Recognition (ASR) system has been designed to predict the perceptual scores of intelligibility. 
The perceptual evaluation showed similar effects of NIHL simulation on the male, female and child speakers. In addition, the automatic intelligibility measure, based on automatic speech recognition scores, was proven to well predict the effects of the different severity levels of NIHL. Indeed, high correlation coefficients were obtained between the automatic and perceptual intelligibility measures on both speech repetition tasks: 0.94 for isolated words task and 0.97 for sentences task.",0.0,4.1125
698,2446,4.85    3.35    2.4    5.85    ,Coherence Models for Dialogue,"Coherence across multiple turns is a major challenge for state-of-the-art dialogue models.
Arguably the most successful approach to automatically learning text coherence is the entity grid, which relies on modelling patterns of distribution of entities across multiple sentences of a text.  Originally applied to the evaluation of automatic summaries and the news genre, among its many extensions, this model has also been successfully used to assess dialogue coherence. 
Nevertheless, both the original grid and its extensions do not model intents, a crucial aspect that has been studied widely in the literature in connection to dialogue structure.
We propose to augment the original grid document representation for dialogue with the intentional structure of the conversation. 
Our models outperform the original grid representation on both text discrimination and insertion, the two main standard tasks for coherence assessment across three different dialogue datasets, confirming that intents play a key role in modelling dialogue coherence.",0.0,4.1125
775,94,3.5    5.8    4.2    2.95    ,Modeling Singing F0 with Neural Network Driven Transition-Sustain Models,"This study focuses on generating fundamental frequency (F0) curves of singing voice from musical scores stored in a midi-like notation. Current statistical parametric approaches to singing F0 modeling meet difficulties in reproducing vibratos and the temporal details at note boundaries due to the oversmoothing tendency of statistical models. This paper presents a neural network based solution that models a pair of neighboring notes at a time (the transition model) and uses a separate network for generating vibratos (the sustain model). Predictions from the two models are combined by summation after proper enveloping to enforce continuity. In the training phase, mild misalignment between the scores and the target F0 is addressed by back-propagating the gradients to the networks' inputs. Subjective listening tests on the NITech singing database show that transition-sustain models are able to generate F0 trajectories close to the original performance.",1.0,4.1125
655,2352,1.5    5.05    5    4.9    ,Voice Source Contribution to Prominence Perception: Rd Implementation,"This paper explores the contribution of voice source modulation to the perception of prominence, following on previous analyses of accentuation, focus and deaccentuation. A listening test was carried out on a sentence of Irish with three accented, prominent syllables (P1, P2, P3). Using inverse filtering and resynthesis, a ‘flattened’ version was generated, with only slight declination of f0 and other voice source parameters. The global waveshape parameter Rd was modulated to provide (i) source boosting (tenser phonation) on either P1 or P2, and/or (ii) source attenuation (laxer phonation) following (Post-attenuation) or preceding (Pre-attenuation) P1 or P2. Rd variation was achieved in two different ways to generate two series of stimuli. f0 was not varied in either series. Twenty-nine listeners rated the prominence level of all syllables in the utterance. Results show that the phrasal position (P1 vs. P2) makes a large difference to prominence judgements. P1 emerged as overall more prominent and more readily ‘enhanced’ by the source modifications. Post-attenuation was particularly important for P1, with effects equal to or greater than local P-boosting. In the case of P2, Pre-attenuation was much more important than Post-attenuation.",0.0,4.112500000000001
467,1840,4.1    4.15    4.1    ,Expectation-Maximization Algorithms for Itakura-Saito Nonnegative Matrix Factorization,"This paper presents novel expectation-maximization (EM) algorithms for estimating the nonnegative matrix factorization model with Itakura-Saito divergence. Indeed, the common EM-based approach exploits the space-alternating generalized EM (SAGE) variant of EM but it usually performs worse than the conventional multiplicative algorithm. We propose to explore more exhaustively those algorithms, in particular the choice of the methodology (standard EM or SAGE variant) and the latent variable set (full or reduced). We then derive four EM-based algorithms, among which three are novel. Speech separation experiments show that one of those novel algorithms using a standard EM methodology and a reduced set of latent variables outperforms its SAGE variants and competes with the conventional multiplicative algorithm.",0.0,4.116666666666666
1203,2135,4.1    4.35    3.9    ,Fidelity of Automatic Speech Processing for Adult Speech Classifications Using the Language ENvironment Analysis (LENA) System,"The Language ENvironment Analysis (LENA) system is a wearable audio recorder that collects daylong recordings; it identifies and classifies speech, providing automated measures of adult word counts and other vocalization metrics. Clinicians and researchers adopted LENA for analysis of the at-home language and acoustic environments of children at risk for speech-language delay or disorder. A primary issue for researchers and clinicians is the reliability of LENA derived speech classification and adult word count (AWC). We tested classification and AWC reliability in LENA recordings from 15 families with young children who were typically developing, hearing impaired with a hearing aid, or were profoundly deaf with a cochlear implant. The analysis focused on samples of audio classified by LENA as containing speech and samples classified as non-speech within one recording from each family. Human listeners identified start and end points of speech by adult female or male talkers and child vocalizations, as well as the number of words produced by adult talkers during speech intervals. Our results suggest marginal reliability for LENA’s classifications, with approximately a 2:1 ratio of adult speech found vs. missed. These results suggest that LENA’s automatic measures of AWC and other vocalization metrics should be interpreted cautiously.",1.0,4.116666666666666
809,1084,5.25    4.15    2.95    ,Analyzing Learned Representations of a Deep ASR Performance Prediction Model,"This paper addresses a relatively new task: prediction of  ASR performance on unseen broadcast programs. 
In a previous paper, we presented an ASR performance prediction system using CNNs that encode both text (ASR transcript) and speech, in order to predict word error rate. 
This work is  dedicated to the analysis of speech signal embeddings and text embeddings learnt by the CNN while training  our prediction model.
We try to better understand which information is captured by the deep model and its relation with different conditioning factors.
It is shown that hidden layers convey a clear signal about speech style, accent and broadcast type. We then try to leverage these 3 types of information at training time through multi-task learning. Our experiments show that this allows to train slightly more efficient ASR-performance prediction systems that - in addition - simultaneously tag the analyzed utterances according to their speech style, accent and broadcast program origin.",1.0,4.116666666666667
386,1668,3.55    5.1    5    2.85    ,Text-Dependent Speech Enhancement for Small-Footprint Robust Keyword Detection,"Keyword detection (KWD), also known as keyword spotting, is in great demand in small devices in the era of Internet of Things. Albeit recent progresses, the performance of KWD, measured in terms of precision and recall rate, may still degrade significantly when either the non-speech ambient noises or the human voice and speech-like interferences (e. g., TV, background competing
talkers) exists. In this paper, we propose a general solution to address all kinds of environmental interferences. A novel text-dependent speech enhancement (TDSE) technique using a recurrent neural network (RNN) with long short-term memory (LSTM) is presented for improving the robustness of the small-footprint KWD task in the presence of environmental noises and interfering talkers. On our large simulated and recorded noisy and far-field evaluation sets, we show that TDSE significantly improves the quality of the target keyword speech and performs particularly well under speech interference conditions. We demonstrate that KWD with TDSE frontend significantly outperforms the baseline KWD system with or without a generic speech enhancement in terms of equal error rate (EER) in the keyword detection evaluation.",0.0,4.125
1096,1847,4.75    5.1    3.25    3.4    ,Durational and Pitch Aspects of Mandarin Chinese Rhythm in Read and Spontaneous Speech,"This study investigates Mandarin Chinese rhythm in read speech and spontaneous speech, using both rhythm metrics and melody metrics. Rhythm metrics (%V, deltaC, VarcoC, rPVI and nPVI) are calculated to measure the durational variation, and melody metrics (mean intervals and slopes of F0 rising and falling) are calculated to measure the F0 variation. Though some of the rhythm metrics confirm the syllable-timed impression of Mandarin Chinese, several metrics (e.g. nPVI) based on spontaneous speech are similar to stress-timed languages. The results of melody metrics reveal differences between read speech and spontaneous speech. A difference is also found between genders. The results of the two aspects suggest that the rhythm characteristics of a language can change due to speaking style. A further point is made that the calculation of rhythm/melody metrics based on acoustic measurements without account of linguistic structures cannot fully capture the characteristics of speech rhythm.",1.0,4.125
602,2191,4.25    4.15    4.95    3.15    ,An Investigation of Mixup Training Strategies for Acoustic Models in ASR,"Mixup is a recently proposed technique that creates virtual training examples by combining existing ones. It has been successfully used in various machine learning tasks. This paper
focuses on applying mixup to automatic speech recognition (ASR). More specifically, several strategies for acoustic model training are investigated, including both conventional
cross-entropy and novel lattice-free MMI models. Considering mixup as a method of data augmentation as well as regularization, we compare it with widely used speed perturbation and dropout techniques. Experiments on Switchboard-1, AMI and TED-LIUM datasets shows consistent improvement of word error rate up to 13% relative. Moreover, mixup is found to be particularly effective on test data mismatched to the training data.",0.0,4.125
135,1176,4.9    5.65    1.95    4    ,Resyllabification in Indian Languages and Its Implications in Text-to-speech Systems,"Resyllabification is a phonological process in continuous speech in which the coda of a syllable is converted into the onset of the following syllable, either in the same word or in the subsequent word. This paper presents an analysis of 
resyllabification across words in different Indian languages and its implications in Indian language text-to-speech (TTS) 
synthesis systems. The evidence for resyllabification is evaluated based on the acoustic analysis of a read speech corpus of the corresponding language.  This study shows that the resyllabification obeys the maximum onset principle and introduces the notion of prominence resyllabification in Indian languages. This paper finds acoustic evidence for total resyllabification. 

The resyllabification rules obtained are applied to TTS systems. The correctness of the rules is evaluated quantitatively by comparing the acoustic log-likelihood scores of the speech utterances with the original and resyllabified texts, and by performing a pair comparison (PC) listening test on the synthesized speech output. An improvement in the log-likelihood score with the resyllabified text is observed, and the synthesized speech with the resyllabified text is preferred 3 times over those without resyllabification.",0.0,4.125
753,39,3.3    4.9    4.9    3.4    ,A Refined Query-by-Example Approach to Spoken-Term-Detection (STD) on ESL Learners' Speech,"A refined Query-by-Example (QbE) approach is proposed to improve Spoken-Term-Detection (STD) performance on L2 English learners' speech data. A Hidden Markov Model (HMM) is built for each keyword and a computationally efficient, iterative Viterbi decoding is adopted to detect spoken keywords in test. The approach is evaluated on an English as Second Language (ESL) speech database collected over L2 learners with different English proficiency levels. The experimental results show that the new approach achieves a performance much better than the traditional DTW-based QbE. Also, it is comparable to that of an LVCSR-based STD but with significant lower complexities and computations. The refined QbE and LVCSR approach to STD are complementary to each other. By fusing the two systems together, we can further improve the MAP and MP@N performance by ~10% in testing sets of 3 different English proficiency levels over the best performance of either system.",1.0,4.125
274,1432,3.95    4.95    3.55    4.05    ,Speech Emotion Recognition from Variable-Length Inputs with Triplet Loss Function,"Automatic emotion recognition is a crucial element on understanding human behavior and interaction. Prior works on speech emotion recognition focus on exploring various feature sets and models. Compared with these methods, we propose a triplet framework based on Long Short-Term Memory Neural Network (LSTM) for speech emotion recognition. The system learns a mapping from acoustic features to discriminative embedding features, which are regarded as basis of testing with SVM. The proposed model is trained with triplet loss and supervised loss simultaneously. The triplet loss makes intra-class distance shorter and inter-class distance longer, and supervised loss incorporates class label information. In view of variable-length inputs, we explore three different strategies to handle this problem, and meanwhile make better use of temporal dynamic process information. Our experimental results on the Interactive Emotional Motion Capture (IEMOCAP) database reveal that the proposed methods are beneficial to performance improvement. We demonstrate promise of triplet framework for speech emotion recognition and present our analysis.",0.0,4.125
1195,2110,4.25    3.25    4.05    4.95    ,Real Time Keyword Spotting and Retrieval of Spoken Data from Telugu Broadcasted News,"Real Time Automatic Speech Recognition System is developed for spotting keywords from the broadcasted Telugu TV news and also retrieving the audio/video by searching the voice/keywords in the archived audio/video data of broadcasted news.
The user can set the required keywords and the system will continuously track the audio and alert the user as and when the keywords appeared in the audio. Of course the system is continuously transcribing and indexing every word with time. As and when a user wanted to retrieve the data with particular word, it will search the transcribed text in the archives and play the corresponding audio, video along with scrolling of transcribed text of audio.
The system is developed fully with GUI for giving the keywords either through microphone or keyboard by Transliteration in Telugu. The ASR system is developed using the annotated speech corpus of 65 hours duration with two hundred thousand unique words. The acoustic models are developed using SGMMs. The Language modeling is developed with Witten Bell and Kneser-Ney smoothing techniques.",1.0,4.125
493,1904,3.35    4.15    4.05    4.95    ,A Neural Model to Predict Parameters for a Generalized Command Response Model of Intonation,"The Generalised Command Response (GCR) model is a time-local model of intonation that has been shown to lend itself to (cross-language) transfer of emphasis. In order to generalise the model to longer prosodic sequences, we show that it can be driven by a recurrent neural network emulating a spiking neural network. We show that a loss function for error backpropagation can be formulated analogously to that of the Spike Pattern Association Neuron (SPAN) method for spiking networks. The resulting system is able to generate prosody comparable to a state-of-the-art deep neural network implementation, but potentially retaining the transfer capabilities of the GCR model.",0.0,4.125
144,1203,4.15    3.05    4.45    4.85    ,Information Bottleneck Based Percussion Instrument Diarization System for Taniavartanam Segments of Carnatic Music Concerts,"An approach to diarize taniavartanam segments of a Carnatic music concert is proposed in this paper. Information bottleneck (IB) based approach used for speaker diarization is applied for this task. IB system initializes the segments to be clustered uniformly with fixed duration. The issue with diarization of percussion instruments in taniavartanam is that the stroke rate varies highly across the segments. It can double or even quadruple within a short duration, thus leading to variable information rate in different segments. To address this issue, the IB system is modified to use the stroke rate information to divide the audio into segments of varying durations. These varying duration segments are then clustered using the IB approach which is then followed by Kullback-Leibler hidden Markov model (KL-HMM) based realignment of the instrument boundaries. Performance of the conventional IB system and the proposed system is evaluated on standard Carnatic music dataset. The proposed technique shows a best case absolute improvement of 8.2% over the conventional IB based system in terms of diarization error rate.",0.0,4.125
452,1807,3.25    4    4.35    4.9    ,Who Said That? a Comparative Study of Non-negative Matrix Factorization Techniques,"In noisy environments it is difficult for a computer to understand what a person is saying especially when there are multiple speakers. In this paper we concentrate on separating overlapping speech. Non-negative matrix factorisation (NMF) is a method of doing source separation without needing a lot of data. The choice of cost function can have a significant impact on the performance of NMF. We evaluate NMF using three different cost functions (Euclidean, Itakura-Saito and Kullback-Leibler) including modifications using sparsity, convolution or additional information in the form of the direction of arrival. We conduct this evaluation on three different speech corpora. Adding directional information to NMF in the form of non-negative tensor factorisation (NTF) gives us the best result on the map task and vocalization corpora and the Itakura-Saito cost function performs best on the acoustic-camera corpus. In this paper, we show that the Itakura-Saito cost function is the most robust cost function when the recording contains noise. We do this by applying acoustic evaluation measurements.",0.0,4.125
750,35,3.55    3.9    4.05    5    ,Do WaveNets Dream of Acoustic Waves?,"Various sources have reported the WaveNet deep learning architecture being able to generate high-quality speech, but to our knowledge there haven't been studies on the interpretation or visualization of trained WaveNets. This study investigates the possibility that WaveNet understands speech by unsupervisedly learning an acoustically meaningful latent representation of the speech signals in its receptive field; we also attempt to interpret the mechanism by which the feature extraction is performed. Suggested by singular value decomposition and linear regression analysis on the activations and known acoustic features (e.g. F0), the key findings are (1) activations in the higher layers are highly correlated with spectral features; (2) WaveNet explicitly performs pitch extraction despite being trained to directly predict the next audio sample and (3) for the said feature analysis to take place, the latent signal representation is converted back and forth between baseband and wideband components.",1.0,4.125
546,2031,4.75    3.45    4.2    ,Implementation of Digital Hearing Aid as a Smartphone Application,"Hearing aids for persons with sensorineural hearing loss aim to compensate for degraded speech perception caused by frequency-dependent elevation of hearing thresholds, reduced dynamic range, abnormal loudness growth, and increased temporal and spectral masking. A digital hearing aid is implemented as a smartphone application as an alternative to ASIC-based hearing aids. The implementation provides user-configurable processing for background noise suppression and dynamic range compression. Speech enhancement technique using spectral subtraction based on geometric approach and noise spectrum estimation based on dynamic quantile tracking is implemented to improve speech perception. To compensate for reduced dynamic range and frequency-dependent elevation of hearing thresholds, a sliding-band dynamic range compression technique is used. Both processing blocks are imple­mented for real-time processing using single FFT-based analysis-synthesis. Implementation as a smartphone applica­tion has been carried out using Nexus 5X with Android 7.1 Nougat OS. A touch-controlled graphical user interface enables the user to fine tune the processing parameters in an inte­ractive and real-time mode. The audio latency is 45 ms, making it suitable for face-to-face communication.",0.0,4.133333333333333
971,1517,3.85    5.15    4.15    3.4    ,An Empirical Study of English Monophthongs Acquisition of EFL Learners in Tianjin and Zibo,"This paper aims to investigate the convergence and divergence on the acquisition of English vowels from a typological perspective. We focus on the acoustic features of monophthongs produced by speakers learning English as a Foreign Language Learners (EFL). The learners are mainly from non-Mandarin area, e.g., Tianjin (TJ) and Zibo (ZB) dialectal area, both of which belong to the Jilu dialect. The acoustic features of vowels of leaners are examined in comparison with those producing by the American (AM) speakers. The research further explores the degree of phonetic transfer of dialects (L1) and Mandarin (L2) to English (L3). Ten English monophthongs and three similar vowels (/i/, /u/, /a/) are selected as target samples and their F1 and F2 formants are employed as parameters. Specifically, the results of the three similar vowels show that: /i/ and /a/ is more affected by Mandarin for ZB learners; /u/ is more similar to dialect, while for TJ learners, /a/ and /i/ is closer to dialect than to Mandarin and /u/ produced by TJ learners is more affected by Mandarin. The Speech Learning Model (SLM) is further employed to explain the analysis results.",1.0,4.1375
690,2430,3.45    4.8    5    3.3    ,Speaker Recognition with Nonlinear Distortion: Clipping Analysis and Impact,"Speech, speaker, and language systems have traditionally relied on carefully collected speech material for training acoustic models. There is an overwhelming abundance of publicly accessible audio material available for training. A major challenge, however, is that such found data is not professionally recorded, and therefore may contain a wide diversity of background noise, nonlinear distortions, or other unknown environmental based contamination or mismatch. There is a critical need for automatic analysis to screen such unknown data sets before acoustic model development, or to perform input audio purity screening prior to classification. In this study, we propose a waveform based clipping detection algorithm for naturalistic audio streams and analyze the impact of clipping at different severities on speech quality measures and automatic speaker recognition systems. We use the TIMIT and NIST SRE-08 corpora as case studies. The results show, as expected, that clipping introduces a nonlinear distortion into clean speech data, which reduces both speech quality and speaker recognition performance. We also investigate what degree of clipping can be present to sustain effective speech system performance. The proposed detection system, which will be released, could contribute to massive new audio collections for speech and language technology development.",0.0,4.1375
823,1129,4.95    4.2    4.15    3.25    ,Subspace Based Sequence Discriminative Training of LSTM Acoustic Models with Feed-Forward Layers,"State-of-the-art automatic speech recognition (ASR) systems use sequence discriminative training for improved performance over frame-level cross-entropy (CE) criterion. Even though sequence discriminative training improves long short-term memory (LSTM) recurrent neural network (RNN) acoustic models (AMs), it is not clear whether these systems achieve the optimal performance due to overfitting. This paper investigates the effect of state-level minimum Bayes risk (sMBR) training on LSTM AMs and show that the conventional way of performing sMBR by updating all LSTM parameters is not optimal. We investigate two methods to improve the performance of sequence discriminative training of LSTM AMs. First to include more feed-forward (FF) layers between the last LSTM layer and the output layer so those additional FF layers may benefit more from the sMBR training. Second, to estimate a subspace as an interpolation of rank-1 matrices when performing sMBR for LSTM layers of the AM. Our methods are evaluated in benchmark  AMI single distance microphone (SDM) task. We find that proposed approaches provides 1.6% absolute improvement over a strong sMBR trained LSTM baseline.",1.0,4.1375
618,2238,3.85    5.1    4.25    3.35    ,Cross-Corpora Convolutional Deep Neural Network Dereverberation Preprocessing for Automatic Speaker Verification and Speech Enhancement,"Deep neural network (DNN) dereverberation preprocessing has been shown to be a viable strategy for speech enhancement and increasing the accuracy of automatic speech recognition and automatic speaker verification. In this paper, an improved DNN technique based on convolutional neural networks is presented and compared to existing methods for speech enhancement and speaker verification in the presence of reverberation. This new technique is first shown to enhance speech quality as compared to other existing methods. Then, a more thorough set of experiments is presented that assesses cross-corpora speaker verification performance on data that contains real reverberation and noise. A discussion of the applicability and generalizability of such techniques is given.",0.0,4.1375
136,1178,4.2    3.95    4.2    4.2    ,Code-switching in Indic Speech Synthesisers,"Most Indians are inherently bilingual or multilingual owing to the diverse linguistic culture in India. As a result, code-switching is quite common in conversational speech. The objective of this work is to train good quality text-to-speech (TTS) synthesisers that can seamlessly handle code-switching. To achieve this, bilingual TTSes that are capable of handling phonotactic variations across languages are trained using combinations of monolingual data in a unified framework. In addition to segmenting Indic speech data using signal processing cues in tandem with hidden Markov model-deep neural network (HMM-DNN), we propose to segment Indian English data using the same approach after NIST syllabification. Then, bilingual HTS-STRAIGHT based systems are trained by randomizing the order of data so that the systematic interactions between the two languages are captured better. Experiments are conducted by considering three language pairs: Hindi+English, Tamil+English and Hindi+Tamil. The code-switched systems are evaluated on monolingual, code-mixed and code-switched texts. Degradation mean opinion score (DMOS) for monolingual sentences shows marginal degradation over that of an equivalent monolingual TTS system, while the DMOS for bilingual sentences is significantly better than that of the corresponding monolingual TTS systems.",0.0,4.1375
338,1561,4.35    5.1    4.45    2.7    ,Incremental TTS for Japanese Language,"Simultaneous lecture translation requires speech to be translated in real time before the speaker has spoken an entire sentence since a long delay will create difficulties for the listeners trying to follow the lecture. The challenge is to construct a full-fledged system with speech recognition, machine translation, and text-to-speech synthesis (TTS) components that could produce high-quality speech translations ‘on the fly’. Specifically for a TTS, this poses problems as a conventional framework commonly requires the language-dependent contextual linguistics of a full sentence to produce a natural-sounding speech waveform. Several studies have proposed ways for an incremental TTS (ITTS), in which it can estimate the target prosody from only partial knowledge of the sentence. However, most investigations are being done only in French, English, and German. French is a syllable-timed language and the others are stress-timed languages. The Japanese language, which is a mora-timed language, has not been investigated so far. In this paper, we evaluate the quality of Japanese synthesized speech based on various linguistic and temporal incremental units. Experimental results reveal that an accent phrase incremental unit (a group of moras) is essential for a Japanese ITTS as a trade-off between quality and synthesis units.",0.0,4.1499999999999995
683,2417,5    3.2    5.1    4.8    2.65    ,Spanish Statistical Parametric Speech Synthesis Using a Neural Vocoder,"During the 2000s decade, unit-selection based text-to-speech was the dominant commercial technology. Meanwhile, the TTS research community has made a big effort to push statistical-parametric speech synthesis to get similar quality and more flexibility on the generated voice. During last years, deep learning advances applied to speech synthesis have filled the gap, specially when neural vocoders substitute traditional signal-processing based vocoders. 
In this paper we substitute the waveform generation vocoder of MUSA, our Spanish TTS, with SampleRNN, a neural vocoder which was recently proposed as a deep autoregressive raw waveform generation model. MUSA uses recurrent neural networks to predict vocoder parameters (MFCC and logF0) from linguistic features. Then, the Ahocoder vocoder is used to recover the speech waveform out of the predicted parameters. In the first system SampleRNN is extended to generate speech conditioned on the Ahocoder generated parameters, where two configurations have been considered to train the system. First, the parameters derived from the signal using Ahocoder are used. Secondly, the system is trained with the parameters predicted by MUSA, where SampleRNN and MUSA are jointly optimized. The subjective evaluation shows that the second system outperforms both the original Ahocoder and SampleRNN as an independent neural vocoder.",0.0,4.1499999999999995
273,1431,5.75    4.15    2.55    ,Robust Voice Activity Detection Using Frequency Domain Long-Term Differential Entropy,"We propose a novel voice activity detection (VAD) scheme employing differential entropy at each frequency bin of power spectral estimates of past and present overlapping speech frames. Here, the power spectral estimate is obtained by employing the Bartlett-Welch method. Later, we add entropies across frequency bins and denote this as the frequency domain long-term differential entropy (FLDE). Long-term averaging enhances VAD performance under low signal-to-noise-ratio (SNR). We evaluate the performance of proposed FLDE scheme, considering 12 types of noises and 5 different SNRs which are artificially added to speech samples from the SWITCHBOARD corpus. We present VAD performance of FLDE and compare with existing VAD algorithms, such as ITU-T G.729B, likelihood ratio test, long-term signal variability, and long-term spectral flatness measure based algorithms. Finally, we demonstrate that our FLDE-based VAD performs with best average accuracy and speech hit-rate among the VAD algorithms considered for evaluation.",0.0,4.1499999999999995
589,2149,1.9    4.8    4.9    5    ,Investigating Utterance Level Representations for Detecting Intent from Acoustics,"Recognizing paralinguistic cues from speech has applications in varied domains of speech processing. In this paper we present approaches to identify the expressed intent from acoustics in the context of INTERSPEECH2018 ComPare challenge. We have made submissions in three sub-challenges: prediction of 1) self-assessed affect, 2) atypical affect and 3) types of crying. Since emotion and intent are perceived at suprasegmental levels, we explore a variety of utterance level embeddings. The work includes experiments with both automatically derived as well as knowledge-inspired features that capture spoken intent at various acoustic levels. Incorporation of utterance level embeddings at the text level using an off the shelf phone decoder has also been investigated. Our experiments impose constraints and manipulate the training procedure using heuristics from the data distribution. We conclude by presenting the preliminary results on the development and blind test sets.",0.0,4.15
939,1435,5    3.3    5    3.3    ,Cantonese Tones Produced Using Reverse Phonation,"Acoustical and perceptual characteristics associated with the six lexical tones of Cantonese produced using normal phonation has been studied quite extensively. However, little is known about how tones are produced using reverse phonation - an inward flow of air. In the present study, Cantonese lexical tones produced by native speakers using normal phonation (NP) and reverse phonation (RP) were examined acoustically and perceptually. Fundamental frequency (F0) values associated with the syllable /ji/ produced at six contrastive tones using NP and RP by forty native Cantonese speakers were calculated. In addition, perception of NP and RP lexical tones by ten naïve listeners of Cantonese were also investigated. Results showed a higher perceptual accuracy in the tones produced using NP than those produced using RP. Higher accuracy of tone identification was found for high rising tone (tone 2) and low rising tone (tone 5) when produced using RP. F0 contours revealed similar patterns between NP and RP rising tones, while NP and RP falling tones showed the most differences. The success in producing rising, level and falling tones using RP was likely to be related to the physiology of the vocal mechanism during RP.",1.0,4.15
545,2030,4.7    4.85    4.45    2.6    ,"A Study of Enhancement, Augmentation, and Autoencoder Methods for Domain Adaptation in Distant Speech Recognition","Speech recognizers trained on close-talking speech do not generalize to distant speech and the word error rate degradation can be as large as 40% absolute. Most studies focus on tackling distant speech recognition as a separate problem, leaving little effort to adapting close-talking speech recognizers to distant speech. In this work, we review several approaches from a domain adaptation perspective. These approaches, including speech enhancement, multi-condition training, data augmentation, and autoencoders, all involve a transformation of the data between domains. We conduct experiments on the AMI data set, where these approaches can be realized under the same controlled setting. These approaches lead to different amounts of improvement under their respective assumptions. The purpose of this paper is to quantify and characterize the performance gap between the two domains, setting up the basis for studying adaptation of speech recognizers from close-talking speech to distant speech. Our results also have implications for improving distant speech recognition.",0.0,4.15
30,92,4.8    2.8    3.9    5.1    ,Analysis of Length Normalization in End-to-End Speaker Verification System,"The classical i-vectors and the latest end-to-end deep speaker embeddings are the two representative categories of utterance-level representations in automatic speaker verification systems. Traditionally, once i-vectors or deep speaker embeddings are extracted, we rely on an extra length normalization step to normalize the representations into unit-length hyperspace before back-end modeling. In this paper, we explore how the neural network learns length-normalized deep speaker embeddings in an end-to-end manner. To this end, we add a length normalization layer followed by a scale layer before the output layer of the common classification network. We conducted experiments on the verification task of the Voxceleb1 dataset. The results show that integrating this simple step in the end-to-end training pipeline significantly boosts the performance of speaker verification. In the testing stage of our L2-normalized end-to-end system, a simple inner-product can achieve the state-of-the-art.",0.0,4.15
83,1086,3.25    4    5.1    4.25    ,Extending Recurrent Neural Aligner for Streaming End-to-End Speech Recognition in Mandarin,"End-to-end models have been showing superiority in Automatic Speech Recognition (ASR). At the same time, the capacity of streaming recognition has become a growing requirement for end-to-end models. Following these trends, an encoder-decoder recurrent neural network called Recurrent Neural Aligner (RNA) has been freshly proposed and shown its competitiveness on two English ASR tasks. However, it is not clear if RNA can be further improved and applied to other spoken language. In this work, we explore the applicability of RNA in Mandarin Chinese and present four effective extensions: In the encoder, we redesign the temporal down-sampling and introduce a powerful convolutional structure. In the decoder, we utilize a regularizer to smooth the output distribution and conduct joint training with a language model. On two Mandarin Chinese conversational telephone speech recognition (MTS) datasets, our Extended-RNA obtains promising performance. Particularly, it achieves 27.7% character error rate (CER), which is superior to current state-of-the-art result on the popular HKUST task.",0.0,4.15
1320,2571,4.95    4.35    3.15    ,Attention Based Fully Convolutional Network for Speech Emotion Recognition,"Speech emotion recognition is a challenging task for three main reasons: 1) human emotion is abstract, which means it is hard to distinguish; 2) in general, human emotion can only be detected in some specific moments during a long utterance; 3) speech data with emotional labeling is usually limited. In this paper, we present a novel attention based fully convolutional network for speech emotion recognition. We employ fully convolutional network as it is able to handle variable-length speech, free of the demand of segmentation to keep critical information not lost. The proposed attention mechanism can make our model be aware of which time-frequency region of speech spectrogram is more emotion-relevant. Considering limited data, the transfer learning is also adapted to improve the accuracy. Especially, it's interesting to observe obvious improvement obtained with natural scene image based pre-trained model. Validated on the publicly available IEMOCAP corpus, the proposed model outperformed the state-of-the-art methods with a weighted accuracy of 70.4% and an unweighted accuracy of 63.9% respectively.",1.0,4.15
872,1249,6    3.5    3.8    3.3    ,Speech Emotion Recognition via Siamese Architecture,"Speech emotion recognition is an important aspect of human-computer interaction. Prior work proposed various models and audio features to improve the performance of speech emotion classifiers. However, they ignore the in-class distance and the between-class distance. In this paper, we introduce a new loss function – the contrastive loss function, which is combined with the siamese architecture for emotion recognition. By optimizing with this loss function in the training process, we can decrease the distance of positive pairs and increase the distance of negative pairs. Moreover, various processing methods are tested, such as feature selection methods and pairwise sample selection methods. To verify the performance of the proposed system, we conduct experiments on The Interactive Emotional Dyadic Motion Capture (IEMOCAP) database – a common evaluation corpus. Experimental results reveal the advantages of the proposed method, which reaches 62.19% in the weighted accuracy and 63.21% in the unweighted accuracy. It outperforms the baseline system that is optimized without the contrastive loss function with 1.14% and 2.55% in the weighted accuracy and the unweighted accuracy, respectively.",1.0,4.15
1269,2351,4.35    3.65    5.15    3.45    ,Deep Neural Network Based Mizo Tone Recognition Using Acoustic Features,"Mizo is an under-studied Tibeto-Burman tonal language of the
North-East India. Preliminary research findings have confirmed
that along with four distinct tones of Mizo (High, Low, Rising
and Falling), a tone sandhi also appears in the language result-
ing from a Rising tone, preceding a High or a Falling tone. In
this work, an attempt is made to automatically recognize four
phonological tones in Mizo and a tone sandhi distinctively using
acoustic parameters as features. F0 variation, slope, height are
considered as features and a Deep Neural Network based classi-
fier is implemented for automatic tone recognition. A database
consisting of 37, 982 iterations of the four Mizo tones along
with the sandhi tone, collected from 18 speakers, spoken in tri-
syllabic phrases, was used in this work. A five-way classifica-
tion of tones was attempted with balanced (equal number of iter-
ations per tone category) and unbalanced (unequal number of it-
erations per tone category) datasets for each tone category. The
results showed best performance on the balanced dataset with
an accuracy of 81.12% in correctly recognizing four phonolog-
ical and one sandhi derived tone.",1.0,4.15
763,74,3.5    5.15    2.85    5.1    ,Bag-of-Audio-Word Feature Representation of ASR DNN Posteriors for General Paralinguistic Classification,"Bag-of-Audio-Word (or BoAW) representation is an utterance-level feature representation approach that was successfully applied in the past in various computational paralinguistic tasks. Here, we extend the BoAW feature extraction process with the use of Deep Neural Networks: first we train a DNN acoustic model on an acoustic dataset consisting of 22 hours of speech for phoneme identification, then we evaluate this DNN on a standard paralinguistic dataset. To construct utterance-level features from the frame-level posterior vectors, we calculate their BoAW representation. We found that this approach can be utilized even on its own, although the results obtained lag behind those of the standard paralinguistic approach, and the optimal size of the extracted feature vectors tends to be large. Our approach, however, can be easily and efficiently combined with the standard paralinguistic one, resulting in significant improvements in the Unweighted Average Recall (UAR) scores for a general paralinguistic dataset.",1.0,4.15
1151,1985,2.25    4.35    4.9    5.15    ,How Deep Neural Networks Treat Non-modal and Pathological Speech,"Deep neural networks are increasingly more popular for speech processing in medical applications, but their full use is hampered by a lack of data. One solution to the problem is to train a system on normal speech, apply it on disordered speech and look for differences. This article presents a study on using a DNN-based speech recognizer in the tasks of voice modality classification and voice pathology detection. The principal idea was to employ the outputs of neurons from hidden layers as features. Our hypothesis was that a discriminative model trained on normal speech will treat non-modal and pathological speech as unseen data. It was also expected the model would normalize out the paralynguistic information, as it was originally trained for recognizing speech content. The first task was to classify modal, breathy, strained and rough speech. Outputs from the first hidden layer yielded the best results as each subsequent layer normalized out the perceived voice quality. The utterance-level accuracy reached 75.2\%. The second task was to evaluate the suitability using a speech recognizer for assessing the presence and severity of voice disorders. The experiment proved that improving one's voice status by a medical treatment reduces the word error rate significantly.",1.0,4.1625
321,1528,4.15    3.9    4.4    4.2    ,High-quality Voice Conversion Using Spectrogram-Based WaveNet Vocoder,"Waveform generator is a key component in voice conversion. Recently, WaveNet waveform generator conditioned on the Mel-cepstrum (Mcep) has shown better quality over standard vocoder. In this paper, an enhanced WaveNet model based on spectrogram is proposed to further improve voice conversion performance. Here, Mel-frequency spectrogram is converted from source speaker to target speaker using an LSTM-RNN based frame-to-frame feature mapping. To evaluate the performance, the proposed approach is compared to an Mcep based LSTM-RNN voice conversion system. Both STRAIGHT vocoder and Mcep-based WaveNet vocoder are elected to produce the converted speech for Mcep conversion system. The fundamental frequency (F0) of the converted speech in different systems is analyzed. The naturalness, similarity and intelligibility are evaluated in subjective measures. Results show that the spectrogram based WaveNet waveform generator can achieve better voice conversion quality compared to traditional WaveNet approaches. The Mel-spectrogram based voice conversion can achieve significant improvement in speaker similarity and inherent F0 conversion.",0.0,4.1625000000000005
1231,2214,5.2    3.45    4.65    3.35    ,Automatic Diagnosis of Alzheimer's Disease Using Neural Network Language Models,"In today's aging society, the number of neurodegenerative diseases such as Alzheimer's disease (AD) increases. Since currently no cure exists, reliable tools for automatic diagnosis and monitoring of AD are necessary. For that, language is an indicator, which is useful and reasonable. We present a way to improve the method introduced by Wankerl et. al.
[1]. The purely statistical approach of n-gram language models(LMs) is enhanced by using the rwthlm toolkit to create neural network language models (NNLMs) with Long Short
Term-Memory (LSTM) cells. The prediction is solely based on evaluating the perplexity of transliterations of descriptions of the Cookie Theft picture from DementiaBank's Pitt Corpus. Each transliterations is evaluated on Language models (LM) of both control and Alzheimer speakers in a leave-one-speaker-out cross-validation scheme. The resulting perplexity values reveal enough discrepancy to classify patients on just those two values
with an accuracy of 85.6% at Equal-Error-Rate compared to 77.1% when using trigram LMs. Further, this approach yieldsa higher correlation between the calculated perplexities and the AD test subject's MMSE scores.",1.0,4.1625000000000005
483,1878,2.4    5    5.1    4.2    ,Automatic Detection of Multi-speaker Fragments with High Time Resolution,"Interruptions and simultaneous talking represent important patterns of speech behavior. However, there is a lack of approaches to their automatic detection in continuous audio data. We have developed a solution for automatic labeling of multi-speaker fragments using harmonic traces analysis. Since harmonic traces in multi-speaker intervals form an irregular pattern as opposed to the structured pattern typical for a single speaker, we resorted to computer vision methods to detect multi-speaker fragments.

A convolutional neural network was trained on synthetic material to differentiate between single-speaker and multi-speaker fragments. For evaluation of the proposed method the SSPNet Conflict Corpus with provided manual diarization was used. We also examined factors affecting algorithm performance.

The main advantages of the proposed method are calculation simplicity and high time resolution.
With our approach it is possible to detect segments with minimum duration of 0.5 seconds. The proposed method demonstrates highly accurate results and may be used for speech segmentation, speaker tracking, content analysis such as conflict detection, and other practical purposes.",0.0,4.175
1279,2386,5.1    4.15    4.2    3.25    ,Towards Disease-specific Speech Markers for Differential Diagnosis in Parkinsonism,"Parkinsonism refers to Parkinson’s Disease (PD) and Atypical Parkinsonian Syndromes (APS), such as Progressive Supranuclear Palsy (PSP) and Multiple System Atrophy (MSA).
Discrimination between PD and APS and within APS groups in early disease stages is a very challenging task. Interestingly, speech disorder is frequently an early and prominent clinical
feature of both PD and APS. This renders speech/voice analysis a promising tool for the development of an objective marker to assist neurologists in their diagnosis.
This paper is a continuation of a recent work on speech-based differential diagnosis within APS.
We address the difficult problem of defining disease-specific speech features
which is crucial in the perspective of early differential diagnosis. We investigate this problem
by considering the constraint that only a small amount of training data can be available in
this setting. To do so, we perform univariate statistical analysis followed by a supervised learning  that forces the designed new features to be 1-dimensional.
We show that linear classification models allow the definition of new scalar variables which can be considered as speech features which are specific to each disease, MSA and PSP. We support our results by running experiments in the task of discrimination between MSA and PSP.",1.0,4.175
500,1920,3.55    2.9    5.1    5.15    ,On Training and Evaluation of Grapheme-to-Phoneme Mappings with Limited Data,"When scaling to low resource languages for speech synthesis or speech recognition in an industrial setting, a common challenge is the absence of a readily available pronunciation lexicon. Common alternatives are handwritten letter-to-sound rules and data-driven grapheme-to-phoneme (G2P) models, but without a pronunciation lexicon it is hard to even determine their quality. We identify properties of a good quality metric and note drawbacks of naive estimates of G2P quality in the domain of small test sets. We demonstrate a novel method for reliable evaluation of G2P accuracy with minimal human effort. We also compare behavior of known state-of-the-art approaches for training with limited data. Finally we evaluate a new active learning approach for training G2P models in the low resource setting.",0.0,4.175
624,2263,4.85    2.8    4.1    4.95    ,Lexical and Acoustic Deep Learning Model for Personality Recognition,"Deep learning has been very successful on labeling tasks such as image classification and neural network modeling, but there has not yet been much work on using deep learning for automatic personality recognition. In this study, we propose two deep learning structures for the task of personality recognition using acoustic-prosodic, psycholinguistic, and lexical features, and present empirical results of several experimental configurations, including a cross-corpus condition to evaluate robustness. Our best models match or outperform state-of-the-art on the well-known myPersonality corpus, and also set a new state-of-the-art performance on the more difficult CXD corpus.",0.0,4.175
293,1471,4.6    5.15    2.5    4.45    ,Testing Paradigms for Assistive Hearing Devices in Diverse Acoustic Environments,"Hearing-aids and cochlear-implants are a common choice for the restoration and rehabilitation of the auditory function. Audio sound processors in CIs and HAs operate within the prescribed limits that are assigned by an audiologist. Operation within these prescribed limits is critical not only for acceptable sound perception but also for safety reasons. Signal processing research engineers, therefore, need to follow best design practices to ensure reliable performance and incorporate necessary safety checks in the design which ensure safety limits are never exceeded irrespective of the acoustic environments and design space. This paper proposes a comprehensive testing and evaluation paradigm to investigate the behavior of audio devices that specifically address the safety concerns in diverse acoustical conditions. This is achieved by characterizing the performance of devices with large amounts of acoustic inputs and monitoring the output behavior. As an example, we have used, CCi-Mobile research interface (used for CI and HA research) to demonstrate the testing/assessment paradigm. Factors such as pulse width are estimated to evaluate the impact of AHDs on comfort and sound quality. The practices described in this paper could potentially serve as a blueprint to characterize audio devices in terms of listening perception and biological safety.",0.0,4.175
315,1519,4.3    3.5    2.9    6    ,Feature Representation of Short Utterances Based on Knowledge Distillation for Spoken Language Identification,"The performance of spoken language identification (LID) on short utterances is drastically degraded even though model is completely trained on short utterance data set. The degradation is because of the large pattern confusion caused by the large variation of feature representation on short utterances. In this paper, we propose a teacher-student network learning algorithm to explore discriminative features for short utterances. With the teacher-student network learning, the feature representation for short utterances (explored by the student network) are normalized to their representations corresponding to long utterances (provided by the teacher network). With this learning algorithm, the feature representation on short utterances is supposed to reduce pattern confusion. Experiments on a 10-language LID task were carried out to test the algorithm. Our results showed the proposed algorithm significantly improved the performance.",0.0,4.175
908,1356,4.2    2.8    4.7    5    ,First-pass Techniques for Very Large Vocabulary Speech Recognition of Morphologically Rich Languages,"In many automatic speech recognition applications, it is essential to recognize the speech in a single decoding pass.
A single left-to-right decoding pass avoids the situation, where the text output has to wait for the recording and the first pass of input sentence to finish.
In this paper we target morphologically rich languages where the language model accuracy is severely limited by the data sparsity and out-of-vocabulary words.
For a vocabulary of millions of words, an effective application of word classes and subword units is needed.
We survey efficient graph construction and decoding architectures.
We then present novel techniques for language model look-ahead and model interpolation between word and subword units.
The methods improve the accuracy of very large-vocabulary first-pass decoding while allowing real-time recognition.",1.0,4.175
13,52,4.25    3.95    5.9    2.6    ,Acoustic and Textual Data Augmentation for Improved ASR of Code-Switching Speech,"In this paper, we describe several techniques for improving the acoustic and language model of an automatic speech recognition (ASR) system operating on code-switching (CS) speech. We focus on the recognition of Frisian-Dutch radio broadcasts where one of the mixed languages, namely Frisian, is an under-resourced language. In previous work, we have proposed several automatic transcription strategies for CS speech to increase the amount of available training speech data. In this work, we explore how the acoustic modeling (AM) can benefit from monolingual speech data belonging to the high-resourced mixed language. For this purpose, we train state-of-the-art AMs, which were ineffective due to lack of training data, on a significantly increased amount of CS speech and monolingual Dutch speech. Moreover, we improve the language model (LM) by creating code-switching text, which is in practice almost non-existent, by (1) generating text using recurrent LMs trained on the transcriptions of the training CS speech data, (2) adding the transcriptions of the automatically transcribed CS speech data and (3) translating Dutch text extracted from the transcriptions of a large Dutch speech corpora. We report significantly improved CS ASR performance due to the increase in the acoustic and textual training data.",0.0,4.175
392,1680,2.25    4.15    5.2    5.1    ,I-vector Transformation Using Conditional Generative Adversarial Networks for Short Utterance Speaker Verification,"I-vector based text-independent speaker verification (SV) systems often have poor performance with short utterances, as the biased phonetic distribution in a short utterance makes the extracted i-vector unreliable. This paper proposes an i-vector compensation method using a generative adversarial network (GAN), where its generator network is trained to generate a compensated i-vector from a short-utterance i-vector and its discriminator network is trained to determine whether an i-vector is generated by the generator or the one extracted from a long utterance. Additionally, we assign two other learning tasks to the GAN to stabilize its training and to make the generated i-vector more speaker-specific. Speaker verification experiments on the NIST SRE 2008 “10sec-10sec” condition show that
after applying our method, the equal error rate reduced by 11.3% from the conventional i-vector and PLDA system.",0.0,4.175000000000001
1191,2102,4.35    4.45    2.8    5.1    ,"Inter-speech Event Classification and Clustering: Clicks, Breaths, Silences.","This work reports on automatic classification of conversational events that occur in pause intervals between speech activity. The available classes are: audible breathing, oral clicks and silences. We implement a supervised algorithm (SVM) on labeled data of one speaker with 92 % testing accuracy on the same speaker. Additionally, we explore unsupervised methods such as DBSCAN with t-SNE-based dimensionality reduction on that speaker and a large conversational corpus. We provide visual representations of unsupervised clustering and statistics that perform best in distinguishing the classes.",1.0,4.175000000000001
920,1393,3.45    3.6    4.55    5.1    ,Automatic Syllable Segmentation by Classifying Peaks in Short-Term Energy Signal,"In this paper, we present a new method for segmenting syllables from continuous speech. Our approach is based on a relatively simple principle: vowel phonemes tend to have high energy. Therefore, we will learn a function that maps each peak in the short-term energy signal of a speech utterance onto either the vowel or consonant class. The features used for classification are derived from both the short-term energy signal and the spectrum of original speech signal. The identified vowel peaks will act as the nucleus of our syllable segments. Specifically, we draw segmentation boundaries between two vowel peaks, which are separated by at least 150 ms. We demonstrate the effectiveness of our method using nested cross validation on 25 unique test utterances from the TIMIT dataset, containing 351 syllables in total. Our simple method achieves better detection performance and a lower false alarm rate than current state-of-the-art approaches for syllable segmentation.",1.0,4.175000000000001
7,45,2.55    4.15    5.85    4.15    ,A Novel Normalization Method for Autocorrelation Function for Pitch Detection and for Speech Activity Detection,"Autocorrelation functions (ACF) have been used in various pitch detection algorithms (PDA) and voicing-feature based speech activity detection (SAD) techniques. Speech is assumed to be stationary over a short-term window, and a Hanning window is typically applied in the calculation of ACF. As a result of windowing, the ACF tapers as the autocorrelation lags increase. Boersma demonstrated that the tapering effect could be compensated for by dividing the ACF of the windowed signal by the autocorrelation of the windowing function itself, referred to as wACF hereafter. We recently found that wACF could cause overcompensation and therefore, result in errors in pitch detection. In this paper, a novel normalization method, eACF, is proposed that can both mitigate the tapering effect and minimize the overcompensation. The new method is evaluated on synthetic speech and on the TIMIT database with various types of additive noise at different signal-to-noise (SNR) ratios. The results show that the new method leads to better performance both in terms of pitch detection and speech activity detection. In this paper, we also investigate the scenarios where applying the wACF method is advantageous and where it is not.",0.0,4.175000000000001
68,1057,4.25    4.15    4.15    4.15    ,Structured Word Embedding for Low Memory Neural Network Language Model,"Neural network language model (NN LM), such as long short term memory (LSTM) LM, has been increasingly popular due to its promising performance. However, the model size of an uncompressed NN LM is still too large to be used in embedded or portable devices. The dominant part of memory consumption of NN LM is the word embedding matrix. Directly compressing the word embedding matrix usually leads to performance degradation. In this paper, a product quantization based structured embedding approach is proposed to significantly reduce memory consumption of word embeddings without hurting LM performance. Here, each word embedding vector is cut into partial embedding vectors which are then quantized separately. Word embedding matrix can then be represented by an index vector and a code-book tensor of the quantized partial embedding vectors. Experiments show that the proposed approach can achieve 10 to 20 times embedding parameter reduction rate with negligible performance loss.",0.0,4.175000000000001
610,2213,3.6    4.3    4.65    ,Artificial Bandwidth Extension with Memory Inclusion Using Semi-supervised Stacked Auto-encoders,"Artificial bandwidth extension (ABE) algorithms have been
developed to improve quality when wideband devices receive
speech signals from narrowband devices or infrastructure. The
utilisation of contextual information in the form of dynamic features or explicit memory captured from neighbouring frames is
common to ABE research, however the use of additional cues
augments complexity and can introduce latency. Previous work
shows that unsupervised, linear dimensionality reduction techniques help to reduce complexity. This paper reports a semisupervised, non-linear approach to dimensionality reduction using a stacked auto-encoder. In further contrast to previous
work, it operates on raw spectra from which a low dimensional
narrowband representation is learned in a data-driven manner.
Three different objective speech quality measures show that the
new features can be used with a standard regression model to
improve ABE performance. Improvements in the mutual information between learned features and missing higher frequency
components are also observed whereas improvements in speech
quality are corroborated by informal listening tests.",0.0,4.183333333333334
373,1644,3.3    5.8    4.3    3.35    ,Automatic Miscue Detection Using RNN Based Models with Data Augmentation,"This study proposes a method of using data augmentation to address the problem of data shortages in miscue detection tasks. Three main steps were taken. First, a phoneme classifier was developed to acquire force-aligned data, which would be used for miscue classification and data augmentation. In order to create the phoneme classifier, phonetic features of “Seoul Reading Speech” (SRS) corpus were extracted by using grapheme-to-phoneme (G2P) to train CNN-based models. Second, to obtain miscue labeled corpus, we performed data augmentation using the phoneme classifier output, which is artificially generated miscue corpus of SRS (modified-SRS). This miscue corpus was created by randomly deleting or modifying sound sections according to three miscue categories; extension (EXT), pause (PAU), and pre-correction (PRE). Third, the performance of the miscue classifier was tested after training three types of RNN based models (LSTM, BiLSTM, BiGRU) with the modified-SRS corpus. The results show that the BiGRU model performed best at 0.819 in F1-score on augmented data, while BiLSTM model performed best at 0.512 on real data.",0.0,4.1875
1112,1885,4.25    4.25    4.25    4    ,Acoustic Correlates of Aspiration in Fricatives and Nasals,"This paper focuses on the phonetic analysis of Korean and Rabha fricatives and Angami nasals. Though frication and aspiration in aspirated consonants have been studied, very few literature were found for the comparative study of aspirated fricatives and aspirated nasals. Previous literature has suggested the presence of aspirated fricatives. As there are limited studies on aspirated nasals, this paper tries to investigate the properties of aspirated nasals by comparing it with the aspiration in Korean aspirated fricative and analyses the feature that might distinguish between the aspirated and unaspirated counterparts of both the consonants. The features such as Intensity, Duration, Centre of Gravity (COG), F1 onset and Spectral tilt(H1-H2) are used to investigate whether there is a distinction between the aspirated and unaspirated fricatives and nasals. Results confirm that COG is a distinctive acoustic cue to discriminate the aspirated and unaspirated counterparts.",1.0,4.1875
188,1271,4.9    2.55    4.25    5.05    ,Phoneme Resistance and Phoneme Confusion in Noise: Impact of Dyslexia,"Understanding speech in noisy environments is a challenge for almost everyone and particularly so for people with dyslexia. To better understand the phonological processing deficit, which has been posited as a core trait of dyslexia, we wanted to further characterize the impact of noise on speech perception. In this paper we investigated phoneme resistance to noise for dyslexic and control adults and explored the pattern of errors produced by noise interference. Our aim was to examine differences between phoneme confusion matrices of the two populations. 
Disyllabic nouns were embedded in noise and participants had to perform an auditory word identification task. Error rates, phoneme resistance and phoneme confusions were compared between a dyslexic and a group of matched controls. 
Error rate was higher in the dyslexic group. However, no qualitative differences in the profile of errors were found. The coronals /ʃ and s/ were the most resistant phoneme in both groups while the labials /f, m and v/ were the most vulnerable. Although dyslexics showed a more scattered pattern of confusions, the matrices were correlated. Our results confirm a phonological deficit in dyslexia whereas they do not support the hypothesis of qualitative differences in phonological representation between the two groups.",0.0,4.1875
425,1753,5.15    3.95    5.2    2.45    ,LSTBM: a Novel Sequence Representation of Speech Spectra Using Restricted Boltzmann Machine with Long Short-Term Memory,"In this paper, we propose a novel probabilistic model, namely long short-term Boltzmann memory (LSTBM), to represent sequential data like speech spectra. The LSTBM is an extension of a restricted Boltzmann machine (RBM) that has generative long short-term memory (LSTM) units. The original RBM automatically learns relationships between visible and hidden units and is widely used as a feature extractor, a generator, a classifier, a pre-training method of deep neural networks, etc. However, the RBM is not sufficient to represent sequential data because it assumes that each frame from sequential data is completely independent of the others. Unlike conventional RBMs, the LSTBM has connections over time via LSTM units and represents time dependencies in sequential data. Our speech coding experiments demonstrated that the proposed LSTBM outperformed the other conventional methods: an RBM and a temporal RBM.",0.0,4.1875
25,78,4.35    2.55    4.25    5.6    ,Active Memory Networks for Language Modeling,"Making predictions of the following word given the back history of words may be challenging without meta-information such as the topic. Standard neural network language models have an implicit representation of the topic via the back history of words. In this work a more explicit form of topic representation is used via an attention mechanism. Though this makes use of the same information as the standard model, it allows parameters of the network to focus on different aspects of the task. The attention model provides a form of topic representation that is automatically learned from the data. Whereas the recurrent model deals with the (conditional) history representation. The combined model is expected to reduce the stress on the standard model to handle multiple aspects. Experiments were conducted on the Penn Tree Bank and BBC Multi-Genre Broadcast News (MGB) corpora, where the proposed approach outperforms standard forms of recurrent models in perplexity. Finally, N-best list rescoring for speech recognition in the MGB3 task shows word error rate improvements over comparable standard form of recurrent models.",0.0,4.1875
29,91,3.3    4.25    5.15    4.05    ,Emotional Prosody Perception in Mandarin-speaking Congenital Amusics,"Congenital amusia, which is a neurogenetic disorder affecting musical pitch processing, was found recently to affect not only human speech perception, but also emotional perception. Since previous studies only examined participants with non-tonal languages, they cannot easily generalize the finding to people with tonal language background, due to the fact that those people utilize pitch cues much more heavily in daily communication compared with others. To make clear the doubt, this paper investigates emotional prosody perception of Mandarin speakers with congenital amusia. We tried to recruit 19 amusics and matched control group of similar number of normal speakers, and carried out emotional perception experiments in which speech and non-speech stimuli with six kinds of emotions were used, including happy, sad, fear, angry, surprise, and neutral. Results showed that the amusics performed significantly worse than matched controls. This indicated that tone-language expertise cannot compensate for pitch deficits in amusia for emotional perception. Further analyses demonstrated that there was a positive correlation between emotion prosody performance and pitch perceptional ability. These findings further support previous hypothesis that music and language share cognitive and neural resources, and provide a new perspective on the proposition of the relation between music and language.",0.0,4.1875
623,2261,5.05    3.15    3.55    5    ,Self-Assessed Affect Recognition Using Fusion of Attentional BLSTM and Static Acoustic Features,"In this study, we present a computational framework to participate in the Self-Assessed Affect Sub-Challenge in the INTERSPEECH 2018 Computation Paralinguistics Challenge. The goal of this sub-challenge is to classify the valence scores given by the speaker themselves into three different levels, i.e., low, medium, and high. We explore fusion of Bi-directional LSTM with baseline SVM models to improve the recognition accuracy. In specifics, we extract frame-level acoustic LLDs as input to the BLSTM with a modified attention mechanism, and separate SVMs are trained using the standard ComParE_16 baseline feature sets with minority class upsampling. These diverse prediction results are then further fused using a decision-level score fusion scheme to integrate all of the developed models. Our proposed approach achieves a 62.94% and 67.04% unweighted average recall (UAR), which is an 6.24% and 1.04% absolute improvement over the best baseline provided by the challenge organizer. We further provide a detailed comparison analysis between different models.",0.0,4.1875
394,1687,3.45    4.1    4.15    5.05    ,Novel Variable Length Energy Separation Algorithm Using Instantaneous Amplitude Features for Replay Detection,"Voice-based speaker authentication or Automatic Speaker Verification
(ASV) system is now becoming practical reality after
several decades of research. However, still this technology
is very much susceptible to various spoofing attacks. Among
various spoofing attacks, replay is the most challenging attack.
In this paper, we propose a novel feature set based on our recently
introduced Variable length Energy Separation Algorithm
(VESA) during INTERSPEECH 2017. The key idea of this
paper is to capture the Instantaneous Amplitude (IA) obtained
from the instantaneous energy fluctuations. The replay speech
is affected by acoustic environment and distortions of intermediate
device. Thus, the noise added in replayed speech is important
to detect. The Amplitude Modulations (AM) are more
susceptible to noise and multipath interferences that may result
due to replay mechanism. The experiments are performed
on various dependency index (DI) and lower EER of 6.12 %
and 11.94 % is found on dev and eval set, respectively, of ASV
Spoof 2017 Challenge database. Furthermore, we compare our
results with CQCC, LFCC, MFCC, and VESA-IFCC feature
sets. The score-level fusion VESA-IFCC and proposed feature
set further reduced the EER to 0.19 % and 7.11 % on dev and
eval set, respectively.",0.0,4.1875
875,1257,4.3    3.35    5.1    4.2    4    ,Calibrated Prediction Intervals for Neural Network Regressors,"Ongoing developments in neural network models are continually advancing the state-of-the-art in terms of system accuracy. However, the predicted labels should not be regarded as the only core output; also important is a well calibrated estimate of the prediction uncertainty. Such estimates and their calibration is critical in relation to robust handling of out of distribution events not observed in training data. Despite their obvious aforementioned advantage in relation to accuracy, contemporary neural networks can, generally, be regarded as poorly calibrated and as such do not produce reliable output probability estimates. Further, while post-processing calibration solutions can be found in the relevant literature, these tend to be for systems performing classification. In this regard, we herein present a method for acquiring calibrated predictions intervals for neural network regressors by posing the regression task as a multi-class classification problem and applying one of three proposed calibration methods on the classifiers' output. Testing our method on two exemplar tasks -- speaker age prediction and signal-to-noise ratio estimation -- indicates the suitability of the classification-based regression models and that post-processing by our proposed Empirical Calibration or Temperature Scaling methods yields well calibrated prediction intervals.",1.0,4.1899999999999995
174,1250,5.05    2.45    5.2    4.1    ,Data Independent Sequence Augmentation Method for Acoustic Scene Classification,"Augmenting datasets by transforming inputs in a way such as vocal tract length perturbation (VTLP) is a crucial ingredient of the state of the art methods for speech recognition tasks. In contrast to speech, sounds coming from realistic environments have no speaker to speaker variations. Thus VTLP is invalid for acoustic scene classification tasks. This paper investigates a novel sequence augmentation method for long short-term memory (LSTM) acoustic modeling to deal with data sparsity in acoustic scene classification tasks. The audio sequences are randomly rearranged and concatenated during training, but at test time, a prediction is made by the original audio sequence. The rearrangement is well-designed to adapt to the long short-term dependency in LSTM models. Experiments on acoustic scene classification task show performance improvements of the proposed methods. The classification errors in LITIS ROUEN dataset and DCASE2016 dataset are reduced by 18.1% and 6.4% relatively.",0.0,4.199999999999999
458,1821,3.75    3.3    4.9    4.85    ,Early Detection of Continuous and Partial Audio Events Using CNN,"Sound event detection is an extension of the static auditory classification task into continuous environments, where performance depends jointly upon the detection of overlapping events and their correct classification.
Several approaches have been published to date which either develop novel classifiers or employ well-trained static classifiers with a detection front-end. 
This paper takes the latter approach, by combining a proven CNN classifier acting on spectrogram image features, with time-frequency shaped energy detection that identifies seed regions within the spectrogram that are characteristic of auditory energy events. Furthermore, the shape detector is optimised to allow early detection of events as they are developing. Since some sound events naturally have longer durations than others, waiting until completion of entire events before classification may not be practical in a deployed system. The early detection capability of the system is thus evaluated for the classification of partial events.
Performance for continuous event detection is shown to be good, with accuracy being maintained well when detecting partial events.",0.0,4.199999999999999
1079,1794,5.1    5.55    4.15    2    ,"""Hidden Camera"" Scenario for Measuring Socio-Affective Bias on Acoustic Spatial Perception","When using a telepresence robot, some acoustic artefacts emerge in the signals, due to the features of the acoustic channel and the necessity to transmit the voice signal, so that it is adapted to the change of context. In particular, the volume of the voice must be regulated, so the voice remains intelligible in the remote environment and socially coherent with the speaker’s intention. However, the impact of these artefacts on the interlocutors is not well known. It would be therefore interesting to control not only voice intensity, but the ""social value"" of the vocal signal, so the eventual artefacts of telecommunication could be counterbalanced. To this end, we propose an experiment for measuring, analyzing and modeling how human beings perceive their interlocutor spatially, according to socio-affective variations in the vocal productions of this interlocutor. The used methodology is based on a ""hidden camera"" scenario to achieve ecological measures of human acoustic perception. First results show already correlations between the behavior of the interlocutor and the perception of the subjects.",1.0,4.199999999999999
445,1797,4.1    4.2    4.3    ,Hierarchical Recurrent Neural Networks for Acoustic Modeling,"Recurrent neural network (RNN)-based acoustic models are widely used in speech recognition, and end-to-end training with CTC (connectionist temporal classification) shows good performance. In order to improve the ability to keep temporarily distant information, we employ hierarchical recurrent neural networks (HRNNs) to the acoustic modeling in speech recognition. HRNN consists of multiple RNN layers that operate on different time-scales, and the frequency of operation at each layer is controlled by learned gates from training data. We employ gate activation regularization techniques to control the operation of the hierarchical layers. When tested with the WSJ eval92, our best model obtained the word error rate of 5.19% with beam search decoding using RNN based character-level language models. Compared to an LSTM based acoustic model with a similar parameter size, we achieved a relative word error rate improvement of 10.5%. Even though this model employs uni-directional RNN models, it showed the performance improvements over the previous bi-directional RNN based acoustic models.",0.0,4.2
408,1721,3.4    4.05    5.15    ,Weighting Time-Frequency Representation of Speech Using Auditory Saliency for Automatic Speech Recognition,"This paper proposes a new method for weighting two-dimensional (2D) time-frequency (T-F) representation of speech using auditory saliency for noise-robust automatic speech recognition (ASR). Auditory saliency is estimated via 2D auditory saliency maps which model the mechanism for allocating human auditory attention. These maps are used to weight T-F representation of speech, namely the 2D magnitude spectrum or spectrogram, prior to features extraction for ASR. Experiments on Aurora-4 corpus demonstrate the effectiveness of the proposed method for noise-robust ASR. In multi-stream ASR, relative word error rate (WER) reduction of up to 5.3% and 4.0% are observed when comparing the multi-stream system using the proposed method with the baseline single-stream system not using T-F representation weighting and that using conventional spectral masking noise-robust technique, respectively. Combining the multi-stream system using the proposed method and the single-stream system using the conventional spectral masking technique reduces further the WER.",0.0,4.2
236,1363,4.35    4.2    4.65    3.6    ,An Unsupervised Neural Prediction Framework for Learning Speaker Embeddings Using Recurrent Neural Networks,"This paper presents an unsupervised training framework for learning a speaker-specific embedding using a Neural Predictive Coding (NPC) technique. We employ a Recurrent Neural Network (RNN) trained on unlabeled audio with multiple and unknown speaker change points. We assume short-term speaker stationarity and hence that speech frames in close temporal proximity originated from a single speaker. In contrast, two random short speech segments from different audio streams are assumed to originate from two different speakers. Based on this hypothesis, a binary classification scenario of predicting whether an input pair of short speech segments comes from the same speaker or not, is developed. An RNN based deep siamese network is trained and the resulting embeddings, extracted from a hidden layer representation of the network, are employed as speaker embeddings. The experimental results on speaker change points detection show the efficacy of the proposed method to learn short-term speaker-specific features. We also show the consistency of these features via a simple statistics-based utterance-level speaker classification task. The proposed method outperforms the MFCC baseline for speaker change detection, and both MFCC and i-vector baselines for speaker classification.",0.0,4.2
870,1231,3.5    4.1    5    ,Voice Activity Detection Based on Time-Delay Neural Networks,"Voice activity detection (VAD) is an important preprocessing part of many speech applications. Context information is important for VAD. Time-delay neural networks (TDNNs) capture long context information with a few parameters. This paper investigates a TDNN based VAD framework. A simple chunk based decision method is proposed to smooth raw posteriors and decide border points of utterances. To evaluate decision performance, a metric intersection-over-union (IoU) is introduced from image object detection. The experiment results are evaluated on Wall Street Journal (WSJ0) corpus. Frame classification performance is measured by area under the curve (AUC) and equal error rate (EER). Compared with long short-term memory baseline, the TDNN based system achieves a 41.26% EER relative reduction on average in matched noise condition, and relative improvement of average AUC is 3.82%. Proposed decision method achieves an 18.74% IoU relative improvement on average compared with moving average method on average.",1.0,4.2
354,1598,3.75    4.85    3.2    5    ,Acoustic Modeling Using Adversarially Trained Variational Recurrent Neural Network for Speech Synthesis,"In this paper, we propose a variational recurrent neural network (VRNN) based method for modeling and generating speech parameter sequences. In recent years, the performance of speech synthesis systems has been improved over conventional techniques thanks to deep learning-based acoustic models. Among the popular deep learning techniques, recurrent neural networks (RNNs) has been successful in modeling time-dependent sequential data efficiently. However, due to the deterministic nature of RNNs prediction, such models do not reflect the full complexity of highly structured data, like natural speech. In this regard, we propose adversarially trained variational recurrent neural network (AdVRNN) which use VRNN to better represent the variability of natural speech for acoustic modeling in speech synthesis. Also, we apply adversarial learning scheme in training AdVRNN to overcome oversmoothing problem. We conducted comparative experiments for the proposed VRNN with the conventional gated recurrent unit which is one of RNNs, for speech synthesis system. It is shown that the proposed AdVRNN based method performed better than the conventional GRU technique.",0.0,4.2
597,2173,5    2.75    5    4.05    ,Subband Weighting for Binaural Speech Source Localization,"We consider the task of speech source localization from a binaural recording using interaural time difference (ITD). A typical approach is to process binaural speech using gammatone filters and calculate frame-level ITD in each subband. The ITDs in each gammatone subband are statistically modelled using Gaussian mixture models (GMMs) for every direction during training. Given a binaural test-speech, the source is localized using maximum likelihood (ML) criterion. In this work, we propose a subband weighting scheme where subband likelihoods are weighted based on their reliability. We measure the reliability of a subband using the average frame level localization error obtained for the respective subbands. These reliability values are used as the weights for each subband likelihood prior to combining the likelihoods for ML estimation. We also introduce non-linear warping of these weights to accommodate and analyse a larger space of possible subband weights. Experiments on Subject_003 from the CIPIC database reveal that weighting the subbands is better than the unweighted scheme of combining likelihoods.",0.0,4.2
11,50,3.15    3.6    5.75    3.5    5    ,Leveraging Mixture Structure from Multivariate Von Mises-Fisher Distributions for Informed Speaker Clustering for Naturalistic Audio Streams,"Speaker Diarization (i.e. determining who spoke and when?)
for multi-speaker naturalistic interactions such as Peer-Led
Team Learning (PLTL) sessions is a challenging task. In this
study, we propose robust speaker clustering based on mixture
of multivariate von Mises-Fisher distributions. Our diarization
pipeline has two stages: (i) ground-truth segmentation; (ii) proposed
speaker clustering. The ground-truth speech activity information
is used for extracting i-Vectors from each speechsegment.
We post-process the i-Vectors with principal component
analysis for dimension reduction followed by lengthnormalization.
Normalized i-Vectors are high-dimensional unit
vectors possessing discriminative directional characteristics.
We model the normalized i-Vectors with a mixture model consisting
of multivariate von Mises-Fisher distributions. K-means
clustering with cosine distance is chosen as baseline approach.
The evaluation data is derived from: (i) CRSS-PLTL corpus;
and (ii) three-meetings subset of AMI corpus. The CRSSPLTL
data contain audio recordings of PLTL sessions which
is student-led STEM education paradigm. Proposed approach
is consistently better than baseline leading to upto 44.48% and
53.68% relative improvements for PLTL and AMI corpus, respectively.",0.0,4.2
1055,1734,3.15    4.3    5.05    4.3    ,Novel MMSE-Wasserstein Generative Adversarial Network Using Vocal Tract Length Warped Posterior Features for Voice Conversion,"The ability of Generative Adversarial Network (GAN) in generating the realistic samples, resulted in a significant performance improvement in the area of Voice Conversion (VC). The GAN-based VC framework first finds the latent variable representation from the source speaker's training data and then employs GAN as a synthesizer to generate the target samples, closely following the target speech distribution. In this paper, we propose to use speaker-independent unsupervised Vocal Tract Length Normalization (VTLN) posteriorgram for the latent representation, over the Gaussian Mixture Model (GMM) posteriorgram (both evaluated in the GAN framework). The key limitation of the vanilla GAN-based system is in generating the samples that may not correspond to the given source speaker's utterance. To address this issue, we recently proposed to use Minimum Mean Squared Error as a regularizer to the vanilla GAN (MMSE-GAN), for speech enhancement task.  Motivated by this study, we propose to exploit the significance of MMSE regularizer in the Wasserstein GAN (MMSE-WGAN) in the VC task, due to its increased training stability. The experimental results on the Voice Conversion Challenge (VCC) 2018 database, clearly indicate the effectiveness of the proposed MMSE-WGAN using VTLN posteriorgram over the baseline MMSE-GAN.",1.0,4.2
1276,2376,5.9    2.5    3.5    4.9    ,Multi-Scale CNN Based Framework for Customer Satisfaction Prediction Using Text and Speech,"Measuring Customer Satisfaction is critical in determining customer
sentiment and the outcome of a call in a call center. Typically,
customer satisfaction is determined using a post-call survey
presented to the customer, which is of limited value given
that only a subset of customers actually completes this survey.
In this paper, we present a multi-scale Convolution Neural Network
(CNN)-based framework for automatically predicting the
overall Customer Satisfaction of calls in a call center. We will
extract two kinds of features for this task based on Automatic
Speech Recognition (ASR) transcripts and audio segmentation.
Firstly, we show the usefulness of CNN, operating on ASR transcripts,
for this task. Secondly, features extracted using audio
segmentation and speech turns, called as temporal features,
have proven to be useful for this task. We then show that model-based
fusion of these two features improves the accuracy by
9.26% absolute over text only system. We evaluated our systems
on the dataset consisting of US English telephone speech
from call-centers.",1.0,4.2
204,1299,3.5    3.3    5.8    4.2    ,A Compact and Discriminative Feature Based on Auditory Summary Statistics for Acoustic Scene Classification,"One of the biggest challenges of acoustic scene classification (ASC) is to find proper features to better represent and characterize environmental sounds. Environmental sounds generally involve more sound sources while exhibiting less structure in temporal spectral representations. However, the background of an acoustic scene exhibits temporal homogeneity in acoustic properties, suggesting it could be characterized by distribution statistics rather than temporal details. In this work, we investigated using auditory summary statistics as the feature for ASC tasks. The inspiration comes from a recent neuroscience study, which shows the human auditory system tends to perceive sound textures through time-averaged statistics. Based on these statistics, we further proposed to use linear discriminant analysis to eliminate redundancies among these statistics while keeping the discriminative information, providing an extreme com-pact representation for acoustic scenes. Experimental results show the outstanding performance of the proposed feature over the conventional handcrafted features.",0.0,4.2
345,1575,5.9    3.5    4.2    3.2    ,Korean Singing Voice Synthesis Based on LSTM Recurrent Neural Network,"Singing voice synthesis (SVS) systems generate the singing voice from a musical score. Similar to the text-to-speech synthesis (TTS) field, SVS systems have also been greatly improved since the deep neural network (DNN) framework was introduced. Although they share many parts of the framework, the main difference between TTS and SVS systems is that the feature composing method, between linguistic and musical features, is important for SVS systems. In this paper, we propose a Korean SVS system based on a long-short term memory recurrent neural network (LSTM-RNN). At the feature composing stage, we propose a novel composing method, based on Korean syllable structure. At the synthesis stage, we adopt LSTM-RNN for the SVS. According to our experiments, our composed feature improved the naturalness of the voice, specifically in any part that has to be pronounced for a long time. Furthermore, LSTM-RNN outperformed the DNN based SVS system in both quantitative and qualitative evaluations.",0.0,4.2
1243,2247,4.95    4.75    4.85    2.3    ,Far-field ASR Using Low-rank and Sparse Soft Targets from Parallel Data,"Far-field automatic speech recognition (ASR) of conversational speech is often considered to be a very challenging task due to the poor quality of alignments available for training the DNN acoustic models. A common way to aleviate this problem is to use clean alignments obtained from parallelly recorded close-talk speech data. In this work, we advance the parallel data approach by obtaining enhanced low-rank and sparse soft targets from a close-talk ASR system and using them for training more accurate far-field acoustic models. Specifically, we exploit \textit{eigenposteriors} and sparse dictionaries to learn low-dimensional senone subspaces in DNN posterior space, and enhance close-talk DNN posteriors to achieve high quality soft targets for training far-field DNN acoustic models. Enhanced soft targets encode the structural and temporal inter-relationships among senone classes and improve the mapping of far-field acoustics to close-talk senone classes. The experiments are performed on AMI meeting corpus where our approach achieves 4.4% absolute (8% rel.) WER reduction as compared to a system which doesn't use parallel data, and 2.1% absolute (4% rel.) WER reduction as compared to system which uses close-talk hard alignments.",1.0,4.2124999999999995
1232,2216,4.9    4.85    3.7    3.4    ,Predicting Group Satisfaction in Meeting Discussions,"We address the task of automatically predicting group satisfaction in meetings using acoustic, lexical, and turn-taking features. Participant satisfaction is measured using individual post-meeting questionnaires from the AMI corpus. We focus on predicting two aspects of satisfaction: overall participant satisfaction with the meeting, and more specifically whether participants felt that everyone received sufficient attention during the meeting. All predictions are made at the aggregated group level. In general,  we find that combining features across modalities improves prediction performance.  In particular, acoustic and lexical models benefit from the addition of  turn-taking features.  Our experiments also  show how data-driven methods can be used to explore how different facets of group satisfaction are expressed through different modalities. For example, inclusion of prosodic features improves prediction of feeling sufficient attention but hinders prediction of overall satisfaction, and vice-versa for lexical features.   Moreover, feelings of sufficient attention were better reflected by acoustic features  than by speaking time.  Overall, this study indicates that group dynamics can be revealed as much by how participants speak, as by what they say.",1.0,4.2124999999999995
803,1069,4.85    3.45    3.4    5.15    ,Restricted Boltzmann Machine Vectors for Speaker Clustering,"Restricted Boltzmann Machines (RBMs) have been used both in the front-end and backend of speaker verification systems. In this work, we apply RBMs as a front-end in the context of speaker clustering. Speakers' utterances are transformed into a vector representation by means of RBMs. These vectors, referred to as RBM vectors, have shown to preserve speaker-specific information and are used for the task of speaker clustering. In this work, we perform the traditional bottom-up Agglomerative Hierarchical Clustering (AHC). Using the RBM vector representation of speakers, the performance of speaker clustering is improved. The evaluation has been performed on the audio recordings of Catalan TV Broadcast shows. The experimental results show that our proposed system outperforms the baseline i-vectors system in terms of Equal Impurity (EI). Using cosine scoring, a relative improvement of 15% and 19% are achieved for average and single linkage clustering algorithms respectively. Using PLDA scoring, the RBM vectors achieve a relative improvement of 12% compared to i-vectors for the single linkage algorithm.",1.0,4.2125
266,1419,3.2    6    3.3    4.35    ,A Discriminative Acoustic-Prosodic Approach for Measuring Local Entrainment,"Acoustic-prosodic entrainment describes the tendency of humans to align or adapt their speech acoustics to each other in conversation. This alignment of spoken behavior has important implications for conversational success. However, modeling the subtle nature of entrainment in spoken dialogue continues to pose a challenge. In this paper, we propose a straightforward definition for local entrainment in the speech domain and operationalize an algorithm based on this: acoustic-prosodic features that capture entrainment should be maximally different between real conversations involving two partners and sham conversations generated by randomly mixing the speaking turns from the original two conversational partners. We propose an approach for measuring local entrainment that quantifies alignment of behavior on a turn-by-turn basis, projecting the differences between interlocutors' acoustic-prosodic features for a given turn onto a discriminative feature subspace that maximizes the difference between real and sham conversations. We evaluate the method using the derived features to drive a classifier aiming to predict an objective measure of conversational success (i.e., low versus high), on a corpus of task-oriented conversations. The proposed entrainment approach achieves 72% classification accuracy using a Naive Bayes classifier, outperforming three previously established approaches evaluated on the same conversational corpus.",0.0,4.2125
857,1197,4.2    3.15    4.9    4.6    ,Syllable-based End-to-end Text-to-speech Synthesis for Mandarin Chinese,"The complex pipeline design of a conventional text-to-speech (TTS) system requires much significant domain expertise and human engineering. Building such a system is nontrivial. This paper presents a neural approach towards end-to-end Mandarin Chinese text-to-speech synthesis based on syllables. We propose an improved model based on Google's Tacotron to cope with this challenging task. Our model can synthesize speech directly from Mandarin tonal syllables (a.k.a. pinyin), achieving a mean opinion score (MOS) of 3.81 in naturalness. We also present a multi-speaker variant which synthesizes speech with each speaker's timbre and speaking style well retained with only some simple modifications to the decoder. Furthermore, speaker adaption is investigated based on the trained multi-speaker model. Experiments show it can transfer to a new voice producing acceptable quality using as little as 10 minutes of data, making it a promising way to quickly build a TTS system.",1.0,4.2125
37,999,2.4    5.15    5    4.3    ,Articulatory-to-speech Conversion Using Bi-directional Long Short-term Memory,"Methods for synthesizing speech sounds from the motion of articulatory organs can be used to produce substitute speech for people who have undergone laryngectomy. To achieve this goal, feature parameters representing the spectral envelope of speech, directly related to the acoustic characteristics of the vocal tract, has been estimated from articulatory movements. Within this framework, speech can be synthesized by driving the filter obtained from a spectral envelope with noise signals. In the current study, we examined an alternative method that generates speech sounds directly from the motion pattern of articulatory organs based on the implicit relationships between articulatory movements and the source signal of speech. These implicit relationships were estimated by considering that articulatory movements are involved in phonological representations of speech that are also related to sound source information such as the temporal pattern of pitch and voiced/unvoiced flag. We developed a method for simultaneously estimating the spectral envelope and sound source parameters from articulatory data obtained with an electromagnetic articulography (EMA) sensor.  Furthermore, objective evaluation of estimated speech parameters and subjective evaluation of the word error rate were performed to examine the effectiveness of our method.",0.0,4.2125
1094,1839,3.4    5.15    4.1    ,Feedback Cancellation Using Hybrid Adaptive Filters,"Adaptive feedback cancellation (AFC) using prediction error method
(PEM) is commonly used to increase the stable gain in hearing
aids. This method requires a model of the input signal, which
in practice needs to be estimated. Thus to implement an AFC
with PEM both the input model and the feedback need to be
estimated at the same time. In this paper, we have investigated
hybrid combination algorithms which combine a standard NLMS
update with a pre-filtered error update using either a variable stepsize
algorithm or a simplified Kalman filter. Through simulations,
it has been shown that the hybrid combination is efficient in terms
of convergence, re-convergence, and misalignment for real speech
over a range of gains and changing feedback channels.",1.0,4.216666666666667
685,2420,3.6    5.9    3.15    ,Wide Learing for Auditory Comprehension,"Classical linguistic, cognitive, and engineering models for speech recognition and human auditory comprehension posit representations for sounds and words that mediate between the acoustic signal and interpretation. Recent advances in automatic speech recognition have shown, using deep learning, that state-of-the-art performance is obtained without such units. We present a cognitive model of auditory comprehension based on wide rather than deep learning that was trained on 20 to 80 hours of TV news broadcasts. Just as deep network models, our model is an end-to-end system that does not make use of phonemes and phonological wordform representations. Nevertheless, it performs well on the difficult task of single word identification (model accuracy 11.37%, Mozilla DeepSpeech: 4.45%). The
architecture of the model is a simple two-layered wide neural network with weighted connections between the acoustic frequency band features as inputs and lexical outcomes (pointers to semantic vectors) as outputs. Model performance shows hardly any degredation when trained on speech in noise rather than on clean speech. Performance was further enhanced by adding a second network to a standard wide network. The present word recognition module is designed to become part of a larger system modeling the comprehension of running speech.",0.0,4.216666666666667
1326,2589,5.75    2.55    3.35    5.25    ,Many-to-Many Voice Conversion Based on Bottleneck Features with Variational Autoencoder for Non-parallel Training Data,"abstract
This paper proposes a novel approach to many-to-many (M2M) voice conversion from non-parallel data. In the proposed approach, we first obtain bottleneck features (BNFs) as speaker representation from a deep neural network (DNN). Then, a variational autoencoder (VAE) implements the mapping function (i.e., a reconstruction process) using both the latent semantic information and the speaker representation. Furthermore, we propose an adaptive scheme by intervening the training process of DNN, which can enrich the target speaker's€ personality feature space in the case of limited training data. Our approach has three advantages: 1) neither parallel training data nor explicit frame alignment process is required; 2) consolidates multiple pair-wise systems into a single M2M model (many-source speakers to many-target speakers); 3) expands M2M conversion task from closed set to open set when the training data of target speaker is very limited. The objective and subjective evaluations show that our proposed approach outperforms the baseline system.",1.0,4.225
28,84,4.35    3.25    3.6    5.7    ,What to Expect from Expected Kneser-Ney Smoothing,"Kneser-Ney smoothng on expected counts was proposed recently. In this paper we revisit this technique and suggest a number of optimizations and extensions. We then analyse its performance in several practical speech recognition scenarios that depend on fractional sample counts, such as training on uncertain data, language model adaptation and Word-Phrase-Entity models. We show that the proposed approach to smoothing outperforms known alternatives by a significant margin.",0.0,4.225
1040,1695,4.45    4.1    2.6    5.75    ,Exploring Complementary Information of GMM/CNN/LSTM Classifiers for Replay Detection,"Voice biometrics technology assures the user authentication by
using the speakers’ voice with help of machines. It reduces the
cost and risk associated with authentication that requires passwords or sharing of sensitive data. On the other hand, the voice
samples are also challenging to secure, as only a few minutes
of audio samples are required to replay the voice convincingly
to have access to the system and steal the secured information.
Replay is one of the spoofing attacks that is responsible to have
a threat for Automatic Speaker Verification (ASV) system and
thus, a standalone countermeasure is required for spoof detection. In this paper, we present a comparison of various feature
sets with different classifiers for replay detection. Experimental
results conducted on ASV spoof 2017 Challenge database show
that Gaussian Mixture Model (GMM) and Convolutional Neural Network (CNN)-based classifiers gave comparable results
than Long Short-Term Memory (LSTM) classifier. Furthermore, to explore complementary information present in classifiers, classifier-level fusion was performed. The lower EER
obtained was with GMM and CNN classifier reducing the individual Equal Error Rate (EER) to 01.78 % and 06.47 % on
development and evaluation dataset with the proposed Amplitude Weighted Frequency Cosine Coefficients (AWFCC) feature sets, respectively.",1.0,4.225
113,1134,3.4    3.4    4.3    5.8    ,Bidirectional Long-Short Term Memory Network-based Estimation of Reliable Spectral Component Locations,"An accurate Ideal Binary Mask (IBM) estimate is essential for Missing Feature Theory (MFT)-based speaker identification, as incorrectly labelled spectral components (where a component is either reliable or unreliable) will degrade the performance of an Automatic Speaker Identification (ASI) system adversely in the presence of noise. In this work a Bidirectional Recurrent Neural Network (BRNN) with Long-Short Term Memory (LSTM) cells is proposed for improved IBM estimation. The proposed system had an average IBM estimate accuracy improvement of 4.5% and an average MFT-based speaker identification accuracy improvement of 3.1% over all tested SNR dB levels, when compared to the previously proposed Multilayer Perceptron (MLP)-IBM estimator. When used for speech enhancement the proposed system had an average MOS-LQO (objective quality measure) improvement of 0.32 and an average QSTI (objective intelligibility measure) improvement of 0.01 over all tested SNR dB levels, when compared to the MLP-IBM estimator. The results presented in this work highlight the effectiveness of the proposed BRNN-IBM estimator for MFT-based speaker identification and IBM-based speech enhancement.",0.0,4.225
306,1500,4.2    5    3.6    4.1    ,Joint Learning of J-Vector Extractor and Joint Bayesian Model for Text Dependent Speaker Verification,"J-vector and joint Bayesian have been proved to be very effective in text dependent speaker verification with short-duration speech. However current state-of-the-art framework often consider training the J-vector extractor and the joint Bayesian classifier separately. Such an approach will result in information loss for j-vector learning and also fail to exploit an end-to-end framework. In this paper we present a integrated approach to text dependent speaker verification, which consists of a siamese deep neural network that takes two variable length speech segments and maps them to the likelihood score and speaker/phrase labels, where the likelihood score as a loss guide is computed by a variant joint Bayesian model. The likelihood loss guide can constrain the j-vector extractor for improving the verification performance. Since the strengths of j-vector and joint Bayesian analysis appear complementary the joint learning significantly outperforms traditional separate training scheme. Our experiments on the the public RSR2015 part I data corpus demonstrate that this new training scheme can produce more discriminative j-vectors and leading to performance improvement on the speaker verification task.",0.0,4.225
154,1224,5.05    2.5    4.2    5.15    ,Singing Voice Phoneme Segmentation by Hierarchically Inferring Syllable and Phoneme Onset Positions,"In this paper, we tackle the singing voice phoneme segmentation problem in the singing training scenario by using language-independent information -- onset and prior coarse duration. We propose a two-step method.  In the first step, we jointly calculate the syllable and phoneme onset detection functions (ODFs) using a convolutional neural network (CNN). In the second step, the syllable and phoneme boundaries and labels are inferred hierarchically by using a duration-informed hidden Markov model (HMM). To achieve the inference, we incorporate the a priori duration model as the transition probabilities and the ODFs as the emission probabilities into the HMM. The proposed method is designed in a language-independent way such that no phoneme class labels are used. For the model training and algorithm evaluation, we collect a new jingju (also known as Beijing or Peking opera) solo singing voice dataset and manually annotate the boundaries and labels at phrase, syllable and phoneme levels. The dataset is publicly available. The proposed method is compared with a baseline method based on hidden semi-Markov model (HSMM) forced alignment. The evaluation results show that the proposed method outperforms the baseline by a large margin regarding both segmentation and onset detection tasks.",0.0,4.225
220,1328,4.25    3.25    5.05    4.35    ,An Optimization Based Approach for Solving Spoken CALL Shared Task,"In this paper, we are describing our developed systems for the 2018 SLaTE CALL Shared Task on grammatical and linguistic assessment of English spoken by German-speaking Swiss teenagers. The English spoken response is converted to text using baseline English DNN-HMM ASR trained on the shared task training data and another two commercial ASRs (Google and Microsoft Bing). The produced transcription is assessed in terms of language and meaning errors. In this work, we focused on the text-processing component. Grammatical errors are detected using English grammar checker, part of speech analysis and extracting incorrect bi-grams from grammatically incorrect responses. Errors related to the meaning are detected using novel approaches which measure the similarity between the given response and stored set of reference responses.

  The outputs of several systems have been fused together into one overall system, where the fusion weights and parameters are tuned using genetic algorithm. The best result on the 2018 shared task test dataset is D-score of 14.41, which was achieved by the fused system and the optimized set of incorrect bi-grams.",0.0,4.225
745,2587,6    4.35    3.25    3.3    ,Joint Learning of Facial Expression and Head Pose from Speech,"Natural movement plays a significant role in realistic speech animation, and numerous studies have demonstrated the contribution visual cues make to the degree human observers find an animation acceptable. Natural, expressive, emotive, and prosodic speech exhibits motion patterns that are difficult to predict with considerable variation in visual modalities. Recently, there have been some impressive demonstrations of face animation derived in some way from the speech signal. Each of these methods have taken unique approaches, but none have included rigid head pose in their predicted output.

We observe a high degree of correspondence with facial activity and rigid head pose during speech, and exploit this observation to jointly learn full face animation and head pose rotation and translation combined. From our own corpus, we train Deep Bi-Directional LSTMs (BLSTM) capable of learning long-term structure in language to model the relationship that speech has with the complex activity of the face. We define a model architecture to encourage learning of rigid head motion via the latent space of the speaker's facial activity. The result is a model that can predict lip sync and other facial motion along with rigid head motion directly from audible speech.",0.0,4.225
141,1198,4.25    4.25    3.4    5    ,Learning and Modeling Unit Embeddings for Improving HMM-based Unit Selection Speech Synthesis,"This paper presents a method of learning and modeling unit embeddings using deep neutral networks (DNNs) to improve the performance of HMM-based unit selection speech synthesis. First, a DNN with an embedding layer is built to learn a fixed-length embedding vector for each phone-sized candidate unit in the corpus from scratch. Then, another two DNNs are constructed to map linguistic features toward the extracted unit vector of each phone. One of them employs the unit vectors of preceding phones as model input. At synthesis time, the L2 distances between the unit vectors predicted by these two DNNs and the ones derived from candidate units are integrated into the target cost and the concatenation cost of HMM-based unit selection speech synthesis respectively. Experimental results demonstrate that the unit vectors estimated using only acoustic features display phone-dependent clustering properties. Furthermore, integrating unit vector distances into cost functions, especially the concatenation cost, improves the naturalness of HMM-based unit selection speech synthesis in our experiments.",0.0,4.225
833,1155,3.4    4.5    5.2    3.8    ,Variational Mode Decomposition Based Feature for Emotion Classification,"In this paper, a novel method is proposed for emotion classification from the speech signal using variational mode decomposition (VMD). VMD decomposes the speech signal into number
of sub-signals or modes. Each sub-signal has different bandwidth and center frequency. Different emotions have different impact on the frequency bands. The sub-signals of VMD may
therefore be better exploit the emotion information of each band for characterization of different emotions. Center frequency, peak amplitude, energy, spectral entropy, renyi’s entropy and state space correlation entropy from each sub-signal are evaluated, and used as the proposed feature for emotion classification. The performance is evaluated using three databases, EMODB, IEMOCAP and FAU AIBO. SVM classifier is used for classification purpose. In terms of classification rates, the proposed feature out-performs the mel-frequency cepstral coefficients (MFCC), prosodic and Teager-energy-operator (TEO) based 
TEO-CB-Auto-Env features.",1.0,4.2250000000000005
325,1536,4.2    4.15    2.65    5.9    ,Auditory Filterbank Learning Using ConvRBM for Infant Cry Classification,"The infant cry classification is a socially-relevant problem where the task is to classify the normal vs. pathological cry signals. Since the cry signals are very different from the speech signals in terms of temporal and spectral content, there is a need for better feature representation for infant cry signals. In this paper, we propose to use unsupervised auditory filterbank learning using Convolutional Restricted Boltzmann Machine (ConvRBM). Analysis of the subband filters shows that most of the subband filters are Fourier-like basis functions. The infant cry classification experiments were performed on the two databases, namely, DA-IICT Cry and Baby Chillanto. The experimental results show that the proposed features perform better than the standard Mel Frequency Cepstral Coefficients (MFCC) using various statistically meaningful performance measures. In particular, our proposed ConvRBM-based features obtained an absolute improvement of 2 % and 0.58 % in the classification accuracy on the DA-IICT Cry and the Baby Chillanto database, respectively.",0.0,4.2250000000000005
650,2334,5    4.25    3.45    ,Multilingual Bottleneck Features for Subword Modeling in Zero-resource Languages,"How can we effectively develop speech technology for languages where no transcribed data is available? Many existing approaches use no annotated resources at all, yet it makes sense to leverage information from large annotated corpora in other languages, for example in the form of multilingual bottleneck features (BNFs) obtained from a supervised speech recognition system. In this work, we evaluate the benefits of BNFs for subword modeling (feature extraction) in six unseen languages on a word discrimination task. First we establish a strong unsupervised baseline by combining two existing methods: vocal
tract length normalisation (VTLN) and the correspondence autoencoder (cAE). We then show that BNFs trained on a single language already beat this baseline; including up to 10 languages results in additional improvements which cannot be matched by just adding more data from a single language. Finally, we show that the cAE can improve further on the BNFs if high-quality same-word pairs are available.",0.0,4.233333333333333
190,1280,4.5    5.15    3.25    4.05    ,A Generalization of PLDA for Joint Modeling of Speaker Identity and Multiple Nuisance Conditions,"Probabilistic linear discriminant analysis (PLDA) is the leading method for computing scores in speaker recognition systems. The method models the vectors representing each audio sample as a sum of three terms: one that depends on the speaker identity, one that models the within-speaker variability, and one that models any remaining variability. The last two terms are assumed to be independent across samples. We recently proposed an extension of the PLDA method, which we termed Joint PLDA (JPLDA), where the second term is considered dependent on the type of nuisance condition present in the data (e.g., the language or channel). The proposed method led to significant gains for multilanguage speaker recognition when taking language as the nuisance condition. In this paper, we present a generalization of this approach that allows for multiple nuisance terms. We show results using language and several nuisance conditions describing the acoustic characteristics of the sample and demonstrate that jointly including all these factors in the model leads to better results than including only language or acoustic condition factors. Overall, we obtain relative improvements in detection cost function between 5% and 47% for various systems and test conditions with respect to standard PLDA approaches.",0.0,4.2375
831,1146,3.9    5    5.1    2.95    ,(In)activation of Contrastive Alternative with L+H* Pitch Accent in PP Attachment Ambiguity Resolution,"This study investigated an influence of contrastive L+H* pitch accent in prepositional phrase (PP) attachment ambiguity resolution. Using sentences such as The boy will touch the frog with the flower, we examined whether contrastive pitch accent within the ambiguous PP would highlight a contrast between two objects in a visually presented context and listeners use the cue to adopt one of the two possible interpretations between NP-attached modifier interpretation (the frog has the flower) or VP-attached instrument interpretation (touch the frog by using the flower). The two possibilities were tested in the study: (i) the contrastive pitch accent within the ambiguous PP highlights a contrast in the context and leads listeners to adopt contextually intended interpretation; (ii) the contrastive pitch accent highlights a contrast only when the two objects in the context makes a strong contrast. The results from an eye-tracking visual-world experiment showed the contrastive pitch accent led listeners to adopt contextually intended analysis only when the contrastive set in the scene was animate. The same contrastive pitch accent did not emphasize the contrast when the contrastive set in the scene was inanimate and less salient - even though participants were aware of the contrast between the two objects.",1.0,4.2375
614,2224,4.6    4    4.25    4.1    ,Wuxi Speakers’ Production and Perception of Coda Nasals in Mandarin,"Wuxi natives speak a dialect of Wu, which has only one coda nasal /n/ but allows allophones depending on the pre-nasal vowel (Qian, 1992), whereas in Mandarin, there are two coda nasals—alveolar /n/ and velar /ŋ/. Two perception experiments were conducted to investigate Wuxi speakers’ perception and production of coda nasals in their second language (L2) Mandarin. First, two groups of Wuxi native speakers, age around 20 and 50, produced monosyllabic words with nasal coda in Mandarin and their production was used as the stimuli for native Mandarin speakers to identify. Second, the same Wuxi speakers participated in an identification task to judge the place of articulation of the nasal coda in monosyllabic words in standard Mandarin. The results of the first experiment indicate that young Wuxi speakers’ Mandarin production was identified with higher accuracy by native Mandarin speakers than older Wuxi speakers’, suggesting the young speakers produced more nativelike Mandarin than the older speakers. The results of the second experiment reveal that young Wuxi speakers identified coda nasals in Mandarin more accurately than older Wuxi speakers did, suggesting Wuxi speakers’ production of Mandarin coda nasals is associated with their perception.",0.0,4.2375
159,1232,3.4    4.3    3.5    5.75    ,S4D: Speaker Diarization Toolkit in Python,"In this paper, we present S4D, a new open-source Python toolkit dedicated to speaker diarization. S4D provides various state-of-the-art components and the possibility to easily develop end-to-end diarization prototype systems. S4D offers a large panel of clustering, segmentation, scoring and visualization algorithms. S4D has been thought to be easily understood, installed, modified and used in order to allow fast transfers of diarization technologies to industry and facilitate development of new approaches.
Examples, benchmarks on standard tasks and tutorials are provided in this paper.
S4D is an extension of the open-source toolkit for speaker recognition: SIDEKIT.",0.0,4.2375
363,1615,5.1    4.2    4.2    3.45    ,Sparsity-Constrained Weight Mapping for Individualization of Head-Related Transfer Functions from Anthropometric Features,"Head-related transfer functions (HRTFs) describe the propagation of sound waves from the sound source to ear drums, which contain most of information for localization. However, HRTFs are highly individual-dependent, and thus because of the difference of anthropometric features between subjects, individualization of HRTFs is a great challenge for accurate localization perception in virtual auditory displays (VAD). In this paper, we propose a sparsity-constrained weight mapping method termed SWM to obtain individual HRTFs. The key idea behind SWM is to obtain optimal weights to combine HRTFs from the training subjects based on the relationship of anthropometric features between the target subject and the training subjects. To this end, SWM learns two sparse representations between the target subject and the training subjects in terms of anthropometric features and HRTFs, respectively. A non-negative sparse model is used for this purpose when considering the non-negative property of the anthropometric features. Then, we build a mapping between the two weight vectors using a nonlinear regression. Furthermore, an iterative data extension method is proposed in order to increase training samples for mapping model. The objective and subjective experimental results show that the proposed method outperforms other methods in terms of log-spectral distortion (LSD) and localization accuracy.",0.0,4.2375
749,31,3.75    5.25    3.05    4.9    ,The Perception of Multi-modal Emotional Cues by Children in Artificial Background Noise,"Ones capacity to perceive emotion from speech allows for more successful human-human interactions. Although this ability can be natural for most typically developed individuals, factors such as age or environmental conditions can restrict that inherent skill. Noise pollution and multimedia over-stimulation are common components of contemporary society, and have shown to particularly impair a child's interpersonal skills. Assessing the influence of such features of daily-life on the perception of emotion over different developmental stages will advance child-related research according to contemporary urgency. The presented work evaluates how background noise and emotionally connoted visual stimuli affect a child's perception of emotional speech. A total of 80 subjects from 4 to 14 years old evaluated 20 multi-modal instances of nonsense emotional speech, linked to several environmental and visual conditions. Results from this study suggest that visual stimulation does not influence a child's perception; however, background noise does compromise their ability to correctly identify emotion in speech, a phenomenon that seems to decrease with age.",1.0,4.237500000000001
1280,2388,5.1    3.75    3.95    4.15    ,Measuring Speech Rate in Disfluent Children's Speech,"Stuttering speech is characterized by the occurrence of disfluencies such as repetitions (of individual speech sounds, syllables, words, etc.), prolongations and blocks (non-volitional hesitations or stoppages). These disfluencies cause the speech to appear less fluent compared to normal speech. A common measure to quantify fluency is speech rate, measured in syllables per second. Automatic speech rate determination algorithms are typically based on the detection of syllable nuclei, e.g. using peak picking on relevant energy trajectories.

In this paper we evaluate techniques for automated syllable nuclei detection on non-fluent children's spontaneous speech. Even though these algorithms were optimized for adult fluent speech, the estimated speech rate values correlated strongly with the true values, which were calculated using annotations from experienced listeners.",1.0,4.237500000000001
365,1625,4.25    4.95    2.5    5.2    4.3    ,Transfer Learning for Improving Speech Emotion Classification Accuracy,"The majority of existing speech emotion recognition research focuses on automatic emotion detection using training and testing data from same corpus collected under the same conditions. The performance of such systems has been shown to drop significantly in cross-corpus and cross-language scenarios. To address the problem, this paper exploits a transfer learning technique to improve the performance of speech emotion recognition systems that is novel in cross-language and cross-corpus scenarios. Evaluations on five different corpora in three different languages show that Deep Belief Networks (DBNs) offer better accuracy than previous approaches on cross-corpus emotion recognition, relative to a Sparse Autoencoder and Support Vector Machine (SVM) baseline system. Results also suggest that using a large number of languages for training and using a small fraction of the target data in training can significantly boost accuracy compared with baseline also for the corpus with limited training examples.",0.0,4.24
385,1665,4.65    4.95    2.8    4.6    ,Detection of Glottal Activity Errors in Production of Stop Consonants in Children with Cleft Lip and Palate,"Individuals with cleft lip and palate (CLP) alter the glottal activity characteristics during the production of stop consonants. The presence/absence of glottal vibrations during the production of unvoiced/voiced stops is referred as glottal activity error (GAE).  In this work, acoustic-phonetic and production based knowledge of stop consonants are exploited to propose an algorithm for the automatic detection of GAE. The algorithm uses zero frequency filtered and band-pass (500-4000 Hz) filtered speech signals to identify the syllable nuclei positions, followed by the detection of glottal activity characteristics of consonant present within the syllable. Based on the identified glottal activity characteristics of consonant and $a~ priori$ voicing information of target stop consonant, the presence or absence of GAE is detected. The algorithm is evaluated over the database containing the responses of normal children and children with repaired CLP for the target consonant-vowel-consonant-vowel  words with stop consonants.",0.0,4.25
1252,2274,3.35    4.3    5.1    ,Prosodic Organization and Focus Realization in Taiwan Mandarin,"Cross-linguistically, the way that focus is marked through prosody can depend on a variety of factors, including local constraints on prosodic organization or the position of a word within the larger focus constituent. Here we report on a production study that explores the possible influence of prosodic organization and position on focus realization in Taiwan Mandarin. The materials consisted of sentences in which the syntactic subject consisted of a monosyllabic numeral, classifier, and noun. The context was manipulated to elicit narrow information focus (i.e., using wh-questions) on either the numeral, the noun, or the entire NP. The resulting target syllables were then analyzed in terms of their F0 characteristics, duration, and amplitude. The results revealed clear asymmetries in how the numeral and noun were realized in their corresponding single-word narrow focus condition versus in the NP focus condition, though the pattern differed for Tone 1 versus Tone 3 words. Moreover, the classifier did not always conform to its expected status when external to the focus constituent. These findings suggest that focus marking in Taiwan Mandarin is conditioned by independent constraints on local prosodic structure. We compare our results to previously reported findings for Beijing Mandarin.",1.0,4.25
574,2112,4.25    4.2    4.35    4.2    ,Building Large-vocabulary Speaker-independent Lipreading Systems,"Constructing a viable lipreading system is a challenge because it is claimed that only 30\% of information of speech production is visible on the lips. Nevertheless, in small vocabulary tasks, there have been several reports of high accuracies. However, investigation of larger vocabulary tasks is much rarer.   

This work examines constructing a large vocabulary lipreading system using an approach based-on Deep Neural Network Hidden Markov Models (DNN-HMMs). We tackle the problem of lipreading an unseen speaker. We investigate the effect of employing several steps to pre-process visual features. Moreover, we examine the contribution of language modelling in a lipreading system where we use longer $n$-grams to recognise visual speech. Our lipreading system is constructed on the 6000-word vocabulary TCD-TIMIT audiovisual speech corpus. The results show that visual speech recognition can definitely reach 50\% word accuracy on large vocabularies. We actually achieved a mean of 53.83\% measured via three-fold cross-validation on the speaker independent setting of the TCD-TIMIT corpus using bigrams.",0.0,4.25
99,1111,4.45    2.65    5.05    4.85    ,Improving Language Modeling with an Adversarial Critic for Automatic Speech Recognition,"Recurrent neural network language models (RNN LMs) trained via the maximum likelihood principle suffer from the exposure bias problem in the inference stage. Therefore, potential recognition errors limit their performance on re-scoring N-best lists of the speech recognition outputs. Inspired by the generative adversarial net (GAN), this paper proposes a novel approach to alleviate this problem. We regard the RNN LM as a generative model in the training stage. And an auxiliary neural critic is used to encourage the RNN LM to learn long-term dependencies from corrupted contexts by forcing it generating valid sentences. Since the vanilla GAN has limitations when generating discrete sequences, the proposed framework is optimized though the policy gradient algorithm. Experiments were conducted on two mandarin speech recognition tasks. Results show the proposed method achieved lower character error rates on both datasets compared with the maximum likelihood method, whereas it increased perplexities slightly. Finally, we visualised the sentences generated from the RNN LM. Results demonstrate the proposed method really helps the RNN LM to learn long-term dependencies and alleviates the exposure bias problem.",0.0,4.25
47,1021,3.3    5.1    3.6    5    ,Unsupervised and Efficient Vocabulary Expansion for Recurrent Neural Network Language Models in ASR,"In automatic speech recognition (ASR) systems, recurrent neural network language models (RNNLM) are used to rescore a word lattice or N-best hypotheses list. Due to the expensive training, the RNNLM’s vocabulary set accommodates only small shortlist of most frequent words. This leads to suboptimal performance if an input speech contains many out-of-shortlist (OOS) words.
An effective solution is to increase the shortlist size and retrain the entire network which is highly inefficient. Therefore, we propose an efficient method to expand the shortlist set of a pretrained RNNLM without incurring expensive retraining and using additional training data. Our method exploits the structure of RNNLM which can be decoupled into three parts: input projection layer, middle layers, and output projection layer. Specifically, our method expands the word embedding matrices in projection layers and keeps the middle layers unchanged. In this approach, the functionality of the pretrained RNNLM will be correctly maintained as long as OOS words are properly modeled in two embedding spaces. We propose to model the OOS words by borrowing linguistic knowledge from appropriate in-shortlist words. Additionally, we propose to generate the list of OOS words to expand vocabulary in unsupervised manner by automatically extracting them from ASR output.",0.0,4.25
522,1972,3.55    4.95    5.2    3.3    ,Comparison of Unsupervised Modulation Filter Learning Methods for ASR,"The widespread deployment of automatic speech recognition (ASR) system in  consumer centric applications such as voice interaction and voice search demands the need for noise robustness in such systems. One approach to this problem is to achieve the desired robustness in speech representations used in the ASR. Motivated from studies on robust human speech recognition, we analyse the unsupervised data-driven temporal modulation filter learning for robust feature extraction. In this paper, we compare various unsupervised models for data driven filter learning like convolutional autoencoder (CAE),  generative adversarial network (GAN) and convolutional restricted Boltzmann machine (CRBM). The unsupervised models are designed to learn a set of filters from long temporal trajectories of speech sub-band energy. The filters learnt from these models are used for modulation filtering of the input spectrogram before the ASR training. The ASR experiments are performed on Wall Street Journal (WSJ) Aurora-4 database with clean and multi condition training setup. The experimental results obtained from the modulation filtered representations shows considerable robustness to noise, channel distortions and reverberant conditions compared to other feature extraction methods. Among the three approaches compared in this paper, the GAN approach provides the most consistent improvements in ASR accuracy in different training scenarios.",0.0,4.25
557,2066,4.8    3.9    5.9    2.4    ,The Effect of Real-Time Constraints on Automatic Speech Animation,"Machine learning has previously been applied successfully to speech-driven facial animation. To account for carry-over and anticipatory coarticulation a common approach is to predict the facial pose using a symmetric window of acoustic speech that includes both past and future context. Using future context limits this approach for animating the faces of characters in real-time and networked applications, such as online gaming. An acceptable latency for conversational speech is 200ms and typically network transmission times will consume a significant part of this. Consequently, we consider asymmetric windows by investigating the extent to which decreasing the future context effects the quality of predicted animation using both deep neural networks (DNNs) and bi-directional LSTM recurrent neural networks (BiLSTMs). Specifically we investigate future contexts from 170ms (fully-symmetric) to 0ms (fully-asymmetric). We find that a BiLSTM trained using 70ms of future context is able to predict facial motion of equivalent quality as a DNN trained with 170ms, while introducing increased processing time of only 5ms. Subjective tests using the BiLSTM show that reducing the future context from 170ms to 50ms does not significantly decrease perceived realism. Below 50ms, the perceived realism begins to deteriorate, generating a trade-off between realism and latency.",0.0,4.25
382,1661,4.8    4.1    4.25    3.45    4.7    ,Novel Empirical Mode Decomposition Cepstral Features for Replay Spoof Detection,"The advances in Automatic Speaker Verification (ASV) system
for voice biometric purpose comes with the danger of spoofing
attacks. The replay attack is the most accessible attack,
where the attacker imitates speaker’s identity by replaying the
pre-recorded speech samples of the target speaker. Most of the
conventional features, such as Mel Frequency Cepstral Coefficients
(MFCC), Instantaneous Frequency Cepstral Coefficients
(IFCC), etc. uses filterbank structure for feature extraction purpose.
In this paper, we propose a novel Empirical Mode Decomposition
Cepstral Coefficient (EMDCC) feature set, where
the filterbank in MFCC is replaced with the Empirical Mode
Decomposition (EMD) to obtain the subband signals. The proposed
feature set takes an advantage of using EMD that acts as a
dyadic filterbank and handles the nonlinear and non-stationary
nature of the speech signal. The stand-alone EMDCC feature
set gives the Equal Error Rate (EER) of 28.06 % compared to
the baseline CQCC and MFCC system with EER of 29.18 %
and 31.3 %, respectively on the evaluation set of ASV Spoof
2017 Challenge database. Furthermore, the proposed feature
set is fused with the Linear Frequency Modified Group Delay
Cepstral Coefficient (LFMGDCC) at score-level and we obtain
a reduced EER of 18.36 % on evaluation set.",0.0,4.26
165,1240,5    2.3    4.85    4.9    ,On Learning to Identify Genders from Raw Speech Signal Using CNNs,"Automatic Gender Recognition (AGR) is the task of identifying the gender of a speaker given a speech signal. Standard approaches extract features like fundamental frequency and cepstral features from the speech signal and train a binary classifier. Inspired from recent works in the area of automatic speech recognition (ASR), speaker recognition and presentation attack detection, we present a novel approach where relevant features and classifier are jointly learned from the raw speech signal in end-to-end manner. We propose a convolutional neural networks (CNN) based gender classifier that consists of: (1) convolution layers, which can be interpreted as a feature learning stage and (2) a multilayer perceptron (MLP), which can be interpreted as a classification stage. The system takes raw speech signal as input, and outputs gender posterior probabilities. Experimental studies conducted on two datasets, namely AVspoof and ASVspoof 2015, with different architectures show that with simple architectures the proposed approach yields better system than standard acoustic features based approach. Further analysis of the CNNs show that the CNNs learn formant and fundamental frequency information for gender identification.",0.0,4.262499999999999
632,2290,2.75    5.05    4.6    4.65    ,Real-time Single-channel Dereverberation and Separation with Time-domain Audio Separation Network,"We investigate the recently proposed Time-domain Audio Separation Network (TasNet) in the task of real-time single-channel speech dereverberation. Unlike systems that take time-frequency representation of the audio as input, TasNet learns an adaptive front-end in replacement of the time-frequency representation by a time-domain convolutional non-negative autoencoder. We show that by formulating the dereverberation problem as a denoising problem where the direct path is separated from the reverberations, a TasNet denoising autoencoder can outperform a deep LSTM baseline on log-power magnitude spectrogram input in both causal and non-causal settings. We further show that adjusting the stride size in the convolutional autoencoder helps both the dereverberation and separation performance.",0.0,4.262499999999999
932,1416,4.85    3.65    5    3.55    ,The DKU-JN-EMA Electromagnetic Articulography Database on Mandarin and Chinese Dialects with Tandem Feature Based Acoustic-to-Articulatory Inversion,"This paper describes the acquisition of the Duke Kunshan University Jinan University Electromagnetic Articulography (DKU-JN-EMA) database in terms of aligned acoustics and articulatory data on Mandarin and Chinese dialects. This database currently includes data from multiple individuals in Mandarin and three Chinese dialects, namely Cantonese, Hakka, Teochew. There are 2-7 native speakers for each language or dialect.  Acoustic data is obtained by one head-mounted close talk microphone while articulatory data is obtained by the NDI electromagnetic articulography wave research system. The DKU-JN-EMA corpus is now in preparation for public release to help advance research in areas of acoustic-to-articulatory inversion, speech production, dialect recognition, and experimental phonetics.  Along with the corpus, we propose an acoustic-to-articulatory inversion baseline using deep neural networks. Moreover, we show that by concatenating the dimension reduced phoneme posterior probability feature with MFCC feature at the feature level as tandem feature, the inversion system performance is enhanced significantly.",1.0,4.2625
904,1338,2.75    3.5    5.8    5    ,Spectral Mapping Using Residual Networks and Mimic Loss,"The application of deep learning to robust speech recognition has led to significant gains recently. One approach, spectral mapping, uses a modular deep neural network (DNN) system to map directly from noisy speech to clean speech; this system can be used as a generic front-end denoiser to any speech recognition pipeline. More recently, it has been found that both jointly training a denoiser with an acoustic model, and addition of noise-robust features in a customized pipeline, can bring down word error rates (WERs) considerably. However, using these techniques severely reduces the ability of the denoiser to be decoupled from a particular acoustic model. In contrast, we use residual networks (ResNets), in place of DNNs, to perform spectral mapping without the use of joint training or additional features. We feed our denoised features to an off-the-shelf Kaldi recipe, and achieve a WER of 11.1\% on the CHiME-2 corpus despite having fewer parameters than other front-end-only systems. Training this architecture with the recently-proposed mimic loss brings the WER to 10.7\% which matches the state-of-the-art results for speech enhancement techniques on this corpus using a much simpler production architecture.",1.0,4.2625
87,1093,4.75    4.15    5.9    2.25    ,The Perception and Analysis of the Likeability and Human Likeness of Synthesized Speech,"The synthesized voice has become an ever present aspect of daily life. Heard through our smart-devices and from public announcements, engineers continue in an endeavour to achieve naturalness in such voices. Yet, the degree to which these methods can produce likeable, human like voices, has not been fully evaluated. With recent advancements in synthetic speech technology suggesting that human like imitation is more obtainable, this study asked 25 listeners to evaluate both the likeability and human likeness of a corpus of 13 German male voices, produced via 5 synthesis approaches (from formant to hybrid unit selection, deep neural network systems), and 1 Human control. Results show that unlike visual artificially intelligent elements - as posed by the concept of the Uncanny Valley -likeability consistently improves along with human likeness for the synthesized voice, with recent methods achieving substantially closer results to human speech than older methods. A small scale acoustic analysis shows that the F0 of hybrid systems correlates less closely to human speech with a higher standard deviation for F0. This analysis suggests that limited variance in F0 is linked to a reduction in human likeness, resulting in lower likeability for conventional synthetic speech methods.",0.0,4.2625
137,1182,4.35    5.1    3.4    4.2    ,Improving Cross-Lingual Knowledge Transferability Using Multilingual TDNN-BLSTM with Language-Dependent Pre-Final Layer,"Multilingual acoustic modeling for improved automatic speech recognition (ASR) has been extensively researched. It's widely acknowledged that the shared-hidden-layer multilingual deep neural network (SHL-MDNN) acoustic model (AM) could outperform the conventional monolingual AM, due to its effectiveness in cross-lingual knowledge transfer. In this work, two research aspects are investigated, with the goal of improving multilingual acoustic modeling. Firstly, in the  SHL-MDNN architecture, the shared hidden layer configuration is replaced by a combined TDNN-BLSTM structure. Secondly, the improvement of cross-lingual knowledge transferability is achieved through adding the proposed language-dependent pre-final layer under each network output. The pre-final layer, rarely adopted in past works, is expected to increase nonlinear modeling capability between universal transformed features generated by shared hidden layers and language-specific outputs. Experiments are carried out with CUSENT, WSJ and RASC-863 corpora, covering Cantonese, English and Mandarin. A Cantonese ASR task is chosen for evaluation. Experimental results show that SHL-MTDNN-BLSTM achieves the best performance. The proposed additional language-dependent pre-final layer brings moderate while consistent performance gains in various multilingual training corpora settings, thus demonstrates its effectiveness in improving cross-lingual knowledge transferability.",0.0,4.2625
1137,1957,5.15    3.65    3.15    5.1    ,Discretized Phone Duration Prediction with Multi-Task Dilated Convolutional Neural Networks and Effective Duration Decoding,"This paper explores a more accurate and faster method alternative to BLSTM for discrete phone duration prediction: dilated CNN, which have better capacity than traditional CNN. Specifically, we discretize duration at different resolutions and   model them jointly with a multi-task dilated CNN. In the training stage, to discriminate the distance between the ``nearby''  and ``further'' class, we use a cost-sensitive cross entropy loss that places more weight on frames whose prediction is far from the reference class label. In the testing stage, to avoid erratic label transitions and make more accurate and robust phone duration series estimation, two novel and effective duration decoding methods that incorporate both long- and short-term temporal properties of different labels are applied. Objective results show the proposed multi-task dilated CNN based approaches significantly outperform the current published method (BLSTM-CRF), with a lower RMSE by 2.6 ms, and nearly 4 times faster. The subjective evaluation results further indicate the effectiveness of the proposed methods. Meanwhile, we give a detailed analysis of the proposed multi-task dilated CNN based duration model to figure out the contribution of each component.",1.0,4.2625
80,1080,5    4    4.9    3.15    ,Implementation of Respiration in Articulatory Synthesis Using a Pressure-Volume Lung Model,"In previous studies of the 1D vocal tract model of articulatory synthesis, subglottal pressure is typically regarded as constant, ignoring its dynamics.
However, human vocalization is initially generated by glottal airflow via subglottal pressure change.
This change is caused by the expansion and contraction of the lungs.
In the current study, we propose a new pressure-volume model that relates pressure changes to volume changes of the human lung.
Using this model, the behavior of the human lung can be integrated with articulatory synthesis.
This model produces positive and negative subglottal pressure corresponding to expiration and inspiration respectively.
In addition, breathing could be implemented in the proposed model. This implementation would expand the possibilities for articulatory synthesis.",0.0,4.2625
95,1105,5.25    4.1    4.25    3.45    ,Estimation of the Vocal Tract Length of Vowel Sounds Based on the Frequency of the Significant Spectral Valley,"Estimating the vocal tract length (VTL), given the acoustic signal of a vowel sound, is an important problem, which is useful in speaker normalization for vowel recognition, in the inversion problem and in acoustic-phonetic studies. The common approach of using the formant data to estimate VTL works for a neutral vowel approximating a uniform tube. However, for natural vowels, formant data shift considerably away from the resonant frequencies of a uniform tube. The proposed method is motivated from these observations: (a) the frequency of a spectral valley, F_v, depends inversely on VTL; (b) there is much smaller shift in F_v, across vowels, from the corresponding valley frequency of a uniform tube; (c) F_v can be estimated from the spectral envelope itself. VTL has been estimated for the Peterson and Barney (33 male and 28 female speakers) and the TIMIT (326 male and 136 female speakers) databases. When the estimated F_v is used for normalization, the spread in the formant data due to gender differences is considerably reduced. The normalization procedure is vowel and speaker intrinsic. Additionally, we report applications such as Front/Back classification, gender recognition and phonetic feature mapping.",0.0,4.2625
1099,1853,2.95    5    5.5    3.6    ,Enhancing Spoken Language Recognition by Leveraging Speech Attribute Features,"Deep bottleneck features (BNF) have proven to significantly boost the spoken language recognition performance over basic spectral features. However, BNFs are commonly extracted using tied context-dependent phone states as learning targets, which are inherently language-dependent. In this work, we propose to utilize another set of phonetically motivated features, which are designed to be intrinsically language-independent. We achieve this goal by training a deep neural architecture over a common set of fundamental units that can be  ""universally"" defined across all spoken languages. In particular, we adopt manner and place of articulation as ""universal"" speech units (a.k.a., speech attributes). Experimental evidence, collected on the recent NIST Language Recognition Evaluation (LRE) 2017 challenge, confirms our intuition, that is, a beneficial effect on the recognition accuracy can be attained leveraging manner-, and place-based information. Finally, the F1 metric can be brought from 58.8% up to 63.2% by combining the conventional baseline SDCs with the proposed attribute-based detection scores.",1.0,4.2625
391,1679,4.05    5.75    3.35    3.9    ,Comparison of an End-to-end Trainable Dialogue System with a Modular Statistical Dialogue System,"This paper presents a comparison of two dialogue systems:
one is end-to-end trainable and the other uses a more
traditional, modular architecture.
End-to-end trainable dialogue systems recently attracted
a lot of attention because they offer several advantages over traditional systems.
One of them is the avoidance to train each system module
independently, by creating a single network architecture
which maps an input to the corresponding output without
the need for intermediate representations.
While the end-to-end system investigated here had been
tested in a text-in/out scenario it remained an open question
how the system would perform in a speech-in/out scenario,
with noisy input from a speech recognizer and
output speech generated by a speech synthesizer.

To evaluate this, both dialogue systems were trained on the same corpus,
including human-human dialogues in the Cambridge restaurant domain,
and then compared in both scenarios by human evaluation.
The results show, that in both interfaces
the end-to-end system receives significantly higher
ratings on all metrics than the traditional modular system,
an indication that it enables users to reach their goals faster
and experience both a more natural system response
and a better comprehension by the dialogue system.",0.0,4.2625
340,1563,5.25    4.85    2.7    ,Inference-Invariant Transformation of Batch Normalization for Domain Adaptation of Acoustic Models,"Batch normalization, or batchnorm, is a popular technique often used to accelerate and improve training of deep neural networks. When existing models that use this technique via batchnorm layers, are used as initial models for domain adaptation or transfer learning, the novel input feature distributions of the adapted domains, considerably change the batchnorm transformations learnt in the training mode from those which are applied in the inference mode. We empirically find that this mismatch can degrade the performance of domain adaptation for acoustic modeling. To mitigate this degradation, we propose an inference-invariant transformation of batch normalization, a method which reduces the mismatch between training mode and inference mode transformations without changing the inference results. This invariance property is achieved by adjusting the weight and bias terms of the batchnorm to compensate for differences in the mean and variance terms when using the adaptation data. Experimental results show that our proposed method performs the best on several acoustic model adaptation tasks with up to 5% relative improvement in recognition performances in both supervised and unsupervised domain adaptation settings.",0.0,4.266666666666667
1030,1672,3.5    3.7    4.35    4.2    5.6    ,Gradient-based Spectral Visualization of Raw Waveform-based CNNs,"Deep architectures have demonstrated state-of-the-art results in many speech-related pattern recognition tasks. However, the performance improvement comes at the cost of interpretability. There is a need for understanding and visualization of what is learned by various deep architectures as a whole. While recently there have been many visualization approaches proposed in the context of images, there are very few in the context of speech/audio signal especially for architectures dealing with raw waveforms. Moreover, unlike images where spatial patterns are easily interpretable, it is better suited to analyze spectral patterns for speech/audio signals. To this aim, this work proposes a gradient-based spectral visualization technique to visualize and analyze the global characteristics of deep systems trained on raw audio signals. In particular, the proposed approach is an adaptation and spectral variant of gradient-based backpropagation approach, originally developed for image applications. Given an input, the approach highlights a spectral pattern or in other words the frequency bins that have the highest impact on the prediction score. The advantages of the proposed approach are illustrated using an example on speaker identification and on speech recognition.",1.0,4.2700000000000005
518,1962,4.7    4.3    4.2    3.9    ,Language-Dependent Melody Embeddings,"The paper explores the perspectives of applying the distributional approach to prosodic typology of languages.
The method discussed here is an adaptation of the distributional semantics approach, as suggested by Mikolov, to melodic features of speech.
The paper contains a detailed description of the new method, as well as a comparison of five European languages (English, Czech, German, Russian, and Finnish) in terms of melody embeddings. The total amount of speech data was over 500 hours.
The experimental results show that melody embeddings are language dependent. The proposed melody embedding model has shown reasonable results in language comparison.",0.0,4.2749999999999995
222,1333,5.25    5    4.2    2.65    ,Statistical Model Compression for Small-Footprint Natural Language Understanding,"In this paper we investigate statistical model compression applied to natural language understanding (NLU) models. Small-footprint NLU models are important for enabling offline systems on hardware restricted devices, and for decreasing on-demand model loading latency in cloud-based systems. To compress NLU models, we present two main techniques, parameter quantization and perfect feature hashing. These techniques are complementary to existing model pruning strategies such as L1 regularization. We performed experiments on a large scale NLU system. The results show that our approach achieves 14-fold reduction in memory usage compared to the original models with minimal predictive performance impact.",0.0,4.2749999999999995
505,1937,5    5.1    3.5    3.5    ,Multicomponent 2-D AM-FM Modeling of Speech Spectrograms,"In contrast to 1-D short-time analysis of speech, 2-D modeling of spectrograms provides a characterization of speech attributes directly in the joint time-frequency plane. Building on existing 2-D models to analyze a spectrogram patch, we propose a multicomponent 2-D AM-FM representation for spectrogram decomposition. The components of the proposed representation comprise a DC, a fundamental frequency carrier and its harmonics, and a spectrotemporal envelope, all in 2-D. The number of harmonics required is patch-dependent. The estimation of the AM and FM is done using the Riesz transform, and the component weights are estimated using a least-squares approach. The proposed representation provides an improvement over existing state-of-the-art approaches, for both male and female speakers. This is quantified using reconstruction SNR and perceptual evaluation of speech quality (PESQ) metric. Further, we perform an overlap-add on the DC component, pooling all
the patches and obtain a time-frequency (t-f) aperiodicity map for the speech signal. We verify its effectiveness in improving speech synthesis quality by using it in an existing state-of-the-art vocoder.",0.0,4.275
704,2461,3.4    4.35    3.45    5.9    ,Adversarial Feature-Mapping for Speech Enhancement,"Feature-mapping with deep neural networks is commonly used for single-channel speech enhancement, in which a feature-mapping network directly transforms the noisy features to the corresponding enhanced ones and is trained to minimize the mean square errors between the enhanced and clean features. In this paper, we propose an adversarial feature-mapping (AFM) method for speech enhancement which advances the feature-mapping approach with adversarial learning. An additional discriminator network is introduced to distinguish the enhanced features from the real clean ones. The two networks are jointly optimized to minimize the feature-mapping loss and simultaneously mini-maximize the discrimination loss. The distribution of the enhanced features is further pushed towards that of the clean features through this adversarial multi-task training. To achieve better performance on ASR task, senone-aware (SA) AFM is further proposed in which an acoustic model network is jointly trained with the feature-mapping and discriminator networks to optimize the senone classification loss in addition to the AFM losses. Evaluated on the CHiME-3 dataset, the proposed AFM achieves 16.95% and 5.27% relative word error rate (WER) improvements over the real noisy data and the feature-mapping baseline respectively and the SA-AFM achieves 9.85% relative WER improvement over the multi-conditional acoustic model.",0.0,4.275
1170,2024,4.2    3.45    4.35    5.1    ,Bilinear Similarity and Mahalanobis Distance with Metric Learning for Speaker Verification,"The cosine similarity and PLDA scoring methods are widely used in the stat-of-the-art i-vector model for speaker recognition. The cosine similarity scoring is simple and the PLDA can get better performance generally. We proposed the bilinear similarity and mahalanobis distance scorings with metric learning in the i-vector framework. The distance metric learning is to learn a distance metric from a given collection of pair of similar/dissimilar points that preserves the distance relation among the training data. We use the subspace similarity metric learning algorithms and the KISS (keep it simple and straightforward!) to learning the distance metrics, and the results can also be fused for better performance. The metric learning algorithm’s inputs are the similar/dissimilar pairs of samples, so we propose a new Euclidean distance-based way to form the training set, which select the similar pairs with farthest distance and dissimilar pairs with nearest distance. The results on NIST 2014 i-vector challenge show that the performance of the new scoring methods can get better performance and be simpler than the classical PLDA method.",1.0,4.275
1179,2049,3.8    3.2    5.1    5    ,End-to-End MMI Training and Voting for ASR,"State-of-the-art speech recognition systems follow the HMM-DNN hybrid approach. End-to-end systems, such as CTC, provide simplicity in the training procedure, and computational advantages in decoding. Yet, the performance of these systems still falls short compared to the hybrid approach. In this paper, we consider end-to-end training with the MMI objective function. Through experimentation, we demonstrate consistent improvements in performance compared to CTC, and additional improvements in robustness, decoding time, disk footprint and quality of alignments. The last enables the use of a straightforward ensemble method, inapplicable to CTC systems. The ensemble method yields a considerable reduction in the word error rate. These additional advantages on top of CTC systems, make the utilization of end-to-end training with MMI ideal in many cases.",1.0,4.275
62,1044,4.95    4.05    3.05    5.05    ,Training Utterance-level Embedding Networks for Speaker Identification and Verification,"Encoding speaker-specific characteristics from speech signals into fixed length vectors is a key component of speaker identification and verification systems. This paper presents a deep neural network architecture for speaker embedding models where similarity in embedded utterance vectors explicitly approximates the similarity in vocal patterns of speakers. The proposed architecture contains an additional speaker embedding lookup table to compute loss based on embedding similarities. Furthermore, we propose a new feature sampling method for data augmentation. Experimentation based on two databases demonstrates that our model is more effective at speaker identification and verification when compared to a fully connected classifier and an end-to-end verification model.",0.0,4.275
416,1739,4.9    4.85    4.15    3.2    ,Multi-talker Speech Separation Based on Permutation Invariant Training and Beamforming,"The recently proposed Permutation Invariant Training (PIT) technique addresses the label permutation problem for multi-talker speech separation. It has shown to be effective for the single-channel separation case. In this paper, we propose to extend the PIT-based technique to the multichannel multi-talker speech separation scenario. PIT is used to train a neural network that outputs masks for each separate speaker which is followed by a Minimum Variance Distortionless Response (MVDR) beamformer. The beamformer utilizes the spatial information of different speakers and alleviates the performance degradation due to misaligned labels. Experimental results show that the proposed PIT-MVDR-based technique leads to higher Signal-to-Distortion Ratios (SDRs) compared to the single-channel speech separation method when tested on two-speaker and three-speaker mixtures.",0.0,4.275
231,1350,5    2.8    6    3.3    ,Investigating the Role of L1 in Automatic Pronunciation Evaluation of L2 Speech,"Automatic pronunciation evaluation plays an important role in pronunciation training and second language education. This field draws heavily on concepts from automatic speech recognition (ASR) to quantify how close the pronunciation of non-native speech is to native-like pronunciation. However, it is known that the formation of accent is related to pronunciation patterns of both the target language (L2) and the speaker's first language (L1). In this paper, we propose to use two native speech acoustic models, one trained on L2 speech and the other trained on L1 speech. We develop two sets of measurements that can be extracted from two acoustic models given accented speech. A new utterance-level feature extraction scheme is used to convert these measurements into a fixed-dimension vector which is used as an input to a statistical model to predict the accentedness of a speaker. On a data set consisting of speakers from 4 different L1 backgrounds, we show that the proposed system yields improved correlation with human evaluators compared to systems only using the L2 acoustic model.",0.0,4.275
501,1921,3.35    4.35    5.15    ,Analysis of Sparse Representation Based Feature on Speech Mode Classification,"Traditional phone recognition systems are developed using read speech. But, in reality, the speech that needs to be processed by machine is not always in read mode. Therefore to handle the phone recognition in realistic scenarios, three broad modes of speech: read, conversation and extempore are considered in this study. The conversation mode includes informal communication in an unconstrained environment between two or more individuals. In the extempore mode, a person speaks with confidence without the help of notes. Read mode is a formal type of speech in a rigid environment. In this work, we have proposed a sparse based feature for speech mode classification. The effectiveness of sparse representation depends on the dictionary. Therefore, we have learned multiple overcomplete dictionaries by using parallel atom-update dictionary learning (PAU-DL) technique to capture the discrimination characteristics present in the considered speech modes. Further, sparse features correspond to the sequence of speech frames are derived using the learned dictionary by applying the orthogonal matching pursuit (OMP) algorithm. The proposed sparse features are evaluated on speech corpora consisting of six Indian languages by performing classification of speech modes. The results with the proposed sparse features outperform the standard spectral, excitation source and prosodic features.",0.0,4.283333333333333
1241,2240,5.1    5.25    5.15    1.65    ,Prosody Modifications for Question-Answering in Voice-Only Settings,"Many popular form factors of digital assistants --such as Amazon Echo, Apple
Homepod or Google Home-- enable the user to hold a conversation with the
assistant based only on the speech modality. The lack of a screen from which the
user can read text or watch supporting images or video presents unique
challenges. In order to satisfy the information need of a user, we believe that
the presentation of the answer needs to be optimized for such voice-only
interactions.

In this paper we propose a task of evaluating usefulness of
prosody modifications for the purpose of voice-only question answering.
We propose a crowd-sourcing setup where we evaluate the quality
of these modifications along multiple dimensions
corresponding to the informativeness, naturalness,
and ability of the user to identify the key part of the answer.

In addition, we propose a set of simple prosodic modifications that highlight
important parts of the answer using various acoustic cues.
%While clearly more work is needed on the prosody side,
Our initial results suggest that some of the modifications lead to better
comprehension at the expense of slightly degraded naturalness.",1.0,4.2875
149,1211,4.25    5.15    2.5    5.25    ,Data Augmentation Improves Recognition of Foreign Accented Speech,"Speech recognition of foreign accented speech remains a challenge to the state-of-the-art. The most common approach to address this scenario involves the collection and transcription of accented speech, and incorporating this into the training data. However, the amount of accented data is dwarfed by the amount of material from native speakers, limiting the impact of the additional material. In this work, we address this problem via data augmentation. We create modified copies of two accents, Latin American and Asian accented English speech with voice transformation, noise addition, and speed modification. We investigate both supervised and unsupervised approaches to using the accented data and associated augmentations. We find that all augmentations provide improvements, with the largest gains coming from speed modification, then voice transformation and noise addition providing the least improvement. The improvements from supervised adaptation and training accent specific models with the augmented data are substantial. Overall, we find speed modification to be a remarkably reliable data augmentation technique for improving recognition of foreign accented speech. Our strategies with associated augmentations provide WER reductions of up to 30% relative over a baseline trained with only the accented data.",0.0,4.2875
513,1948,4.6    2.75    4.9    4.9    ,Respiratory and Respiratory Muscular Control in JL1’S and JL2’S Text Reading Utilizing 4-RSTs and a Soft Respiratory Mask with a Two-Way Bulb,"To investigate how respiratory muscles and respiration are controlled during L1 and L2 text readings, experiments were conducted to acquire data on (A) upper- and lower- chest and upper- and lower- abdominal movements, utilizing four respiratory strain-gauge transducers (4-RSTs), and (B) inspiratory and expiratory volumes, utilizing a soft respiratory mask with a two-way bulb.  Speech were recorded for (A).  In addition, the subjects’ pulmonic vital capacities were measured. Five male students read two kinds of text materials in either Japanese (L1) or English (L2) five times. Thus, we acquired 200 data for chest and abdominal movements, 100 data for respiration, and 100 data for the recorded voice in addition to his pulmonic data and made quantitative and statistical analyses. Our findings were: (1) text reading speed was more stable in L1 than in L2, (2) the lower abdomen is controllable. This may indicate “stomach respiration,” (3) the inspiratory and expiratory controls during speech differed largely from those in quiet breathing, indicating acute active muscular movements, (4) the vital capacity volume did not sufficiently correlate with the subjects’ expiratory and inspiratory air volume during speech, and (5) a better English pronunciation might be supported by alternative control of inspiration and respiration.",0.0,4.2875
676,2400,2.7    4.25    5.2    5    ,Experiments with Training Corpora for Statistical Text-to-speech Systems.,"Common text-to-speech (TTS) systems rely on training data for modelling human speech. The quality of this data can range from professional voice actors recording hand-curated sentences in high-quality studio conditions, to found voice data representing arbitrary domains. For years, the unit selection technology dominant in the field required many hours of data that was expensive and time-consuming to collect. With the advancement of statistical methods of waveform generation, there have been experiments with more noisy and often much larger datasets, testing the inherent flexibility of such systems. In this paper we examine the relationship between training data and speech synthesis quality. We then hypothesise that statistical text-to-speech benefits from high acoustic quality corpora with high level of prosodic variation, but that beyond the first few hours of training data we do not observe quality gains. We then describe how we engineered a training dataset containing optimized distribution of features, and how these features were defined. Lastly, we present results from a series of evaluation tests. These confirm our hypothesis and show how a carefully engineered training corpus of a smaller size yields the same speech quality as much larger datasets, particularly for voices that use WaveNet.",0.0,4.2875
16,61,5.2    3.35    3.5    5.1    ,Voice Comparison and Rhythm: Behavioral Differences between Target and Non-target Comparisons,"It is common to see voice recordings being presented as a forensic trace in court. Generally, a forensic expert is asked to analyze both suspect and criminal's voice samples in order to indicate whether the evidence supports the prosecution  or defence hypotheses. This process is known as Forensic Voice Comparison. Since the emergence of the DNA typing model, the likelihood-ratio framework has become the new “golden standard” in forensic sciences. The LR not only supports one of the hypotheses but also quantifies the strength of its support. However, the LR accepts some practical limitations due to its estimation process itself. It is particularly true when Automatic Speaker Recognition systems are considered as they are outputting a score in all situations regardless of the case specific conditions. Indeed, several factors are not taken into account by the estimation process which put into question the validity and reliability of FVC. In our recent study, we showed the importance of the phonemic content and we highlighted interesting differences between inter-speakers effects and intra-speaker's ones. In this article, we wish to take our previous analysis a step farther and investigate the impact of rhythm variation separately on target and non-target trials.",0.0,4.2875
675,2398,5.05    3.3    6    2.8    ,The Use of Machine Learning and Phonetic Endophenotypes to Discover Genetic Variants Associated with Speech Sound Disorder,"Thirty-four (34) children with reported speech sound disorders (SSD) were recruited for a prior study, as well as 31 of their siblings, many of whom also showed SSD. Using data-clustering techniques, we assigned each child to one or more endophenotypes defined by the number and type of speech errors made on the GFTA-2. The genetic samples of 53 of the participants underwent whole exome sequencing. Variant alleles were detected, filtered, and annotated from the sequences, and the data were filtered using quality checks, annotations, and phenotypes. We then used Random Forest classification to search for associations between variants and endophenotypes. In this preliminary report, we highlight one promising association with a common variant of COMT, a dopamine metabolizer in the brain.",0.0,4.2875
267,1422,5    4.3    3.65    4.2    ,Latent Factor Analysis of Deep Bottleneck Features for Speaker Verification with Random Digit Strings,"Speaker verification with prompted random digit strings has been a challenging task due to very short test utterance. This work investigates how to
combine methods from deep bottleneck features (DBF) and latent factor analysis (LFA) to result in a new state-of-the-art approach for such task. In order
to provide a wider temporal context, a stacked DBF is extracted to replace the traditional MFCC feature in the derivation of the supervector representations and leads to a significant improvement for the speaker verification. The LFA is used to model these stacked DBFs in both digit and
utterance scales. Based on this learned LFA model, two kinds of supervector representations are extracted for utterance and local digits
respectively.
Since the strengths of DBF and LFA appear complementary, the combination significantly outperforms either of its components.
Experiments have been conducted on the public RSR2015 part III data corpus, the results showed that our approach can achieve 1.40\% EER and 1.55\% EER on
male and female respectively.",0.0,4.2875000000000005
43,1016,5.15    3.5    4.3    4.25    ,LSTM Based Attentive Fusion of Spectral and Prosodic Information for Keyword Spotting in Hindi Language,"In this paper, a DNN based keyword spotting framework, that utilizes both spectral as well as prosodic information present in the speech signal, is proposed. A DNN is first trained to learn a set of hierarchical non-linear transformation parameters that project the original spectral and prosodic feature vectors onto a feature space where the distance between similar syllable pairs is small and between dissimilar syllable pairs is large. These transformed features are then fused using an attention-based long short-term memory (LSTM) network. As a side result, a deep denoising autoencoder based fine-tuning technique
is used to improve the performance of sequence predictions. A sequence matching method called the sliding syllable protocol is also developed for keyword spotting. Syllable recognition and keyword spotting (KWS) experiments are conducted specifically for the Hindi language which is one of the widely spoken languages across the globe but is not addressed significantly by the speech processing community. The proposed framework indicates reasonable improvements when compared to baseline methods available in the literature.",0.0,4.3
208,1306,5.1    3.55    4.3    4.25    ,Acoustic-dependent Phonemic Transcription for Text-to-speech Synthesis,"Text-to-speech synthesis (TTS) purpose is to produce a speech signal from an input text. 
This implies the annotation of speech recordings with word and phonemic transcriptions.
The overall quality of TTS highly depends on the accuracy of phonemic transcriptions.
However, they are generally automatically produced by grapheme-to-phoneme conversion systems, which don't deal with speaker variability.
In this work, we explore ways to obtain signal-dependent phonemic transcriptions.
We investigate forced-alignment with enriched pronunciation lexicon and multimodal phonemic transcription.
We then apply our results on error detection of grapheme-to-phoneme conversion hypotheses in order to find where the phonemic transcriptions may be erroneous.
On a French TTS dataset, we show that we can detect up to 90.5% of errors of a state-of-the-art grapheme-to-phoneme conversion system by annotating less than 15.8% of phonemes as erroneous.
This can help a human annotator to correct most of grapheme-to-phoneme conversion errors without checking a lot of data.
In other words, our method can significantly reduce the cost of high quality TTS data creation.",0.0,4.3
92,1100,4.35    3.4    5.15    ,Play Duration Based User-Entity Affinity Modeling in Spoken Dialog System,"Multimedia streaming services over spoken dialog systems have become ubiquitous. User-entity affinity modeling is critical for the system to understand and disambiguate user intents and personalize user experiences. However, fully voice-based interaction demands quantification of novel behavioral cues to determine user affinities. In this work, we propose using play duration cues to learn a matrix factorization based collaborative filtering model. We first binarize play durations to obtain implicit positive and negative affinity labels. The Bayesian Personalized Ranking objective and learning algorithm are employed in our low-rank matrix factorization approach. To cope with uncertainties in the implicit affinity labels, we propose to apply a weighting function that emphasizes the importance of high confidence samples. Based on a large-scale database of Alexa music service records, we evaluate the affinity models by computing Spearman correlation between play durations and predicted affinities. Comparing different data utilizations and weighting functions, we find that employing both positive and negative affinity samples with a convex weighting function yields the best performance. Further analysis demonstrates the model's effectiveness on individual entity level and provides insights on the temporal dynamics of observed affinities.",0.0,4.3
576,2115,2.9    4.95    5.2    4.15    ,Analyzing Thai Tone Distribution through Functional Data Analysis,"This paper reports an analysis of tonal properties of Thai using a method based on functional data analysis on a large collection of TIMIT-like corpus. Both density estimation pooled across syllable-wise F0 contours and Functional Principle Component Analysis (FPCA) were applied. The results suggest that the simple two dimensional representation of tones: pitch target height and contour slope, is not able to capture context dependent variations of tonal contour within and across tone categories. In addition, the shape and timing of pitch target are also crucial both in differentiating tonal categories and explaining variations associated with syllable structure. The third and fourth dimension of the functional basis have been shown to be able to represent these higher-order properties. Thus FPCA can provide a compact yet interpretable low dimension representation for the tonal property of Thai. These findings are also helpful for understanding tone distribution properties and coarticulation.",0.0,4.300000000000001
454,1811,5.85    2.75    4.2    4.4    ,Speech Emotion Recognition Using Spectrogram & Phoneme Embedding,This paper proposes a speech emotion recognition method based on phoneme sequence and spectrogram. Both phoneme sequence and spectrogram retain emotion contents of speech which is missed if the speech is converted into text. We performed various experiments with different kinds of deep neural networks with phoneme and spectrogram as inputs. Three of those network architectures are presented here that helped to achieve better accuracy when compared to the state-of-the-art methods on benchmark dataset. A phoneme and spectrogram combined CNN model proved to be most accurate in recognizing emotions on IEMOCAP data. We achieved more than 4% increase in overall accuracy and average class accuracy as compared to the existing state-of-the-art methods.,0.0,4.300000000000001
297,1476,3.55    5.1    3.65    4.95    ,Homogeneity vs Heterogeneity in Indian English Prosody: Investigating Influences of L1 on F0 Range,"We present an exploratory analysis of several long-term distributional measures of f0 range in the speech of university-educated speakers of Indian English from four L1 backgrounds (Telugu, Tamil, Hindi and Bengali). The aim of this study is to investigate the degree of homogeneity in Indian English prosody and any similarities between the speakers’ productions in English and their L1. Following recent studies, we examine three aspects of f0 range: pitch level (relative height of habitual f0), pitch span and pitch dynamism. Overall, across varieties, pitch level measures reveal individual speaker differences and only weak L1 effects on max f0 and median f0. Some speakers show higher f0 in their L1 productions compared to their English productions. More robust patterns were found for pitch span and dynamism: for all measures (maximum-minimum f0, pitch dynamism quotient and standard deviation), significant differences were found between L1 and English (p<0.001) for Bengali and Telugu L1 speakers. The relative weakness of L1 effects would suggest a degree of homogeneity in Indian English, at least for the prosodic parameters investigated. Evidence of a shift in pitch span when talking in English, regardless of L1, further suggests a convergent speech variety.",0.0,4.3125
807,1075,2.2    5.1    4.95    5    ,Posterior Calibration for Multi-Class Paralinguistic Classification,"Computational paralinguistics is an area which contains diverse
classification tasks. In many cases the class distribution of these
tasks is highly inbalanced by nature, as the phenomena needed to
detect in human speech do not occur uniformly. To ignore this
inbalance, it is common to measure the efficiency of classification
approaches via the Unweighted Average Recall (UAR) metric in this
area. However, general classification methods such as Support-Vector
Machines (SVM) and Deep Neural Networks (DNNs) were shown to focus
on traditional classification accuracy, which might lead to a
suboptimal performance for inbalanced datasets. In this study we
show that by performing posterior calibration, this effect can be
countered and the UAR scores obtained might be improved: on two
multi-class paralinguistic datasets we achieved improvements of 4%
and 14% expressed in the form of relative error reduction, while we
also outperformed the traditional downsampling approach.",1.0,4.3125
72,1062,6    5.1    2.6    3.55    ,On the Usefulness of the Speech Phase Spectrum for Pitch Extraction,"Most frequency domain techniques for pitch extraction such as cepstrum, harmonic product spectrum (HPS) and summation residual harmonics (SRH) operate on the magnitude spectrum
and turn it into a function in which the fundamental frequency emerges as argmax. In this paper, we investigate the extension of these three techniques to the phase and group delay (GD) domains. Our extensions exploit the observation that the bin at which F (magnitude) becomes maximum, for some monotonically increasing function F , is equivalent to bin at which F (phase) has maximum negative slope and F (group delay) has the maximum value. To extract the pitch track from speech phase spectrum, these techniques were coupled with the source-filter model in the phase domain that we proposed in earlier publications and a novel voicing detection algorithm proposed here. The accuracy and robustness of the phase-based pitch extraction techniques are illustrated and compared with their magnitude-
based counterparts using six pitch evaluation metrics. On average, it is observed that the phase spectrum can be successfully employed in pitch tracking with comparable accuracy and robustness to the speech magnitude spectrum.",0.0,4.3125
155,1225,5.8    5.25    3.05    3.15    ,Vowels and Diphthongs in Hangzhou Wu Chinese Dialect,"This paper gives an acoustic phonetic description of the vowels and diphthongs in Hangzhou Wu Chinese dialect. Data from 12 speakers, 6 male and 6 female, were measured and analyzed. Monophthongs were investigated in CV, CVN, and CVC syllables; diphthongs were examined in terms of temporal organization, spectral properties, and dynamic aspects. Results suggest that falling diphthongs tend to have a single dynamic target, while rising diphthongs have two static spectral targets.",0.0,4.3125
59,1039,3.05    4.85    5.15    4.2    ,Cross-Lingual Multi-Task Neural Architecture for Spoken Language Understanding,"Cross-lingual spoken language understanding (SLU) systems traditionally require machine translation services for language portability and liberation from human supervision. However, restriction exists in parallel corpora and model architectures. Assuming reliable data are provided with human-supervision, which encourages non-parallel corpora and alleviate translation errors, this paper aims to explore cross-lingual knowledge transfer from multiple levels by taking advantage of neural architectures. We first investigate a joint model of slot filling and intent determination for SLU, which alleviates the out-of-vocabulary problem and explicitly models dependencies between output labels by combining character and word representations, bidirectional Long Short-Term Memory and conditional random fields together, while attention-based classifier is introduced for intent determination. Knowledge transfer is further operated on character-level and sequence-level, aiming to share morphological and phonological information between languages with similar alphabets by sharing character representations, and characterize the sequence with language-general and language-specific knowledge adaptively acquired by separate encoders. Experimental results on the MIT-Restaurant-Corpus and the ATIS corpora in different languages demonstrate the effectiveness of the proposed methods.",0.0,4.3125
666,2373,5.15    4.2    2.65    5.25    ,The Diphthongs of Formal Nigerian English: a Preliminary Acoustic Analysis,"AbstractPostcolonial varieties of English, used in countries such as Nigeria, India and Singapore, are subject to both local (“endonormative”) and external (“exonormative”) forces, the latter often in the form of British/American English. This gives rise to a stylistic continuum, where informal speech is more endonormatively oriented than formal/educated speech, which, nevertheless, is clearly distinguishable from British/American English. The formal end of the continuum is often regarded as the incipient local standard. 
Nigerian English (NigE) is the most widely spoken African variety of English, but empirical/quantitative descriptions are rare. In this pilot study, we present an acoustic analysis of eight phonological diphthongs produced in formal contexts by nine educated speakers of NigE with L1 Yoruba and drawn from the ICE Nigeria corpus.
Results show that the NigE speakers produced more monophthongal realisations of English phonological diphthongs than speakers of BrE do, as measured by trajectory length in F1-F2 space. Phonetically, most of these vowels can be considered monophthongs.
The results can be explained through two factors at work during the foundation phase of NigE: (1) historical L1 influence and (2) the native English input present in the country, which involved more monophthongal realisations of some phonological diphthongs than in present-day BrE.",0.0,4.3125
371,1637,4.8    4.2    4.2    4.05    ,Time Aggregation Operators for Multi-label Audio Event Detection,"In this paper, we present a state-of-the-art system for audio event detection. The labels on the training (and evaluation) data specify the set of events occurring in each audio clip, but neither the time spans nor the order in which they occur. Specifically, our task of weakly supervised learning is the “Detection and Classification of Acoustic Scenes and Events (DCASE) 2017” challenge. We use the winning entry in this challenge given by Xu et al. [9] as our starting point and identify several important modifications that allow us to improve on their results significantly. Our techniques pertain to aggregation and consolidation over time and frequency signals over a (temporal) sequence before decoding the labels. In general, our work is also relevant to other tasks involving learning from weak labeling of sequential data.",0.0,4.3125
349,1584,4.15    3.75    5    4.35    ,"An Exploration towards Joint Acoustic Modeling for Indian Languages: IIIT-H Submission for Low Resource Speech Recognition Challenge for Indian Languages, INTERSPEECH 2018","India being a multilingual society, a multilingual Automatic speech recognition system is widely appreciated. Despite different orthographies, Indian languages share same phonetic space. To exploit this property, a common phone set is employed in training joint acoustic models for multilingual ASR system. Three Indian languages namely Telugu, Tamil and, Gujarati have been considered for the study. This work studies the amenability of two different acoustic modeling approaches for training joint acoustic models. Sub-space Gaussian mixture models (SGMM), and recurrent neural networks (RNN) trained with connectionst temporal classification (CTC) objective function are explored for training joint acoustic models. From the experimental results, it can be observed joint acoustic models trained with RNN-CTC have performed better than SGMM based system even on 120 hours of data (approx 40 per language). The performance of joint acoustic model trained with RNN-CTC has performed better than the monolingual models, which can be attributed to efficient data sharing across the languages. Conditioning the joint model with language ID has a minimal advantage. In training RNN-CTC models sub-sampling the features by a factor of 2 has reduced the training times and has performed better.",0.0,4.3125
353,1597,4.95    4.6    3.6    4.1    ,Investigation on the Combination of Batch Normalization and Dropout in BLSTM-based Acoustic Modeling for ASR,"The Long Short-Term Memory (LSTM) architecture is a very special kind of recurrent neural network for modeling sequential data like speech. It has been widely used in the large scale acoustic model estimation recently and performs better than many other neural networks. Batch normalization(BN) is a good way to accelerate network training and improve the generalization performance of neural networks. However, applying batch normalization in the LSTM model is more complicated and challenging than in the feed-forward network. In this paper, we explored some novel approaches to add batch normalization to the LSTM model in bidirectional mode. Then we investigated some ways to combine the BN-BLSTM model with dropout, which is a traditional method to alleviate the overfitting problem in neural network training. We evaluated the proposed methods on several speech recognition tasks. Experiments show that the best performance on Switchboard task achieves 9.8% relative reduction on word error rate compared to the baseline, using the total Hub5’2000 evaluation dataset. Additionally, it is easy to implement and brings little extra computation.",0.0,4.3125
176,1252,3.15    4.8    5    ,ZCU-NTIS Speaker Diarization System for the DIHARD 2018 Challenge,"In this paper, we present the system developed by the team from the New Technologies for the Information Society (NTIS) research center of the University of West Bohemia, for the First DIHARD Speech Diarization Challenge. The base of our system follows the currently-standard approach of segmentation, i-vector extraction, clustering, and resegmentation. Here, we describe the modifications to the system which allowed us to apply it to data from a range of different domains. The main contribution to our achievement is an ANN-based domain classifier, which categorizes each conversation into one of the ten domains present in the development set. This classification determines the specific system configuration, such as the expected number of speakers and the stopping criterion for the hierarchical clustering. At the time of writing of this abstract, our best submission achieves a DER of 26.90% and an MI of 8.34 bits on the evaluation set (gold speech/nonspeech segmentation).",0.0,4.316666666666666
649,2331,4.85    5.1    3    ,Influences of Fundamental Oscillation on Speaker Identification in Vocalic Utterances by Humans and Computers,"We tested the influence of fundamental oscillation (fo) on human and machine speaker recognition performance in vocalic test utterances. In experiment I, we trained a Gaussian-Mixture model on 15 speakers (80 multi-word utterances each) and tested it with sustained vowel utterances (/a:/, /i:/ and /u:/) under six fo conditions, three changing (fall, rise, fall-rise) and three steady-state (high, mid, low). Results revealed better performance for the steady-state compared to the changing conditions and within the steady-state condition, performance was poorest for high fo. In experiment II, we tested 9 human listeners on a subset of 4 speakers from experiment I. They went through two training tasks (training 1: multi-word utterances; training 2: words). In the test, they recognized speakers based on the same vocalic utterances as in experiment I (for these 4 speakers). Results showed that performance was about equally high for the changing and steady-state vowels, however, in the steady-state condition performance was best for high fo vowels. The experiments suggest that (a) fo has an influence on the strength of speaker specific characteristics in vowels and (b) humans - compared to machines - pay attention to different acoustic information in vocalic utterances for speaker recognition.",0.0,4.316666666666666
715,2486,4.05    5.1    4.2    3.95    ,Semi-supervised and Active-learning Scenarios: Efficient Acoustic Model Refinement for a Low Resource Indian Language,"We address the problem of efficient acoustic-model refinement (continuous retraining) using semi-supervised and active learning for a low resource Indian language, wherein the low resource constraints are having i) a small labeled corpus from which to train a baseline `seed' acoustic model, and ii) a large training corpus without orthographic labeling or from which to perform a data selection for manual labeling at low costs. The proposed semi-supervised learning decodes the unlabeled large training corpus using the seed model and through various protocols, selects the decoded utterances with high reliability using confidence levels (that correlate to the WER of the decoded utterances) and iterative bootstrapping. The proposed active learning protocol uses confidence level based metric to select the decoded utterances from the large unlabeled corpus for further labeling. The semi-supervised learning protocols can offer a WER reduction, from a poorly trained seed model, by as much as 50% of the best WER-reduction realizable from the seed model's WER, if the large corpus were labeled and used for acoustic-model training. The active learning protocols allow that only 60% of the entire training corpus be manually labeled, to reach the same performance as the entire data.",0.0,4.324999999999999
130,1162,3.75    5.2    4.2    4.15    ,Active Learning for LF-MMI Trained Neural Networks in ASR,"This paper investigates how active learning (AL) effects the training of neural network acoustic models based on Lattice-free Maximum Mutual Information (LF-MMI) in automatic speech recognition (ASR). To fully exploit the most informative examples from fresh datasets, different data selection criterions based on the heterogeneous neural networks were studied. In particular, we examined the relationship among the transcription cost of human labeling, example informativeness and data selection criterions for active learning. As a comparison, we tried both semi-supervised training (SST) and active learning to improve the acoustic models. Experiments were performed for both the small-scale and large-scale ASR systems. Experimental results suggested that, our AL scheme can benefit much more from the fresh data than the SST in reducing the word error rate (WER).The AL yields 6～13% relative WER reduction against the baseline trained on a 4000 hours transcribed dataset, by only selecting 1.2K hrs informative utterances for human labeling via active learning.",0.0,4.324999999999999
329,1543,4.85    3    4.35    5.1    ,Compressing End-to-end ASR Networks by Tensor-Train Decomposition,"End-to-end deep learning has become a popular framework for automatic speech recognition (ASR) tasks, and it has proven itself to be a powerful solution. Unfortunately, network structures commonly have millions of parameters, and large computational resources are required to make this approach feasible for training and running such networks. Moreover, many applications still prefer lightweight models of ASR that can run efficiently on mobile or wearable devices. To address this challenge, we propose an approach that can reduce 
the number of ASR parameters. Specifically, we perform Tensor-Train decomposition on the weight matrix of the gated recurrent unit (TT-GRU) in the end-to-end ASR framework. Experimental results on LibriSpeech data reveal that the compressed ASR with TT-GRU can maintain good performance while greatly reducing the number of parameters.",0.0,4.324999999999999
705,2462,3.45    3.85    5.9    4.1    ,Exploring How Phone Classification Neural Networks Learn Phonetic Information by Visualising and Interpreting Bottleneck Features,"Neural networks have a reputation for being ``black boxes'', which it has been suggested that
techniques from user interface development, and visualisation in particular, could help lift.
In this paper, we explore 9-dimensional bottleneck features (BNFs) that have been shown in our earlier work to well represent speech in the context of speech recognition, and 2-dimensional BNFs directly extracted from bottleneck neural networks. The 9-dimensional BNFs obtained from a phone classification neural network are visualised in 2-dimensional spaces using linear discriminant analysis (LDA) and t-distributed stochastic neighbour embedding (t-SNE). The 2-dimensional BNF space is analysed with regard to phonetic features. A back-propagation method is used to create ``cardinal'' features for each phone under a particular neural network. Both the visualisations of 9-dimensional and 2-dimensional BNFs show distinctions between most phone categories. Particularly, the 2-dimensional BNF space seems to be a union of phonetic category related subspaces that preserve local structures within each subspace where the organisations of phones appear to correspond to phone production mechanisms. By applying LDA to the features of higher dimensional non-bottleneck layers, we observe a triangular pattern which may indicate that silence, friction and voicing are the three main properties learned by the neural networks.",0.0,4.325
278,1442,4.55    4.15    4.4    4.2    ,Prediction of Turn-taking Using Multitask Learning with Prediction of Backchannels and Fillers,"We address prediction of turn-taking considering related behaviors such as backchannels and fillers. Backchannels are used by listeners to acknowledge that the current speaker can hold the turn. On the other hand, fillers are used by prospective speakers to indicate a will to take a turn.  We propose a turn-taking model based on multitask learning in conjunction with prediction of backchannels and fillers.  The multitask learning of LSTM neural networks shared by these tasks allows for efficient and generalized learning, and thus improves prediction accuracy.  Evaluations with two kinds of dialogue corpora of human-robot interaction demonstrate that the proposed multitask learning scheme outperforms the conventional single-task learning.",0.0,4.325
52,1027,4.45    4.6    3.1    5.15    ,Postfiltering Using Log-Magnitude Spectrum for Speech and Audio Coding,"Advanced coding algorithms yield high quality signals with good coding efficiency within their target bit-rate ranges, but their performance suffer outside the target range. At lower bitrates, the degradation in performance is because the decoded signals are sparse, which gives a perceptually muffled and distorted characteristic to the signal. Standard codecs reduce such distortions by applying noise filling and post-filtering methods. In this paper, we propose a post-processing
method based on modeling the inherent time-frequency correlation in the log-magnitude spectrum. The goal is to improve the perceptual SNR of the decoded signals and, to reduce the distortions caused by signal sparsity. Objective measures show an average improvement of 1.5 dB for input perceptual SNR in range 4 to 18 dB. The improvement is especially prominent in components which had been quantized to zero.",0.0,4.325
568,2084,3.65    4.35    4.35    4.95    ,Analyzing Vocal Tract Movements During Speech Accommodation,"When two people engage in verbal interaction, they tend to accommodate on a variety of linguistic levels. Although recent attention has focused on to the acoustic characteristics of convergence in speech, the underlying articulatory mechanisms remain to be explored. Using 3D electromagnetic articulography (EMA), we simultaneously recorded articulatory movements in two speakers engaged in an interactive verbal game, the domino task. In this task, the two speakers take turn in chaining bi-syllabic words according to a rhyming rule. By using a robust speaker identification strategy, we identified for which specific words speakers converged or diverged. Then, we explored the different vocal tract features characterizing speech accommodation. Our results suggest that tongue movements tend to slow down during convergence whereas maximal jaw opening during convergence and divergence differs depending on syllable position.",0.0,4.325
359,1608,4.95    4.2    3.15    5    ,Avoiding Speaker Overfitting in End-to-End DNNs Using Raw Waveform for Text-Independent Speaker Verification,"In this research, we propose a novel raw waveform end-to-end DNNs for text-independent speaker verification. For speaker verification, many studies utilize the speaker embedding scheme, which trains deep neural networks as speaker identifiers to extract speaker features. However, this scheme has an intrinsic limitation in which the speaker feature, trained to classify only known speakers, is required to represent the identity of unknown speakers. Owing to this mismatch, speaker embedding systems tend to well generalize towards unseen utterances from known speakers, but are overfitted to known speakers. This phenomenon is referred to as speaker overfitting. In this paper, we investigated regularization techniques, a multi-step training scheme, and a residual connection with pooling layers in the perspective of mitigating speaker overfitting which lead to considerable performance improvements. Technique effectiveness is evaluated using the VoxCeleb dataset, which comprises over 1,200 speakers from various uncontrolled environments. To the best of our knowledge, we are the first to verify the success of 
end-to-end DNNs directly using raw waveforms in text-independent scenario. It shows an equal error rate of 7.4%, which is lower than i-vector/probabilistic linear discriminant analysis and end-to-end DNNs that use spectrograms.",0.0,4.325
160,1233,3.9    4.2    4.95    4.25    ,Age-related Effects on Sensorimotor Control of Speech Production,"The current study investigates the effect of aging on the speech motor control, more specifically the labial and lingual system. We provide an acoustic and articulatory analysis comparing younger (20-30 years old) and older speakers (70-80 years old) of German, all of them recorded with electromagnetic articulography. We analyzed target words in contrastive focus condition.
In the acoustic domain, target syllables were not prolonged in the productions of the older speakers. However, when looking at the articulatory domain, we found systematic modifications: Especially vocalic gestures, requiring movements of the lingual system, showed slower peak velocities for older subjects. Furthermore, we found age-related effects on the symmetry of articulatory gestures. Older subjects produce longer deceleration and shorter acceleration phases leading to a strong asymmetry of the movement components. Variability between and across speakers were considerably higher in the group of older speakers compared to younger ones.
Our results on age-related effects on speech motor control are comparable with those from general motor control, where e.g. prolonged deceleration phases are an indicator for a decrease in sensory feedback control.",0.0,4.325
478,1864,5.1    3.85    4.8    3.55    ,Improved Accented Speech Recognition Using Accent Embeddings and Multi-task Learning,"One of the major remaining challenges in modern automatic speech recognition (ASR) systems for English is to be able to handle speech from users with a diverse set of accents. ASR systems that are trained on speech from multiple English accents still underperform when confronted with a new speech accent. In this work, we explore how to use accent embeddings and multi-task learning to improve speech recognition for accented speech. We propose a multi-task architecture that jointly learns an accent classifier and a multi-accent acoustic model. We also consider augmenting the speech input with accent information in the form of embeddings extracted by a separate network. These techniques together give significant relative performance improvements of 15% and 10% over a multi-accent baseline system on test sets containing seen and unseen accents, respectively.",0.0,4.325
88,1096,3.4    5.25    4.35    4.3    ,Punctuation Prediction Model for Conversational Speech,"An ASR system usually does not predict any punctuation or capitalization. Lack of punctuation causes problems in result presentation and confuses both the human reader and off-the-shelf natural language processing algorithms. To overcome these limitations, we train two variants of Deep Neural Network (DNN) sequence labelling models - a Bidirectional Long Short-Term Memory (BLSTM) and a Convolutional Neural Network (CNN), to predict the punctuation. The models are trained on the Fisher corpus which includes punctuation annotation. In our experiments, we combine time-aligned and punctuated Fisher corpus transcripts using a sequence alignment algorithm. The neural networks are trained on Common Web Crawl GloVe embedding of the words in Fisher transcripts aligned with conversation side indicators and word time infomation. The CNNs yield a better precision and BLSTMs tend to have better recall. While BLSTMs make fewer mistakes overall, the punctuation predicted by the CNN is more accurate - especially in the case of question marks. Our results constitute significant evidence that the distribution of words in time, as well as pre-trained embeddings, can be useful in the punctuation prediction task.",0.0,4.325
247,1378,4.85    4.9    4.95    2.6    ,Leveraging Native Language Information for Improved Accented Speech Recognition,"Recognition of accented speech is a long-standing challenge for ASR systems, given the increasing worldwide population of bi-lingual speakers with English as their second language. If we consider foreign-accented speech as an interpolation of the native language(L1) and English(L2), using a model that can simultaneously recognize both languages would perform better at the acoustic level for accented speech. In this study, we explore how an end-to-end recurrent neural network (RNN) trained system with English and native languages (Spanish and Indian languages) could leverage the data of native languages to perform better for accented English speech. To this end, we examine using pre-training with native languages, as well as multitask learning in which the main task is trained with native English data and the secondary task is trained with Spanish or Indian Languages. We show that the multitask setting performs better than the former approach. We suggest a new setting for multitask learning in which the secondary task is trained with both English and the native language, using the same output set. This proposed scenario yields better performance than the first setting which provides +11.95% and +17.55% character error rate (CER) gain over the baseline, for Hispanic and Indian accents, respectively.",0.0,4.325
404,1711,5.7    4.35    3.05    4.2    ,Multilingual Neural Network Acoustic Modelling for ASR of Under-Resourced English-isiZulu Code-Switched Speech,"Although isiZulu speakers code-switch with English as a matter of course, extremely little appropriate data is available for acoustic modelling. Recently, a small five-language corpus of code-switched South African soap opera speech was compiled. We used this corpus to evaluate the application of multilingual neural network acoustic modelling to English-isiZulu code-switched speech recognition. Our aim was to determine whether English-isiZulu speech recognition accuracy can be improved by incorporating three other language pairs in the corpus: English-isiXhosa, English-Setswana and English-Sesotho. Since isiXhosa, like isiZulu, belongs to the Nguni language family, while Setswana and Sesotho belong to the more distant Sotho family, we could also investigate the merits of additional data from within and across language groups. Our experiments using both fully connected DNN and TDNN-LSTM architectures show that English-isiZulu speech recognition accuracy as well as language identification after code-switching is improved more by the incorporation of English-isiXhosa data than by the incorporation of the other language pairs. However additional data from the more distant language group remained beneficial, and the best overall performance was always achieved with a multilingual neural network trained on all four language pairs.",0.0,4.325
200,1295,4.2    4.25    4.9    3.95    ,Learning Structured Dictionaries for Exemplar-based Voice Conversion,"Incorporating phonetic information has been shown to improve the performance of exemplar-based voice conversion. A standard approach is to build a phonetically structured dictionary, where exemplars are categorized into sub-dictionaries according to their phoneme labels. However, acquiring phoneme labels can be expensive, and the phoneme labels can have inaccuracies. The latter problem becomes more salient when the speakers are non-native speakers. This paper presents an iterative dictionary-learning algorithm that avoids the need for phoneme labels, and instead learns the structured dictionaries in an unsupervised fashion. At each iteration, two steps are alternatively performed: cluster update and dictionary update. In the cluster update step, each training frame is assigned to a cluster whose sub-dictionary represents it with the lowest residual. In the dictionary update step, the sub-dictionary for a cluster is updated using all the speech frames in the cluster. We evaluate the proposed algorithm through objective and subjective experiments on a new corpus of non-native English speech. Compared to previous studies, the proposed algorithm improves the acoustic quality of voice-converted speech while retaining the target speaker’s identity.",0.0,4.325
591,2156,4.2    5.15    3.65    ,Speech Emotion Recognition by Combining Amplitude and Phase Information Using Convolutional Neural Network,"Previous studies of speech emotion recognition utilize convolutional neural network (CNN) directly on amplitude spectrogram to extract features. CNN combines with bidirectional long short term memory (BLSTM) has become the state-of-the-art model. However, phase information has been ignored in this model. The importance of phase information in speech processing field is gathering attention. In this paper, we propose feature extraction of amplitude spectrogram and phase information using CNN for speech emotion recognition. The modified group delay cepstral coefficient (MGDCC) and relative phase are used as phase information. Firstly, we analyze the influence of phase information on speech emotion recognition. Then we design a CNN-based feature representation using amplitude and phase information. Finally,  experiments were conducted on EmoDB to validate the effectiveness of phase information. Integrating amplitude spectrogram with phase information, the relative emotion error recognition rates are reduced by over 33% in comparison with using only amplitude-based feature.",0.0,4.333333333333334
360,1610,4.65    4.05    5.85    2.8    ,Attention-based Sequence Classification for Affect Detection,"This paper presents the Cogito submission to the Interspeech Computational Paralinguistics Challenge (ComParE), for the second sub-challenge.  The aim of this second sub-challenge is to recognize self-assessed affect from short clips of speech-containing audio data.  We adopt a sequence classification-based approach where we use a long-short term memory (LSTM)
network for modeling the evolution of low-level spectral coefficients, with added attention mechanism to emphasize salient regions of the audio clip. Additionally to deal with the underrepresentation of the negative valence class we use a combination of mitigation strategies including oversampling and loss function weighting. Our experiments demonstrate improvements in detection accuracy when including the attention mechanism and class balancing strategies in combination, with the best models outperforming the best single challenge baseline model.",0.0,4.3374999999999995
218,1326,4.15    3.6    5.15    4.45    ,Low-Resource Speech-to-Text Translation,"Speech-to-text translation has many potential applications for low-resource languages, but the typical approach of cascading speech recognition with machine translation is often impossible, since the transcripts needed to train a speech recognizer are usually not available for low-resource languages. Recent work has found that neural encoder-decoder models can learn to directly translate foreign speech in high-resource scenarios, without the need for intermediate transcription. We investigate whether this approach also works in settings where both data and computation are limited. To make the approach efficient, we make several architectural changes, including a change from character-level to word-level decoding. We find that this choice yields crucial speed improvements that allow us to train with fewer computational resources, yet still performs well on frequent words. We explore models trained on between 20 and 160 hours of data, and find that although models trained on less data have considerably lower BLEU scores, they can still predict words with relatively high precision and recall around 50% for a model trained on 50 hours of data, versus around 60% for the full 160 hour model. Thus, they may still be useful for some low-resource scenarios.",0.0,4.3375
662,2364,4.95    3.45    5.15    3.8    ,Learning Word Embeddings: Unsupervised Methods for Fixed-size Representations of Variable-length Speech Segments,"Fixed-length embeddings of words are very useful for a variety of tasks in speech and language processing. Here we systematically explore two methods of computing fixed-length embeddings for variable-length sequences. We evaluate their susceptibility to phonetic and speaker-specific variability on English, a high resource language, and Xitsonga, a low resource language, using two evaluation metrics: ABX word discrimination and ROC-AUC on same-different phoneme n-grams. We show that a simple downsampling method supplemented with length information can outperform the variable-length input feature representation on both evaluations. Recurrent autoencoders, trained without supervision, can yield even better results at the expense of increased computational complexity.",0.0,4.3375
295,1473,5    4.1    3.85    4.4    ,Detection of Replay-Spoofing Attacks Using Frequency Modulation Features,"Prevention of malicious spoofing attacks is currently acknowledged as a priority area of investigation for the deployment of automatic speaker verification systems. Various features of speech signals have been used to fight counterfeit attacks. Among the different spoofing attack variants, replay attacks pose a significant threat as they do not require any expert knowledge and are difficult to detect. This paper proposes the use of a spectral centroid based frequency modulation (FM) features that we term spectral centroid deviation (SCD) for replay attack detection. Spectral centroid frequency (SCF) and spectral centroid magnitude coefficient (SCMC) features extracted from the same front-end as SCD are also investigated as complementary features. Evaluations on the ASVspoof 2017 dataset indicate that the proposed SCD features with a Gaussian Mixture Model (GMM) back-end is highly capable of discriminating genuine from replay spoofed speech, providing an equal error rate improvement greater than 60% relative to the CQCC baseline system from the ASVspoof 2017 challenge. Interestingly, experiments also reveal that the proposed SCD features exhibit an increased variance for replay spoofed speech relative to genuine speech, particularly for the lowest and highest frequency subbands.",0.0,4.3375
275,1436,5    3.35    5.75    3.25    ,Spoken Keyword Detection Using Joint DTW-CNN,"A method to detect spoken keywords in a given
speech utterance is proposed, called as joint Dynamic Time
Warping (DTW)- Convolution Neural Network (CNN). It is a
combination of DTW approach with a strong classifier like CNN.
Both these methods have independently shown significant results
in solving problems related to optimal sequence alignment and
object recognition, respectively. The proposed method modifies
the original DTW formulation and converts the warping matrix
into a gray scale image. A CNN is trained on these images to
classify the presence or absence of keyword by identifying the
texture of warping matrix. The TIMIT corpus has been used
for conducting experiments and our method shows significant
improvement over other existing techniques.",0.0,4.3375
400,1702,4.45    4.1    4.9    4.3    4    ,Novel Linear Frequency Residual Cepstral Features for Replay Attack Detection,"Replay attack poses the most difficult challenge for the development of countermeasures for spoofed speech detection (SSD)
system. Earlier researchers mainly used vocal tract-based (segmental) information for replay detection. However, during replay, excitation source-based information also gets affected (in
particular, degradation in pitch source harmonics at higher frequency regions) due to recording environment and replay devices. Hence, in addition to the vocal tract-based system information, we have also explored the excitation source-based
informations for SSD. In particular, we have used Linear Frequency Residual Cepstral Coefficients (LFRCC) for replay detection. The objective of this paper is to explore possible complementary excitation (glottal) source information present in
the Linear Prediction residual-based features. Experiments performed on the ASV Spoof 2017 Challenge database with Gaussian Mixture Model (GMM) and Convolutional Neural Network (CNN) classifiers. When we combined the source and
system-based information, we obtained on an average 28.77%
and 42.72% relative decrease in Equal Error Rate (EER) on development and evaluation set, respectively. Furthermore, when
we perform score-level fusion of feature sets (for a fixed classifier) followed by a classifier-level fusion of GMM and CNN
(for a fixed feature set), we obtained reduced EER of 2.40% and
9.06% on dev and eval set, respectively.",0.0,4.35
245,1376,4.2    5    4.25    3.95    ,Phone Recognition Using a Non-Linear Manifold with Broad Phone Class Dependent DNNs,"Although it is generally accepted that different broad phone classes (BPCs) have different production mechanisms and are better described by different types of features, most automatic speech recognition (ASR) systems use the same features and decision criteria for all phones. Motivated by this observation, this paper proposes a two-level DNN structure, referred to as a BPC-DNN, inspired by the notion of a topological manifold. In the first level, several small separate BPC-dependent DNNs are applied to different broad phonetic classes, and in the second level the outputs of these DNNs are fused to obtain senone-dependent posterior probabilities, which can be used for frame level classification or integrated into Viterbi decoding for phone recognition. In a previous paper using this approach we reported improved frame classification accuracy on the TIMIT corpus compared with a conventional DNN. The contribution of the present paper is to demonstrate that this advantage extends to full phone recognition. Our most recent results show that the BPC-DNN achieves reductions in error rate relative to a conventional DNN of 16% and 8% for frame classification and phone recognition, respectively.",0.0,4.35
926,1409,5.1    3.75    2.6    5.95    ,DialEdit: a New Corpus on Spoken Conversational Image Editing,"We present a spoken dialogue corpus on ""conversational image editing"", where people edit an image interactively by giving spoken language instructions. Our corpus contains spoken conversations between two human participants: users requesting changes to images and experts performing these modifications in real time. We describe our data collection process and novel dialogue act labeling scheme. Our annotation scheme consists of 26 dialogue act labels covering instructions, requests, and feedback. The corpus supports research and development in areas such as incremental intent recognition, visual reference resolution, image-grounded dialogue modeling, dialogue state tracking, and user modeling.",1.0,4.35
69,1058,4.15    4.85    5.15    3.25    ,An End-to-End Text-Independent Speaker Identification System on Short Utterances,"In the field of speaker recognition, text-independent speaker identification on short utterances is still a challenging task, since it is rather tough to extract a robust and dicriminative speaker feature in short duration condition. This paper explores an end-to-end speaker identification system, which maps utterances to a speaker identity subspace where the similarity of speakers can be measured by Euclidean distance. To be specific, we apply GRU architectures to extract utterance-level feature. Then it is assumed that one’s various utterances can be viewed as transformations of a single object in an ideal speaker identity subspace. Based on this assumption, the ResCNN architecture is utilized to model the transformation, and the whole system is jointly optimized by speaker identity subspace loss. Experimental results demonstrate the effectiveness of our proposed system and superiority over pervious methods. For example, the GRU learned feature reduces the equal error rate by 27.53% relatively and the speaker identity subspace loss further brings 7.22% relative reduction compared to softmax loss.",0.0,4.35
494,1905,4.05    4.15    4.8    4.4    ,Robust Acoustic Event Classification Using Bag-of-Visual-Words,"This paper presents a novel Bag-of-Visual-Words (BoVW) approach, to represent the grayscale spectrograms of acoustic events. Such, BoVW representations are referred as histograms of visual features, used for Acoustic Event Classification (AEC). Further, Chi-square distance between histograms of visual features evaluated, which generates kernel to Support Vector Machines (Chi-square SVM) classifier. Evaluation of the proposed histograms of visual features together with Chi-square SVM classifier is conducted on different categories of acoustic events from UPC-TALP corpora in clean and different noise  conditions. Results show that proposed approach is more robust to noise and achieves improved recognition accuracy compared to other methods.",0.0,4.35
735,2537,5.1    5.2    3.65    3.45    ,Towards Automatic Speech Identification from Vocal Tract Shape Dynamics in Real-time MRI,"Vocal tract configurations play a vital role in generating distinguishable speech sounds, by modulating the airflow and creating different resonant cavities in speech production. They contain abundant information that can be utilized to better understand the underlying speech production mechanism. As a step towards automatic mapping of vocal tract shape geometry to acoustics, this paper employs effective video action recognition techniques, like Long-term Recurrent Convolutional Networks (LRCN) models, to identify different vowel-consonant-vowel (VCV) sequences from dynamic shaping of the vocal tract. Such a model typically combines a CNN based deep hierarchical visual feature extractor with Recurrent Networks, that ideally makes the network spatio-temporally deep enough to learn the sequential dynamics of a short video clip for video classification tasks. We use a database consisting of 2D real-time MRI of vocal tract shaping during VCV utterances by 17 speakers. The comparative performances of this class of algorithms under various parameter settings and for various classification tasks are discussed. Interestingly, the results show a marked difference in the model performance in the context of speech classification with respect to generic sequence or video classification tasks.",0.0,4.3500000000000005
668,2381,5    4.1    2.7    5.1    4.9    ,Studying Vowel Variation in French-Algerian Arabic Code-switched Speech,"Algerian Arabic-French bilinguals show phonetic variation with respect to vowel timber in both their languages. Our study aims to identify vowel variants frequently produced by Algerian Arabic-French bilinguals. To that end, the speech corpus FACST, containing French and Algerian Arabic code-switched speech, was analyzed. A second corpus with native French speakers (NCCFr) was used to provide a reference baseline and to compare vowel variants across the two speaker groups.
Three experiments were carried out: first, the French speech of both corpora was aligned with a French acoustic model including parallel vowel variants in its pronunciation dictionary. Second, the Arabic speech was aligned using using the same acoustic model as in the first experiment including parallel vowel variants in its dictionary. Finally, we tested whether peripheral vowels in Algerian Arabic-French bilinguals are more often centralized than in French native speech by allowing schwa as a competing variant.
The results show that, French natives and Algerian Arabic-French bilinguals have globally a comparable amount of vowel variation in French. However, French natives have stable high vowels whereas bilinguals tend to produce stable low and back vowels. In the centralization experiment, Algerian bilinguals particularly favor the centralization of mid, open and back vowels.",0.0,4.359999999999999
1060,1745,5.1    5.05    3.4    3.9    ,Feature Identification and Automated Detection of Articulation Accuracy,"Early detection of misarticulations by children with speech disorders is essential to facilitate normal language development. Automated detection can provide easy access to parents and caregivers, but training a traditional automated speech recognition system requires access to large datasets of misarticulated words, which are rare.  With access to a small set of recordings by children, with and without speech delay disorder that took the Goldman-Fristoe Test of Articulation (GFTA), we developed an automated misarticulation recognition system using elastic net regularization. Using this technique, we were not only able to  accurately identify instances of misarticulations, but also select features that help discriminate between the various classes of mispronunciations. Our modeling approach used only correctly pronounced data for training and was able to recognize with high accuracy misarticulations as well as previously unseen correct pronunciations. In addition, due to use of the regularization technique we isolated features that characterize the subtle differences between normal and misarticulated phone classes and found this subset of features to correlate with known acoustic-phonetic characteristics of the various consonants.",1.0,4.3625
197,1291,4.75    5.85    3.7    3.15    ,Multi-resolution Gammachirp Envelope Distortion Index for Intelligibility Prediction of Noisy Speech,A multi-resolution version of the gammachirp envelope distortion index (mr-GEDI) is proposed for the intelligibility prediction of noisy speech processed using speech enhancement algorithms. The proposed model calculates the short-time signal-to-distortion ratio in the temporal envelope modulation extracted from the output of the gammachirp auditory filterbank. The predictions were compared with human subjective results for various signal-to-noise ratio conditions with pink and babble noise. The mr-GEDI predicts the intelligibility curves better than the hearing-aid speech perception index (HASPI).,0.0,4.3625
6,43,5.9    5.9    2.85    2.8    ,Frequency Domain Variants of Velvet Noise and Their Application to Speech Processing and Synthesis,"We propose a new excitation source signal for VOCODERs and an all-pass impulse response for post-processing of synthetic sounds and pre-processing of natural sounds for data-augmentation. The proposed signals are variants of velvet noise, which is a sparse discrete signal consisting of a few non-zero (1 or -1) elements and sounds smoother than Gaussian white noise. One of the proposed variants, FVN (Frequency domain Velvet Noise) applies the procedure to generate a velvet noise on the cyclic frequency domain of DFT (Discrete Fourier Transform). Then, by smoothing the generated signal to design the phase of an all-pass filter followed by inverse Fourier transform yields the proposed FVN. Temporally variable frequency weighted mixing of FVN generated by frozen and shuffled random number provides a unified excitation signal which can span from random noise to a repetitive pulse train. The other variant, which is an all-pass impulse response, significantly reduces “buzzy” impression of VOCODER output by filtering. Finally, we will discuss applications of the proposed signal for watermarking and psychoacoustic research.",0.0,4.3625
540,2022,5.05    5.1    4.15    3.2    ,Comparison of BLSTM-Layer-Specific Affine Transformations for Speaker Adaptation,"Bidirectional Long Short-Term Memory (BLSTM) Recurrent Neural Networks (RNN) acoustic models have demonstrated superior performance over Deep feed-forward Neural Networks (DNN) models in speech recognition and many other tasks. Although, a lot of work has been reported on DNN model adaptation, very little has been done on BLSTM model adaptation.

This work presents a systematic study on the adaptation of BLSTM acoustic models by means of learning affine transformations within the neural network on small amounts of unsupervised adaptation data.

Through a series of experiments on two major speech recognition benchmarks (Switchboard and CHiME-4), we investigate the significance of the position of the transformation in a BLSTM Network using a separate transformation for the forward- and backward-direction. We observe that applying affine transformations result in consistent relative word error rate reductions ranging from 6% to 11% depending on the task and the degree of mismatch between training and test data.",0.0,4.375
60,1042,5.1    4.75    4.75    2.9    ,Spoofing Detection Using Adaptive Weighting Framework and Clustering Analysis,"Security of Automatic Speaker Verification (ASV) systems against imposters are now focusing on anti-spoofing countermeasures. Under the severe threat of various speech spoofing techniques, ASV systems can easily be 'fooled' by spoofed speech which sounds as real as human-beings. As two effective solutions, the Constant Q Cepstral Coefficients (CQCC) and the Scattering Cepstral Coefficients (SCC) perform well on the detection of artificial speech signals, especially for attacks from speech synthesis (SS) and voice conversion (VC). However, for spoofing subsets generated by different approaches, a low Equal Error Rate (EER) cannot be maintained. In this paper, an adaptive weighting based standalone detector is proposed to address the selective detection degradation. The clustering property of the genuine and the spoofed subsets are analysed for the selection of suitable weighting factors. With a Gaussian Mixture Model (GMM) classifier as the back-end, the proposed detector is evaluated on the ASVspoof 2015 database. The EERs of 0.01% and 0.20% are obtained on the known and the unknown attacks, respectively. This presents an essential complementation between the CQCC and the SCC and also promotes the future research on generalized countermeasures.",0.0,4.375
581,2124,4.9    3.2    3.5    5.9    ,Investigating Speech Features for Continuous Turn-Taking Prediction Using LSTMs,"For spoken dialog systems to conduct fluid conversational interactions with users, the systems must be sensitive to turn-taking cues produced by a user. Models should be designed so that effective decisions can be made as to when it is appropriate, or not, for the system to speak. Traditional end-of-turn models, where decisions are made at utterance end-points, are limited in their ability to model fast turn-switches and overlap. A more flexible approach is to model turn-taking in a continuous manner using RNNs, where the system predicts speech probability scores for discrete frames within a future window. The continuous predictions represent generalized turn-taking behaviors observed in the training data and can be applied to make decisions that are not just limited to end-of-turn detection. In this paper, we investigate optimal speech-related feature sets for making predictions at pauses and overlaps in conversation. We find that while traditional acoustic features perform well, part-of-speech features generally perform worse than word features. We show that our current models outperform previously reported baselines.",0.0,4.375
455,1812,5.55    1.5    4.8    5.65    ,Investigating the Effect of Face and Voice Familiarity in Recognising Speech in Noise,"The speech of a familiar talker is better recognized in noise than an unfamiliar one, suggesting that listeners access talker-specific models to assist with degraded input. This study investigated whether a talker model could be accessed by presenting the face of a talker. In the experiment, participants were trained in recognizing three talkers’ faces and voices to ceiling-level. Participants were then given a speech in noise recognition task consisting of four talker conditions: familiar face then familiar voice; unfamiliar face then familiar voice, familiar face then unfamiliar voice; and unfamiliar face then unfamiliar voice. A talker familiarity effect was found, i.e., speech perception was more accurate in the familiar face and familiar voice condition than all other ones. A familiar voice did not produce a talker familiarity effect when paired with an unfamiliar face. The familiar face and unfamiliar voice condition had the poorest performance, indicating that pairing a familiar face and unfamiliar voice had a disruptive effect. The results suggest that listeners develop a talker model that includes details of both the voice and the face; and that accessing this model can in some circumstances be wholly determined by face cues.",0.0,4.375
206,1302,4.9    4.2    3.4    5    ,BUT System for Low Resource Indian Language ASR,"This paper describes the BUT `Jilebi' team's speech recognition systems created for the 2018 low resource speech recognition challenge for Indian languages. We investigate modifications of multilingual time-delay neural network (TDNN) architectures with transfer learning and compare them to bi-directional residual memory networks (BRMN) and bi-directional LSTM. Our best submission based on system combination achieved word error rates of 13.92\% (Tamil), 14.71\% (Telugu) and 14.06\% (Gujarati).  We present the details of submitted systems and also the post-evaluation analysis done for lexicon discovery using unsupervised word segmentation.",0.0,4.375
427,1755,4.15    4.35    4.15    4.85    ,Automatic Question Detection from Acoustic and Phonetic Features Using Feature-wise Pre-training,"This paper presents a novel question detection method from natural speech using acoustic and phonetic features. The conventional methods based on Recurrent Neural Networks (RNNs) use only acoustic features. However, lexical cues are essential to identify some questions such as declarative questions. To this end we propose a new RNN-based question detection model which utilizes both acoustic and lexical information. Phonetic features which are suitable to describe interrogative cues are used as lexical information. Furthermore, we also propose a new training framework named feature-wise pre-training (FP) to combine the acoustic and phonetic features effectively. FP attempts to acquire interrogative cues in individual features instead of the combination of the features, which makes the model training more stable. The estimation models of the interrogatives are then integrated and fine-tuning is applied to obtain the unified comprehensive model. Experiments show that the proposed method offers better performance than the conventional benchmarks.",0.0,4.375
952,1479,5    3.45    5.1    3.95    ,WaveNet MH-SRU: Deep and Wide Multiple-history Simple Recurrent Unit for Speech Recognition,"Recently we proposed the high-order multiple-history recurrent neural network (MH-RNN) for speech recognition. The high-order connections to an MH-RNN cell come from signals similar to the original one but running with progressively longer time lags. Each time-lagged signal keeps a slightly different history of the input, and the history ensemble helps improve the model's robustness against mis-labeling or mis-alignments in the training targets. When an MH-RNN cell is unfolded in time, it becomes a deep neural network with wider and wider layers at the bottom. Although a wider MH-RNN is more resilient to noisy data, we notice that when it is too wide, it is under-trained and the increased histories are not effectively utilized. This paper proposes replacing each unfolded MH-RNN layer by a WaveNet block which makes use of dilated causal convolutions to provide a wider receptive field that can capture wider history contexts more effectively. Meanwhile simple recurrent units (SRUs) are introduced to parallelize most computations in an MH-RNN. Experimental results on NTIMIT and CHiME-2 demonstrate the effectiveness of the new WaveNet MH-SRU. For example, it achieves 1.8\% and 0.6\%  absolute WER reductions over the CHiME-2 SRU and MH-SRU baseline models that have similar number of model parameters.",1.0,4.375
381,1660,5.1    4.25    4.05    4.1    ,Single-channel Late Reverberation Power Spectral Density Estimation Using Denoising Autoencoders,"In order to suppress the late reverberation in the spectral domain, many single-channel dereverberation techniques rely on an estimate of the late reverberation power spectral density (PSD). In this paper, we propose a novel approach to late reverberation PSD estimation using a denoising autoencoder (DA), which is trained to learn a mapping from the microphone signal PSD to the late reverberation PSD. Simulation results show that the proposed approach yields a high PSD estimation accuracy and generalizes well to unseen data. Furthermore, simulation results show that the proposed DA-based PSD estimate yields a higher PSD estimation accuracy and a similar dereverberation performance than a state-of-the-art statistical PSD estimate, which additionally also requires knowledge of the reverberation time.",0.0,4.375
396,1692,5.1    2.9    5    4.45    4.45    ,Acoustic-Prosodic Features of Tabla Bol Recitation and Correspondence with the Tabla Imitation,"The recitation of a tabla composition using vocalic syllables (bols) plays an important role in the oral tradition of pedagogy in North Indian classical music. However human speech and the tabla are two different acoustic systems with distinct production mechanisms, each with their own articulatory parameters and acoustic space. There have been empirical studies on the phonetic features of isolated bol utterances and the corresponding isolated strokes produced on the tabla. In this work, we consider acoustic properties of bol recitation beyond the segmental measurements related to the syllables. The recitation of a tabla composition is typically quite expressive in nature being marked by pitch variations, loudness dynamics and voice quality variations across a sequence or phrase.
Given the distinct spaces of acoustic variation of the voice and tabla, we study acoustic-prosodic variation in the recitation and investigate the corresponding acoustic variations in the drumming. An available large dataset of recordings of selected tabla compositions, each aligned with the corresponding bol recitation, is employed in the analyses. We find that while the recitation reliably encodes intensity variations across bols in a cycle, the observed pitch variations are meaningful only for the pitch-varying drum strokes of the left drum.",0.0,4.38
344,1574,4.35    4.8    4.1    4.3    ,Frequency Domain Linear Prediction Features for Replay Spoofing Attack Detection,"Automatic speaker verification (ASV) systems are vulnerable to various types of spoofing attacks such as speech synthesis, voice conversion and replay attacks. Recent research has highlighted the need for more effective countermeasures for replay attacks, which can be very challenging to detect, however replayed speech has previously shown frequency band-specific differences when compared with genuine speech. In this paper, we propose the use of long-term temporal envelopes of subband signals using a frequency domain linear prediction (FDLP) framework. This flexible framework makes use of temporal envelope information, which has not previously been investigated for replay spoofing detection. Evaluations of the proposed system and its fusion with other subsystems were carried out on the ASVspoof 2017 database. Interestingly, smoother temporal envelopes, based on very long windows of up to 1 second, seem to be most successful and show good prospects for performance improvements via fusion.",0.0,4.387499999999999
423,1750,3.3    3.5    4.85    5.9    ,Neural Speech Turn Segmentation and Affinity Propagation for Speaker Diarization,"Speaker diarization is the task of determining who speaks when in an audio stream. Most diarization systems rely on statistical models to address four sub-tasks: speech activity detection (SAD), speaker change detection (SCD), speech turn clustering, and re-segmentation. First, following the recent success of recurrent neural networks (RNN) for SAD and SCD, we propose to address re-segmentation with Long-Short Term Memory (LSTM) networks. Then, we propose to use affinity propagation on top of neural speaker embeddings for speech turn clustering, outperforming regular Hierarchical Agglomerative Clustering (HAC). Finally, all these modules are combined and jointly optimized to form a speaker diarization pipeline in which all but the clustering step are based on RNNs. We provide experimental results on the French Broadcast dataset ETAPE where we reach state-of-the-art performance.",0.0,4.387499999999999
24,73,2.3    5.05    5.05    5.15    ,COSMO SylPhon: a Model to Assess Phonological Learning,"During speech development, babies learn to perceive and produce speech units, especially syllables and phonemes. However, the mechanisms underlying the acquisition of speech units still remain unclear. We propose a Bayesian model of speech communication, named “COSMO SylPhon”, for studying the acquisition of both syllables and phonemes. In this model, speech development involves a sensory learning phase mainly related to perception development and a motor learning phase mainly related to production development. We analyze how an agent can learn speech units during these two phases through an unsupervised learning process based on syllable stimuli. We show that the learning process enables to efficiently learn the distribution of syllabic stimuli provided in the environment. Importantly, we show that if agents are equipped with a bootstrap process inspired by the Frame-Content Theory of speech development, they learn to associate consonants to specific articulatory gestures, providing the basis for consonantal articulatory invariance.",0.0,4.387499999999999
352,1593,3.35    4.25    4.95    5    ,Cosine Metric Learning for Speaker Verification in the I-vector Space,"It is known that the equal-error-rate (EER) performance of a speaker verification system is determined by the overlap region of the decision scores of true and imposter trials. Also, the cosine similarity scores of the true or imposter trials produced by the state-of-the-art i-vector front-end approximate to a Gaussian distribution, and the overlap region of the two classes of trials depends mainly on their between-class distance. Motivated by the above facts, this paper presents a cosine similarity learning (CML) framework for speaker verification, which combines classical compensation techniques and the cosine similarity scoring for improving the EER performance. CML minimizes the overlap region by enlarging the between-class distance while introducing a regularization term to control the within-class variance, which is initialized by a traditional channel compensation technique such as linear discriminant analysis. Experiments are carried out to compare the proposed CML framework with several traditional channel compensation baselines on the NIST speaker recognition evaluation data sets. The results show that CML outperforms all the studied initialization compensation techniques.",0.0,4.3875
721,2505,1.85    4.9    5.9    4.9    ,Perceptual Sensitivity to Spectral Change in Australian English Close Front Vowels: an Electroencephalographic Investigation,"Speech scientists have long noted that the qualities of naturally-produced vowels do not remain constant over their durations – regardless of being nominally “monophthongs” or “diphthongs”. Recent acoustic corpora show that there are consistent patterns of first (F1) and second (F2) formant frequency change across different vowel categories. The three Australian English (AusE) close front vowels /iː, ɪ, ɪə/ provide a striking example: while their midpoint or mean F1 and F2 frequencies are virtually identical, their spectral change patterns distinctly differ. 
The present study utilizes a pre-attentive discrimination paradigm with electroencephalography to assess AusE listeners’ perceptual sensitivity to close front vowels with different F1 × F2 trajectory lengths (TLs) and directions (TDs). When TLs are modest, there is an asymmetry in perceptual sensitivity: closing vowels, e.g., /iː/ whose trajectory terminates high in the F1 × F2 vowel space, are perceptually prominent, whereas centering vowels, e.g., /ɪ, ɪə/ whose trajectories end more centrally, are not. However, when TLs are exaggerated, the asymmetry in the perceptual sensitivity to the two TDs is substantially reduced.
The results indicate that, despite the distinct patterns of spectral change of AusE /iː, ɪ, ɪə/ in production, its perceptual relevance is not uniform, but rather vowel-category dependent.",0.0,4.3875
595,2169,3.55    3.45    4.75    5.8    ,Multi-Lingual Depression-Level Assessment from Conversational Speech Using Acoustic and Text Features,"Depression is a common mental health problem around the world with a large burden on economies, well-being, hence productivity, of individuals. Its early diagnosis and treatment are critical to reduce the costs and even save lives. One key aspect to achieve that goal is to use voice technologies and monitor
depression remotely and relatively inexpensively using automated agents.
Although there has been efforts to automatically assess depression levels from audiovisual features, use of transcriptions along with the acoustic features has emerged as a more recent research venue. Moreover, difficulty in data collection and the limited amounts of data available for research are also challenges that are hampering the success of the algorithms. One of the novel contributions in this paper is to exploit the databases from multiple languages for feature selection. Since a large number of features can be extracted from speech, and given the small amounts of training data available, effective data selection is critical for success. Our proposed multi-lingual method was effective at selecting better features and significantly improved the depression assessment accuracy. We also use text-based features for assessment and propose a novel strategy to fuse the text- and speech-based classifiers which further boosted the performance.",0.0,4.3875
440,1778,4.3    5.05    3.2    5    ,Cross-lingual Speech Emotion Recognition through Factor Analysis,"Conventional speech emotion recognition based on the extraction of high level descriptors emerging from low level descriptors seldom delivers promising results in cross-corpus experiments. Therefore it might not perform well in real-life applications. Factor analysis, proven in the fields of language identification and speaker verification, could clear a path towards more robust emotion recognition. This paper proposes an iVector-based approach operating on acoustic MFCC features with a separate modeling of the speaker and emotion variabilities respectively. The speech analysis extracts two fixed-length low-dimensional feature vectors corresponding to the two mentioned sources of variation. To model the speaker-related nuisance variability speaker factors are extracted using an eigenvoice matrix. After compensating for this speaker variability in the supervector space, the emotion factors (one per targeted emotion) are extracted using an emotion variability matrix. The emotion factors are then fed to a basic emotion classifier. Leave-one-speaker-out cross-validation on the Berlin Database of Emotional Speech EMO-DB (German) and IEMOCAP (English) datasets lead to results that are competitive with the current state-of-the-art. Cross-lingual experiments demonstrate the excellent robustness of the method: the classification accuracies degrade less than 15% relative when emotion models are trained on one corpus and tested on the other.",0.0,4.3875
640,2305,4.3    4.9    4    ,Triplet Network with Attention for Speaker Diarization,"In automatic speech processing systems, speaker diarization is a crucial front-end component to separate segments from different speakers. Inspired by the recent success of deep neural networks (DNNs) in semantic inferencing, triplet loss-based architectures have been successfully used for this problem. However, existing work utilizes conventional i-vectors as the input representation and builds simple fully connected networks for metric learning, thus not fully leveraging the modeling power of DNN architectures. This paper investigates the importance of learning effective representations from the sequences directly in metric learning pipelines for speaker diarization. More specifically, we propose to employ attention models to learn embeddings and the metric jointly in an end-to-end fashion. Experiments are conducted on the CALLHOME conversational speech corpus. The diarization results demonstrate that, besides providing a unified model, the proposed approach achieves improved performance when compared against existing approaches.",0.0,4.3999999999999995
305,1499,4.75    4.85    5.1    2.9    ,Wavelet Analysis of Speaker Dependent and Independent Prosody for Voice Conversion,"Thus far, voice conversion studies are mainly focused on the conversion of spectrum. However, speaker identity is also characterized by its prosody features, such as fundamental frequency (F0) and energy contour. We believe that with a better understanding of speaker dependent/independent prosody features, we can devise an analytic approach that addresses voice conversion in a better way.  We consider that speaker dependent features reflect speaker's individuality, while speaker independent features reflect the expression of linguistic content. Therefore, the former is to be converted while the latter is to be carried over from source to target during the conversion. To achieve this, we provide an analysis of speaker dependent and speaker independent prosody patterns in different temporal scales by using wavelet transform. The centrepiece of this paper is based on the understanding that a speech utterance can be characterized by speaker dependent and independent features in its prosodic manifestations. Experiments show that the proposed prosody analysis scheme improves the prosody conversion performance consistently under the sparse representation framework.",0.0,4.3999999999999995
90,1098,4.2    5    4    ,Detecting Packet-Loss Concealment Using Formant Features and Decision Tree Learning,"One of the main quality impairments in today's packet-based voice services are interruptions caused by transmission errors. Therefore, most codecs comprise concealment algorithms that attempt to reduce the perceived quality degradation of missing speech packets. In case the algorithm fails to properly synthesize the lost speech, interruptions or unnatural sounds are usually perceivable by the user. When measuring the quality of a voice network, there are excellent tools available, which can predict the perceived speech quality. However, they offer only little insight into the technical cause of a quality degradation. A packet-loss detection model could explain the influence of transmission errors on the speech quality and state a packet-loss rate. Thus, making it easier to identify technical problems in the network. In this paper, we examine a new approach for detecting (perceived) packet-loss of transmitted speech by audio analysis. After finding a lost packet, the model classifies in a second stage if the loss was perceivable as a quality degradation. In the model, we use meaningful features that are easy to interpret, and obtained promising results in a simulated environment. Therefore, this detector could also be used to evaluate new packet-loss concealment algorithms and help in optimizing the same.",0.0,4.3999999999999995
117,1140,4.2    5    3.35    5.05    ,Monoaural Audio Source Separation Using Variational Autoencoders,"We introduce a monaural audio source separation framework using a latent generative model. Traditionally, discriminative training for source separation is proposed using
deep neural networks or non-negative matrix factorization. In this paper, we propose a principled generative approach using variational autoencoders (VAE) for audio source separation. VAE computes efficient Bayesian inference which leads to a continuous latent representation of the input data(spectrogram). It contains a probabilistic encoder which projects an input data to latent space and a probabilistic decoder which projects data
from latent space back to input space. This allows us to learn a robust latent representation of sources corrupted with noise and other sources. The latent representation is then fed to the decoder to yield the separated source. Both encoder and decoder
are implemented via multilayer perceptron (MLP). In contrast to prevalent techniques, we argue that VAE is a more principled approach to source separation. Experimentally, we find
that the proposed framework yields reasonable improvements when compared to baseline methods available in the literature i.e. DNN and RNN with different masking functions and autoencoders. We show that our method performs better than best of the relevant methods with ∼ 2 dB improvement in the source to distortion ratio.",0.0,4.3999999999999995
636,2298,4.85    5.05    5.1    2.6    ,LSTM Based Cross-corpus and Cross-task Acoustic Emotion Recognition,"Acoustic emotion recognition is a popular and central research direction in paralinguistic analysis, due its relation to a wide range of affective states/traits and manifold applications. Developing highly generalizable models still remains as a challenge for researchers and engineers, because of multitude of nuisance factors. To assert generalization, deployed models need to handle spontaneous speech recorded under different acoustic conditions compared to the training set. This requires that the models are tested for cross-corpus robustness. In this work, we first investigate the suitability of Long-Short-Term-Memory (LSTM) models trained with time- and space-continuously annotated affective primitives for cross-corpus acoustic emotion recognition. We next employ an effective approach to use the frame level valence and arousal predictions of LSTM models for utterance level affect classification and apply this approach on the ComParE 2018 challenge corpora. The proposed method alone gives motivating results both on development and test set of the Self-Assessed Affect Sub-Challenge. On the development set, the cross-corpus prediction based method gives a boost to performance when fused with top components of the baseline system. Results indicate the suitability of the proposed method for both time-continuous and utterance level cross-corpus acoustic emotion recognition tasks.",0.0,4.3999999999999995
414,1732,5.9    3.85    3.5    4.35    ,Speech Source Separation Using ICA in Constant Q Transform Domain,"In order to separate individual sources from convoluted speech mixtures, complex-domain independent component analysis (ICA) is employed on the individual frequency bins of time frequency representations of the speech mixtures, obtained using short-time Fourier transform (STFT). The frequency components computed using STFT are separated by constant frequency difference with a constant frequency resolution. However, it is well known that the human auditory mechanism offers better resolution at lower frequencies. Hence, the perceptual quality of the extracted sources critically depends on the separation achieved in the lower frequency components. In this paper, we propose to perform source separation on the time-frequency representation computed though constant Q transform (CQT), which offers non uniform logarithmic binning in the frequency domain. Complex-domain ICA is performed on the individual bins of the CQT in order to get separated components in each frequency bin which are suitably scaled and permuted to obtain separated sources in the CQT domain. The estimated sources are obtained by applying inverse constant Q transform to the scaled and permuted sources. In comparison with the STFT based frequency domain ICA methods, there has been a consistent improvement of 3dB or more in the Signal to Interference Ratios of the extracted sources.",0.0,4.4
237,1364,4.45    4.15    5    4    ,Multimodal Speaker Segmentation and Diarization Using Lexical and Acoustic Cues via Sequence to Sequence Neural Networks,"While there has been substantial amount of work in speaker diarization recently, there are few efforts in jointly employing lexical and acoustic information for speaker segmentation. Towards that, we investigate a speaker diarization system using a sequence-to-sequence neural network trained on both lexical and acoustic features. We also propose a loss function that allows for selecting not only the speaker change points but also the best speaker at any time by allowing for different speaker groupings. We incorporate Mel Frequency Cepstral Coefficients (MFCC) as an acoustic feature stream alongside lexical information that are obtained from conversations from the Fisher dataset. Thus, we show that acoustics provide complementary information to the lexical modality. The experimental results show that sequence-to-sequence system trained on both word sequences and MFCC can improve on speaker diarization result compared to the system that only relies on lexical modality or the baseline MFCC-based system. In addition, we test the performance of our proposed method with Automatic Speech Recognition (ASR) transcripts. While the performance on ASR transcripts drops, the Diarization Error Rate (DER) of our proposed method still outperforms the traditional method based on Bayesian Information Criterion (BIC).",0.0,4.4
120,1149,3.15    4.1    5.25    5.1    ,User Information Augmented Semantic Frame Parsing Using Progressive Neural Networks,"Semantic frame parsing is a crucial component in spoken language understanding (SLU) to build spoken dialog systems. It has two main tasks: intent detection and slot filling. Although state-of-the-art approaches showed good results, they require large annotated training data and long training time. In this paper, we aim to alleviate these drawbacks for semantic frame parsing by utilizing the ubiquitous user information. We design a novel progressive deep neural network model to incorporate prior knowledge of user information intermediately to better and quickly train a semantic frame parser. Due to the lack of benchmark dataset with real user information, we synthesize the simplest type of user information (location and time) on ATIS benchmark data. The results show that our approach leverages such simple user information to outperform state-of-the-art approaches by 0.25% for intent detection and 0.31% for slot filling using standard training data. When using smaller training data, the performance improvement on intent detection and slot filling reaches up to 1.35% and 1.20% respectively. We also show that our approach can achieve similar performance as state-of-the-art approaches by using less than 80% annotated training data. Moreover, the training time to achieve the similar performance is also reduced by over 60%.",0.0,4.4
380,1655,5.15    4.25    3.45    4.75    ,Multi-Head Decoder for End-to-End Speech Recognition,"This paper presents a new network architecture called multi-head decoder for end-to-end speech recognition as an extension of a multi-head attention model.
In the multi-head attention model, multiple attentions are calculated, and then, they are integrated into a single attention.
On the other hand, instead of the integration in the attention level, our proposed method uses multiple decoders for each attention and integrates their outputs to generate a final output.
Furthermore, in order to make each head to capture the different modalities, different attention functions are used for each head, leading to the improvement of the recognition performance with an ensemble effect.
To evaluate the effectiveness of our proposed method, we conduct an experimental evaluation using Corpus of Spontaneous Japanese.
Experimental results demonstrate that our proposed method outperforms the conventional methods such as location-based and multi-head attention models, and that it can capture different speech/linguistic contexts within the attention-based encoder-decoder framework.",0.0,4.4
226,1343,4.35    2.6    5.75    4.9    ,Dual Language Models for Code Switched Speech Recognition,"In this work, we present a simple and elegant approach to language modeling for bilingual code-switched text. Since code-switching is a blend of two or more different languages, a standard bilingual language model can be improved upon by using structures of the monolingual language models. We propose a novel technique called dual language models, which involves building two complementary monolingual language models and combining them using a probabilistic model for switching between the two. We evaluate the efficacy of our approach using a conversational Mandarin-English speech corpus. We prove the robustness of our model by showing significant improvements in perplexity measures over the standard bilingual language model without the use of any external information. Similar consistent improvements are also reflected in automatic speech recognition error rates.",0.0,4.4
58,1038,4.2    5    4.55    3.85    ,Articulatory Consequences of Vocal Effort Elicitation Method,"Articulatory features from two datasets, Slovak and Swedish, were compared to see whether different methods of eliciting loud speech (ambient noise vs. visually presented loudness target) result in different articulatory behavior. The features studied were temporal and kinematic characteristics of lip separation within the closing and opening gestures of bilabial consonants, and of the tongue body movement from /i/ to /a/ through a bilabial consonant. The results indicate larger hyper-articulation in the speech elicited with visually presented target. While individual articulatory strategies are evident, the speaker groups agree on increasing the kinematic features consistently within each gesture in response to the increased vocal effort. Another concerted strategy is keeping the tongue response considerably smaller than that of the lips, presumably to preserve acoustic prerequisites necessary for the adequate vowel identity. While the method of visually presented loudness target elicits larger span of vocal effort, the two elicitation methods achieve comparable consistency per loudness conditions.",0.0,4.4
194,1285,4.1    4.4    5.1    4    ,Visual Timing Information in Audiovisual Speech Perception: Evidence from Lexical Tone Contour,"The present study investigated whether duration of lip movement could improve intelligibility of lexical pitch contours under noisy condition. Eighteen Chinese speakers were asked to identify a Mandarin lexical tone in one pair of tones under auditory only (AO) and audiovisual (AV) condition. Two types of tone pairs were used in the study: maximum contrastive pair (falling vs. dipping tones, the durational difference of lip movement was 100ms) and minimum contrastive pair (rising vs. falling tones, the difference was 33ms). The results showed that duration of lip movement enhanced discrimination in the maximum pair whereas the similar lengths of rising and dipping tones attenuated such visual benefit. The finding suggested that visual timing information could be a specific cue for audiovisual lexical tone perception.",0.0,4.4
39,1007,4.25    4.25    4.25    4.85    ,Follow-up Question Generation Using Pattern-based Seq2Seq with a Small Corpus for Interview Coaching,"In this study, as only a small interview corpus is available, a pattern-based sequence to sequence model is adopted for follow-up question generation. First, word clustering is employed to automatically transform the question/answer sentences into sentence patterns, in which each pattern is composed of word classes, to decrease the complexity of the sentence structures. Next, the convolutional neural tensor network model is used to select a sentence in an interviewee’s answer turn for follow-up question generation. In order to generate the follow-up question pattern, the selected target sentence pattern is fed to a Seq2seq model to obtain the corresponding follow-up question pattern. Then the word class positions in the generated follow-up question sentence pattern is filled in with the words using a word class table obtained from the training corpus. Finally, the n-gram language model is used to rank the candidate follow-up questions and choose the most suitable one as the response to the interviewee. This study collected 3390 follow-up question and answer sentence pairs for training and evaluation. Five-fold cross validation was employed and the experimental results show that the proposed method outperformed the traditional word-based method, and achieved a more favorable performance based on a statistical significance test.",0.0,4.4
611,2215,4.15    4.85    4.3    4.3    ,Audio-Visual Prediction of Head-Nod and Turn-Taking Events in Dyadic Interactions,"Head-nods and turn-taking both significantly contribute conversational dynamics in dyadic interactions. Timely prediction and use of these events is quite valuable for  dialog management systems in human-robot interaction. In this study, we present an audio-visual prediction framework for the head-nod and turn-taking events that can also be utilized in real-time systems. Prediction systems based on Support Vector Machines (SVM) and Long Short-Term Memory Recurrent Neural Networks (LSTM-RNN) are trained on human-human conversational data. Unimodal and multimodal classification performances of head-nod and turn-taking events are reported over the IEMOCAP dataset.",0.0,4.4
498,1910,3.6    5.05    4.85    4.1    ,Self-Attentional Acoustic Models,"Self-attention is a method of encoding sequences of vectors by relating these vectors to each-other based on pairwise similarities.
These models have recently shown promising results for modeling discrete sequences, but they are non-trivial to apply to acoustic modeling due to computational and modeling issues.
In this paper, we apply self-attention to acoustic modeling, proposing several improvements to mitigate these issues:
First, self-attention memory grows quadratically in the sequence length, which we address through a downsampling technique.
Second, we find that previous approaches to incorporate position information into the model are unsuitable and explore other representations and hybrid models to this end.
Third, to stress the importance of local context in the acoustic signal, we propose a Gaussian biasing approach that allows explicit control over the context range.
Experiments find that our model approaches a strong baseline based on LSTMs with network-in-network connections while being much faster to compute.
Besides speed, we find that interpretability is a strength of self-attentional acoustic models, and demonstrate that self-attention heads learn a linguistically plausible division of labor.",0.0,4.4
736,2544,3.15    5.05    4.3    5.1    ,Automatic Evaluation of Soft Articulatory Contact for Stuttering Treatment,"We describe a new method for the automatic discrimination and evaluation of phonation beginning with a consonant with soft articulatory contact, which is used in the treatment of stuttering, and normal phonation. Soft articulatory contact is trained to relax articulators and remove hard contacts that occur during stuttering. We use features related to the changes in acoustic characteristics and the voice quality under the hypothesis that the slowing down of articulatory movement of the initial consonant and the relaxing of phonatory muscles co-occur with soft articulatory contact. The results of an experimental evaluation showed that high accuracy was obtained when acoustic features were related to the peaks of the first derivative of the mel frequency cepstral coefficients (MFCCs) corresponded to the slowing down of the movement of the articulators. The features of vocal quality only slightly contributed to the classification.",0.0,4.4
358,1606,2.55    3.9    5.15    6    ,Speaker Activity Detection and Minimum Variance Beamforming for Source Separation,"This work proposes a framework that renders minimum variance beamforming blind allowing for source separation in real world environments with an ad-hoc multi-microphone setup using no assumptions other than knowing the number of speakers. The framework allows for multiple active speakers at the same time and estimates the activity of every single speaker at flexible time resolution. These estimated speaker activities are subsequently used for the calibration of the beamforming algorithm. This framework is tested with three different speaker activity detection (SAD) methods, two of which use classical algorithms and one that is event-driven. Our methods, when tested in real world reverberant scenarios, can achieve very high signal-to-interference ratio (SIR) of around 20 dB and sound quality of 0.85 in short-time objective intelligibility (STOI) close to optimal beamforming results of 22 dB SIR and 0.89 in STOI.",0.0,4.4
203,1298,2.7    4.45    4.95    5.1    4.85    ,Learning Conditional Acoustic Latent Representation with Gender and Age Attributes for Automatic Pain Level Recognition,"Pain is an unpleasant internal sensation caused by bodily damages or physical illnesses with varied expressions conditioned on personal attributes. In this work, we propose an age-gender embedded latent acoustic representation learned using conditional maximum mean discrepancy variational autoencoder (MMD-CVAE). The learned MMD-CVAE embeds personal attributes information directly in the latent space. Our method achieves a 70.7% in extreme set classification (severe versus mild) and 47.7% in three-class recognition (severe, moderate, and mild) by using these MMD-CVAE encoded features on a large-scale real patients pain database. Our method improves a relative of 11.34% and 17.51% compared to using acoustic representation without age-gender conditioning in the extreme set and the three-class recognition respectively. Further analyses reveal under severe pain, females have higher maximum of jitter and lower harmonic energy ratio between F0, H1 and H2 compared to males, and the minimum value of jitter and shimmer are higher in the elderly compared to the non-elder group.",0.0,4.410000000000001
447,1800,4.05    3.4    5.15    5.05    ,Completely Unsupervised Phoneme Recognition by Adversarially Learning Mapping Relationships from Audio Embeddings,"Unsupervised discovery of acoustic tokens from audio corpora without annotation and learning vector representations for these tokens have been widely studied. Although these techniques have been shown successful in some applications such as query-by-example Spoken Term Detection (STD), the lack of mapping relationships between these discovered tokens and real phonemes has limited the down-stream applications. This paper represents probably the first attempt towards the goal of completely unsupervised phoneme recognition, or mapping audio signals to phoneme sequences without phoneme-labeled audio data. The basic idea is to cluster the embedded acoustic tokens and learn the mapping between the cluster sequences and the unknown phoneme sequences with a Generative Adversarial Network (GAN). An unsupervised phoneme recognition accuracy of 36% was achieved in the preliminary experiments.",0.0,4.4125
938,1434,3.85    4.8    4.55    4.45    ,Emotion Recognition of Overlapping Speech with Deep Clustering and Recurrent Neural Networks,"Speech emotion recognition is an important task for many applications. In real-world scenarios, overlapping speech is a common occurrence. However, no existing speech emotion recognition systems have directly addressed the problem of classifying the emotion of multiple overlapping speakers.

In this paper, we propose the first system for overlapping speech emotion recognition. Our approach consists of separating overlapping speech sources using deep clustering, then feeding each separated source to a recurrent neural network-based emotion classifier. To evaluate the system, we build an overlapping speech dataset from IEMOCAP and experiment with a variety of training strategies. We find that our system is very effective for overlapping speech emotion recognition, achieving 94.4% of the classification accuracy of single-speaker emotion recognition.",1.0,4.4125
517,1959,4.9    4.1    4.05    4.6    ,Neural Network Architecture That Combines Temporal and Summative Features for Infant Cry Classification in the Interspeech 2018 Computational Paralinguistics Challenge,"This paper describes the application of a novel deep neural network architecture to the classification of infant vocalisations as part of the Interspeech 2018 Computational Paralinguistics Challenge. Previous approaches to infant cry classification have either applied a statistical classifier to summative features of the whole cry, or applied a syntactic pattern recognition technique to a temporal sequence of features. In this work we explore a deep neural network architecture that exploits both temporal and summative features to make a joint classification. The temporal input comprises centi-second frames of low-level signal features which are input to LSTM nodes, while the summative vector comprises a large set of statistical functionals of the same frames that are input to MLP nodes. The combined network is jointly optimized and evaluated using leave-one-speaker-out cross-validation on the challenge training set. Results are compared to independently-trained temporal and summative networks and to a baseline SVM classifier. The combined model outperforms the other models and the challenge baseline on the training set. While problems remain in finding the best configuration and training protocol for such networks, the approach seems promising for future signal classification tasks.",0.0,4.4125
100,1113,4.2    5.15    4.95    3.35    ,Expressive Speech Synthesis via Modeling Expressions with Variational Autoencoder,"Recent advances in neural autoregressive models have improve the performance of speech synthesis (SS).
However, as they lack the ability to model global characteristics of speech (such as speaker individualities or speaking styles), particularly when these characteristics have not been labeled, making neural autoregressive SS systems more expressive is still an open issue.
In this paper, we propose to combine VoiceLoop, an autoregressive SS model, with Variational Autoencoder (VAE).
This approach, unlike traditional autoregressive SS systems, uses VAE to model the global characteristics explicitly, enabling the expressiveness of the synthesized speech to be controlled in an unsupervised manner.
Experiments using the VCTK and Blizzard2012 datasets show the VAE helps VoiceLoop to generate higher quality speech and to control the experssions in its synthesized speech by incorporating global characteristics into the speech generating process.",0.0,4.4125000000000005
648,2330,4.1    4    5.15    ,Speech Database and Protocol Validation Using Waveform Entropy,"The assessment of performance for any number of speech processing tasks calls for the use of a suitably large, representative dataset. Dataset design is crucial so as to ensure that any significant variation unrelated to the task in hand is adequately normalised or marginalised. Most datasets are partitioned into training, development and evaluation subsets. Depending on the task, the nature of these three subsets should normally be close to identical. With speech signals being subject to a multitude of different influences, e.g. speaker gender and age, language, dialect, utterance length, etc., the design and validation of speech datasets can become especially challenging. Even if many sources of variation unrelated to the task in hand can easily be marginalised, other sources of more subtle variation can easily be overlooked. Imbalances between training, development and evaluation partitions, can bring into question findings derived from their use. Stringent dataset validation procedures are required. This paper reports a particularly straightforward approach to dataset validation that is based upon waveform entropy.",0.0,4.416666666666667
654,2350,3.8    4.45    5.9    3.55    ,Investigating Speech Enhancement and Perceptual Quality for Speech Emotion Recognition,"In this study, the performance of two enhancement algorithms is investigated in terms of perceptual quality as well as in respect to their impact on speech emotion recognition (SER). The SER system adopted is based on the same benchmark system provided for the AVEC Challenge 2016. The three objective measures adopted are the speech-to-reverberation modulation energy ratio (SRMR), the perceptual evaluation of speech quality (PESQ) and the perceptual objective listening quality assessment (POLQA). Evaluations are conducted on speech files from the RECOLA dataset, which provides spontaneous interactions in French of 27 subjects. Clean speech files are corrupted with different levels of background noise and reverberation. Results show that applying enhancement prior to the SER task can improve SER performance in more degraded scenarios. We also show that quality measures can be an important asset as indicator of enhancement algorithms performance towards SER, with SRMR and POLQA providing the most reliable results.",0.0,4.425
265,1417,4.45    5.1    3.95    4.2    ,Semi-Orthogonal Low-Rank Matrix Factorization for Deep Neural Networks,"Time Delay Neural Networks (TDNNs), also known as one-dimensional Convolutional Neural Networks (1-d CNNs), are an efficient and well-performing neural network architecture for speech recognition. We introduce a factored form of TDNNs (TDNN-F) which is structurally the same as a TDNN whose layers have been compressed via SVD, but is trained from a random start with one of the two factors of each matrix constrained to be semi-orthogonal. This gives substantial improvements over TDNNs and performs about as well as TDNN-LSTM hybrids.",0.0,4.425
183,1265,4.9    5.05    3.6    4.15    ,Transfer Learning Based Progressive Neural Networks for Acoustic Modeling in Statistical Parametric Speech Synthesis,"The fundamental frequency and the spectrum parameters of the speech are correlated thus one of their learned mapping from the linguistic features can be leveraged to help determine the other. The conventional methods treated all the acoustic features as one stream for acoustic modeling. And the multi-task learning methods were applied to acoustic modeling with several targets in a global cost function. To improve the accuracy of the acoustic model, the progressive deep neural networks (PDNN) is applied for acoustic modeling in statistical parametric speech synthesis (SPSS) in our method. Each type of the acoustic features is modeled in different sub-networks with its own cost function and the knowledge transfers through lateral connections. Each sub-network in the PDNN can be trained step by step to reach its own optimum. Experiments are conducted to compare the proposed PDNN-based SPSS system with the standard DNN methods. The multi-task learning (MTL) method is also applied to the structure of PDNN and DNN as the contrast experiment of the transfer learning. The computational complexity, prediction sequences and quantity of hierarchies of the PDNN are investigated. Both objective and subjective experimental results demonstrate the effectiveness of the proposed technique.",0.0,4.425
559,2071,5    5    3.45    4.25    ,Analysis of Language Dependent Front-End for Speaker Recognition,"In Deep Neural Network (DNN) i-vector based speaker recognition systems,  acoustic models trained for Automatic Speech Recognition are employed to estimate sufficient statistics for i-vector modeling.
The DNN based acoustic model is typically trained
on a well-resourced language like English.  In evaluation conditions where the
enrollment and test data are not in English, as in the NIST SRE 2016 dataset, a DNN acoustic model generalizes poorly. In such conditions, a conventional Universal Background Model/Gaussian Mixture Model (UBM/GMM) based i-vector extractor performs better than the DNN based i-vector system.
In this paper, we address the scenario in which one can develop a 
Automatic Speech Recognizer with limited resources for a language present in the evaluation condition, thus enabling the use of a DNN acoustic model instead of UBM/GMM.
Experiments are performed on the Tagalog subset of the NIST SRE 2016 dataset assuming an open training condition.
With a DNN i-vector system trained for Tagalog, a relative improvement of 12.1% is obtained over a baseline system trained for English.",0.0,4.425
405,1712,3.85    4.25    4.95    4.1    5    ,Unsupervised Vocal Tract Length Warped Posterior Features for Non-Parallel Voice Conversion,"In the non-parallel Voice Conversion (VC) with the Iterative combination of Nearest Neighbor search step and Conversion step Alignment (INCA) algorithm, the occurrence of one-to-many and many-to-one pairs in the training data will deteriorate the performance of the stand-alone VC system. The work on handling these pairs during the training is less explored. In this paper, we establish the relationship via intermediate speaker-independent posteriorgram representation, instead of directly mapping the source spectrum to the target spectrum. To that effect, a Deep Neural Network (DNN) is used to map the source spectrum to posteriorgram representation and another DNN is used to map this posteriorgram representation to the target speaker’s spectrum. In this paper, we propose to use unsupervised Vocal Tract Length Normalization (VTLN)-based warped Gaussian posteriorgram features as the speaker-independent representations. We performed experiments on a small subset of publicly available Voice Conversion Challenge (VCC) 2016 database. We obtain the lower Mel Cepstral Distortion (MCD) values with the proposed approach compared to the baseline as well as the supervised phonetic posteriorgram feature-based speaker-independent representations. Furthermore, subjective evaluation gave relative improvement of 13.3 % with the proposed approach in terms of Speaker Similarity (SS).",0.0,4.43
497,1909,4.3    5.05    5    4.95    2.85    ,A Deep Identity Representation for Noise Robust Spoofing Detection,"The issue of the spoofing attacks which may affect automatic speaker verification systems (ASVs) has recently received an increased attention, so that a number of countermeasures have been developed for detecting high technology attacks such as speech synthesis and voice conversion. However, the performance of anti-spoofing systems degrades significantly in noisy conditions. To address this issue, we propose a deep learning framework to extract spoofing identity vectors, as well as the use of soft missing-data masks. The proposed feature extraction employs a convolutional neural network (CNN) plus a recurrent neural network (RNN) in order to provide a single deep feature vector per utterance. Thus, the CNN is treated as a convolutional feature extractor that operates at the frame level. On top of the CNN outputs, the RNN is employed to obtain a single spoofing identity representation of the whole utterance. Experimental evaluation is carried out on both a clean and a noisy version of the ASVSpoof2015 corpus. The experimental results show that our proposals clearly outperforms other methods recently proposed such as the popular CQCC+GMM system or other similar deep feature systems for both seen and unseen noisy conditions.",0.0,4.430000000000001
356,1602,4.4    4.85    4.05    ,Stress Distribution of Given Information in Chinese Reading Texts,"Using information structure annotation System, i.e., RefLex Scheme, the present study annotates the information structure of Chinese reading discourse and explores the relationship between information status and stress distribution. Results show that given information could bear stresses as well as new information. Specifically, the stress distribution of given information is significantly affected by the sub-category of information status on the referential level, i.e., r-given, r-given-generic and r-given-displaced show different stress distribution. However, the sub-category of information status on lexical level exhibit no such effect. Besides, as the given information of the proper noun and personal pronoun on lexical level can attract stresses. The reasons is that the proper noun is the topic of a sentence and the personal pronoun processes a center shift. Furthermore, the inconsistency of information status in both referential and lexical level is the reason for the stress on the given information unit.",0.0,4.433333333333334
548,2040,3.2    4.2    5.9    ,How Did You like 2017? Detection of Language Markers of Depression and Narcissism in Personal Narratives,"Language analyses reveals crucial information about an individual’s current state of mind. Maladaptive psychological functioning appears in cognition, emotional experience and behaviour. In the time of the internet of things, a vast number of text and speech is available; subsequently, the interest in the automated detection of psychological functioning via language is rising. The current study indicates that depression and narcissism can be predicted through word use in personal narratives. Both conditions are characterised by an altered word count regarding anxiety and we (LIWC-based). While depressive individuals use less social words and more anxiety-related words, narcissists do the opposite. This might reflect the verbal correlate of the cognitive triad in depression. In contrast, narcissists’ word use mirrors their excommunicated anxiety of being an undesired self and their inability to reach long-term goals due to a lack of impulse control. The automated recognition of mental state through word use could improve early detection of mental disease, monitoring of disease course, delivery of tailored interventions and evaluation of therapy outcome.",0.0,4.433333333333334
126,1156,4.05    5    4.25    ,Leveraging Second-Order Log-Linear Model for Improved Deep Learning Based ASR Performance,"Gaussian generative models have been shown to be equivalent
to discriminative log-linear models under weak assumptions for
acoustic modeling in speech recognition systems. In this paper,
we note that the output layer of deep learning model consists
of a first-order log-linear model, also known as logistic regression,
which induces a set of homoscedastic distributions in the
generative model space, resulting in linear decision boundaries.
We leverage the above equivalence to make the deep learning
models more expressive by replacing the first order log-linear
model with a second-order model, which leads to heteroscedastic
distributions, as a result, the linear decision boundaries are
replaced with quadratic ones. We observe that the proposed
architecture yields a significant improvement in speech recognition
accuracy compared to the conventional model having a
comparable number of parameters. Relative improvement of
8.37% and 3.92% in word error rate (WER) is obtained for shallow
and deep feed-forward networks respectively. Moreover,
with Long Short-Term Memory (LSTM) networks with projection
matrix, we obtain significant relative improvement in WER
over the standard architecture.",0.0,4.433333333333334
541,2025,4.65    3.85    4.25    5    ,Interactions between Vowels and Nasal Codas in Mandarin Speakers’ Perception of Nasal Finals,"Previous studies have shown vowels’ effect on the perception of nasal codas from the perspectives of acoustics and phonetics with evidence from Mandarin. While few studies investigated the processing interactions between vowels and nasal codas during the perceptual processing of nasal Finals. Using a speeded classification paradigm which requires participants to attend to one dimension selectively and to ignore other dimensions, we aimed to explore the processing interactions between vowels and nasal codas in the process of nasal Finals’ perception by Mandarin speakers. Results of the speeded classification experiment showed that there was a mutual and symmetrical pattern of interference effect between vowel and nasal coda dimensions. More specifically, participants were affected by irrelevant variation of the nasal coda dimension when they were attending to the vowel dimension and affected by irrelevant variation of the vowel dimension when they were attending to the nasal coda dimension. The extent of such mutual interference effect was equal. Results were discussed in terms of phonemic differences in acoustic properties and relative discriminability between vowels and nasal codas. The present study may be helpful to second language learners of Mandarin during their course of acquiring nasal Finals.",0.0,4.4375
426,1754,5.25    4.9    3.65    3.95    ,Dysarthric Speech Recognition Using Time-delay Neural Network Based Denoising Autoencoder,"Dysarthria is a manisfestation of the disruption in the neuro-
muscular physiology resulting in uneven, slow, slurred, harsh
or quiet speech. Dysarthric speech poses serious challenges to
automatic speech recognition, considering this speech is diffi-
cult to decipher for both humans and machines. The objective
of this work is to enhance dysarthric speech features to match
that of healthy control speech. We use a Time-Delay Neural
Network based Denoising Autoencoder (TDNN-DAE) to en-
hance the dysarthric speech features. The dysarthric speech thus
enhanced is recognized using a DNN-HMM based Automatic
Speech Recognition (ASR) engine. This methodology was eval-
uated for speaker-independent (SI) and speaker-adapted (SA)
systems. Absolute improvements of 13% and 3% was observed
in the ASR performance for SI and SA systems respectively as
compared with unenhanced dysarthric speech recognition.",0.0,4.4375
643,2318,4.15    3.5    5    5.1    ,Domain-Adversarial Training for Session Independent EMG-based Speech Recognition,"We present our research on continuous speech recognition based on Surface Electromyography (EMG), where speech information is captured by electrodes attached to the speaker's face. This method allows speech processing without requiring that an acoustic signal is present; however, reattachment of the EMG electrodes causes subtle changes in the recorded signal, which degrades the recognition accuracy and thus poses a major challenge for practical application of the system. Based on the growing body of recent work in domain-adversarial training of neural networks, we present a system which adapts the neural network frontend of our recognizer to data from a new recording session, without requiring supervised enrollment.",0.0,4.4375
672,2391,2.35    5.9    5.15    4.35    ,Whistle-blowing ASRs: Evaluating the Need for More Inclusive Speech Recognition Systems,"Speech is a complex process that can break in many different ways and lead to a variety of voice disorders. Dysarthria is a voice disorder where individuals are unable to control one or more of the aspects of speech—the articulation, breathing, voicing, or prosody—leading to less intelligible speech. In this paper, we evaluate the accuracy of state-of-the-art automatic speech recognition systems (ASRs) on two dysarthric speech datasets and compare the results to ASR performance on control speech. The limits of ASR performance using different voices have not been explored since the field has shifted from generative models of speech recognition to deep neural network architectures. To test how far the field has come in recognizing disordered speech, we test two different ASR systems: (1) Carnegie Mellon University’s Sphinx Open Source Recognition, and (2) Google Speech Recognition. While (1) uses generative models of speech recognition, (2) uses deep neural networks. As expected, while (2) achieved lower word error rates (WER) on dysarthric speech than (1), control speech had a WER 59% lower than dysarthric speech. Future studies should be focused not only on making ASRs robust to environmental noise, but also more robust to different voices.",0.0,4.4375
529,1990,4.35    3.45    5    4.95    ,Fast Language Adaptation Using Phonological Information,"Phoneme-based multilingual connectionist temporal classification (CTC) model is easily extensible to a new language by concatenating parameters of the new phonemes to the output layer. In the present paper, we improve cross-lingual adaptation in the context of phoneme-based CTC models by using phonological information. A universal (IPA) phoneme classifier is first trained on phonological features generated from a phonological attribute detector. When adapting the multilingual CTC to a new, never seen, language, phonological attributes of the unseen phonemes are derived based on phonology and fed into the phoneme classifier. Posteriors given by the classifier are used to initialize the parameters of the unseen phonemes when extending the multilingual CTC output layer to the target language. Adaptation experiments show that the proposed initialization approaches further improve the cross-lingual adaptation on CTC models and yield significant improvements over Deep Neural Network / Hidden Markov Model (DNN/HMM)-based adaptation using limited data.",0.0,4.4375
457,1820,2.7    5.15    4.8    5.1    ,A Three-Layer Emotion Perception Model for Valence and Arousal-Based Detection from Multilingual Speech,"Automated emotion detection from speech has recently shifted from monolingual to multilingual tasks for human-like interaction in real-life where a system can handle more than a single input language. However, most work on monolingual emotion detection is difficult to generalize in multiple languages, because the optimal feature sets of the work differ from one language to another. Our study proposes a framework to design, implement, and validate an emotion detection system using multiple corpora. A continuous dimensional space of valence and arousal is first used to describe the emotions. A three-layer model incorporated with fuzzy inference systems is then used to estimate two dimensions. Speech features derived from prosodic, spectral, and glottal waveform are examined and selected to capture emotional cues. The results of this new system outperformed the existing state-of-the-art system by yielding a smaller mean absolute error and higher correlation between estimates and human evaluators. Moreover, results for speaker independent validation are comparable to human evaluators.",0.0,4.4375
555,2062,4.2    4.05    4.4    5.1    ,Automatic DNN Node Pruning Using Mixture Distribution-based Group Regularization,"In this paper, we address a constrained training for deep neural network-based acoustic model size reduction.
While the L2 regularizer is used as a modeling approach to shrinking parameters, we cannot cut down the unimportant parts because it does not assume any group structure.
The Group Lasso regularizer is used for the model size reduction approach. Group Lasso can set arbitrary group parameters (e.g. the column vector norms of the parameter matrices) as unimportant parts, and make the parameters sparse. Therefore, we can prune the unimportant parameters whose group parameter norm is nearly zero. However, Group Lasso does not suggest a clear rule for separating parameters close to zero and large in the group parameter space and hence is unsuitable for the model size reduction.
To solve these problems, we propose a mixture distribution-based regularizer which assumes distributions of norms in the group parameter space.
We evaluate our method on a NTT real recorded voice search data containing 1600 hours.
Our proposal achieves 27.0% reduction compared to the pruned model by Group Lasso while keeping recognition performance.",0.0,4.4375
145,1204,4.15    5.05    4.5    4.05    ,Compact Feedforward Sequential Memory Networks for Small-footprint Keyword Spotting,"Due to limited resource on devices and complicated scenarios, a compact model with high precision, low computational cost and latency is expected for small-footprint keyword spotting tasks. To fulfill these requirements, in this paper, compact Feed-forward Sequential Memory Network (cFSMN) which combines low-rank matrix factorization with conventional FSMN is investigated for a far-field keyword spotting task. The effect of its architecture parameters is analyzed. Towards achieving lower computational cost, multiframe prediction (MFP) is applied to cFSMN. For enhancing the modeling capacity, an advanced MFP is attempted by inserting small DNN layers before output layers. The performance is measured by area under the curve (AUC) for detection error tradeoff (DET) curves. The experiments show that compared with a well-tuned long short-term memory (LSTM) which needs the same latency and twofold computational cost, the cFSMN achieves 18.11% and 29.21% AUC relative decreases on the test sets which are recorded in quiet and noisy environment respectively. After applying advanced MFP, the system gets 0.48% and 20.04% AUC relative decrease over conventional cFSMN on the quiet and noisy test sets respectively, while the computational cost relatively reduces 46.58%.",0.0,4.4375
924,1402,2.75    4.1    5    5.9    ,Generating Mandarin and Cantonese F0 Contours with Decision Trees and BLSTMs,"This paper models the fundamental frequency contours on both Mandarin and Cantonese speech with decision trees and DNNs (deep neural networks). Different kinds of f0 representations and model architectures are tested for decision trees and DNNs. A new model called Additive-BLSTM (additive bidirectional long short term memory) that predicts a base f0 contour and a residual f0 contour with two BLSTMs is proposed. With respect to objective measures of RMSE and correlation, applying tone-dependent trees together with sample normalization and delta feature regularization within decision tree framework performs best. While the new Additive-BLSTM model with delta feature regularization performs even better. Subjective listening tests on both Mandarin and Cantonese comparing Random Forest model (multiple decision trees) and the Additive-BLSTM model were also held and confirmed the advantage of the new model according to the listeners' preference.",1.0,4.4375
604,2195,5    4.1    5.05    3.6    ,Conditional Computation-Based Recurrent Neural Networks for Computationally Efficient Acoustic Modelling,"The first step in Automatic Speech Recognition (ASR) is a fixed-rate segmentation of the  acoustic signal into overlapping windows of fixed length. Although this procedure allows to achieve excellent recognition accuracy, it is far from being computationally efficient,
in that it may produce a highly redundant signal (i.e, almost identical spectral vectors may span many observation windows) that converts into computational overload. The reduction of such overload can be very beneficial for application such as offline ASR on mobile devices.  

In this paper we present a principled way for saving numerical operations during ASR by using conditional-computation methods in deep bidirectional Recurrent Neural Networks (RNNs) for acoustic modelling.
The methods rely on learned binary neurons that allow hidden layers to be updated only when necessary or to keep their previous value.

We (i) evaluate, for the first time, conditional computation-based recurrent architectures on a speech recognition task, and (ii) propose a novel model specifically designed for speech data that inherently builds a multi-scale temporal structure in the hidden layers. 
Results on the TIMIT dataset show that conditional mechanisms in recurrent architectures can reduce hidden layer updates up to 40% at the cost of about 20%  relative phone error rate increase.",0.0,4.4375
264,1413,4.15    4.35    4.8    4.45    ,Recurrent Neural Network Language Model Adaptation for Conversational Speech Recognition,"We propose two adaptation models for recurrent neural network language models (RNNLMs) to capture topic effects and long-distance triggers for conversational automatic speech recognition (ASR). We use a fast marginal adaptation (FMA) framework to adapt a RNNLM. Our first model is effectively a cache model -- the word frequencies are estimated by counting words in a conversation (with utterance-level hold-one-out) from 1st-pass decoded word lattices, and then is interpolated with a background unigram distribution. In the second model, we train a deep neural network (DNN) on conversational transcriptions to predict word frequencies given word frequencies from 1st-pass decoded word lattices. The second model can in principle model trigger and topic effects but is harder to train. Experiments on three conversational corpora show modest WER and perplexity reductions with both adaptation models.",0.0,4.4375
713,2478,3.05    5.1    5.1    4.5    ,Preference Learning with Qualitative Agreement for Sentence Level Emotional Annotations,"The perceptual evaluation of emotional attributes is noisy due to inconsistencies between annotators. The low inter-evaluator agreement arises due to the complex nature of emotions. Conventional approaches average scores provided by multiple annotators. While this approach reduces the influence of dissident annotations, previous studies have showed the value of considering individual evaluations to better capture the underlying ground-truth. One of these approaches is the qualitative agreement (QA) method, which provides an alternative framework that captures the inherent trends amongst the annotators. While previous studies have focused on using the QA method for time-continuous annotations from a fixed number of annotators, most emotional databases are annotated with attributes at the sentence-level (e.g., one global score per sentence). This study proposes a novel formulation based on the QA framework to estimate reliable sentence-level annotations for preference-learning. The proposed relative labels between pairs of sentences capture consistent trends across evaluators. The experimental evaluation shows that preference-learning methods to rank-order emotional attributes trained with the proposed QA-based labels achieve significantly better performance than the same algorithms trained with relative scores obtained by averaging absolute scores across annotators. These results show the benefits of QA-based labels for preference-learning using sentence-level annotations.",0.0,4.4375
719,2498,4.25    3.2    5.2    5.1    ,Breathy to Tense Voice Discrimination Using Zero-Time Windowing Cepstral Coefficients,"In this paper, we consider breathy to tense voices, which are often considered to be opposite ends of a voice quality continuum. Along with these, other aspects of a speaker's voice play an important role to convey the information to the listener such as mood, attitude and emotional state. The glottal pulse characteristics in different phonation types vary due to the tension of laryngeal muscles together with the respiratory effort. In the present study, we are deriving the features that can capture effects of excitation on the vocal tract system through a signal processing method, called as zero-time windowing (ZTW) method. The ZTW method gives the instantaneous spectrum which captures the changes in the speech production mechanism, providing higher spectral resolution. The cepstral coefficients derived from ZTW method are used for the classification of phonation types. Along with zero-time windowing cepstral coefficients (ZTWCCs), we use the excitation source features derived from zero frequency filtering (ZFF) method. The excitation features used are: strength of excitation, energy of excitation, loudness measure and ZFF signal energy. Classification experiments using ZTWCC and excitation features reveal a significant improvement in the detection of phonation type compared to the existing voice quality features and MFCC features.",0.0,4.4375
225,1342,5    2.95    5.05    5    4.2    ,Adding New Classes without Access to the Original Training Data with Applications to Language Identification,"In this study we address the problem of adding new classes to an existing neural network classifier. We assume that new training data with the new classes is available.
In many applications, dataset used to train machine learning algorithms contain confidential information that cannot be accessed during the process of extending the class set. We propose a method for training an extended class-set classifier using only examples with labels from the new classes while avoiding the problem of  forgetting the original classes. This incremental training method is applied to the problem of language identification. We report results on the 50 languages NIST 2015 dataset where we were able to classify all the languages even though only part of the classes was available during the first training phase and the other languages were only available during the second phase.",0.0,4.4399999999999995
444,1795,4.85    5.1    2.6    5.25    ,Cultural Differences in Pattern Matching: Multisensory Recognition of Socio-affective Prosody,"This study focuses on the cross-cultural differences in perception of audio visual prosodic recordings of Japanese social affects. The study compares cultural differences of perceptual patterns of 21 Japanese subjects with 20 French subjects who have no knowledge of Japanese language or Japanese social affects. The test material is a semantically affectively neutral utterance expressed in 9 various social affects by 2 Japanese speakers (one male, one female) who were chosen as best performers in our previous recognition experiment. The task was to create a specific audio-visual affect by choosing one video stimuli among 9 choices and one audio stimuli, again among 9 choices. The participants could preview each audio and video stimuli individually and also the combination of chosen stimuli. The results reveal that native subjects can correctly combine auditory and visually expressed social affects, showing some confusion inside semantic categories. Different matching patterns are observed for non-native subjects especially for a type of cultural-specific politeness.",0.0,4.449999999999999
421,1748,4.9    4    5.65    3.25    ,Multimodal Name Recognition in Live TV Subtitling,"In this paper, we present a method of combining a visual text reader with a system of automatic speech recognition to suppress errors when encountering out-of-vocabulary words -- specifically names. The visual text reader outputs detected words that are mapped into a large list of names via the Levenshtein distance. The detected names are inserted into the class-based language model on the fly which improves recognition results. To demonstrate the effect on the real speech recognition task we use data from sports TV broadcasting where a lot of names are present in both the audio and video streams. We superseded manual vocabulary editing in live TV subtitling through respeaking by an automated online process. Further, we show that automatically adding the names to the recognition vocabulary online and with forgetting lowers the WER relatively by 39 \% in comparison with the case when names of all sportsmen are added to the vocabulary beforehand and by 15 \% when only the relevant names are added beforehand.",0.0,4.45
534,2011,2.7    4.35    5.8    4.95    ,Multimodal Polynomial Fusion for Detecting Driver Distraction,"Distracted driving is deadly, claiming 3,477 lives in the U.S. in 2015 alone. Although there has been a considerable amount of research on modeling the distracted behavior of drivers under various conditions, accurate automatic detection using multiple modalities and especially the contribution of using the speech modality to improve accuracy has received little attention. This paper introduces a new multimodal dataset for distracted driving behavior and discusses automatic distraction detection using features from three modalities: facial expression, speech and car signals. Detailed multimodal feature analysis shows that adding more modalities monotonically increases the predictive accuracy of the model. Finally, a simple and effective multimodal fusion technique using a polynomial fusion layer shows superior distraction detection results compared to the baseline SVM and neural network models.",0.0,4.45
407,1714,4.05    5    4.3    ,Spoken SQuAD: a Study of Mitigating the Impact of Speech Recognition Errors on Listening Comprehension,"Reading comprehension has been widely studied. One of the most representative reading comprehension tasks is Stanford Question Answering Dataset (SQuAD), on which machine is already comparable with human. On the other hand, accessing large collections of multimedia or spoken content is much more difficult and time-consuming than plain text content for humans. It's therefore highly attractive to develop machines which can automatically understand spoken content. In this paper, we propose a new listening comprehension task – Spoken SQuAD. On the new task, we found that speech recognition errors have catastrophic impact on machine comprehension, and several approaches are proposed to mitigate the impact.",0.0,4.45
54,1032,4.1    5.2    5.1    3.4    ,Discriminating between Nasals and Approximants in English Language Using Zero Time Windowing,"Nasals and approximants consonants are often confused with each other. Despite the distinction in the production mechanism, these two sound classes exhibit a similar low frequency behavior, and lack significant high frequency content. The present study uses a spectral representation obtained using the zero time windowing (ZTW) analysis of speech, for the task of distinction between these two. The instantaneous spectral representation has good resolution at resonances, which helps to highlight the difference in the acoustic vocal tract system response for these sounds. The ZTW spectra around the regions of glottal closure instants are averaged to derive parameters for their classification in continuous speech. A set of parameters based on the dominant resonances, center of gravity, band energy ratio, and cumulative spectral sum in low frequencies, is derived from the average spectrum. The paper proposes classification using a knowledge--based approach and training a support vector machine. These classifiers are tested on utterances from different English speakers in the TIMIT dataset. The proposed methods result in an average classification accuracy of 90% between the two classes in continuous speech.",0.0,4.45
428,1756,4.15    4.25    4.9    4.5    ,Automated Classification of Vowel-Gesture Parameters Using External Broadband Excitation,"External broadband signal excitation applied at the speaker (or singer)’s mouth has previously been successfully used to estimate acoustic resonances of the vocal tract during speaking and singing. In this study, we used a modified, low cost, light-weight, pocket-sized and simplified version of this measurement technique, with reduced sampling time and improved low frequency detection, so that such vocal tract measurements may be easily deployed ‘in the field’ and facilitate a more ‘ecological/natural’ tracking of phonatory gestures. This system was investigated with 6 volunteer speakers phonating 17 English vowels, and the relative impedance spectrum γ (‘gamma’) was measured. Although the γ(f) signal measured here for each phonatory gesture is somewhat noisier than the original technique, it is still believed to carry some important cues associated with vocal tract configuration that produce these vowels. Features were identified both in the amplitude and phase of γ(f) and three ensemble classifiers namely random forest, gradient boosting and adaboost were trained using them. The prediction output from these classifiers were combined using soft voting to predict a class label (front-central-back; open-close). This yielded an accuracy exceeding 80% in classifying the six nominal regions of the vowel plane.",0.0,4.45
334,1553,4.85    5    5    3.65    3.75    ,DA-IICT/IIITV System for Low Resource Speech Recognition Challenge 2018,"This paper presents an Automatic Speech Recognition (ASR) system, in the Gujarati language, developed for Low Resource Speech Recognition Challenge for Indian Languages in INTERSPEECH 2018. For front-end, Amplitude Modulation (AM) features are extracted using the standard and data-driven auditory filterbanks. Recurrent Neural Network Language Models (RNNLM) are used for this task. There is a relative improvement of 36.18 % and 40.95 % in perplexity on the test and blind test sets, respectively, compared to 3-gram LM. TimeDelay Neural Network (TDNN) and TDNN-Long Short-Term Memory (LSTM) models are employed for acoustic modeling.
The statistical significance of proposed approaches is justified using a bootstrap-based % Probability of Improvement (POI) measure. RNNLM rescoring with 3-gram LM gave an absolute reduction of 0.69-1.29 % in Word Error Rate (WER) for various feature sets. AM features extracted using the gammatone filterbank (AM-GTFB) performed well on the blind test set compared to the FBANK baseline (POI>70 %). The combination of ASR systems further increased the performance with an absolute reduction of 1.89 and 2.24 % in WER for test and blind test sets, respectively (100 % POI).",0.0,4.45
639,2304,4.2    4.35    4.8    ,"Joint Discriminative Embedding Learning, Speech Activity and Overlap Detection for the DIHARD Speaker Diarization Challenge","The DIHARD is a new, annual speaker diarization challenge focusing on hard domains, i.e. datasets in which current state-of-the-art systems are expected to perform poorly. We present our diarization system, which is a neural network jointly optimized for speaker embedding learning, speech activity and overlap detection. We present our network topology and the affinity matrix loss objective function responsible for learning the frame-wise speaker embeddings. The outputs of the network are then clustered with KMeans, and each frame classified with speech activity is assigned to one or two speakers, depending on the overlap detection. For the training data, we used two well-know meeting corpora - the AMI and the ICSI datasets, together with the provided samples from the DIHARD challenge. To further enhance our system, we present three data augmentation settings: the first is a naive concatenation of isolated speaker utterances from non-diarization datasets, which generates artificial diarization prompts. The second is a simple noise addition with sampled signal-to-noise ratios. The third is using noise suppression over the development data. All training setups are compared in terms of diarization error rate and mutual information in the evaluation set of the challenge.",0.0,4.45
558,2067,4.8    4.25    4.7    4.05    ,Engagement Recognition in Spoken Dialogue via Neural Network by Aggregating Different Annotators' Models,"This paper addresses engagement recognition based on four multimodal listener behaviors - backchannels, laughing, eye-gaze, and head nodding.
Engagement is an indicator of how much a user is interested in the current dialogue.
Multiple third-party annotators give ground truth labels of engagement in a human-robot interaction corpus.
Since perception of engagement is subjective, the annotations are sometimes different between individual annotators.
Conventional methods directly use integrated labels, such as those generated through simple majority voting, and do not consider each annotator's recognition.
We propose a two-step engagement recognition where each annotator's recognition is modeled and the different annotators' models are aggregated to recognize the integrated label.
The proposed neural network consists of two parts.
The first part corresponds to each annotator's model which is trained with the corresponding labels independently.
The second part aggregates the different annotators' models to obtain one integrated label.
After each part is pre-trained, the whole network is fine-tuned through back-propagation of prediction errors.
Experimental results show that the proposed network outperforms baseline models which directly recognize the integrated label without considering differing annotations.",0.0,4.45
242,1373,4.35    5    3.45    5    ,Cross-cultural (A)symmetries in Audio-visual Attitude Perception,"This paper evaluates results from a cross-cultural and cross-language experiment series employing short audio-visual utterances produced with varying attitudinal expressions. German and Cantonese-speaking participants freely labeled such utterances in the two languages and assigned to each stimulus a verbal label. Based on the results of the four experiments we were able to establish to what degree the attitudinal frames of reference of the two groups overlap and how they differ. Verbal labels were assessed regarding their emotional content in terms of valence, activation and dominance, and for the linguistic opposition between assertive and interrogative speech act, and hence permit to abstract from the language of the rater and ultimately even abstract from the attitudinal categories used when eliciting the stimuli. Instead we regard each utterance as a data-point in the emotional space. We found that the judgments of the two rater groups agree well with respect to the valence of attitudinal expressions and diverge most as to the perceived activation of the stimulus presenter. Cantonese speaking participants seem to mirror Germans’ ratings of German stimuli better than vice versa, which suggests an interesting asymmetry of attitudinal perception. The acoustic channel primarily transmits linguistic information and the visual channel emotional content.",0.0,4.45
221,1331,5.05    3.8    4.8    4.2    ,"Vocalic, Lexical and Prosodic Cues for the INTERSPEECH 2018 Self-Assessed Affect Challenge","The INTERSPEECH 2018 Self-Assessed Affect Challenge consists in the prediction of the affective state of mind from speech. Experiments were conducted on the Ulm State-of-Mind in Speech database (USoMS) where subjects self-report their affective state. Dimensional representation of emotion (valence) is used for labeling. We have investigated cues related to the perception of the emotional valence according to three main relevant linguistic levels: phonetics, lexical and prosodic. For this purpose we studied: the degree-of-articulation, the voice quality, an affect lexicon and the expressive prosodic contours. For the phonetics level, a set of gender-dependent audio-features was computed on vowel analysis (voice quality and speech articulation measurements). At the lexical level, an affect lexicon was extracted from the automatic transcription of the USoMS database. This lexicon has been assessed for the Challenge task comparatively to a reference polarity lexicon. In order to detect expressive prosody, N-gram models of the prosodic contours were computed from an intonation labeling system. At last, an emotional valence classifier was designed combining ComParE and eGeMAPS feature sets with other phonetic, prosodic and lexical features. Experiments have shown an improvement of 2.4% on the Test set, compared to the baseline performance of the Challenge.",0.0,4.4624999999999995
211,1310,4.95    4.3    4.3    4.3    ,Harmonic-Percussive Source Separation of Polyphonic Music by Suppressing Impulsive Noise Events,"In recent years, harmonic-percussive source separation methods are gaining importance because of their potential applications in many music information retrieval tasks. The goal of the decomposition methods is to achieve near real-time separation, distortion and artifact free component spectrograms and their equivalent time domain signals for potential music applications. In this paper, we propose a decomposition method based on filtering/suppressing the impulsive interference of percussive source on the harmonic components and impulsive interference of the harmonic source on the percussive components by modified moving average filter in the Fourier frequency domain. The significant advantage of the proposed method is that it minimizes the artifacts in the separated signal spectrograms. In this work, we have proposed Affine and Gain masking methods to separate the harmonic and percussive components to achieve minimal spectral leakage. The objective measures and separated spectrograms showed that the proposed method is better than the existing rank-order filtering based harmonic-percussive separation methods.",0.0,4.4625
230,1349,5.25    4.2    5.15    3.25    ,An Open Source Emotional Speech Corpus for Human Robot Interaction Applications,"For further understanding the wide array of emotions embedded
in human speech, we are introducing a strictly-guided simulated emotional
speech corpus. In contrast to existing speech corpora, this was constructed
by maintaining an equal distribution of 4 long vowels in New Zealand English. This balance is to facilitate emotion related formant and glottal source feature comparison studies.  Also, the corpus has 5 secondary emotions and 5 primary emotions.  Secondary emotions are important in  Human-Robot Interaction (HRI) to model natural conversations among humans and robots. But there are few existing speech resources to study these emotions, which has motivated the creation of this corpus. A large scale perception test with 120 participants showed that the corpus has approximately 70% and 40% accuracy in the correct classification of primary and secondary emotions respectively. The reasons behind the differences in perception accuracies of the two  emotion types is further investigated. A preliminary prosodic analysis of corpus shows significant differences among the emotions. The corpus is made public at: github.com/tli725/JL-Corpus.",0.0,4.4625
114,1135,4.15    4.2    4.5    5    ,Joint Noise and Reverberation Adaptive Learning for Robust Speaker DOA Estimation with an Acoustic Vector Sensor,"Deep neural network (DNN) based DOA estimation (DNN-DOAest) methods report superior performance but the degradation is observed under stronger additive noise and room reverberation conditions. Motivated by our previous work with an acoustic vector sensor (AVS) and the great success of DNN based speech denoising and dereverberation (DNN-SDD), a unified DNN framework for robust DOA estimation task is thoroughly investigated in this paper. First, a novel DOA cue termed as sub-band inter-sensor data ratio (Sb-ISDR) is proposed to efficiently represent DOA information for training a DNN-DOAest model. Second, a speech-aware DNN-SDD is presented, where coherence vectors denoting the probability of time-frequency points dominated by speech signals are used as additional input to facilitate the training to predict complex ideal ratio masks. Last, by stacking the DNN-DOAest on the DNN-SDD with a joint part, the unified network is jointly fine-tuned, which enables DNN-SDD to serve as a pre-processing front-end to adaptively generate ‘clean’ speech features that are easier to be correctly classified by the following DNN-DOAest for robust DOA estimation. Experimental results on simulated and recorded data confirm the effectiveness and superiority of our proposed methods under different noise and reverberations compared with baseline methods.",0.0,4.4625
525,1979,3.6    5.9    5.15    3.2    ,Efficient Keyword Spotting Using Time Delay Neural Networks,"This paper describes a novel method of live keyword spotting using a two-stage time delay neural network. This model is trained using transfer learning: initial training with phone targets from a large speech corpus is followed by training with keyword targets from a smaller data set. The accuracy of the system is evaluated on two separate tasks. The first is the freely available Google Speech Commands dataset. The second is an in-house task specifically developed for keyword spotting. The results show significant improvements in false accept and false reject rates in both clean and noisy environments when compared with previously known techniques. Furthermore, we investigate various techniques to reduce computation in terms of multiplications per second of audio. Compared to recently published work, the proposed system provides up to 89% savings on computational complexity.",0.0,4.4625
922,1397,5.75    3.5    4.35    4.25    ,Speech Assessment Using Generative Adversarial Networks,"Most automatic speech assessment techniques for Computer-Assisted Pronunciation Training (CAPT) use acoustic features and exploit higher-level information like articulatory attributes. Recent breakthrough in the deep generative modeling includes generative adversarial networks (GANs). GANs have achieved a good level of success in the computer vision and speech enhancement field, however, they have not yet been applied to speech assessment or speech learning tasks. In this paper, we propose to use the adversarial framework to generate speech signals and assess its goodness. In contrast to traditional or current techniques, we operate at the waveform level, training the model end-to-end. We use 300 Korean words produced by 50 native and 123 non-native speakers of diverse backgrounds and learning levels to train the network. Experimental results confirm the viability of the discriminator as an effective evaluator of goodness-of-pronunciation. This experiment not only opens possibilities of generative architectures for speech assessment, but also of extending the work to mispronunciation diagnosis by using the corrective ability of the generator.",1.0,4.4625
195,1286,4.15    4.75    4.75    4.2    ,A Weighted Superposition of Functional Contours Model for Modelling Contextual Prominence of Elementary Prosodic Contours,"The way speech prosody encodes linguistic, paralinguistic and non-linguistic information via multiparametric representations of the speech signals is still an open issue. The Superposition of Functional Contours (SFC) model proposes to decompose prosody into elementary multiparametric functional contours through the iterative training of neural network contour generators using analysis-by-synthesis. Each generator is responsible for computing multiparametric contours that encode one given linguistic, paralinguistic and non-linguistic information on a variable scope of rhythmic units. The contributions of all generators' outputs are then overlapped and added to produce the prosody of the utterance. We propose an extension of the contour generators that allows them to model the prominence of the elementary contours based on contextual information. WSFC jointly learns the patterns of the elementary multiparametric functional contours and their weights dependent on the contours' contexts. The experimental results show that the proposed weighted SFC (WSFC) model can successfully capture contour prominence and thus improve SFC modelling performance. The WSFC is also shown to be effective at modelling the impact of attitudes on the prominence of functional contours cuing syntactic relations in French, and that of emphasis on the prominence of tone contours in Chinese.",0.0,4.4625
98,1110,5.2    5.15    4.95    2.55    ,L2-ARCTIC: a Non-native English Speech Corpus,"In this paper, we introduce L2-ARCTIC, a speech corpus of non-native English that is intended for research in voice conversion, accent conversion, and mispronunciation detection. This initial release includes recordings from ten non-native speakers of English whose first languages (L1s) are Hindi, Korean, Mandarin, Spanish, and Arabic, each L1 containing recordings from one male and one female speaker. Each speaker recorded approximately one hour of read speech from the Carnegie Mellon University ARCTIC prompts, from which we generated orthographic and forced-aligned phonetic transcriptions. In addition, we manually annotated 150 utterances per speaker to identify three types of mispronunciation errors: substitutions, deletions, and additions, making it a valuable resource not only for research in voice conversion and accent conversion but also in computer-assisted pronunciation training. The corpus is publicly accessible at https://psi.engr.tamu.edu/l2-arctic-corpus/.",0.0,4.4625
615,2225,4.1    5.15    4.3    4.3    ,Acoustic and Perceptual Characteristics of Mandarin Speech in Homosexual and Heterosexual Male Speakers,"The present study investigated both acoustic and perceptual characteristics of Mandarin speech in homosexual and heterosexual male speakers. Acoustic analyses of monosyllabic words showed significant differences between the two groups in F0 features (including the mean, the max, and the range), F1 and F2 of vowels, aspiration/frication duration of consonants, and center of gravity as well as skewness for /s/. Especially, the patterns were found to be opposite between Mandarin and American English speakers, which might be due to social psychological differences between the two societies. The perceptual experiment showed that the perceived score of gayness differed significantly between the speeches of the two groups. Among those acoustic parameters showing significant differences, fricative duration may be the most salient cue for sexual orientation of Mandarin male speakers.",0.0,4.4625
185,1267,5.1    3.3    5    ,Automatic Pronunciation Evaluation of Singing,"In this work, we develop a strategy to automatically evaluate pronunciation of singing. We apply singing-adapted automatic speech recognizer (ASR) in a two-stage approach for evaluating pronunciation of singing. First, we force-align the lyrics with the sung utterances to obtain the word boundaries. We improve the word boundaries by a novel lexical modification technique. Second, we investigate the performance of the phonetic posteriorgram (PPG) based template independent and dependent methods for scoring the aligned words. To validate the evaluation scheme, we obtain reliable human pronunciation evaluation scores using a crowd-sourcing platform. We show that the automatic evaluation scheme offers quality scores that are close to human judgments.",0.0,4.466666666666666
451,1806,4.3    5.05    4.05    ,Mining Multimodal Repositories for Speech Affecting Diseases,"The motivation for this work is to contribute to the collection of large in-the-wild multimodal datasets in which the speech of the subject is affected by certain medical conditions. Our mining effort is focused on video blogs (vlogs), and as a proof-of-concept we have selected three target diseases: Depression, Parkinson's disease, and cold.

Given the large scale nature of the online repositories, we take advantage of existing retrieval algorithms to narrow the pool of candidate videos for a given query related with the disease (e.g. depression vlog), and on top of that we apply several filtering techniques. These techniques explore both audio, video, text and metadata cues, in order to retrieve vlogs that include a single speaker which, at some point, admits that he/she is currently affected by a given disease. The use of straightforward NLP techniques on the automatically transcribed data showed that distinguishing between narratives of present and past experiences is harder than distinguishing between narratives of self experiences and of someone else's.

The three resulting speech datasets were tested with neural networks trained with speech data collected in controlled conditions, yielding results only slightly below the ones achieved with the original test datasets.",0.0,4.466666666666666
175,1251,5.1    3.4    4.9    ,Pitch-Adaptive Front-end Feature for Hypernasality Detection,"Hypernasality in cleft palate (CP) children is due to the velopharyngeal insufficiency. The vowels get nasalized in hypernasal speech and the nasality evidence are mainly present in low-frequency region around the first formant $(F_{1})$ of vowels. The detection of hypernasality using Mel-frequency cepstral coefficient (MFCC) feature may get affected because the feature might not be able to capture the nasality evidence present in the low-frequency region. This is due to the fact that the MFCC feature extracted from high pitched children speech contains the pitch harmonics effect of magnitude spectrum. The pitch harmonics effect results in high variance for the higher dimensions of MFCC coefficients. This problem may increase due to high perturbation in pitch of CP speech. So in this work, a pitch-adaptive MFCC feature is used for hypernasality detection. The feature is derived from the cepstral smooth spectrum instead of magnitude spectrum. A pitch-adaptive low time liftering is done to smooth out the pitch harmonics. This feature when used for the detection of hypernasality using support vector machine (SVM) gives an accuracy of 83.45 \%, 88.04 and \%, 85.58 \% for /a/, /i/ and /u/ vowels respectively, which is better than the accuracy of MFCC feature.",0.0,4.466666666666667
472,1849,5.75    5    3.95    3.2    ,A Preliminary Study on Tonal Coarticulation in Continuous Speech,"Tonal variations in continuous speech are complicated in nature and it is a challenge to identify the effect of tonal coarticulation given several influencing factors. To address the issue, the present study proposes a scheme for labeling tonal coarticulation in Mandarin continuous speech by applying Hypo- and Hyper-articulation theory. We assume that the bidirectional tonal coarticulation (both carryover and anticipatory effects) as patterns of Hypo-articulation results from the economical articulatory rule. The effects may partially disappear under the influence of specific stress patterns and become unidirectional (carryover or anticipatory). At a prosodic boundary, the effects of tonal coarticulation may completely disappear and lead to the occurrence of patterns of Hyper-articulation. Based on the scheme, we have labeled the data in the Annotated Speech Corpus of Chinese Discourse. It is shown that: three annotators are consistent at a fairly high level (86.2%) on average, and the acoustic parameters of four kinds of tonal coarticulation are significantly different. Therefore, we conclude that the proposal is feasible for investigating tonal coarticulation in Mandarin continuous speech. Though the labeling scheme is language dependent, it may well have cross-linguistic implications.",0.0,4.475
260,1404,6    4.4    4.25    3.25    ,Gestural Lenition of Rhotics Captures Variation in Brazilian Portuguese,"The goal of this study is to examine the rhotics in Brazilian Portuguese (BP), /ɾ,ʁ/ and the ‘archetypal’ coda /R/, to determine if: (1) they can be characterized as a coordination of the tongue dorsum and tongue body or tip and (2) manipulation of the gestural settings accounts for rhotic allophony in BP.

Six native speakers of BP participated in an ultrasound experiment and produced target phonemes in #CV, VCV, and VC# environments with the vowels /i, e, a, o/. Tongue contours for the rhotics were compared using Smoothing Spline ANOVAs. /ɾ, ʁ/ were produced with a tongue body and dorsum gesture, while /ɾ/ also had an apical gesture. Archetypal /R/ was realized variably, as any of [ɾ, ɻ, ɹ, χ].

BP rhotics can be described as the coordination of a tongue dorsum and a tongue body or tip gesture. ‘Archetypal’ /R/ is posited to be /ɾ/. Allophony between /ɾ/ and [ɻ, ɹ, χ] is due to tongue tip lenition.  Allophony between /ʁ/ and [h] is due to weakening of the tongue dorsum and body gestures. This analysis suggests synchronic and diachronic changes of rhotics result from lenition. It also captures the rarity of diachronic changes from uvulars to alveolars.",0.0,4.475
634,2295,3.9    4.1    4.1    5.8    ,Multimodal I-vectors to Detect and Evaluate Parkinson's Disease,"Parkinson's Disease (PD) is a neurodegenerative disorder characterized by a variety of motor symptoms. PD patients show several motor deficits, including   speech deficits, impaired handwriting, and gait disturbances. In this work we propose a methodology to fuse i-vectors extracted from three different bio-signals: speech, handwriting, and gait. These i-vectors are used to classify Parkinson's Disease patients and healthy controls, and to evaluate the neurological state of the patients. Speech i-vectors are extracted from MFCCs, handwriting i-vectors are extracted from kinematic features, and gait i-vectors are extracted from modified MFCCs computed from inertial sensor signals. Two fusion strategies are tested: concatenating the i-vectors of a subject to form a super-i-vector with information from the three bio-signals and score pooling. The proposed fusion methods leads to better classification results respect to the separate analysis with each bio-signal, reaching an accuracy of up to 85%.",0.0,4.475
48,1023,5.1    2.6    5.2    5    ,MTGAN: Speaker Verification through Multitasking Triplet Generative Adversarial Networks,"In this paper, we propose an enhanced triplet method that improves the encoding process of embeddings by jointly utilizing generative adversarial mechanism and multitasking optimization. We extend our triplet encoder with Generative Adversarial Networks (GANs) and softmax loss function. GAN is introduced for increasing the generality and diversity of samples, while softmax is for reinforcing features about speakers. For simplification, we term our method Multitasking Triplet Generative Adversarial Networks (MTGAN). Experiment on short utterances demonstrates that MTGAN reduces the verification equal error rate (EER) by 67% (relatively) and 32% (relatively)
over conventional i-vector method and state-of-the-art triplet loss method respectively. This effectively indicates that MTGAN outperforms triplet methods in the aspect of expressing the high-level feature of speaker information.",0.0,4.475
96,1107,4.9    4.35    4.35    4.3    ,Syllable-Based Sequence-to-Sequence Speech Recognition with the Transformer in Mandarin Chinese,"Sequence-to-sequence attention-based models have recently
shown very promising results on automatic speech recognition
(ASR) tasks, which integrate an acoustic, pronunciation and
language model into a single neural network. In these models,
the Transformer, a new sequence-to-sequence attentionbased
model relying entirely on self-attention without using
RNNs or convolutions, achieves a new single-model state-ofthe-
art BLEU on neural machine translation (NMT) tasks. Since
the outstanding performance of the Transformer, we extend
it to speech and concentrate on it as the basic architecture of
sequence-to-sequence attention-based model on Mandarin Chinese
ASR tasks. Furthermore, we investigate a comparison between
syllable based model and context-independent phoneme
(CI-phoneme) based model with the Transformer in Mandarin
Chinese. Additionally, a greedy cascading decoder with the
Transformer is proposed for mapping CI-phoneme sequences
and syllable sequences into word sequences. Experiments on
HKUST datasets demonstrate that syllable based model with
the Transformer performs better than CI-phoneme based counterpart,
and achieves a character error rate (CER) of 28.77%,
which is competitive to the state-of-the-art CER of 28.0% by
the joint CTC-attention based encoder-decoder network.",0.0,4.475
64,1047,3.35    5.05    4.35    5.15    ,Comparison of Syllabification Algorithms and Training Strategies for Robust Word Count Estimation across Different Languages and Recording Conditions,"Word count estimation (WCE) from audio recordings has a number of applications, including quantifying the amount of speech that language-learning infants hear in their natural environments, as captured by daylong recordings made with devices worn by infants. To be applicable in a wide range of scenarios and also low-resource domains, WCE tools should be extremely robust against varying signal conditions and require minimal access to labeled training data in the target domain. For this purpose, earlier work has used automatic syllabification of speech, followed by a least-squares-mapping of syllables to word counts. This paper compares a number of previously proposed syllabifiers in the WCE task, including a supervised bi-directional long short-term memory (BLSTM) network that is trained on a language for which high quality syllable annotations are available (a “high resource language”), and reports how the alternative methods compare on different languages and signal conditions. We also explore additive noise and varying-channel data augmentation strategies for BLSTM training, and show how they improve performance in both matching and mismatching languages. Intriguingly, we also find that even though the BLSTM works on languages beyond its training data, the unsupervised algorithms can still outperform it in challenging signal conditions on novel languages.",0.0,4.475
453,1808,5.2    4.2    4.45    4.05    ,Slot Filling with Delexicalized Sentence Generation,"We introduce a novel approach that jointly learns slot filling and delexicalized sentence generation. There have been recent attempts to tackle slot filling as a type of sequence labeling problem, with encoder-decoder attention framework. We further improve the framework by training the model to generate delexicalized sentences, in which words according to slot values are replaced with slot labels. Slot filling with delexicalization shows better results compared to models having a single learning objective of filling slots. The proposed method achieves state-of-the-art slot filling performance on ATIS dataset. We experiment different variants of our model and find that delexicalization encourages generalization by sharing weights among the words with same labels and helps the model to further leverage certain linguistic features.",0.0,4.4750000000000005
688,2427,4.45    4.9    4.3    4.25    ,A Probability Weighted Beamformer for Noise Robust ASR,"We investigate a novel approach to spatial filtering that is adaptive to conditions at different time-frequency (TF) points for noise removal by taking advantage of speech sparsity. Our approach combines a noise reduction beamformer with a minimum variance distortionless response (MVDR) beamformer or Generalized Eigenvalue (GEV) beamformer through TF posterior probabilities of speech presence (PPSP). To estimate PPSP, we study both statistical model-based and neural network based methods, where in the former, we use complex Gaussian mixture modeling (CGMM) on temporally augmented spatial spectral features, and in the latter, we use neural network (NN) based TF masks to initialize speech and noise covariance matrices in CGMM. We have conducted experiments on CHiME-3 task. On its real noisy speech test set, our methods of feature augmentation, TF dependent spatial filter, and NN-based mask initialization on covariances for CGMM have yielded relative word error rate (WER) reductions cumulatively by 8%, 16%, and 25% over the original CGMM based MVDR. On the real test data, the three methods have also produced consistent WER reductions when replacing MVDR by GEV.",0.0,4.4750000000000005
617,2228,5.9    3.4    4.15    ,Emotion Recognition from Variable-Length Speech Segments Using Deep Learning on Spectrograms,"In this work, an approach of emotion recognition is proposed for variable-length speech segments by applying deep neutral network to spectrograms directly. The spectrogram carries comprehensive para-lingual information that are useful for emotion recognition. We tried to extract such information from spectrograms and accomplish the emotion recognition task by combining Convolutional Neural Networks (CNNs) with Recurrent Neural Networks (RNNs). To handle the variable-length speech segments, we proposed a specially designed neural network structure that accepts variable-length speech sentences directly as input. Compared to the traditional methods that split the sentence into smaller fixed-length segments, our method can solve the problem of accuracy degradation introduced in the speech segmentation process. We evaluated the emotion recognition model on the IEMOCAP dataset over four emotions. Experimental results demonstrate that the proposed method outperforms the fixed-length neural network on both weighted accuracy (WA) and unweighted accuracy (UA).",0.0,4.483333333333333
398,1694,4.4    4.35    4.2    5    ,A Hybrid Approach to Grapheme to Phoneme Conversion in Assamese,"Assamese is one of the low resource Indian languages. This paper implements both rule-based and data-driven grapheme to phoneme (G2P) conversion systems for Assamese. The rule-based system is used as the baseline which yields a word error rate of 35.3%. The data-driven systems are implemented using state-of-the-art sequence learning techniques such as —i) Joint-Sequence Model (JSM), ii) Recurrent Neural Networks with LTSM cell (LSTM-RNN) and iii) bidirectional LSTM (BiLSTM). The BiLSTM yields the lowest WER i.e., 18.7%, which
is an absolute 16.6% improvement on the baseline system. We additionally implement the rules of syllabification for Assamese. The surface output is generated in two forms namely i) phonemic sequence with syllable boundaries, and ii) only phonemic sequence. The output
of BiLSTM is fed as an input to Hybrid system. The Hybrid system syllabifies the input phonemic sequences to apply the vowel harmony rules. It also applies the rules of schwa-deletion as well as some rules in which the consonants change their form in clusters. The accuracy of the Hybrid system is 17.3% which is an absolute 1.4% improvement over the BiLSTM based G2P.",0.0,4.4875
129,1160,5    3.7    4.25    5    ,Forward-Backward Attention Decoder,"This paper investigates how forward and backward attentions
can be integrated to improve the performance of attention-based 
sequence-to-sequence (seq2seq) speech recognition systems.
In the proposed approach, speech is decoded from left to
right as well as from right to left utilizing forward and
backward attention vectors, and the best sentence hypothesis
is searched for according to combined probabilities provided
by the decoders of two directions.
Our method takes advantage of two distinct and complementary
ways of extracting information from the asymmetric time
structure of speech. It also mitigates a drawback of
attention-based models that they tend to output less
reliable labels due to error accumulation when the utterance becomes
longer.
We also show the effectiveness of a multitask learning in
which the forward decoder is jointly trained with 
backward decoding sharing a single encoder.
The proposed forward-backward decoding improved word error
rates (WERs) of word-level attention models by up to 12.7 \%
relative in speech recognition experiments using large-scale
spontaneous speech corpora.
They achieve much higher
performances than a state-of-the-art hybrid DNN-HMM system
while retaining the advantage of very low latency.",0.0,4.4875
658,2359,5.2    4.35    4.2    4.2    ,Speaker Adaptive Audio-Visual Fusion for the Open-Vocabulary Section of AVICAR,"This experimental study establishes the first audio-visual speech recognition baseline for the TIMIT sentence portion of the AVICAR dataset, a dataset recorded in a real, noisy car environment. We use an automatic speech recognizer trained on a larger dataset to generate an audio-only recognition baseline for AVICAR. We utilize the forced alignment of the audio modality of AVICAR to get training targets for the convolutional neural network based visual front end. Based on our observation that there is a great amount of variation between visual features of different speakers, we apply feature space maximum likelihood linear regression (fMMLR) based speaker adaptation to the visual features. We find that the quality of fMLLR is sensitive to the quality of the alignment probabilities used to compute it; experimental tests compare the quality of fMLLR trained using audio-visual versus audio-only alignment probabilities. We report the first audio-visual results for TIMIT subset of AVICAR and show that the word error rate of the proposed audio-visual system is significantly better than that of the audio-only system.",0.0,4.4875
148,1210,4.35    4.95    5.85    2.8    ,Collapsed Speech Segment Detection and Suppression for WaveNet Vocoder,"In this paper, we propose a technique to alleviate the quality degradation caused by collapsed speech segments sometimes generated by the WaveNet vocoder. The effectiveness of the WaveNet vocoder for generating natural speech from acoustic features has been proved in recent works. However, it sometimes generates very noisy speech with collapsed speech segments when only a limited amount of training data is available or significant acoustic mismatches exist between the training and testing data. Such a limitation on the corpus and limited ability of the model can easily occur in some speech generation applications, such as voice conversion and speech enhancement.  To address this problem, we propose a technique to automatically detect collapsed speech segments. Moreover, to refine the detected segments, we also propose a waveform generation technique for WaveNet using a linear predictive coding constraint. Verification and subjective tests are conducted to investigate the effectiveness of the proposed techniques. The verification results indicate that the detection technique can detect most collapsed segments. The subjective evaluations of voice conversion demonstrate that the generation technique significantly improves the speech quality while maintaining the same speaker similarity. 
Index Terms: speech generation, WaveNet, vocoder, linear predictive coding, collapsed speech detection, voice conversion",0.0,4.4875
75,1070,5.2    4.15    3.6    5    ,i-Vectors in Language Modeling: an Efficient Way of Domain Adaptation for Feed-Forward Models,"We show an effective way of adding context information to
shallow neural language models. We propose to use Subspace
Multinomial Model (SMM) for context modeling and we add
the extracted i-vectors in a computationally efficient way. By
adding this information, we shrink the gap between shallow
feed-forward network and an LSTM from 65 to 31 points of per-
plexity on the Wikitext-2 corpus (in the case of neural 5-gram
model). Furthermore, we show that SMM i-vectors are suit-
able for domain adaptation and a very small amount of adap-
tation data (e.g. endmost 5 % of a Wikipedia article) brings a
substantial improvement. Our proposed changes are compati-
ble with most optimization techniques used for shallow feed-
forward LMs.",0.0,4.487500000000001
424,1751,3.85    4.95    4.9    4.25    ,Data Augmentation Using Healthy Speech for Dysarthric Speech Recognition,"Dysarthria refers to a speech disorder caused by trauma to the
brain areas concerned with motor aspects of speech giving rise
to effortful, slow, slurred or prosodically abnormal speech. 
Traditional Automatic Speech Recognizers (ASR) perform poorly
on dysarthric speech recognition tasks, owing mostly to 
insufficient dysarthric speech data. Speaker related challenges 
complicates data collection process for dysarthric speech.   
In this paper, we explore data augmentation using temporal and speed
modifications of healthy speech to simulate dysarthric speech.
DNN-HMM based Automatic Speech Recognition (ASR) and
Random  Forest  based  classification  were  used  for  evaluation
of the proposed method. Dysarthric speech generated synthetically 
is classified for severity using a Random Forest classifier that is trained
on  actual  dysarthric  speech.   ASR  trained  on  healthy  speech
augmented  with  simulated  dysarthric  speech  is  evaluated  for
dysarthric speech recognition. All evaluations were carried out
using Universal Access dysarthric speech corpus.  An absolute
improvement of was 4.24% and 2% achieved using tempo based
and speed based data augmentation respectively as compared to
ASR performance using healthy speech alone or training.",0.0,4.487500000000001
1258,2292,5.15    5.75    3.2    3.9    ,The Recognition of Imagined Words Using Dynamic Time Warping on EEG Signals,"The recognition of imagined speech could be the most intuitive type of brain-computer interface for people with severe speech disability. As a consequence, there has been increasing interest in classifying different imagined words from electroencephalogram (EEG) signals. However, in previous studies, the time variations of imagination reflected in EEG signals have not been considered. These variations, caused by differences in the starting time and duration of the imagined words, could have a detrimental affect on the classification accuracy. In this paper, for the first time, such temporal variations are taken into account and minimised using a  dynamic time warping (DTW)-based algorithm, whereby the distances between imagined words after matching by DTW are used as classification features. The proposed algorithm was evaluated using EEG data collected from 10 subjects who imagined five different words. In this experiment, the subjects had control over the imagination length. Our results show that the DTW-based approach using linear discriminant analysis as the classifier yielded an average 52.4% accuracy in classification of five words; a significant (16.6%) improvement over state-of-the-art approaches based on the discrete wavelet transform.",1.0,4.5
694,2439,3.55    4.8    4.4    5.25    ,Concatenative Resynthesis with Improved Training Signals for Speech Enhancement,"Noise reduction in speech signals remains an important area of research with potential for high impact in speech processing domains such as voice communication and hearing prostheses.  We extend and demonstrate significant improvements to our previous work in synthesis-based speech enhancement, which performs concatenative resynthesis of speech signals for the production of noiseless, high quality speech.  Concatenative resynthesis methods perform unit selection through learned non-linear similarity functions between short chunks of clean and noisy signals.  These mappings are learned using deep neural networks (DNN) trained to predict high similarity for the exact chunk of speech that is contained within a chunk of noisy speech, and low similarity for all other pairings.  We find here that more robust mappings can be learned with a more efficient use of the available data by selecting pairings that are not exact matches, but contain similar clean speech that matches the original in terms of acoustic, phonetic, and prosodic content.  The resulting output is evaluated on the small vocabulary CHiME2-GRID corpus and outperforms our original baseline system in terms of intelligibility by combining phonetic similarity with similarity of acoustic intensity, fundamental frequency, and periodicity.",0.0,4.5
599,2185,5.1    4.3    3.6    5    ,Role Play Dialogue Aware Language Models Based on Conditional Hierarchical Recurrent Encoder-Decoder,"We propose role play dialogue-aware language models (RPDA-LMs) that can leverage interactive contexts in role play multi-turn dialogues for estimating the generative probability of words. Our motivation is to improve automatic speech recognition (ASR) performance in role play dialogues such as contact center dialogues and service center dialogues. Although long short-term memory recurrent neural network based language models (LSTM-RNN-LMs) can capture long-range contexts within an utterance, they cannot utilize sequential interactive information between speakers in multi-turn dialogues. Our idea is to explicitly leverage speakers' roles of individual utterances, which are often available in role play dialogues, for neural language modeling. The RPDA-LMs are represented as a generative model conditioned by a role sequence of a target role play dialogue. We compose the RPDA-LMs by extending hierarchical recurrent encoder-decoder modeling so as to handle the role information. Our ASR evaluation in a contact center dialogue demonstrates that RPDA-LMs outperform LSTM-RNN-LMs and document-context LMs in terms of perplexity and word error rate. In addition, we verify the effectiveness of explicitly taking interactive contexts into consideration.",0.0,4.5
61,1043,4.45    4.1    5.1    4.35    ,Designing a Pneumatic Bionic Voice Prosthesis - Statistical Approach for Source Excitation Generation,"This study follows up on our pioneering work in designing a Pneumatic Bionic Voice (PBV) prosthesis for larynx amputees. PBV prostheses are electronic adaptations of the traditional Pneumatic Artificial Larynx (PAL) device. The PAL is a non-invasive mechanical voice source, driven exclusively by respiration and with an exceptionally high voice quality. Following the PAL design closely, the PBV prosthesis is
anticipated to substitute the medical gold standard of voice prostheses by generating a similar voice quality while remaining non-invasive and non-surgical. This paper describes a statistical approach to estimate the excitation waveform of the PBV source using the PAL as a reference. A Gaussian mixture
model of the joint probability density of respiration and PAL voice features is implemented to estimate the excitation waveform of the PBV. The evaluation on a database of more than two hours of continuous speech shows a close match between f0 pattern and mel-cepstra of the estimated PBV source
and the PAL. When used to re-synthesize the original speech, the intelligibility of the PBV speech remains high and is scored 7.1±0.4 compared to 7.9±0.15 of the original PAL source.",0.0,4.5
660,2361,5.8    3.5    4.2    ,Lightly Supervised vs. Semi-supervised Training of Acoustic Model on Luxembourgish for Low-resource Automatic Speech Recognition,"In this work, we focus on exploiting ‘inexpensive’ data in order to to improve the DNN acoustic model for ASR. We explore two strategies: The first one uses untranscribed data from the target domain. The second one is related to the proper selection of excerpts from imperfectly transcribed out-of-domain public data, as parliamentary speeches. We found out that both approaches lead to similar results, making them equally beneficial for practical use. The Luxembourgish ASR seed system had a 38.8% WER and it improved by roughly 4% absolute, leading to 34.6% for untranscribed and 34.9% for lightlysupervised data. Adding both databases simultaneously led to 34.4% WER, which is only a small improvement. As a
secondary research topic, we experiment with semi-supervised state-level minimum Bayes risk (sMBR) training. Nonetheless, for sMBR we saw no improvement from adding the automatically
transcribed target data, despite that similar techniques yield good results in the case of cross-entropy (CE) training.",0.0,4.5
695,2440,3.45    5.15    5.2    4.2    ,Student-Teacher Learning for BLSTM Mask-based Speech Enhancement,"Spectral mask estimation using bidirectional long short-term memory (BLSTM) neural networks has been widely used in various speech enhancement applications, and it has achieved great success when it is applied to multichannel enhancement techniques with a mask-based beamformer. However, when these masks are used for single channel speech enhancement they severely distort the speech signal and make them unsuitable for speech recognition. This paper proposes a student-teacher learning paradigm for single channel speech enhancement. The beamformed signal from multichannel enhancement is given as input to the teacher network to obtain soft masks. An additional cross-entropy loss term with the soft mask target is combined with the original loss, so that the student network with single-channel input is trained to mimic the soft mask obtained with multichannel input through beamforming. Experiments with the CHiME-4 challenge single channel track data shows improvement in ASR performance.",0.0,4.5
1282,2395,5.85    4.25    4.3    3.6    ,"Preliminary Test of a Wireless, Portable Magnetic Tongue Tracking System for Silent Speech Interface","A silent speech interface (SSI) converts non-audio information such as articulatory (e.g., tongue and lips) movement to text that can then drive a text-to-speech synthesizer. In this paper, we present a prototype, wireless, and portable tongue tracking system based on permanent magnet articulography (PMA) for SSI purpose. This device measures the changes in magnetic field induced by the movements of a magnetic tracer attached near the tongue tip by an array of 3-axial magnetometers positioned outside the mouth. The magnetic field recordings are directly mapped to text by a deep neural network-based silent speech recognition algorithm. The recognition accuracy of target phonemes was evaluated in a speaker-dependent task using a dataset collected from six American English speakers. The average phoneme error rate was 37.3% and the word error rate was 32.1%. These encouraging results indicate the potential of this device for SSI or relevant applications. Further improvement of the data quality and wearability will further improve the device’s potential for a practical SSI system.",1.0,4.5
207,1305,4.95    3.2    4.95    4.9    ,Deep Metric Learning for the Target Cost in Unit-Selection Speech Synthesizer,"This paper describes a unified Deep Metric Learning (DML) framework to predict the target cost directly by supervised learning method. The conventional methods to calculate the target cost include two separate steps: feature extraction and standard distance measurement. The proposed DML framework aims to measure the similarity between the candidate units and the target units more reasonably and directly. Firstly, the symmetrical DML framework is pre-trained to learn the metric between pairs of candidate units and target units. The relabeling procedure is added to correct the initial designed labels of the target cost. Secondly, the acoustic features of the target units are removed, which fits the runtime of the unit-selection synthesizer. The asymmetrical DML is fine-tuned to learn the metric between candidate units and target units. Compared with the conventional methods, the proposed unified DML framework can avoid the accumulation of errors in separate steps and improve the accuracy in labeling and predicting the target cost. The evaluation results demonstrate that the naturalness of synthetic speech has been improved by adopting DML framework to predict target cost.",0.0,4.5
527,1987,2.8    4.9    5.2    4.4    5.2    ,An Optimization Framework for Reconstruction of Speech from a Phase-Encoded Spectrogram,"In general, reconstruction of a speech signal from the spectrogram is non-unique because of the unavailability of the phase spectrum. Considering zero phase would result in a minimum-phase reconstruction. This limitation is overcome by computing the recently introduced phase-encoded spectrogram. In this approach, one modifies each frame of a speech signal to possess the causal, delta-dominant (CDD) property prior to computing the spectrogram. In an earlier publication, we showed that finite-length CDD sequences can be retrieved exactly from their magnitude spectra using a cepstrum technique. Although exactness is guaranteed in principle, practical implementations result in a limited, but high, reconstruction accuracy. In this paper, we focus on increasing the reconstruction accuracy. We formulate the reconstruction problem within an optimization framework and deploy a recently proposed iterative, alternating direction method of multipliers (ADMM) algorithm called autocorrelation retrieval—Kolmogorov factorization (CoRK). Experimental validations show that the CoRK algorithm results in a reconstruction accurate up to machine precision. We also show that both CoRK and cepstrum techniques are robust and invariant to the choice of the window duration, the amount of overlap between consecutive speech frames, the strength of the delta used to impart the CDD property, and the presence of noise.",0.0,4.5
351,1590,5.05    5.2    2.8    5    ,A Unified Framework for the Generation of Glottal Signals in Deep Learning-based Parametric Speech Synthesis Systems,"In this paper, we propose a unified training framework for the generation of glottal signals in deep learning (DL)-based parametric speech synthesis systems. 
The glottal vocoding-based speech synthesis system, especially the modeling-by-generation (MbG) structure that we proposed recently, significantly improves the naturalness of synthesized speech by faithfully representing the noise component of the glottal excitation with an additional DL structure. 
Because the MbG method introduces a multistage processing pipeline, however, its training process is complicated and inefficient.
To alleviate this problem, we propose a unified training approach that directly generates speech parameters by merging all the required models, such as acoustic, glottal, and noise models into a single unified network.
Considering the fact that noise analysis should be performed after training the glottal model, we also propose a stochastic noise analysis method that enables noise modeling to be included in the unified training process by iteratively analyzing the noise component in every epoch.
Both objective and subjective test results verify the superiority of the proposed algorithm compared to conventional methods.",0.0,4.5125
212,1312,4.15    5.2    4.4    4.3    ,Impact of ASR Performance on Free Speaking Language Assessment,"In free speaking tests candidates respond in spontaneous speech to prompts. This form of test allows the spoken language proficiency of a non-native speaker of English to be assessed more fully than read aloud tests. As the candidate's responses are unscripted, transcription by automatic speech recognition (ASR) is essential for automated assessment. ASR will never be 100% accurate so any assessment system must seek to minimise and mitigate ASR errors. This paper considers the impact of ASR errors on the performance of free speaking test auto-marking systems. Firstly rich linguistically related features, based on part-of-speech tags from statistical parse trees, are investigated for assessment. Then, the impact of ASR errors on how well the system can detect whether a learner's answer is relevant to the question asked is evaluated. Finally, the impact that these errors may have on the ability of the system to provide detailed feedback to the learner is analysed. In particular, pronunciation and grammatical errors are considered as these are important in helping a learner to make progress. As feedback resulting from an ASR error would be highly confusing, an approach to mitigate this problem using confidence scores is also analysed.",0.0,4.5125
603,2194,5.1    5.2    3.3    4.45    ,Unspeech: Unsupervised Speech Context Embeddings,"We introduce ""Unspeech"" embeddings, which are based on unsupervised learning of context feature representations for spoken language. The embeddings were trained on up to 9500 hours of crawled English speech data without transcriptions or speaker information, by using a straightforward learning objective based on context and non-context discrimination with negative sampling. We use a Siamese convolutional neural network architecture to train Unspeech embeddings and evaluate them on speaker comparison, utterance clustering and as a context feature in TDNN-HMM acoustic models trained on TED-LIUM, comparing it to i-vector baselines. Particularly decoding out-of-domain speech data from the recently released Common Voice corpus shows consistent WER reductions. We release our source code and pre-trained Unspeech models under a permissive open source license.",0.0,4.5125
73,1063,4.9    5.1    3.55    ,Semi-supervised Cross-domain Visual Feature Learning for Audio-Visual Broadcast Speech Transcription,"Visual information can be incorporated into automatic speech recognition (ASR) systems to improve their robustness in adverse acoustic conditions. Conventional audio-visual speech recognition (AVSR) systems require highly specialized audio-visual (AV) data in both system training and evaluation. For many real-world speech recognition applications, only audio information is available. This presents a major challenge to a wider application of AVSR systems. In order to address this challenge, this paper proposes a semi-supervised visual feature learning approach for developing AVSR systems on a DARPA GALE Mandarin broadcast transcription task. Audio to visual feature inversion long short-term memory neural networks (LSTMs) were initially constructed using limited amounts of out of domain  AV data. The acoustic features’ domain mismatch against the broadcast data was further reduced using multi-level domain adaptive deep networks. Visual features were then automatically generated from the broadcast speech audio and used in both AVSR system training and testing time. Experimental results suggest a CNN based AVSR system using the proposed semi-supervised cross-domain audio-to-visual feature generation technique outperformed the baseline audio only CNN ASR system by an average CER reduction of 6.8\% relative. In particular, on the most difficult Phoenix TV subset, a CER reduction of 8.34% relative was obtained.",0.0,4.516666666666667
164,1239,5.45    5    3.1    4.55    ,Naturalness Improvement Algorithm for Reconstructed Glossectomy Patient's Speech Using Spectral Differential Modification in Voice Conversion,"In this paper, we propose an algorithm to improve the naturalness of the reconstructed glossectomy patient's speech that is generated by voice conversion to enhance the intelligibility of speech uttered by patients with a wide glossectomy. While existing VC algorithms make it possible to improve intelligibility and naturalness, the result is still not satisfying. To solve the continuing problems, we propose to directly modify the speech waveforms using a spectrum differential. The motivation is that glossectomy patients mainly have problems in their vocal tract, not in their vocal cords. The proposed algorithm requires no source parameter extractions for speech synthesis, so there are no errors in source parameter extractions and we are able to make the best use of the original source characteristics. In terms of spectrum conversion, we evaluate with both GMM and DNN. Subjective evaluations show that our algorithm can synthesize more natural speech than the vocoder-based method. Judging from observations of the spectrogram, power in high-frequency bands of fricatives and stops is reconstructed to be similar to that of natural speech.",0.0,4.5249999999999995
703,2458,3.55    5.1    5.1    4.35    ,Effectiveness of Single-Channel BLSTM Enhancement for Language Identification,"This paper proposes to apply deep neural network (DNN)-based single-channel speech enhancement (SE) to language identification. The 2017 language recognition evaluation (LRE17) introduced noisy audios from videos, in addition to the telephone conversation from past challenges. Because of that, adapting models from telephone speech to noisy speech from the video domain was required to obtain optimum performance.  However, such adaptation requires knowledge of the audio domain and availability of in-domain data. Instead of adaptation, we propose to use a speech enhancement step to clean up the noisy audio as preprocessing for language identification. We used a bi-directional long short-term memory (BLSTM) neural network, which given log-Mel noisy features predicts a spectral mask indicating how clean each time-frequency bin is. The noisy spectrogram is multiplied by this predicted mask to obtain the enhanced magnitude spectrogram, and it is transformed back into the time domain by using the unaltered noisy speech phase. The experiments show significant improvement to language identification of noisy speech, for systems with and without domain adaptation, while preserving the identification performance in the telephone audio domain. In the best adapted state-of-the-art bottleneck i-vector system the relative improvement is 11.3% for noisy speech.",0.0,4.5249999999999995
708,2467,6    5    3.6    3.5    ,Expressive Speech Synthesis Using Sentiment Embeddings,"In this paper we present a DNN based speech synthesis system trained on an audiobook including sentiment features predicted by the Stanford sentiment parser.
The baseline system uses DNN to predict acoustic parameters based on conventional linguistic features, as they have been used in statistical parametric speech synthesis.
The predicted parameters are transformed into speech using a conventional high-quality vocoder. In the proposed system the conventional linguistic features are enriched using sentiment features. Different sentiment representations have been considered, combining sentiment probabilities with hierarchical distance and context.
After preliminary analysis a listening experiment is conducted, where participants evaluate the different systems. The results show the usefulness of the proposed features and reveal differences between expert and non-expert TTS user.",0.0,4.525
296,1475,5.15    5    4.4    3.55    ,Improving CTC-based Acoustic Model with Very Deep Residual Time-delay Neural Networks,"Connectionist temporal classification (CTC) has shown great potential in end-to-end (E2E) acoustic modeling. The current state-of-the-art architecture for a CTC-based E2E model is based on a deep bidirectional long short-term memory (BLSTM) network that provides frame-wise outputs estimated from both forward and backward directions (BLSTM-CTC). Since this architecture can lead to a serious time latency problem in decoding, it cannot be applied to real-time speech recognition tasks. Considering that the CTC label of one current frame can only be affected by a few neighboring frames, we argue that using BLSTM traversing on a whole utterance from both directions is not necessary. In this paper, we use a very deep residual time-delay (VResTD) network for CTC-based E2E acoustic modeling (VResTD-CTC). The VResTD network provides frame-wise outputs with local bidirectional information without needing to wait for the whole utterance. Speech recognition experiments on Corpus of Spontaneous Japanese were carried out to test our proposed VResTD-CTC and the state-of-the-art BLSTM-CTC model. Comparable performance was obtained while the proposed VResTD-CTC does not suffer from the decoding time latency problem.",0.0,4.525
168,1243,4.9    4.95    5    3.25    ,Unsupervised Temporal Feature Learning Based on Sparse Coding Embedded BoAW for Acoustic Event Recognition,"The performance of an Acoustic Event Recognition (AER) system highly depends on the statistical information and the temporal dynamics in the audio signals. Although the traditional Bag of Audio Words (BoAW) and the Gaussian Mixture Models (GMM) approaches can obtain more statistics information by aggregating multiple frame-level descriptors of an audio segment compared with the frame-level feature learning methods, its temporal information is unreserved. Recently, more and more Deep Neural Networks (DNN) based AER methods have been proposed to effectively capture the temporal information in audio signals, and achieved better performance, however, these methods usually required the manually annotated labels and fixed-length input during feature learning process. In this paper, we proposed a novel unsupervised temporal feature learning method, which can effectively capture the temporal dynamics for an entire audio signal with arbitrary duration by building direct connections between the BoAW histograms sequence and its time indexes using a non-linear Support Vector Regression (SVR) model. Furthermore, to make the feature representation have a better signal reconstruction ability, we embedded the sparse coding approach in the conventional BoAW framework. Compared with the BoAW and Convolutional Neural Network (CNN) baselines, experimental results showed our method brings improvements of 9.7% and 4.1% respectively.",0.0,4.525
219,1327,5.1    5.1    5.15    2.75    ,Stochastic Shake-Shake Regularization for Affective Learning from Speech,"We propose stochastic Shake-Shake regularization based on multi-branch residual architectures to mitigate over-fitting in affective learning from speech. 
Inspired by recent Shake-Shake \cite{gastaldi2017} and ShakeDrop \cite{shakedrop} regularization techniques, we introduce negative scaling into the Shake-Shake regularization algorithm while still maintain a consistent stochastic convex combination of branches to encourage diversity among branches whether they are scaled by positive or negative coefficients. 
In addition, we also employ the idea of stochastic depth to randomly relax the shaking mechanism during training as a method to control the strength of regularization.
Through experiments on speech emotion recognition with various levels of regularization strength, we discover that the shaking mechanism alone contributes much more to constraining the optimization of network parameters than to boosting the generalization power.
However, stochastically relaxing the shaking regularization serves to conveniently strike a balance between them. 
With a flexible configuration of hybrid layers, promising experimental results demonstrate a higher unweighted accuracy and a smaller gap between training and validation, i.e. reduced over-fitting, and shed light on the future direction for pattern recognition tasks with low resource.",0.0,4.525
299,1481,6    4.2    3.4    ,ASe: Acoustic Scene Embedding Using Deep Archetypal Analysis and GMM,"In this paper, we propose a deep learning framework which combines the generalizability of Gaussian mixture models (GMM) and discriminative power of deep matrix factorization to learn acoustic scene embedding (ASe) for the acoustic scene classification task. The proposed approach first builds a Gaussian mixture model-universal background model (GMM-
UBM) using frame-wise spectral representations. This UBM is adapted to a waveform, and the likelihood for each spectral frame representation is stored as a feature matrix. This matrix is fed to a deep matrix factorization pipeline (with audio recording level max-pooling) to compute a sparse-convex discriminative representation. The proposed deep factorization model is based on archetypal analysis, a form of convex NMF, which has been shown to be well suited for audio analysis. Finally, the obtained representation is mapped to a class label using a dictionary based auto-encoder consisting of linear and symmetric encoder and decoder with an efficient learning algorithm. The encoder projects the ASe of a waveform to the label space, while the decoder ensures that the feature can be reconstructed, resulting in better generalization on the test data.",0.0,4.533333333333333
726,2517,5.15    5.15    3.3    ,Improved Training for Online End-to-end Speech Recognition Systems,"Achieving high accuracy with end-to-end speech recognizers requires careful parameter initialization prior to training. Otherwise, the networks may fail to find a good local optimum. This is particularly true for low-latency online networks, such as unidirectional LSTMs. Currently, the best strategy to train such systems is to bootstrap the training from a tied-triphone system. However, this is time consuming, and more importantly, is impossible for languages without a high-quality pronunciation lexicon. In this work, we propose an initialization strategy that uses teacher-student learning to transfer knowledge from a large, well-trained, offline end-to-end speech recognition model to an online end-to-end model, eliminating the need for a lexicon or any other linguistic resources. We also explore curriculum learning and label smoothing and show how they can be combined with the proposed teacher-student learning for further improvements. We evaluate our methods on a Microsoft Cortana personal assistant task and show that the proposed method results in a 19% relative improvement in word error rate compared to a randomly-initialized baseline system.",0.0,4.533333333333334
279,1446,3.65    5.8    5.1    3.6    ,Compensation for Domain Mismatch in Text-independent Speaker Recognition,"Domain mismatch continues to be a major research challenge for speaker recognition in naturalistic audio streams.
This study presents a new technique for domain mismatch compensation within a text-independent speaker recognition scenario. The proposed method is designed for the NIST speaker recognition evaluation 2016 (SRE16) task, where speakers from training, development and evaluation data belong to different sets of languages. An i-vector/PLDA speaker recognition system is adopted for this study. To address the mismatch problem, we propose to append auxiliary features to the i-vectors. These auxiliary features are adapted representations of the i-vectors to the specific in-domain data; therefore, the new feature vector has two parts: (1) i-vectors which represent speaker identity and (2) auxiliary features which are representations of i-vectors in the in-domain data feature space (and may not contain speaker identity information). This new concatenated feature vector (we call this a-vector) is then post-processed with support vector discriminant analysis (SVDA) for further domain compensation.  Evaluations based on the SRE16 confirm the effectiveness of the proposed technique. In terms of minimum Cprimary cost, a-vector outperforms the i-vector consistently. Moreover, comparing to previous single systems introduced for SRE16, we achieved  8.5%-18% improvements in terms of equal error rate.",0.0,4.5375
170,1245,4.3    5.25    4.45    4.15    ,Weighting Pitch Contour and Loudness Contour in Mandarin Tone Perception in Cochlear Implant Listeners,"Previous investigations found that loudness-contours within individual Mandarin monosyllables can drive categorical perception of Mandarin tone for cochlear implant (CI) users, while in normal hearing (NH) subjects the pitch contour is phonologically acknowledged to be the dominant cue. Here we further examine the weighting strategy of pitch induced and loudness induced contour identification on Mandarin tone perception by CI users. Twenty-seven versions of the disyllabic utterance /Lao3 Shi/ with orthogonally manipulated loudness-contour and pitch-contour of the voiced portion of the second monosyllable /Shi/ served as the stimuli to both CI and NH subjects. In Mandarin, if /Shi/ is pronounced with high-flat-pitched Tone 1 the word means “teacher”, with rising Tone 2 it means “well-behaved”, or with falling Tone 4 it means “always”. CI users generally had poorer word-recognition scores and their inter-subject variance was large. While NH subjects recognized tone reliably based on pitch-contour, half of the CI users relied on pitch-contour, the other half on loudness-contour, implying systematic differences in pitch coding in their CI processing. This paradigm of orthogonal manipulation of pitch and loudness contours could be developed into improved audiometric tests of Mandarin tone perception and pitch coding with CIs.",0.0,4.5375
157,1227,4.2    4.9    4.95    4.1    ,Investigating Accuracy of Pitch-accent Annotations in Neural Network-based Speech Synthesis and Denoising Effects,"We investigated the impact of noisy linguistic features on the performance of a Japanese speech synthesis system based on neural network that uses WaveNet vocoder. We compared an ideal system that uses manually corrected linguistic features including phoneme and prosodic information in training and test sets against a few other systems that use corrupted linguistic features. Both subjective and objective results demonstrate that corrupted linguistic features, especially those in the test set, affected the ideal system's performance significantly in a statistical sense due to a mismatched condition between the training and test sets. 
Interestingly, while an utterance-level Turing test showed that listeners had a difficult time differentiating synthetic speech from natural speech, it further indicated that adding noise to the linguistic features in the training set can partially reduce the effect of the mismatch, regularize the model, and help the system perform better when linguistic features of the test set  are noisy.",0.0,4.5375
258,1401,5.1    4.3    3.7    5.05    ,Using Voice Quality Supervectors for Affect Identification,"The voice quality of speech sounds often conveys perceivable information about the speaker’s affect. This study proposes perceptually important voice quality features to recognize affect represented in speech excerpts from individuals with mental, neurological, and/or physical disabilities. The voice quality feature set consists of F0, harmonic amplitude differences between the first, second, fourth harmonics and the harmonic near 2 kHz, the center frequency and amplitudes of the first 3 formants, and cepstral peak prominence. The feature distribution of each utterance was represented with a supervector, and the Gaussian mixture model and support vector machine classifiers were used for affect classification. Similar classification systems using the MFCCs and ComParE16 feature set were implemented. The systems were fused by taking the confidence mean of the classifiers. Applying the fused system to the Interspeech 2018 Atypical Affect subchallenge task resulted in unweighted average recalls of 43.9% and 41.0% on the development and test dataset, respectively. Additionally, we investigated clusters obtained by unsupervised learning to address gender-related differences.",0.0,4.5375
686,2422,4.15    5.8    3.15    5.05    ,Analysis of Phone Errors in Children's ASR through Bottleneck Feature Visualisations,"Previous work aimed to investigate the extent to which errors attributable to phonological effects associated with language acquisition (PEALA) contribute to the output of children's ASR. Opposite to what was intuitively expected, the proportion of errors predictable from PEALA was positively correlated with recognition accuracy, therefore increased across ages. In order to interpret this finding, the present paper employs a DNN-HMM automatic speech recognition system, built on the CSLU children's speech corpus, to produce bottleneck feature (BNF) visualisations of phones and examine how these relate with respect to PEALA. The focus is drawn particularly on ASR errors caused by phone confusions, which are compared against phone substitution pairs indicated by PEALA. The ASR results confirm the previously observed interaction between errors predictable from PEALA and rising accuracy, but also suggest that these errors only account for a small percentage of the total phone substitution error. The BNF visualisations for the most part outline the age progression smoothly and demonstrate clear clusters of neighbouring phones consistently. The distance between PEALA related phones can be partitioned in four sets; two that increase with age (at a higher or lower rate), one that roughly remains constant and one that decreases with age.",0.0,4.5375
78,1078,4.75    4.3    5.1    4    ,Multi-Task Learning of Speech Recognition and Speech Synthesis Parameters for Ultrasound-based Silent Speech Interfaces,"Silent Speech Interface systems apply two different strategies to solve the articulatory-to-acoustic conversion task. The recognition-and-synthesis approach applies speech recognition techniques to map the articulatory data to a textual transcript, which is then converted to speech by a conventional text-to-speech system. The direct synthesis approach seeks to convert the articulatory information directly to speech synthesis (vocoder) parameters. In both cases, deep neural networks are an evident and popular choice to learn the mapping task. Recognizing that the learning of speech recognition and speech synthesis targets (acoustic model states vs. vocoder parameters) are two closely related tasks over the same ultrasound tongue image input, here we experiment with the multi-task training of deep neural networks, which seeks to solve the two tasks simultaneously. 
Our results show that the parallel learning of the two types of targets is indeed beneficial for both tasks. Moreover, we obtained further improvements by using multi-task training as a weight initialization step before task-specific training. Overall, we report a relative error rate reduction of about 7% in both the speech recognition and the speech synthesis tasks.",0.0,4.5375
484,1883,5.05    5    4.15    3.95    ,On Enhancing Speech Emotion Recognition Using Generative Adversarial Networks,"Generative Adversarial Networks (GANs) have gained a lot of attention from machine learning community due to their ability to learn and mimic an input data distribution. GANs consist
of a discriminator and a generator working in tandem playing a min-max game to learn the complex underlying data distribution when fed with data-points sampled from a simpler distribution like Uniform or Gaussian. Once trained, they allow synthetic generation of examples sampled from the learned distribution. We investigate the application of GANs to generate synthetic feature vectors used for speech emotion recognition. Specifically, we investigate two set ups: (i) a vanilla GAN that learns the distribution of a lower dimensional representation of the actual higher dimensional feature vector and (ii) a conditional GAN that learns the distribution of the higher dimensional feature vectors conditioned on the labels or the emotional class to which it belongs. As a potential practical application of these synthetically generated samples, we measure any improvement in a classifier‘s performance when the synthetic data was used along with real for training it. We perform cross validation analyses followed by a cross-corpus study.",0.0,4.5375000000000005
341,1565,4.95    4.2    3.95    5.05    ,Effectiveness of Generative Adversarial Network for Non-Audible Murmur-to-Whisper Speech Conversion,"The murmur produced by the speaker and captured by the NonAudible Murmur (NAM)-one of the Silent Speech Interface (SSI) technique, suffers from the speech quality degradation. This is due to the lack of radiation effect at the lips and lowpass nature of the soft tissue, which attenuates the high frequency related information. In this work, a novel method for NAM-toWhisper (NAM2WHSP) speech conversion incorporating Generative Adversarial Network (GAN) is proposed. The GAN minimizes the distributional divergence between the whispered speech and the generated speech parameters (through adversarial optimization). The objective and subjective evaluation performed on the proposed system, justifies the ability of adversarial optimization over Maximum Likelihood (ML)-based optimization networks, such as a Deep Neural Network (DNN), in preserving and improving the speech quality and intelligibility. The adversarial optimization learns the mapping function with 54.2 % relative improvement in MOS and 29.83 % absolute reduction in % WER w.r.t. the state-of-the-art mapping techniques. Furthermore, we evaluated the proposed framework by analyzing the level of contextual information and the number of training utterances required for optimizing the network parameters, for the given task and database.",0.0,4.5375000000000005
538,2017,5.9    4.45    4.95    2.85    ,Term Extraction via Neural Sequence Labeling a Comparative Evaluation of Strategies Using Recurrent Neural Networks,"Traditionally systems for term extraction use a two stage approach of first identifying candiate terms, and the scoring them in a second process for identifying actual terms. Thus, research in this field has often mainly focused on refining and improving the scoring process of term candidates, which commonly are identified using linguistic and statistical features. Machine learning techniques and especially neural networks are currently only used in the second stage, that is to score candidates and classify them.

In contrast to that we have built a system that identifies terms via directly performing sequence-labeling with a BILOU scheme on word sequences. To do so we have worked with different kinds of recurrent neural networks and word embeddings.

In this paper we describe how one can built a state-of-the-art term extraction systems with this single-stage technique and compare different network types and topologies and also examine the influence of the type of input embedding used for the task. We further investigated which network types and topologies are best suited when applying our term extraction systems to other domains than that of the training data of the networks.",0.0,4.5375000000000005
271,1428,4.3    5.1    4.2    4.6    ,Long Distance Voice Channel Diagnosis Using Deep Neural Networks,"In long distance telephone network, it is time-consuming to detect and locate the problematic devices. Although hints could be given from the types of distortion in the test calls,
it is tedious to manually classify the distortion types from a large number of calls.
In this paper, we present our work on using a deep neural network-based classifier, to automatically detect and identify the type of distortion which often occurs in long distance calls. We verified our approach with data from real telecommunication networks and the results showed that our approach can achieve an average recall rate of 71% in classification. We believe our method can lead to a huge reduction of manpower and time in long distance voice channel troubleshooting.",0.0,4.549999999999999
298,1477,5.9    4.8    3.4    4.1    ,Exploring Spatio-Temporal Representations by Integrating Attention-based Bidirectional-LSTM-RNNs and FCNs for Speech Emotion Recognition,"Automatic emotion recognition from speech, which is an important and challenging task in the field of affective computing, heavily relies on the effectiveness of the speech features for classification. Previous approaches to emotion recognition have mostly focused on the extraction of carefully hand-crafted features. How to model spatio-temporal dynamics for speech emotion recognition effectively is still under active investigation. In this paper, we propose a method to tackle the problem of emotional relevant feature extraction from speech by leveraging Attention-based Bidirectional Long Short-Term Memory Recurrent Neural Networks with fully convolutional networks in order to automatically learn the best spatio-temporal representations of speech signals. The learned high-level features are then fed into a deep neural network (DNN) to predict the final emotion. The experimental results on the Chinese Natural Audio-Visual Emotion Database (CHEAVD) and the Interactive Emotional Dyadic Motion Capture (IEMOCAP) corpora show that our method provides more accurate predictions compared with other existing emotion recognition algorithms.",0.0,4.55
257,1400,5.75    4.95    2.5    5    ,The Conversation: Deep Audio-Visual Speech Enhancement,"Our goal is to isolate individual speakers from multi-talker simultaneous speech in videos. Existing works in this area have focussed on trying to separate utterances from known  speakers in controlled environments. In this paper, we propose a deep audio-visual speech enhancement network that is able to separate a speaker's voice given lip regions in the corresponding video, by predicting both the magnitude and the phase of the target signal. The method is applicable to speakers unheard and unseen during training, and for unconstrained environments. We demonstrate strong quantitative and qualitative results,  isolating extremely challenging real-world examples.",0.0,4.55
193,1284,4.35    4    5.3    ,Should Code-switching Models Be Asymmetric?,"Since the work of Joshi [1], most models of code-switching (C-S) have assumed asymmetry of the participating languages. While there exist patterns of language mixing in which a dominant or matrix language (ML) may not be discernible, these more complex signatures are rarely modeled [2, 3]. We use a series of metrics to characterize the switching in corpora as asymmetrical (insertional C-S) or symmetrical (alternational C-S). We test the efficacy of a linguistic model that assumes no ML in predicting the syntax of C-S in three Spanish–English corpora that vary according to whether the ML is Spanish, English or indeterminate. Our results show that the same constraints on the grammatical junctures and on the directionality of switching hold irrespective of the symmetry of the data. The length of the alternating language spans varies according to POS with noun phrases comprising the shortest spans. This suggests that insertional C-S may be subsumed under alternational C-S, as spontaneous borrowing. These results invite researchers to reconsider the linguistic theories they adopt and to expand the typology of training data used in creating language models and processing tools for C-S.",0.0,4.55
209,1308,4.3    5.8    3.15    4.95    ,Unsupervised Word Segmentation from Speech with Attention,"We present a first attempt to perform attentional word segmentation
from speech signal, with the final goal of automatically
identifying lexical units in a low-resource, unwritten language
(UL). Our methodology assumes a pairing between recordings
in the UL with translations in a well-resourced language. It
uses Acoustic Unit Discovery (AUD) to convert speech into a
pseudo-phones sequence that is segmented using neural soft alignments
(from a neural machine translation model). Evaluation
uses an actual Bantu UL, Mboshi; comparisons to monolingual
and bilingual baselines illustrate the potential of attentional
word segmentation for language documentation.",0.0,4.55
531,2001,4.85    4.1    5    4.25    ,Multiple Phase Information Combination for Replay Attacks Detection,"In recent years, the performance of Automatic Speaker Verification (ASV) systems has been 
improved significantly. However,they are still affected by different kind of spoofing attacks. In
this paper, we propose a method that fused different phase features and amplitude features to detect replay attacks. We propose the mel-scale relative phase feature and apply source-filter
vocal tract feature in phase domain for replay attacks detection. These two phase-based features are combined to get complementary information. In addition to these phase haracteristics, constant Q cepstral coefficients (CQCCs) are used. The proposed methods are evaluated using the ASVspoof 2017 challenge database, and Gaussian mixture model was used as the back-end model. The proposed approach achieved 55.6% relative error reduction rate than the conventional magnitude-based feature.",0.0,4.55
679,2412,5.25    4.9    4.55    3.5    ,LOCUST - Longitudinal Corpus and Toolset for Speaker Verification,"In this paper, we set forth a new longitudinal corpus and a toolset in an effort to address the influence of voice-aging on speaker verification.
We have examined previous longitudinal research of age-related voice changes as well as its applicability to real world use cases. Our findings reveal that scientists have treated age-related voice changes as a hindrance instead of leveraging it to the advantage of the identity validator. Additionally, we found a significant dearth of publicly available corpora related to both the time span of and the number of participants in audio recordings. We also identified a significant bias toward the development of speaker recognition technologies applicable to government surveillance systems compared to speaker verification systems used in civilian IT security systems.
To solve the aforementioned issues, we built an open project with the largest publicly available longitudinal speaker database, which includes 229 speakers with an average talking time exceeding 15 hours spanning across an average of 21 years per speaker. We assembled, cleaned, and normalized audio recordings and developed software tools for speech features extractions, all of which we are releasing to the public domain.",0.0,4.55
122,1151,5.25    4.95    3.45    ,Joint Learning Using Denoising Variational Autoencoders for Voice Activity Detection,"Voice activity detection (VAD) is a challenging task in very low signal-to-noise ratio (SNR) environments. To address this issue, a promising approach is to map noisy speech features to corresponding clean features and to perform VAD using the generated clean features. This can be implemented by concatenating a speech enhancement (SE) and a VAD network, whose parameters are jointly updated. In this paper, we propose denoising variational autoencoder-based (DVAE) speech enhancement in the joint learning framework. Moreover, we feed not only the enhanced feature but also the latent code from the DVAE into the VAD network. We show that the proposed joint learning approach outperforms conventional denoising autoencoder-based joint learning approach.",0.0,4.55
339,1562,5.1    5.1    4.15    3.85    ,Modeling Interpersonal Influence of Verbal Behavior in Couples Therapy Dyadic Interactions,"Dyadic interactions among humans are marked by speakers continuously influencing and reacting to each other in terms of responses and behaviors, among others. Understanding how interpersonal dynamics affect behavior is important for successful treatment in psychotherapy domains. Traditional schemes that automatically identify behavior for this purpose have often looked at only the target speaker. In this work, we propose a text-based Markov model of how a target speaker's behavior is influenced by their own past behavior as well as their perception of their partner's behavior, based on lexical features. Apart from incorporating additional potentially useful information, our model can also control the degree to which the partner affects the target speaker. We evaluate our proposed model on the task of classifying Negative behavior in Couples Therapy and show that it is more accurate than the single-speaker model. Furthermore, we investigate the degree to which the optimal influence relates to how well a couple does on the long-term, via relating to relationship outcomes.",0.0,4.55
269,1424,4.3    5.1    4.25    ,Encoder Transfer for Attention-based Acoustic-to-word Speech Recognition,"Acoustic-to-word speech recognition based on attention-based
encoder-decoder models achieves better accuracies with much lower
latency than the conventional speech recognition systems.
However, acoustic-to-word models require a very large amount of
training data and it is difficult to prepare one 
for a new domain such as elderly speech.
To address the problem, we propose domain adaptation
based on transfer learning with layer freezing.
Layer freezing first pre-trains a network
with the source domain data, and
then a part of parameters is re-trained
for the target domain while the rest is fixed.
In the attention-based acoustic-to-word model, the encoder part is
frozen to maintain the generality, and only the decoder part is
re-trained to adapt to the target domain. 
This substantially allows for adaptation of the latent linguistic capability
of the decoder to the target domain.
Using a large-scale Japanese spontaneous speech corpus as source, 
the proposed method is applied to three target domains: 
a call center task and two voice search tasks by adults and by elderly.
The models trained with the proposed method achieved better accuracy
than the baseline models, which are trained from scratch or entirely 
re-trained with the target domain.",0.0,4.55
178,1256,3.4    5    4.95    4.9    ,Exploring Temporal Reduction in Dialectal Spanish: a Large-scale Study of Lenition of Voiced Stops and Coda-s,"Large scale studies of temporal reduction are of interest to obtain linguistic knowledge and this understanding can potentially help improve speech technologies. ASR supports the processing of very large corpora and can be used to enrich linguistic studies with models of speech production and perception grounded on  commonly observed pronunciations. In return, ASR can benefit from the linguistic findings by including pronunciation variants that reflect the linguistic variability. This study focuses on two reduction phenomena in Peninsular and Latin American varieties of Spanish: lenition of intervocalic voiced stops /bdg/ and s-coda. First, the two phenomena are investigated via a study of transcription errors produced by an ASR system designed for Peninsular Spanish which can potentially be attributed to lenition.  Then automatic forced alignment experiments are conducted using pronunciation variants with and without lenition to measure the extent of the phenomenon as a function of geographical and stylistic repartition. The results show that the distribution of pronunciation variants across Spanish varieties is consistent with trends depicted by classical linguistic studies. Speaking style is the main factor affecting +/-lenition variation. The findings also suggest that including such variants in ASR system's lexicon may improve performance when processing multiple Spanish varieties.",0.0,4.5625
227,1345,4.9    3.95    5.1    4.3    ,Efficient Language Model Adaptation with Noise Contrastive Estimation and Kullback-Leibler Regularization,"Many language modeling (LM) tasks have limited in-domain data for training.
Exploiting out-of-domain data while retaining the relevant in-domain statistics is a desired property in these scenarios.
Kullback-Leibler Divergence (KLD) regularization is a popular method for acoustic model (AM) adaptation.
KLD regularization assumes that the last layer is a softmax that fully activates the targets of both in-domain and out-of-domain models.
Unfortunately, this softmax activation is computationally prohibitive for language modeling where the number of output classes is large, typically 50k to 100K, but may even exceed 800k in some cases.
The computational bottleneck of the softmax during LM training can be reduced by an order of magnitude using techniques such as noise contrastive estimation (NCE), 
which replaces the cross-entropy loss function with a binary classification problem between the target output and random noise samples.
In this work we combine NCE and KLD regularization and offer a fast domain adaptation method for LM training, while also retaining important attributes of the original NCE, such as self-normalization.
Applied to a medical domain-adaptation task it provided a perplexity improvement of 10.1\% relative with respect to a strong LSTM baseline.",0.0,4.5625
406,1713,5.25    3.6    3.55    5.85    ,Detecting Alzheimer’S Disease Using Gated Convolutional Neural Network from Audio Data,"We propose an automatic detection method of Alzheimer's diseases using a gated convolutional neural network (GCNN) from speech data. This GCNN can be trained with a relatively small amount of data and can capture the temporal information in audio paralinguistic features. Since it does not utilize any linguistic features, it can be easily applied to any languages. We evaluated our method using Pitt Corpus. The proposed method achieved the accuracy of 73.6%, which is better than the conventional sequential minimal optimization (SMO) by 7.6 points.",0.0,4.5625
377,1651,4    4.1    5.1    5.05    ,Auditory Filterbank Learning for Temporal Modulation Features in Replay Spoof Speech Detection,"In this paper, we present a standalone replay spoof speech detection (SSD) system to classify the natural vs. replay speech. The replay speech spectrum is known to be affected in the higher frequency range. In this context, we propose to exploit an auditory filterbank learning using Convolutional Restricted Boltzmann Machine (ConvRBM) with the pre-emphasized speech signals. Temporal modulations in amplitude (AM) and frequency (FM)
are extracted from the ConvRBM subbands using the Energy Separation Algorithm (ESA). ConvRBM-based short-time AM and FM features are developed using cepstral processing, denoted as AM-ConvRBM-CC and FM-ConvRBM-CC. Proposed temporal modulation features performed better than the baseline Constant-Q Cepstral Coefficients (CQCC) features. On the evaluation set, an absolute reduction of 7.48 % and 5.28 % in Equal Error Rate (EER) is obtained using AM-ConvRBM-CC and FM-ConvRBM-CC, respectively compared to our CQCC
baseline. The best results are achieved by combining scores from AM and FM cues (0.82 % and 8.89 % EER for development and evaluation set, respectively). The statistics of AM-FM features are analyzed to understand the performance gap and complementary information in both the features.",0.0,4.5625
173,1248,4.85    4.15    5    4.25    ,Multiple Concurrent Sound Source Tracking Based on Observation-Guided Adaptive Particle Filter,"Particle filter (PF) has been proved to be an effective tool to track sound sources. In traditional PF, a pre-defined dynamic model is used to model source motion, which tends to be mismatched due to the uncertainty of source motion. Besides, non-stationary interferences pose a severe challenge to source tracking. To this end, an observation-guided adaptive particle filter (OAPF) is proposed for multiple concurrent sound source tracking. Firstly, sensor signals are processed in the time-frequency domain to obtain the direction of arrival (DOA) observations of sources. Then, by updating particle states with these DOA observations, angular distances between particles and observations are reduced to guide particles to directions of sources. Thirdly, particle weights are updated by an interference-adaptive likelihood function to reduce the impacts of interferences. At last, with the updated particles and the corresponding weights, OAPF is utilized to determine the final DOAs of sources. Experimental results demonstrate that our method achieves favorable performance for multiple concurrent sound source tracking in noisy environments.",0.0,4.5625
1210,2145,4.75    3.5    5.2    4.8    ,Urdu Intonation Patterns in Spontaneous Speech,"A hand-labelled Urdu data set of spontaneous conversational speech is analysed to determine the intonational inventory of Urdu for this genre, and to explore the relationship between pitch accent and part of speech (PoS), which was also annotated for each word in the data set. The basic Urdu intonation patterns observed in the data are summarised and presented using a simplified version of the Rhythm and Pitch (RaP) labelling system.
Urdu intonation exhibits common contours such as falling tone for statements and WH-questions, and yes/no questions ending in a rising tone. Pitch accent distribution is quite free in Urdu, but the data indicate a stronger association of pitch accent with some PoS categories of content word (e.g. Noun) when compared with function words and semantically lighter PoS categories (such as Light Verbs). 
Contrastive focus is realised by an L*+H accent with a relatively large pitch excursion for the +H tone, and longer duration of the stressed syllable. The data suggest that post-focus compression (PFC) is present in Urdu.",1.0,4.5625
131,1165,4.9    3.1    5.1    5.15    ,Using Deep Neural Networks for Identification of Slavic Languages from Acoustic Signal,"This paper investigates the use of deep neural networks (DNNs) for the task of spoken language identification. Various feed-forward fully connected, convolutional and recurrent DNN architectures are adopted and compared against a baseline i-vector based system. Moreover, DNNs are also utilized for extraction of bottleneck features from the input signal. The dataset used for experimental evaluation contains utterances belonging to languages that are all related to each other and sometimes hard to distinguish even for human listeners: it is compiled from recordings of the 11 most widespread Slavic languages. We also released this Slavic dataset to the general public, because a similar collection is not publicly available through any other source. The best results were yielded by a bidirectional recurrent DNN with gated recurrent units that was fed by bottleneck features. In this case, the baseline ER was reduced from 4.2% to 1.2% and Cavg from 2.3% to 0.6%.",0.0,4.5625
89,1097,4.9    4    4.75    4.6    ,Unsupervised Adaptation with Interpretable Disentangled Representations for Distant Conversational Speech Recognition,"The current trend in automatic speech recognition is to leverage large amounts of labeled data to train supervised neural network models. Unfortunately, obtaining data for a wide range of domains to train robust models can be costly. However, it is relatively inexpensive to collect large amounts of unlabeled data from domains that we want the models to generalize to. In this paper, we propose a novel unsupervised adaptation method that learns to synthesize labeled data for the target domain from unlabeled in-domain data and labeled out-of-domain data. We first learn without supervision an interpretable latent representation of speech that encodes linguistic and nuisance factors (e.g., speaker and channel) using different latent variables. To transform a labeled out-of-domain utterance without altering its transcript, we transform the latent nuisance variables while maintaining the linguistic variables. To demonstrate our approach, we focus on a channel mismatch setting, where the domain of interest is distant conversational speech, and labels are only available for close-talking speech. Our proposed method is evaluated on the AMI dataset, outperforming all baselines and bridging the gap between unadapted and in-domain models by over 77% without using any parallel data.",0.0,4.5625
626,2275,5    3.6    5.1    ,Articulatory Feature Classification Using Convolutional Neural Networks,"The ultimate goal of our research is to improve an existing speech-based computational model of human speech recognition on the task of simulating the role of fine-grained phonetic information in human speech processing. As part of this work we are investigating articulatory feature classifiers that are able to create reliable and accurate transcriptions of the articulatory behaviour encoded in the acoustic speech signal. Articulatory feature (AF) modelling of speech has received a considerable amount of attention in automatic speech recognition research. Different approaches have been used to build AF classifiers, most notably multi-layer perceptrons. Recently, deep neural networks have been applied to the task of AF classification. This paper aims to improve AF classification by investigating two different approaches: 1) investigating the usefulness of a deep Convolutional neural network (CNN) for AF classification; 2) integrating the Mel filtering operation into the CNN architecture. The results showed a remarkable improvement in classification accuracy of the CNNs over state-of-the-art AF classification results for Dutch, most notably in the minority classes. Integrating the Mel filtering operation into the CNN architecture did not further improve classification performance.",0.0,4.566666666666666
355,1600,5.9    4.25    3.3    4.85    ,A Study of Lexical and Prosodic Cues to Segmentation in a Hindi-English Code-switched Discourse,"Bilingualism, almost universal in India, routinely appears in communication in many forms. Code-switching with English is very common among city dwellers with the matrix language typically being the speaker's native tongue. While a number of English words have made their way into the lexicon of Indian languages, also prevalent is insertional code-switching where switching is at sentence or clause level. We consider an interesting and widely encountered variety of code-switched speech in the form of public discourses by a popular motivational speaker who uses English, probably for effect, in her Hindi language speeches. We effectively observe three categories of segments in the discourse: Hindi, Hindi with embedded English words and English. In this work, we present the characteristics of our data, and investigate the discrimination potential of lexical and prosodic cues on manually segmented tokens. Lexical cues are obtained via Google Speech API for Indian English recognition. Prosodic cues computed from pitch, intensity and syllable duration estimates are found to demonstrate significant differences between Hindi and English segments, indicating more careful articulation of the embedded language.",0.0,4.574999999999999
214,1316,3    6    5.9    3.4    ,"Data Requirements, Selection and Augmentation for DNN-based Speech Synthesis from Crowdsourced Data","Crowdsourcing speech recordings provides unique opportunities and challenges for personalized speech synthesis as it allows gathering of large quantities of data but with a huge variety in quality. Manual methods for data selection and cleaning quickly become infeasible, especially when producing larger quantities of voices. We present and analyze approaches for data selection and augmentation to cope with this. For differently-sized training sets, we assess speaker adaptation by transfer learning, including layer freezing, and sentence selection using maximum likelihood of forced alignment. The methodological framework utilizes statistical parametric speech synthesis based on Deep Neural Networks (DNNs). We compare objective scores for $576$ voice models, representing all condition combinations. For a constrained set of conditions we also present results from a subjective listening test. We show that speaker adaptation improves overall quality in nearly all cases, sentence selection helps detecting recording errors, and layer freezing proves to be ineffective in our system. We also found that while Mel-Cepstral Distortion (MCD) does not correlate with listener preference across the range of values, the most preferred voices also exhibited the lowest values for MCD. These findings have implications on scalable methods of customized voice building and clinical applications with sparse data.",0.0,4.575
671,2389,3.3    4.9    4.9    5.2    ,Detection of Amyotrophic Lateral Sclerosis (ALS) via Acoustic Analysis,"ALS is a fatal neurodegenerative disease with no cure. Experts typically measure disease progression via the ALSFRS score, which includes measurements of various abilities known to decline. We propose instead the use of speech analysis as a proxy for ALS progression. This technique enables 1) frequent non-invasive, inexpensive, longitudinal analysis, 2) analysis of data recorded in the wild, and 3) creation of an extensive ALS databank for future analysis. Patients and trained medical professionals need not be co-located, enabling more frequent monitoring of more patients from the convenience of their own homes. The goals of this study are the identification of acoustic speech features in naturalistic contexts which characterize disease progression and development of machine models which can recognize the presence and severity of the disease. We evaluated subjects from the Prize4Life Israel dataset, using a variety of frequency, spectral, and voice quality features. The dataset was generated using the ALS Mobile Analyzer, a cell-phone app that collects data regarding disease progress using a self-reported ALSFRS-R questionnaire and several active tasks that measure speech and motor skills. Classification via leave-five-subjects-out cross-validation resulted in an accuracy rate of 79% (61% chance) for males and 83% (52% chance) for females.",0.0,4.575
229,1348,4.35    4.45    5.1    4.4    ,Classification of Correction Turns in Multilingual Dialogue Corpus,"This paper presents a multiclass classification of correction dialog turns using machine learning. The classes are determined by the type of the introduced recognition errors while performing WOz trials and creating the multilingual corpus. Three datasets were obtained using different sets of acoustic-prosodic features on the multilingual dialogue corpus.
The classification experiments were done using different machine learning paradigms: Decision Trees, Support Vector Machines and Deep Learning. After careful experiments setup and optimization on the hyper-parameter space, the obtained classification results were analyzed and compared in the terms of accuracy, precision, recall and F1 score. The achieved results are comparable with those obtained in similar experiments on different tasks and speech databases.
Index Terms: multiclass classification, machine learning, multilingual dialogue corpus",0.0,4.575
289,1460,4.55    4.25    4.2    5.3    ,DNN-based Speech Synthesis for Small Data Sets Considering Bidirectional Speech-Text Conversion,"In statistical parametric speech synthesis, approaches based on deep neural networks (DNNs) have improved qualities of the synthesized speech. General DNN-based approaches require a large amount of training data to synthesize natural speech. However, it is not practical to record speech for many hours from a single speaker. To address this problem, this paper presents a novel pre-training method of DNN-based speech synthesis systems for small data sets. In this method, a Gaussian-Categorical deep relational model (GCDRM), which represents a joint probability of two visible variables, is utilized to describe the joint distribution of acoustic features and linguistic features. During the maximum-likelihood-based training, the model attempts to obtain parameters of a deep architecture considering the bidirectional conversion between 1) generated acoustic features given linguistic features and 2) re-generated linguistic features given acoustic features generated from itself. Owing to considering whether the generated acoustic features are recognizable, our method can obtain reasonable parameters from small data sets. Experimental results show that pre-trained DNN-based systems using our proposed method outperformed randomly-initialized DNN-based systems. This method also outperformed DNN-based systems in a speaker-dependent speech recognition task.",0.0,4.575
868,1221,5.15    4.25    2.95    5.95    ,Improving Dialogue State Tracking Using Deep Reinforcement Learning for Candidate Selection from N-best Speech Recognition Hypotheses,"Most state-of-the-art dialogue state tracking (DST) methods infer the dialogue state based on ground-truth transcriptions of utterances. In real-world situations, utterances are transcribed by automatic speech recognition (ASR) systems, which output the n-best candidate transcriptions  (hypotheses). In certain noisy environments, the best transcription is often imperfect, severely influencing DST accuracy and possibly causing the dialogue system to stall or loop. The missed or misrecognized words can often be found in the runner-up candidate transcriptions from 2 to n, which could be used to improve accuracy of DST. However, looking beyond the top-ranked ASR results poses a dilemma: going too far may introduce noise, while not going far enough may not uncover any useful information. In this paper, we propose a novel approach to automatically determine the optimal time to stop reexamining runner-up ASR transcriptions based on deep reinforcement learning. Our method outperforms the baseline system, which uses only the top-1 ASR result, by 3.1%. Then, we select the top-10 largest word-error-rate utterances, our method can improve DST accuracy by 15.4%, which is five times the overall improvement rate (3.1%). This improvement was expected because our proposed method is able to select informative ASR results at any rank.",1.0,4.575
189,1272,4.9    5.1    5.1    3.2    ,Improving Sparse Representations in Exemplar-Based Voice Conversion with a Phoneme-Selective Objective Function,"The acoustic quality of exemplar-based voice conversion (VC) degrades whenever the phoneme labels of the selected exemplars do not match the phonetic content of the frame being represented. To address this issue, we propose a Phoneme-Selective Objective Function (PSOF) that promotes a sparse representation of each speech frame with exemplars from a few phoneme classes. Namely, PSOF enforces group sparsity on the representation, where each group corresponds to a phoneme class. The sparse representation for exemplars within a phoneme class tends to activate or suppress simultaneously using the proposed objective function. We conducted two sets of experiments on the ARCTIC corpus to evaluate the proposed method. First, we evaluated the ability of PSOF to reduce phoneme mismatches. Then, we assessed its performance on a VC task and compared it against three baseline methods from previous studies. Results from objective measurements and subjective listening tests show that the proposed method effectively reduces phoneme mismatches and significantly improves VC acoustic quality while retaining the voice identity of the target speaker.",0.0,4.575
567,2083,5.15    4.8    3.3    5.05    ,Wavelet Transform Based Mel-scaled Features for Acoustic Scene Classification,"Acoustic scene classification (ASC) is an audio signal processing task where mel-scaled spectral features are widely used by researchers. These features, considered de facto baseline in speech processing, traditionally employ Fourier based transforms. Unlike speech, environmental audio spans a larger range of audible frequency and might contain short high-frequency transients and continuous low-frequency background noise, simultaneously. Wavelets, with a better time-frequency localization capacity, can be considered more suitable for dealing with such signals. This paper attempts ASC by a novel use of wavelet transform based mel-scaled features. The proposed features are shown to possess better discriminative properties than other spectral features while using a similar classification framework. The experiments are performed on two datasets, similar in scene classes but differing by dataset size and length of the audio samples. When compared with two benchmark systems, one based on mel-frequency cepstral coefficients and Gaussian mixture models, and the other based on log mel-band energies and multi-layer perceptron, the proposed system performed considerably better on the test data.",0.0,4.575
1247,2262,5.1    5.05    3.6    ,Analyzing the Relationship between Productivity and Human Communication in an Organizational Setting,"Though it is a truism that communication contributes to organizational productivity, there are surprisingly few empirical studies documenting a relationship between observable interaction and productivity.  This is because comprehensive, direct observation of communication in organizational settings is notoriously difficult. In this work, we use a unique approach to direct and comprehensive observation of communication in organizational settings to analyze the relationship between productivity and communication. Seventy-nine employees working within a software engineering organization wore microphones and had their speech recorded during working hours for a period of approximately 3 years. From the speech data, we infer when any two individuals are talking to each other and use this information to construct a communication graph for the organization for each week. We use the spectral and temporal characteristics of the produced speech and the structure of the resultant communication graphs to predict the productivity of the group, as measured by the number of lines of code produced. The results indicate that the most important speech and graph features for predicting productivity include those that measure the number of unique people interacting within the organization, the frequency of interactions, and the topology of the communication network.",1.0,4.583333333333333
262,1406,5.1    2.75    5.9    ,A Two-Stage Approach to Noisy Cochannel Speech Separation with Gated Residual Networks,"Cochannel speech separation is the task of separating two speech signals from a single mixture. The task becomes even more challenging if the speech mixture is further corrupted by background noise. In this study, we focus on a gender-dependent scenario, where target speech is from a male speaker and interfering speech from a female speaker. We propose a two-stage separation strategy to address this problem in a noise-independent way. In the proposed system, denoising and cochannel separation are performed successively by two modules, which are based on a newly-introduced convolutional neural network for speech separation. The evaluation results demonstrate that the proposed system substantially outperforms one-stage baselines in terms of objective intelligibility and perceptual quality.",0.0,4.583333333333333
539,2019,4.25    4.7    6    3.4    ,"Computational Paralinguistics: Automatic Assessment of Emotions, Mood and Behavioural State from Acoustics of Speech","Paralinguistic analysis of speech remains a challenging task due to the many confounding factors which affect speech production. In this paper, we address the Interspeech 2018 Computational Paralinguistics Challenge (ComParE) which aims to push the boundaries of sensitivity to non-textual information that is conveyed in the acoustics of speech. We attack the problem on several fronts. We posit that a substantial amount of paralinguistic information is contained in spectral features alone. To this end, we use a large ensemble of Extreme Learning Machines for classification of spectral features. We further investigate the applicability of (an ensemble of) CNN-GRUs networks to model temporal variations therein. We report on the details of the experiments and the results for three ComParE sub-challenges: Atypical Affect, Self-Assessed Affect, and Crying. Our results compare favourably and in some cases exceed the published state-of-the-art performance.",0.0,4.5874999999999995
184,1266,4.8    4.35    4.85    4.35    ,Automatic Evaluation of Speech Intelligibility Based on I-vectors in the Context of Head and Neck Cancers,"In disordered speech context, and despite its well-known subjectivity, perceptual evaluation is still the most commonly used method in clinical practice to evaluate the intelligibility level of patients' speech productions.
However, and thanks to increasing computing power, automatic speech processing systems have witnessed a democratization in terms of users and application areas including the medical practice.
In this paper, we evaluate an automatic approach for the prediction of cancer patients' speech intelligibility based on the representation of the speech acoustics in the total variability subspace based on the i-vector paradigm. Experimental evaluations of the proposed predictive approach have shown a very high correlation rate with perceptual intelligibility when applied on the French speech corpora C2SI (r=0.84).
They have also demonstrated the robustness of the approach when using a limited amount of disordered speech per patient, which may lead to the redesign and alleviation of the test protocols usually used in disordered speech evaluation context.",0.0,4.5874999999999995
526,1983,4.75    5.1    3.9    4.6    ,Analysis of L2 Learners’ Progress of Distinguishing Mandarin Tone 2 and Tone 3,"Many studies have shown the effectiveness of perceptual training to improve L2 learners’ ability to distinguish Mandarin tones. In this paper, we quantified learners perceptual characteristics on discriminating the most difficult tone pair, Tone 2 and 3 in Mandarin before and after training. L2 learners’ categorical perception is measured by fitting a sigmoid curve to the identification responses with average F0 height be the acoustic dimension. The boundary location of the two tones in L2 learners’ perception space is significantly improved to a higher F0 height after training. Regression analysis indicated that δF 0 and δt of the initial falling of the concave F0 shape are the key acoustic features for native speakers in discrimination. L2 learners rely on not only the initial fall but also the δF0 of the final rise to discriminate the tones. A detailed analysis using cognitive measurements reports an increasing attention on the initial fall of the F0 contour for L2 learners after perceptual training. These results confirmed that directing the attention to key acoustic features is essential for L2 learners to improve their categorical perception of novel speech contrasts.",0.0,4.5875
707,2466,6    3.2    3.25    5.9    ,Deep Neural Networks for Emotion Recognition Combining Audio and Transcripts,"In this paper, we propose to improve emotion recognition by combining acoustic information and conversation transcripts. On the one hand, an LSTM network was used to detect emotion from acoustic features like f0, shimmer, jitter, MFCC, etc. On the other hand, a multi-resolution CNN was used to detect emotion from word sequences. This CNN consists of several parallel convolutions with different kernel sizes to exploit contextual information at different levels. A temporal pooling layer aggregates the hidden representations of different words into a unique sequence level embedding, from which we compute the emotion posteriors. We optimized a weighted sum of classification and verification losses. The verification loss tries to bring embeddings from same emotions closer while it separates embeddings for different emotions. We also compared our CNN with state-of-the-art text-based hand-crafted features (e-vector). We evaluated our approach on the USC-IEMOCAP dataset as well as the dataset consisting of US English telephone speech. In the former, we used human transcripts while in the latter, we used ASR transcripts. The results showed fusing audio and transcript information improved unweighted accuracy by relative 24% for IEMOCAP and relative 3.4% for the telephone data compared to a single acoustic system.",0.0,4.5875
3,40,2.05    4.7    5.9    5.7    ,Speech Recognition for Medical Conversations,"In this paper we document our experiences with develop-ing  speech  recognition
 formedical  transcription–  a  systemthat automatically transcribes
doctor-patient conversations. To-wards this goal, we built a system along two
different method-ological lines – a Connectionist Temporal Classification
(CTC)phoneme  based  model  and  a  Listen  Attend  and  Spell        (LAS)grapheme
based model.  To train these models we used a cor-pus  of  anonymized 
conversations  representing  approximately14,000 hours of speech. Because of
noisy transcripts and align-ments in the corpus, a significant amount of effort
was investedin data cleaning issues. We describe a two-stage strategy we
fol-lowed for segmenting the data.  The data cleanup and develop-ment of a
matched language model was essential to the successof the CTC based models.  
The LAS based models,  howeverwere found to be resilient to alignment and
transcript noise anddid not require the use of language models.  CTC models
wereable to achieve a word error rate of20.1%, and the LAS modelswere able to
achieve18.3%. Our analysis shows that both mod-els perform well on important
medical utterances and thereforecan be practical for transcribing medical
conversations.",0.0,4.5875
110,1130,4.4    4    5.6    4.35    ,Dialect-geographical Acoustic-Tonetics: Five Disyllabic Tone Sandhi Patterns in Cognate Words from the Wu Dialects of ZhèJiāNg Province,"The typology of tone sandhi patterns on disyllabic tonally cognate words in selected sub-groups of the Wu dialects of the east-central Chinese province of Zhejiang is investigated using data from 48 sites collected over 45 years. A new method of extrinsic z-score normalisation is demonstrated which permits comparison of tones across dialects with different pitch ranges. Five different but typical right-dominant word-tone patterns are identified, acoustically quantified, and their geographical distribution specified. It is hypothesized that changes in isolation tones, and different types of dissimilation of the first tone from the word-final tone, are a possible origin of the observed variation.",0.0,4.5875
641,2306,4.05    5    4.35    4.95    ,Dereverberation and Beamforming in Robust Far-Field Speaker Recognition,"This paper deals with robust speaker verification (SV) in far-field sensing. The robustness is verified on a subset of NIST SRE 2010 corpus retransmitted in multiple real rooms of different acoustics and captured with multiple microphones. We experimented with various data preprocessing steps including different approaches to dereverberation and beamforming applied to ad-hoc microphone arrays. We found that significant improvements in accuracy can be achieved with neural network based generalized eigenvalue beamformer preceded by weighted prediction error dereverberation. We also explored the effect of data augmentation by adding various real or simulated room acoustic properties to the Probabilistic Linear Discriminant Analysis  (PLDA) training dataset. As a result, we developed a speaker recognition system whose performance is stable across different room acoustic conditions. It yields 41.4% relative improvement in performance over the system without multi-channel processing tested on the cleanest microphone data.  With the best combination of data preprocessing and augmentation, we obtained a performance close to the one we achieved with the original clean test data.",0.0,4.5875
119,1147,4.35    5    5    4    ,Glottal Closure Instant Detection from Speech Signal Using Voting Classifier and Recursive Feature Elimination,"In our previous work, we introduced a classification-based method for the automatic detection of glottal closure instants (GCIs) from the speech signal and we showed it was able to perform very well on several test datasets. In this paper, we investigate whether adding more features (voiced/unvoiced, harmonic/noise, spectral etc.) and/or using an ensemble of classifiers such as a voting classifier can further improve GCI detection performance. We show that using additional features leads to a better detection accuracy; best results were obtained when recursive feature elimination was applied on the whole feature set. In addition, a voting classifier is shown to outperform other classifiers and other existing GCI detection algorithms on publicly available databases.",0.0,4.5875
378,1652,5.25    5    3.3    4.8    ,Robust TDOA Estimation Based on Time-Frequency Masking and Deep Neural Networks,"Deep learning based time-frequency (T-F) masking has dramatically advanced monaural speech separation and enhancement. This study investigates its potential for robust time difference of arrival (TDOA) estimation in noisy and reverberant environments. Three novel algorithms are proposed to improve the robustness of conventional cross-correlation-based, beamforming-based and subspace-based algorithms for speaker localization. The key idea is to leverage the power of deep neural networks (DNN) to accurately identify T-F units that are relatively clean for TDOA estimation. All of the proposed algorithms exhibit strong robustness for TDOA estimation in environments with low input SNR, high reverberation and low direction-to-reverberant energy ratio.",0.0,4.5875
700,2454,4.1    5.2    3.35    5.7    ,Cross-language Phoneme Mapping for Low-resource Languages: an Exploration of Benefits and Trade-offs,"Voice-based systems are an essential approach for engaging directly with low-literate and underrepresented populations. Previous work has taken advantage of high-resource speech recognition technology for low-resource language speech recognition through cross-language phoneme mapping. Unfortunately, there is little guidance in how to deploy these systems across a range of languages. We present a systematic exploration of four source languages and five target languages to understand the trade-offs and performance of different source languages and training techniques. We find that one can improve recognition accuracy by selecting a source language that has similar linguistic properties to that of the target language. We also find that the number of alternative pronunciations per word and gender of participants also impact recognition accuracy. Our work will allow other researchers and practitioners to quickly develop high-quality small-vocabulary speech-based applications for under-resourced languages",0.0,4.5875
116,1139,5.05    5.25    4.35    4.05    4.25    ,Rapid Collection of Spontaneous Speech Corpora Using Telephonic Community Forums,"We present a novel technique for rapid collection of spontaneous speech data over mobile phone channel using telephonic community forums. Our public forum allows users to post audio messages, listen to messages posted by others, post votes and audio comments, and share content with friends through subsidized phone calls. The entertainment aspects and sharing features of the forum lead to its viral spread in Pakistan. Within 8 months, it reached 11,017 users and gathered 1,207 hours of speech data comprising 57,454 audio-posts and 130,685 audio-comments, spanning Urdu and 9 regional languages. We trained an ASR using just 9.5 hours of the corpus to obtain 24.19% WER. Community forums automatically overcome common spontaneous speech data collection challenges like speaker recruitment, natural speech elicitation, content diversity, informed consent, sampling real-world ambient noise, and reach (for geographically remote linguistic communities). This technique is especially useful for gathering speech corpora for under-resourced languages hence enabling the development of speech recognition, keyword spotting, speaker ID, and noise classification systems (among others) for such languages. It also allows rapid, automatic preservation of spoken languages and oral aspects of culture. This technique can be extended to collect speech data for endangered languages, oral cultures, and linguistic minorities.",0.0,4.59
533,2005,4.95    5    4.25    4.2    ,A Comparative Study of Statistical Conversion of Face to Voice Based on Their Subjective Impressions,"Recently, various types of Voice-based User Interfaces (VUIs) including smart speakers
have been developed to be on the market. However, many of the VUIs use only synthetic
voices to provide information for users. To realize a more natural interface,
one feasible solution will be personifying VUIs by adding visual features such as face,
but what kind of face is suited to a given quality of voice or what kind of voice quality
is suited to a given face?
In this paper, we test methods of statistical conversion from face to voice
based on their subjective impressions.
To this end, six combinations of two types of face features, one type of speech features,
and three types of conversion models are tested using a parallel corpus developed
based on subjective mapping from face features to voice features.
The experimental results show that
each subject judge one specific and subject-dependent voice quality
as suited to different faces, and that
the optimal number of mixtures of face features is different from
the numbers of mixtures of voice features tested.",0.0,4.6
282,1452,5    4.95    4.35    4.1    ,A Multistage Training Framework for Acoustic-to-Word Model,"Acoustic-to-word (A2W) prediction model based on Connectionist Temporal Classification (CTC) criterion has gained increasing interest in recent studies. Although previous studies have shown that A2W system could achieve competitive Word Error Rate (WER), there is still performance gap compared with the conventional speech recognition system when the amount of training data is not exceptionally large. In this study, we empirically investigate advanced model initializations and training strategies to achieve competitive speech recognition performance on 300 hour subset of the Switchboard task (SWB-300Hr). We first investigate the use of hierarchical CTC pretraining for improved model initialization. We also explore curriculum training strategy to gradually increase the target vocabulary size from 10k to 20k. Finally, joint CTC and Cross Entropy (CE) training techniques are studied to further improve the performance of A2W system. The combination of hierarchical-CTC model initialization, curriculum training and joint CTC-CE training translates to a relative of 12.1\% reduction in WER. Our final A2W system evaluated on Hub5-2000 test sets achieves a WER of 11.4/20.8 for Switchboard and CallHome parts without using language model and decoder.",0.0,4.6
270,1425,5.15    4.35    4.2    4.7    ,Analyzing Effect of Physical Expression on English Proficiency for Multimodal Computer-Assisted Language Learning,"English proficiency is important for communication in English. Computer-Assisted Language Learning (CALL) systems are introduced to provide a convenient and low-cost language learning environment. Most of the conventional speech-based CALL systems concentrate on developing verbal fluency of the learners. However, actual English communication involves not only verbal expressions but also facial expressions and gestures, which could affect the perceived proficiency. The objective of our research is to develop a CALL system that can evaluate fluency of physical expressions as well as the verbal fluency of English.
However, it is not clear how physical expressions affect the overall proficiency of English. Therefore, this study investigates the relationship between the proficiency of English and the fluency of the physical expression by analyzing the dialog data of the multimodal CALL system.",0.0,4.6
542,2027,2.8    4.35    5.8    5.1    4.95    ,Unsupervised Discovery of Non-native Pronunciation Patterns in L2 English Speech for Mispronunciation Detection and Diagnosis,"Second language (L2) speech is often annotated with the native phoneme categories. However, we often observe that an L2 speech segment generally deviates from a canonical phoneme, and sometimes it is very difficult for linguists to annotate with any canonical phoneme label. We refer to these segments as non-native phonetic patterns. Existing approaches to mispronunciation detection and diagnosis (MDD) focus mainly on canonical mispronunciations, i.e. one canonical phoneme is substituted for another, aside from those deleted or inserted. To better represent L2 speech, this work explores non-native phonetic patterns (NN-PPs) of each native phoneme by an unsupervised approach. We apply an optimized k-means algorithm to cluster state-based phonemic posterior-grams, which are generated with a deep neural network. Then, to discover the NN-PPs related to each native phoneme, we perform forced alignment to divide L2 speech into segments grouped by native phonemes. We use the cluster sequences within segments derived from clustering results to represent different phonetic patterns of each native phoneme. Finally, we apply Cluster Sequence Analysis to discover each phoneme's potential NN-PPs. We verified experimentally that NN-PPs can extend the native phoneme categories to better describe L2 speech, which can enrich the existing approaches to MDD for better performance.",0.0,4.6
32,990,5.05    5.1    3.35    4.9    ,Comparing the Max and Noisy-Or Pooling Functions in Multiple Instance Learning for Weakly Supervised Sequence Learning Tasks,"Many sequence learning tasks require the localization of certain events in sequences. Because it can be expensive to obtain strong labeling that specifies the starting and ending times of the events, modern systems are often trained with weak labeling without explicit timing information. Multiple instance learning (MIL) is a popular framework for learning from weak labeling. In a common scenario of MIL, it is necessary to choose a pooling function to aggregate the predictions for the individual steps of the sequences. In this paper, we compare the ""max"" and ""noisy-or"" pooling functions on a speech recognition task and a sound event detection task. We find that max pooling is able to localize phonemes and sound events, while noisy-or pooling fails. We provide a theoretical explanation of the different behavior of the two pooling functions on sequence learning tasks.",0.0,4.6
638,2300,5.1    3.1    5.2    5    ,End-to-end Text-dependent Speaker Verification Using Novel Distance Measures,"This paper explores novel ideas in building end-to-end deep neural network (DNN) based text-dependent speaker verifica- tion (SV) system. The baseline approach consists of map- ping a variable length speech segment to a fixed dimensional speaker vector by estimating the mean of hidden representa- tions in DNN structure. The distance between two utterances is obtained by computing L2 norm between the vectors. This approach performs worse than the conventional Gaussian Mix- ture Model-Universal Background Model (GMM-UBM) based system in publicly available corpora. We believe that poor per- formance is due to the employed averaging operation, which may not capture the phonetic information of an utterance. Past studies indicate that techniques exploiting phonetic information in addition to speaker is beneficial for this task. This paper therefore proposes to incorporate content information of the speech signal by computing distance function with linguistic units co-occuring between enrollment and test data. The whole network is optimized by employing a triplet-loss objective in an end-to-end fashion to output SV scores. Experiments on the RSR2015 dataset show that the proposed approach outperforms GMM-UBM system by 48% and 36% relative equal error rate for fixed-phrase and Random-digit conditions respectively.",0.0,4.6
70,1059,4.2    5.85    3.25    5.1    ,"Dysarthric Speech Classification Using Glottal Features Computed from Non-words, Words and Sentences","Dysarthria is a neuro-motor disorder resulting from the disruption of normal activity in speech production leading to slow, slurred and imprecise (low intelligible) speech. Automatic classification of dysarthria from speech can be used as a potential clinical tool in medical treatment. This paper examines the effectiveness of glottal source parameters in dysarthric speech classification from three categories of speech signals, namely non-words, words and sentences.  In addition to the glottal parameters, two sets of acoustic parameters extracted by the openSMILE toolkit are used as baseline features. A dysarthric speech classification system is proposed by training support vector machines (SVMs) using features extracted from speech utterances and their labels indicating dysarthria/healthy. Classification accuracy results indicate that the glottal parameters contain discriminating information required for the identification of dysarthria. Additionally, the complementary nature of the glottal parameters is demonstrated when these parameters, in combination with the openSMILE-based acoustic features, result in improved classification accuracy. Analysis of classification accuracies of the glottal and openSMILE features for non-words, words and sentences is carried out. Results indicate that in terms of classification accuracy the word level is best suited in identifying the presence of dysarthria.",0.0,4.6
118,1143,4    3.5    5.1    5.8    ,Deep Learning Techniques for Koala Activity Detection,"Automatically detecting koalas in the real-life environment from audio recordings will immensely help ecologists, conservation groups, and government departments interested in their preservation and the protection of their habitat. Inspired by the success of deep learning approaches in various audio classification tasks, in this paper, the feasibility of recognizing koalas' calls using a convolutional recurrent neural network architecture (CNN+RNN) is studied. The benefit of this architecture is twofold: firstly, convolutional layers learn local time-frequency patterns from the audio spectrogram and secondly, recurrent layers model longer temporal dependencies of the extracted features. In our datasets, the performance of CNN+RNN is evaluated and compared with standard convolutional neural networks (CNNs). The experimental results show that hybrid CNN+RNN architecture is beneficial for learning long-term patterns in spectrogram exhibited by koalas' calls in unseen conditions. The proposed method is also applicable for detecting other animal calls such as bird sound where it achieves 87.46% area under curve score on the bird audio detection challenge evaluation data.",0.0,4.6
561,2075,5.25    5.3    4.3    3.55    ,Learning Interpretable Control Dimensions for Speech Synthesis by Using External Data,"There are many aspects of speech that we might want to control when creating text-to-speech (TTS) systems. We present a general method that enables control of arbitrary aspects of speech, which we demonstrate on the task of emotion control. Current TTS systems use supervised machine learning and are therefore heavily reliant on labelled data. If no labels are available for a desired control dimension, then creating interpretable control becomes challenging. We introduce a method that uses external, labelled data (i.e.\ not the original data used to train the acoustic model) to enable the control of dimensions that are not labelled in the original data. Adding interpretable control allows the voice to be manually controlled to produce more engaging speech, for applications such as audiobooks. We evaluate our method using a listening test.",0.0,4.6000000000000005
535,2012,3.95    4.85    4.55    5.05    ,Supervised I-vector Modeling - Theory and Applications,"Over the last decade, the factor analysis based modeling of a variable length speech utterance into a fixed dimensional vector (termed as i-vector) has been prominently used for many tasks like speaker recognition, language recognition and even in speech recognition. The i-vector model is an unsupervised learning paradigm where the data is initially clustered using 
a Gaussian Mixture Universal Background Model (GMM-UBM). The adapted means of the Gaussian mixture components are dimensionality reduced using the Total Variability Matrix (TVM) where the latent variables are modeled with a single Gaussian distribution.  In this paper, we  propose to rework the theory of i-vector modeling using a supervised framework where the speech utterances are associated with a label. Class labels are introduced in the i-vector model using a mixture Gaussian prior. We show that the proposed model is a generalized i-vector model and the conventional i-vector model turns out to be a special case of this model. This model is applied for a language recognition task using the NIST Language Recognition Evaluation (LRE) 2017 dataset. In these experiments, the supervised i-vector model provides significant improvements over the conventional i-vector model (average relative improvements of $5\%$ in terms of $C_{avg}$.",0.0,4.6000000000000005
577,2117,5.1    5.2    4.15    3.95    ,TDNN-based Multilingual Speech Recognition System for Low Resource Indian Languages,"India is a diverse and multilingual country. It has vast linguistic variations, spoken across its billion plus population. Lack of resources in terms of transcribed speech data, phonetic pronunciation dictionary or lexicon, and text collection has hindered the development and improvement of the ASR systems for Indic languages. With the Interspeech 2018 Special Session: Low Resource Speech Recognition Challenge for Indian Languages,
efforts have been made to solve this issue to an extent. In this paper, we explore the fact that the shared phonetic properties of the languages are essential for improved ASR performance. We build a multilingual Time Delay Neural Network (TDNN) system that uses combined acoustic modeling and language-specific information to decode the input test sequences. Using this approach, for Tamil, Telugu and Gujarati language we obtain a Word Error Rate (WER) of 16.07%, 17.14%, 17.69%, respectively, which was the second best system at the challenge.",0.0,4.6000000000000005
647,2326,5.9    4.95    3.25    4.3    ,Music Source Activity Detection and Separation Using Deep Attractor Network,"In music signal processing, singing voice detection and music source separation are widely researched topics. Recent advances in deep neural network based source separation have advanced the state of the performance in the problem of separating out vocal/instrument (sources) from a mixture. However, the problem of joint source activity detection and separation remains unexplored. In this paper, we propose an unsupervised method to perform source activity detection using the high-dimensional embedding generated by Deep Attractor Network (DANet) when trained for music source separation. By defining both tasks together, DANet is able to dynamically estimate the number of outputs depending on the active sources. We propose a fully Expectation-Maximization training paradigm for DANet which further improves the separation performance of the original DANet. Experiments show that our system has comparable separation performance to a strong baseline system and achieves higher accuracy in source activation detection.",0.0,4.6000000000000005
486,1888,4.3    3.35    5.05    5.75    ,End-to-End Speech Command Recognition with Capsule Network,"In recent years, neural networks have become one of the common approaches used in speech recognition(SR), with SR systems based on Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) achieving  the state-of-the-art results in various SR benchmarks. Especially, since CNNs are capable of capturing the local features effectively, they are applied to tasks which have relatively short-term dependencies, such as keyword spotting or phoneme-level sequence recognition. However, one limitation of CNNs is that, with max-pooling, they do not consider the pose relationship between low-level features. Motivated by this problem, we apply the capsule network to capture the spatial relationship and pose information of speech spectrogram features in both frequency and time axes. We show that our proposed end-to-end SR system with capsule networks on one-second speech commands dataset achieves better results on both clean and noise-added test than baseline CNN models.",0.0,4.6125
465,1834,5    4.15    5    4.3    ,A Non-convolutive NMF Model for Speech Dereverberation,"Reverberation corrupts speech recorded using distant microphones, resulting in poor speech intelligibility. We propose a single-channel, supervised non-negative matrix factorization (NMF) based dereverberation method, in contrast to the convolutive NMF (CNMF) based methods in literature.  Recent supervised approaches use a CNMF model for reverberation and a NMF model for clean speech spectrogram to obtain enhanced speech by directly estimating the clean speech activations. In the proposed method, with a separability assumption on the room impulse response (RIR) spectrogram, the reverb speech can be decomposed into bases and activations using conventional NMF. Using these reverb activations, the clean speech activations are estimated to obtain enhanced speech. The proposed model (i) helps in imposing meaningful constraints on the RIR in both frequency- and time-domains to achieve improved enhancement (ii) leads to a framework that can include a NMF model for noise. (iii) gives a better interpretation of the effects of reverberation in the NMF context. We evaluate and compare the enhancement performance of the algorithm on reverb and noisy conditions, simulated using TIMIT utterances and REVERB challenge RIRs. The proposed method performs better than existing C-NMF based methods in objective measures, such as cepstral distance (CD) and speech-to-reverberation modulation energy ratio (SRMR).",0.0,4.6125
1305,2500,4.2    5    3.35    5.9    ,Word Error Rate Estimation for Speech Recognition: e-WER,"Measuring the performance of automatic speech recognition (ASR) systems requires
manually transcribed data in order to compute the word error rate (WER), which is often time-consuming and expensive. In this paper, we propose a novel approach to estimate WER, or e-WER, which does not require a gold-standard transcription of the test set. Our e-WER framework uses a comprehensive set of features: ASR recognised text, grapheme
recognition results to complement recognition output, and internal decoder (glassbox)
features. We report results for the two features; black-box and glass-box using unseen 24 Arabic broadcast programs. Our system achieves 12.3% WER mean absolute error (MAE) and 16.9% WER root mean square error (RMSE) across 1400 sentences.",1.0,4.6125
681,2414,2.6    5.9    5    4.95    ,End-to-End Speech Recognition from the Raw Waveform,"State-of-the-art speech recognition systems rely on fixed, hand-crafted features such as mel-filterbanks to preprocess the waveform before the training pipeline. 
In this paper, we study end-to-end systems trained directly from the raw waveform, building on two alternatives for trainable replacements of mel-filterbanks that use a convolutional architecture. The first one is inspired by gammatone filterbanks  (Hoshen et al., 2015; Sainath et al, 2015), and the second one by the scattering transform (Zeghidour et al., 2017).
We propose two modifications to these architectures and systematically compare them to mel-filterbanks, on the Wall Street Journal dataset. The first modification is the addition of an instance normalization layer, which greatly improves on the gammatone-based trainable filterbanks and speeds up the training of the scattering-based filterbanks. The second one relates to the low-pass filter used in these approaches. These modifications consistently improve performances for both approaches, and remove the need for a careful initialization in scattering-based trainable filterbanks. In particular, we show a consistent improvement in word error rate of the trainable filterbanks relatively to comparable mel-filterbanks. It is the first time end-to-end models trained from the raw signal significantly outperform mel-filterbanks on a large vocabulary task with clean recording conditions.",0.0,4.6125
669,2383,5.1    5.15    4.1    4.1    ,Large Vocabulary Concatenative Resynthesis,"Traditional speech enhancement systems reduce noise by modifying the noisy signal, which suffer from two problems: under-suppression of noise and over-suppression of speech. 
As an alternative, in this paper, we use the recently introduced concatenative resynthesis approach where we replace the noisy speech with its clean resynthesis. 
The output of such a system can produce speech that is both noise-free and high quality. This paper generalizes our previous small-vocabulary system to large vocabulary.  To do so, we employ efficient decoding techniques using fast approximate nearest neighbor (ANN) algorithms. Firstly, we apply ANN techniques on the original small vocabulary task and get 5X speedup.  We then apply the techniques to the construction of a large vocabulary concatenative resynthesis system and scale the system up to 12X larger dictionary. We perform listening tests with five participants to measure subjective quality and intelligibility of the output speech.",0.0,4.6125
729,2525,4.9    5.1    4.2    4.25    ,Investigation of Using Disentangled and Interpretable Representations for One-shot Cross-lingual Voice Conversion,"We study the problem of cross-lingual voice conversion in non-parallel speech corpora and one-shot learning setting. Most prior work require either parallel speech corpora or enough amount of training data from a target speaker. However, we convert an arbitrary sentences of an arbitrary source speaker to target speaker's given only one target speaker training utterance. To achieve this, we formulate the problem as learning disentangled speaker-specific and context-specific representations and follow the idea of \cite{hsu2017unsupervised} which uses Factorized Hierarchical Variational Autoencoder (FHVAE). After training FHVAE on multi-speaker training data, given arbitrary source and target speakers' utterance, we estimate those latent representations and then reconstruct the desired utterance of converted voice to that of target speaker. We investigate the effectiveness of the approach by conducting voice conversion experiments with varying size of training utterances and it was able to achieve reasonable performance with even just one training utterance. We also examine the speech representation and show that World vocoder outperforms Short-time Fourier Transform (STFT) used in \cite{hsu2017unsupervised}. Finally, in the subjective tests, for one language and cross-lingual voice conversion, our approach achieved significantly better or comparable results compared to VAE-STFT and GMM baselines in speech quality and similarity.",0.0,4.6125
556,2065,5.6    4.25    4.3    4.3    ,Towards Automated Single Channel Source Separation Using Neural Networks,"Many applications of single channel source separation (SCSS) including automatic speech recognition (ASR), hearing aids etc.  require an estimation of only one source from a mixture of many sources.  Treating this special case as a regular SCSS problem where in all constituent sources are given equal priority in terms of reconstruction may result in a suboptimal separation performance. In this paper, we tackle the one source separation problem by suitably modifying the orthodox SCSS framework and focus only on one source at a time.  The proposed approach is a generic framework that can be applied to any existing SCSS algorithm, improves performance, and scales well when there are more than two sources in the mixture unlike most existing SCSS methods. Additionally, existing SCSS algorithms rely on fine hyper-parameter tuning hence making them difficult to use in practice. Our framework takes a step towards automatic tuning of the hyper-parameters thereby making our method better suited for the mixture to be separated and thus practically more useful. We test our framework on a neural network based algorithm and the results show an improved performance in terms of SDR and SAR.",0.0,4.6125
712,2476,5.65    2.65    5.05    5.1    ,Investigation on LSTM Recurrent N-gram Language Models for Speech Recognition,"Recurrent neural networks (NN) with long short-term memory (LSTM) are the current state of the art to model long term dependencies. However, recent studies indicate that NN language models (LM) need only limited length of history to achieve excellent performance. In this paper, we extend the previous investigation on LSTM network based n-gram modeling to the domain of automatic speech recognition (ASR). First, applying recent optimization techniques and up to 6-layer LSTM networks, we improve LM perplexities by nearly 50% relative compared to classic count models on three different domains. Then, we demonstrate by experimental results that perplexities improve significantly only up to 40-grams when limiting the LM history. Nevertheless, the ASR performance saturates already around 20-grams despite across sentence modeling. Analysis indicates that the performance gain of LSTM NNLM over count models results only partially from the longer context and cross sentence modeling capabilities. Using equal context, we show that deep 4-gram LSTM can significantly outperform large interpolated count models by performing the backing off and smoothing significantly better. This observation also underlines the decreasing importance to combine state-of-the-art deep NNLM with count based model.",0.0,4.612500000000001
192,1283,3.85    4.9    5.1    ,Topic and Keyword Identification for Low-resourced Speech Using Cross-Language Transfer Learning,"This paper studies topic and keyword identification for languages in which we have no transcribed speech data. We adopt a transfer learning framework to transfer what is learned from rich-resourced languages (RRL) to low-resourced languages (LRL). Specifically, we propose that a convolutional neural network (CNN) trained as a topic classifier in an RRL learns features (hidden layer activations) that can be used for the same purpose in an LRL.  The CNN observes acoustic features, RRL phones, or segment clusters generated by an unsupervised phone clustering system; its hidden layers are retained, and its output layer re-trained from scratch on the LRL. Our results are compared with the state-of-the-art topic classification methods on cross-language ASR transcripts. We also discuss the successful detection of topic dependent keywords and the use of unsupervised learning based clusters in our approach for low-resourced language topic detection.",0.0,4.616666666666666
300,1484,4.2    5.85    3.8    ,Deep Learning for Acoustic Echo Cancellation in Noisy and Double-Talk Scenarios,"Traditional acoustic echo cancellation (AEC) works by identifying an acoustic impulse response using adaptive algorithms. We formulate AEC as a supervised speech separation problem, which separates the loudspeaker signal and the near-end signal so that only the latter is transmitted to the far end. A recurrent neural network with bidirectional long short-term memory (BLSTM) is trained to estimate the ideal ratio mask from features extracted from the mixtures of near-end and far-end signals. A BLSTM estimated mask is then applied to separate and suppress the far-end signal, hence removing the echo. Experimental results show the effectiveness of the proposed method for echo removal in double-talk, background noise, and nonlinear distortion scenarios. In addition, the proposed method can be generalized to untrained speakers.",0.0,4.616666666666667
326,1538,4.15    4.45    5.25    ,Effectiveness of Dynamic Features in INCA and Temporal Context-INCA,"Non-parallel Voice Conversion (VC) has gained significant attention since last one decade. Obtaining corresponding speech frames from both the source and target speakers before learning the mapping function in the non-parallel VC is a key step in the standalone VC task. Obtaining such corresponding pairs, is more challenging due to the fact that both the speakers may have uttered different utterances from same or the different languages. Iterative combination of a Nearest Neighbor search step and a Conversion step Alignment (INCA) and its variant Temporal Context (TC)-INCA are popular unsupervised alignment algorithms. The INCA and TC-INCA iteratively learn the mapping function after getting the Nearest Neighbor (NN) aligned pairs from the intermediate converted and the target spectral features. In this paper, we propose to use dynamic features along with static features to calculate the NN aligned pairs in both the INCA and TC-INCA algorithms (since the dynamic features are known to play a key role to differentiate major phonetic categories). We obtained on an average relative improvement of 13.75 % and 5.39 % with our proposed Dynamic INCA and Dynamic TC-INCA, respectively. This improvement is also positively reflected in the quality of converted voices.",0.0,4.616666666666667
616,2226,4.3    4.6    4.75    4.85    ,Articulatory and Stacked Bottleneck Features for Low Resource Speech Recognition,"In this paper, we discuss the benefits of using articulatory and stacked bottleneck features (SBF) for low resource speech recognition. Articulatory features (AF) which capture the underlying attributes of speech production are found to be robust to channel and speaker variations. However, building an efficient articulatory classifier to extract AF requires an enormous amount of data. In low resource acoustic modeling, we propose to train the bidirectional long short-term memory (BLSTM) articulatory classifier by pooling data from the available low resource Indian languages, namely, Gujarati, Tamil, and Telugu. This is done in the context of Microsoft Indian Language challenge. Similarly, we train a multilingual bottleneck feature extractor and an SBF extractor using the pooled data. To bias, the SBF network towards the target language, a second network in the stacked architecture was trained using the target language alone. The performance of ASR system trained with stand-alone AF is observed to be at par with the multilingual bottleneck features. When the AF and the biased SBF are appended, they are found to outperform the conventional filterbank features in the multilingual deep neural network (DNN) framework and the high-resolution Mel frequency cepstral coefficient (MFCC) features in the time-delayed neural network(TDNN) framework.",0.0,4.625
323,1531,4.4    4.1    4.85    5.15    ,Device-directed Utterance Detection,"In this work, we propose a classifier for distinguishing device-directed queries from background speech in the context of interactions with voice assistants.
Applications include rejection of false wake-ups or unintended interactions as well as enabling wake-word free follow-up queries.
Consider the example interaction: {\it ``Computer, play music'', ``Computer, reduce the volume''}.
In this interaction, the user needs to repeat the wake-word ({\it Computer}) for the second query. To allow for more natural interactions, the device could immediately re-enter listening state after the first query (without wake-word repetition) and accept or reject a potential follow-up as device-directed or background speech.
The proposed model consists of two long short-term memory (LSTM) neural networks trained on 
acoustic features and automatic speech recognition (ASR) 1-best hypotheses, respectively. 
A feed-forward deep neural network (DNN) is then trained to combine the acoustic and 1-best embeddings, derived from the LSTMs, with features from the ASR decoder. 
Experimental results show that ASR decoder, acoustic embeddings, and 1-best embeddings yield an equal-error-rate (EER) of $9.3~\%$, $10.9~\%$ and $20.1~\%$, respectively. 
Combination of the features resulted in a $44~\%$ relative improvement and a final EER of $5.2~\%$.",0.0,4.625
53,1030,5.1    4.9    4.75    3.75    ,Improving Attention Based Sequence-to-Sequence Models for End-to-End English Conversational Speech Recognition,"In this work, we propose two improvements to attention based sequence-to-sequence models for end-to-end speech recognition systems. For the first improvement, we propose to use an input-feeding architecture which feeds not only the previous context vector but also the previous decoder hidden state information as inputs to the decoder. The second improvement is based on a better hypothesis generation scheme for sequential minimum Bayes risk (MBR) training of sequence-to-sequence models where we introduce softmax smoothing into N-best generation during MBR training. We conduct the experiments on both Switchboard-300hrs and Switchboard+Fisher-2000hrs datasets and observe significant gains from both proposed improvements. Together with other training strategies such as dropout and scheduled sampling, our best model achieved WERs of 8.3%/15.5% on the Switchboard/CallHome subsets of Eval2000 without any external language models which is highly competitive among state-of-the-art English conversational speech recognition systems.",0.0,4.625
307,1504,5.1    4.55    3.6    5.25    ,Voice Conversion across Arbitrary Speakers Based on a Single Target-Speaker Utterance,"Developing a voice conversion (VC) system for a particular speaker typically requires considerable data from both the source and target speakers.  This paper aims to effectuate VC across arbitrary speakers, which we call any-to-any VC, with only a single target-speaker utterance. Two systems are studied: (1) the i-vector-based VC (IVC) system and (2) the speaker-encoder-based VC (SEVC) system. Phonetic PosteriorGrams are adopted as speaker-independent linguistic features extracted from speech samples. Both systems train a multi-speaker deep bidirectional long-short term memory (DBLSTM) VC model, taking in additional inputs that encode speaker identities, in order to generate the outputs. In the IVC system, the speaker identity of a new target speaker is represented by i-vectors. In the SEVC system, the speaker identity is represented by speaker embedding predicted from a separately trained model. Experiments verify the effectiveness of both systems in achieving VC based only on a single target-speaker utterance. Furthermore, the IVC approach is superior to SEVC, in terms of the quality of the converted speech and its similarity to the utterance produced by the genuine target speaker.",0.0,4.625
343,1570,5.1    5.85    3.3    4.25    ,Automatic Visual Augmentation for Concatenation Based Synthesized Articulatory Videos from Real-time MRI Data for Spoken Language Training,"For the benefit of spoken language training, concatenation based articulatory video synthesis has been proposed in the past to overcome the limitation in the articulatory data recording. For this, real time magnetic resonance imaging (rt-MRI) video image-frames (IFs) containing articulatory movements have been used. These IFs require a visual augmentation for better understanding. We, in this work, propose an augmentation method using pixel intensities in the regions enclosed by the articulatory boundaries obtained from air-tissue boundaries (ATBs). Since, the pixel intensities reflect the muscle movements in the articulators, the augmented IFs could provide realistic articulatory movements, when we color them accordingly. However, the ATB manual annotation is time consuming; hence, we propose to synthesize ATBs using the ATBs from a few selected frames that have been used in synthesizing the articulatory videos. We augment a set of synthesized articulatory videos for 50 words obtained from the MRI-TIMIT database. Subjective evaluation on the quality of the augmented videos using twenty-one subjects suggests that the videos are visually more appealing than the respective synthesized rt-MRI videos with a rating of 3.75 out of 5, where a score of 5 (1) indicates that the augmented video quality is excellent (poor).",0.0,4.625
42,1015,5.05    2.75    4.85    5.85    ,Learning Discriminative Features for Speaker Identification and Verification,"The success of any Text-Independent Speaker Identification and/or Verification system relies upon the system's capability to learn discriminative features.

  In this paper we propose a Convolutional Neural Network (CNN) Architecture based on the popular Very Deep VGG [1] CNNs, with key modifications to accommodate variable length spectrogram inputs, reduce the model disk space requirements and reduce the number of parameters, resulting in significant reduction in training times. We also propose a unified deep learning system for both Text-Independent Speaker Recognition and Speaker Verification, by training the proposed network architecture under the joint supervision of Softmax loss and Center loss [2] to obtain highly discriminative deep features that are suited for both Speaker Identification and Verification Tasks.

  We use the recently released VoxCeleb dataset [3], which contains hundreds of thousands of real world utterances of over 1200 celebrities belonging to various ethnicities, for benchmarking our approach. Our best CNN model achieved a Top-1 accuracy of 84.6\%, a 4\% absolute improvement over VoxCeleb's approach, whereas training in conjunction with Center Loss improved the Top-1 accuracy to 89.5\%, a ~10\% absolute improvement over Voxceleb's approach.",0.0,4.625
682,2416,4.2    4.35    4.85    5.1    ,Contextual Speech Recognition in End-to-end Neural Network Systems Using Beam Search,"Recent work has shown that end-to-end (E2E) speech recognition architectures such as Listen Attend and Spell (LAS) can achieve state-of-the-art quality results in LVCSR tasks.  One benefit of this architecture is that it does not require a separately trained pronunciation model, language model, and acoustic model. However, this property also introduces a drawback: it is not possible to adjust language model contributions separately from the system as a whole. As a result, inclusion of dynamic, contextual information (such as nearby restaurants or upcoming events) into recognition requires a different approach from what has been applied in conventional systems.

We introduce a technique to adapt the inference process to take advantage of
contextual signals by adjusting the output likelihoods of the neural network at
each step in the beam search. We apply the proposed method to a LAS E2E
model and show its effectiveness in experiments on a voice search task with
both artificial and real contextual information. Given optimal
context, our system reduces WER from 9.2% to 3.8%. The results show that this
technique is effective at incorporating context into the prediction of an
E2E system.",0.0,4.625
510,1943,4.25    3.45    5.8    5    ,Deep Lip Reading: a Comparison of Models and an Online Application,"The goal of this paper is to develop state-of-the-art models for lip reading --
visual speech recognition.  We develop three
architectures and compare their accuracy and training times: (i) a
recurrent model using LSTMs; (ii) a fully convolutional model; and
(iii) the recently proposed transformer model. The recurrent and fully
convolutional models are trained with a Connectionist Temporal
Classification loss and use an explicit language model for decoding, the transformer is
a sequence-to-sequence model.  Our best performing model improves the
state-of-the-art word error rate on the challenging BBC-Oxford
Lip Reading Sentences 2 (LRS2) benchmark dataset by over
20 percent.

As a further contribution we investigate the fully convolutional model
when used for online (real time) lip reading of continuous speech, and show that
it achieves high performance with low latency.",0.0,4.625
418,1743,5.1    5.25    4.75    3.4    ,Depression Detection from Short Utterances via Diverse Smartphones in Natural Environmental Conditions,"Depression is a leading cause of disease burden worldwide, however there is an unmet need for screening and diagnostic measures that can be widely deployed in real-world environments. Voice-based diagnostic methods are convenient, non-invasive to elicit, and can be collected and processed in near real-time using modern smartphones, smart speakers, and other devices. Studies in voice-based depression detection to date have primarily focused on laboratory-collected voice samples, which are not representative of typical user environments or devices. This paper conducts the first investigation of voice-based depression assessment techniques on real-world data from 887 speakers, recorded using a variety of different smartphones. Evaluations on 16 hours of speech show that conservative segment selection strategies using highly thresholded voice activity detection, coupled with tailored normalization approaches are effective for mitigating smartphone channel variability and background environmental noise. Together, these strategies can achieve F1 scores comparable with or better than those from a combination of clean recordings, a single recording environment and long utterances. The scalability of speech elicitation via smartphone allows detailed models dependent on gender, smartphone manufacturer and/or elicitation task. Interestingly, results herein suggest that normalization based on these criteria may be more effective than tailored models for detecting depressed speech.",0.0,4.625
46,1020,5.2    4.1    4.1    5.1    ,Deep Noise Tracking Network: a Hybrid Signal Processing/Deep Learning Approach to Speech Enhancement,"Noise statistics and speech spectrum characteristics are the essential information for the single channel speech enhancement. The signal processing-based methods mainly rely on noise statistics estimation. They perform very well for stationary noise, but have remained difficult to cope with non-stationary noise. While the deep learning-based methods mainly focus on the perception on the spectrum characteristics of speech and have a capacity in dealing with non-stationary noise. However, the performance would degrade dramatically for the unseen noise types, which could be due to the over-reliance on data and the ignorance to domain knowledge of signal process. Obviously, the hybrid signal processing/deep learning scheme may be a smart alternative. In this paper, we incorporate the powerful perceptual capabilities of deep learning in the conventional speech enhancement framework. Deep learning is used to estimate the speech presence probability and the update factor of noise statistics, which are then integrated into the Wiener filter-based speech enhancement structure to enhance the desired speech. All components are jointly optimized by a spectrum approximation objective. Systematic experiments on CHiME-4 and NOISEX-92 demonstrate the proposed hybrid signal processing/deep learning approach to noise suppression in noise-unmatched and noise-matched conditions.",0.0,4.625
395,1688,4.5    4.95    3.2    5.9    ,Deeply Fused Speaker Embeddings for Text-Independent Speaker Verification,"Recently there has been a surge of interest is learning speaker embeddings using deep neural networks. These models ingest time-frequency representations of speech and can be trained to discriminate between a known set speakers. While embeddings learned in this way perform well, they typically require a large number of training data points for learning. In this work we propose deeply fused speaker embeddings - speaker representations that combine neural speaker embeddings with i-vectors. We show that by combining the two speaker representations we are able to learn robust speaker embeddings in a computationally efficient manner.
We compare several different fusion strategies and find that the resulting speaker embeddings show significantly different veri- fication performance. To this end we propose a novel fusion approach that uses an attention model to combine i-vectors with neural speaker embeddings. Our best performing embedding achieves an error rate of 3.17% using a simple cosine distance classifier. Combining our embeddings with a powerful Joint Bayesian classifier, we are able to further improve the performance of our speaker embeddings to 2.22%, which gave a 7.8% relative improvement over the baseline i-vector system.",0.0,4.637499999999999
673,2394,3.5    5.7    5    4.35    ,Investigation on Bandwidth Extension for Speaker Recognition,"In this work, we investigate training speaker recognition systems on wideband (WB) features and compare their performance with narrowband (NB) baselines. NIST speaker recognition evaluations have mainly driven speaker recognition research in the past years. Because of the target application of these evaluations, most data available to train speaker recognition systems is NB telephone speech. Meanwhile, WB data have been more scarce not being enough to train factor 
analysis and PLDA models. Thus, the usual practice when dealing with WB speech consists in downsampling the signal to 8 kHz, which implies potential loss of useful information. Instead, we experimented upsampling the training telephone data and leaving the WB data unchanged. We adopt two techniques to upsample telephone data: (1) using a feed-forward neural network, termed Bandwidth Extension (BWE) network, to predict WB features given NB features as input; and (2) using basic upsampling with a low-pass filter interpolator. While the former intends to estimate the high frequency information, the latter does not. The upsampled features are used to train state-of-the art i-vector and recently proposed x-vector models. We evaluated the systems on Speakers In The Wild (SITW) database obtaining 11.5% relative improvement in detection cost function (DCF) with x-vector model.",0.0,4.637499999999999
734,2533,5.1    4.2    4.15    5.1    ,Analysing the Focus of a Hierarchical Attention Network: the Importance of Enjambments When Classifying Post-modern Poetry,"After overcoming the traditional metrics, modern and postmodern
poetry developed a large variety of ‘free verse prosodies’
that falls along a spectrum from a more fluent to a more disfluent
and choppy style. We present a method to
analyze and classify this ‘free verse spectrum’ into six classes
of poetic styles as well as to detect three types of poems with
enjambments. We use a model for automatic prosodic analysis
of spoken free verse poetry which uses deep hierarchical attention
networks to integrate the source text and audio and predict
the assigned class. We then analyze and fine-tune the model
with a particular focus on enjambments and in two ways: we
drill down on classification performance by analyzing whether
the model focuses on similar traits of poems as humans would,
in particular, whether it internally builds a notion of enjambment.
We find that our model is similarly good as humans in
finding enjambments; however, when we employ the model for
classifying enjambment-dominated poem types, it does not pay
particular attention to those lines. Adding enjambment labels to
the training only marginally improves performance, indicating
that all other lines are similarly informative for the model.",0.0,4.6375
718,2496,4.95    5.1    4.25    4.25    ,Automatic Early Detection of Amyotrophic Lateral Sclerosis from Intelligible Speech Using Convolutional Neural Networks,"Amyotrophic lateral sclerosis (ALS) is a rapidly progressive neurodegenerative disease of the motor system that leads to the impairment of speech and swallowing functions. The lack of a biomarker typically causes a diagnostic delay.  To advance the current diagnostic process, we explored the feasibility of automatic detection of patients with ALS at an early stage from highly intelligible speech. A speech dataset was collected from thirteen newly diagnosed patients with ALS and thirteen age- and gender-matched healthy controls. Convolutional Neural Networks (CNNs), including time-domain CNN and frequency-domain CNN,  were used to classify the intelligible speech produced by patients with ALS and those by healthy individuals. Experimental results indicated both time- and frequency-CNN outperformed standard neural network. The best sample-level sensitivity and specificity were obtained by time-CNN (71.6% and 80.9%, respectively). When multiple samples were used to vote to estimate a person-level performance, the best result was obtained by frequency-CNN (76.9% sensitivity and 92.3% specificity). Results demonstrated the possibility of early detection of ALS from intelligible speech signals.",0.0,4.6375
739,2561,4.8    5    5    3.75    ,Improving Mandarin Tone Recognition Using Convolutional Bidirectional Long Short-Term Memory with Attention,"Automatic tone recognition is useful for Mandarin spoken language processing. However, the complex F0 variations from the tone co-articulations and the interplay effects among tonality make it rather difficult to perform tone recognition of Chinese continuous speech. This paper explored the application of Bidirectional Long Short-Term Memory (BLSTM), which had the capability of modeling time series, to Mandarin tone recognition to handle the tone variations in continuous speech. In addition, we introduced attention mechanism to guide the model to select the suitable context information. The experimental results showed that the performance of proposed CNN-BLSTM with attention mechanism was the best and it achieved the tone error rate (TER) of 9.30% with a 17.6% relative error reduction from the DNN baseline system with TER of 11.28%. It demonstrated that our proposed model was more effective to handle the complex F0 variations than other models.",0.0,4.6375
446,1798,4.9    4.35    4.1    5.2    ,Characterizing Rhythm Differences between Strong and Weak Accented L2 Speech,"This study examined the rhythmic characteristics of accented L2 speech by using two relatively novel measures of prosodic rhythm: The S-AMPH measure, an index of the degree of synchrony between the stress and syllable amplitude modulation rates; and the Allan Factor measure, that determines the nested clustering of temporal events (in this case peaks in the amplitude envelope) over different timescales. An extreme-group design was used to select strong versus weak foreign accent recordings from a group of Korean and French L2 English talkers saying the same 69-word English passage. For the Korean talkers, both the S-AMPH and the Allan Factor measures differed as a function of the strength of foreign accent. This was not the case for the French talkers, where neither measure differed as a function of the strength of the foreign accent. The difference in outcome between the Korean and French talkers suggests that the measures may not be indexing a general property of L2 accent (e.g., production fluency) but rather that they may be picking up a property specific to the strongly accented Korean talkers.",0.0,4.6375
169,1244,4.3    5.2    4.35    4.7    ,Learning to Adapt: a Meta-learning Approach for Speaker Adaptation,"The performance of automatic speech recognition systems can be improved by adapting an acoustic model to compensate for the mismatch between training and testing conditions, for example by adapting to unseen speakers. The success of speaker adaptation methods relies on selecting weights that are suitable for adaptation and using good adaptation schedules to update these weights in order not to overfit to the adaptation data. In this paper we investigate a principled way of adapting all the weights of the acoustic model using a meta-learning. We show that the meta-learner can learn to perform supervised and unsupervised speaker adaptation and that it outperforms a strong baseline adapting LHUC parameters when adapting a DNN AM with 1.5M parameters. We also report initial experiments on adapting TDNN AMs, where the meta-learner achieves comparable performance with LHUC.",0.0,4.6375
253,1391,5.9    4.45    4.35    3.85    ,Ladder Networks for Emotion Recognition: Using Unsupervised Auxiliary Tasks to Improve Predictions of Emotional Attributes,"Recognizing emotions using few attribute dimensions such as arousal, valence and dominance provides the flexibility to effectively represent complex range of emotional behaviors. Conventional methods to learn these emotional descriptors primarily focus on separate models to recognize each of these attributes. Recent work has shown that learning these attributes together regularizes the models, leading to better feature representations. This study explores new forms of regularization by adding unsupervised auxiliary tasks to reconstruct hidden layer representations. This auxiliary task requires the denoising of hidden representations at every layer of an auto-encoder. The framework relies on ladder networks that utilize skip connections between encoder and decoder layers to learn powerful representations of emotional dimensions. The results show that ladder networks improve the performance of the system compared to baselines that individually learn each attribute, and conventional denoising autoencoders. Furthermore, the unsupervised auxiliary tasks have promising potential to be used in a semi-supervised setting, where few labeled sentences are available.",0.0,4.6375
663,2366,5.85    5.2    3.45    4.05    ,Acoustic-prosodic Entrainment in Structural Metadata Events,"This paper presents an acoustic-prosodic analysis of entrainment in a Portuguese map-task corpus. Our aim is to analyze how turn-by-turn entrainment varies with distinct structural metadata events: types of sentence-like units (SU) in consecutive turns (e.g. interrogatives followed by declaratives, or both declaratives), and with the presence of discourse markers, affirmative cue words, and disfluencies in the beginning of turns. Entrainment at turn-exchanges may be observed in terms of pitch, energy, duration, and voice quality. Regarding SU types, question-answer turns are the ones with stronger similarity, and declarative-interrogative pairs are the ones where less entrainment occurs, as expected. Moreover, in question-answer pairs, there is also stronger evidence of entrainment with Yes/No and Tag questions than with Wh- questions. In fact, these subtypes are coded in distinctive prosodic ways (moreover, the first subtype has no associated lexical-syntactic cues in Portuguese, only prosodic). As for turn-initial structures, entrainment is stronger when the second turn begins with an affirmative cue word; less strong with ambiguous structures (such as ‘OK’), emphatic affirmative answers, and negative answers; and scarce with disfluencies and discourse markers. The different degrees of local entrainment may be related with the informative structure of distinct structural metadata events.",0.0,4.6375
77,1076,5.1    4.85    4.35    4.3    ,"General Utterance-Level Feature Extraction for Classifying Crying Sounds, Atypical & Self-Assessed Affect and Heart Beats","In the area of computational paralinguistics, there is a growing
need for general techniques that can be applied in a variety of
tasks, and which can be easily realized using standard and publicly
available tools. In our contribution to the 2018 Interspeech
Computational Paralinguistic Challenge (ComParE), we test four
general ways of extracting features. Besides the standard ComParE
feature set consisting of 6373 diverse attributes, we experiment
with two variations of Bag-of-Audio-Words representations, and
define a simple feature set inspired by Gaussian Mixture Models. Our
results indicate that the UAR scores obtained via the different
approaches vary among the tasks. In our view, this is mainly
because most feature sets tested were local by nature, and
they could not properly represent the utterances of the Atypical
Affect and Self-Assessed Affect Sub-Challenges. On the Crying
Sub-Challenge, however, a simple combination of all four feature
sets proved to be effective.",0.0,4.6499999999999995
620,2250,4.95    4.35    5.9    3.4    ,Dysarthric Speech Recognition Using Convolutional LSTM Neural Network,"Dysarthria is a motor speech disorder that impedes the physical production of speech. Speech in patients with dysarthria is generally characterized by poor articulation, breathy voice, and monotonic intonation. Therefore, modeling the spectral and temporal characteristics of dysarthric speech is critical for better performance in dysarthric speech recognition. Convolutional long short-term memory recurrent neural networks (CLSTM-RNNs) have recently successfully been used in normal speech recognition, but have rarely been used in dysarthric speech recognition. We hypothesized CLSTM-RNNs have the potential to capture the distinct characteristics of dysarthric speech, taking advantage of convolutional neural networks (CNNs) for extracting effective local features and LSTM-RNNs for modeling temporal dependencies of the features. In this paper, we investigate the use of CLSTM-RNNs for dysarthric speech recognition.  Experimental evaluation on a database collected from nine dysarthric patients showed that our approach provides substantial improvement over both standard CNN and LSTM-RNN based speech recognizers.",0.0,4.65
379,1654,5.15    4.45    4.85    4.15    ,Combined Speaker Clustering and Role Recognition in Conversational Speech,"Speaker Role Recognition (SRR) is usually addressed either as an independent classification task, or as a subsequent step after a speaker clustering module. However, the first approach does not take speaker-specific variabilities into account, while the second one results in error propagation. In this work we propose the integration of an audio-based speaker clustering algorithm with a language-aided role recognizer into a meta-classifier which takes both modalities into account. That way, we can treat separately any speaker-specific and role-specific characteristics before combining the relevant information together. The method is evaluated on two corpora of different conditions with interactions between a clinician and a patient and it is shown that it yields superior results for the SRR task.",0.0,4.65
361,1612,3.45    5.8    4.35    5    ,Correlational Networks for Speaker Normalization in Automatic Speech Recognition,"In this paper, we propose using common representation learning(CRL) for speaker normalization in automatic speech recognition (ASR). Conventional methods like feature space maximum likelihood linear regression (fMLLR) require two pass decode and their performance is often limited by the amount of data during test. While i-vectors do not require two-pass decode, a significant number of input frames are required for estimation. Hence, as an alternative, a regression model employing correlational neural networks (CorrNet) for multi-view CRL is proposed. In this approach, the CorrNet training methodology treats normalized and un-normalized features as two parallel views of the same speech data. Once trained, this network generates frame-wise fMLLR-like features, thus overcoming the limitations of fMLLR/i-vectors. The recognition accuracy using the proposed CorrNet-generated features is comparable with the i-vector model counterparts and significantly better than the un-normalized features like filterbank. With CorrNet-features, we get an absolute improvement in word error rate of 2.5% for TIMIT, 2.69% for WSJ84 and 3.2% for Switchboard-33hour over un-normalized features.",0.0,4.65
443,1791,5.05    4.95    5.15    3.45    ,Multimodal Speech Synthesis Architecture for Unsupervised Speaker Adaptation,"This paper proposes a new architecture for speaker adaptation of multi-speaker neural-network speech synthesis systems in which an unseen speaker’s voice can be synthesized using a relatively small amount of speech data without transcriptions for adaptation.  This is sometimes called “unsupervised speaker adaptation”. More specifically, we concatenate the layers to the audio inputs when performing unsupervised speaker adaptation while we concatenate them to the text inputs when synthesizing speech from a text. Two new training schemes for this new architecture are also proposed in this paper. These training schemes are not limited to speech synthesis; other applications are suggested. Experimental results show that the proposed model not only enables adaptation to unseen speakers using untranscribed speech but it also improves the performance of multi-speaker modeling and speaker adaptation using transcribed audio files.",0.0,4.65
657,2358,5.1    4.65    4.85    4    ,Robust Spoken Language Understanding via Paraphrasing,"Learning intents and slot labels from user utterances is a fundamental step in all spoken language understanding (SLU) and dialog systems. State-of-the-art neural network based methods, after deployment, often suffer from performance degradation on encountering paraphrased utterances, and out-of-vocabulary words, rarely observed in their training set. We address this challenging problem by introducing a novel paraphrasing based SLU model which can be integrated with any existing SLU model in order to improve their overall performance. We propose two new paraphrase generators using RNN and sequence-to-sequence based neural networks, which are suitable for our application. Our experiments on existing benchmark and in house datasets demonstrate the robustness of our models to rare and complex paraphrased utterances, even under adversarial test distributions.",0.0,4.65
346,1580,5.1    4.25    4.25    5    ,Fast ASR-free and Almost Zero-resource Keyword Spotting Using DTW and CNNs for Humanitarian Monitoring,"We use dynamic time warping (DTW) as supervision for training a convolutional neural network (CNN) based keyword spotting system using a small set of spoken isolated keywords. The aim is to allow rapid deployment of a keyword spotting system in a new language to support urgent United Nations (UN) relief programmes in parts of Africa where languages are extremely under-resourced and the development of annotated speech resources is infeasible.
First, we use 1920 recorded keywords (40 keyword types, 34 minutes of speech) as exemplars in a DTW-based template matching system and apply it to untranscribed broadcast speech.
Then, we use the resulting DTW scores as targets to train a CNN on the same unlabelled speech. In this way we use just 34 minutes of labelled speech, but leverage a large amount of unlabelled data for training. While the resulting CNN keyword spotter cannot match the performance of the DTW-based system, it substantially outperforms a CNN classifier trained only on the keywords, improving the area under the ROC curve from 0.54 to 0.64. Because our CNN system is several orders of magnitude faster at runtime than the DTW system, it represents the most viable keyword spotter on this extremely limited dataset.",0.0,4.65
93,1102,4.05    5.15    5.25    4.15    ,Analysis of Complementary Information Sources in the Speaker Embeddings Framework,"Deep neural network (DNN)-based speaker embeddings have resulted in new, state-of-the-art text-independent speaker recognition technology. However, very limited effort has been made to understand DNN speaker embeddings. In this study, our aim is analyzing the behavior of the speaker recognition systems based on speaker embeddings toward different front-end features, including the standard Mel frequency cepstral coefficients (MFCC), as well as power normalized cepstral coefficients (PNCC), and perceptual linear prediction (PLP). Using a speaker recognition system based on DNN speaker embeddings and probabilistic linear discriminant analysis (PLDA), we compared different approaches to leveraging complementary information using score-, embeddings-, and feature-level combination. We report our results for Speakers in the Wild (SITW) and NIST SRE 2016 datasets. We found that first and second embeddings layers are complementary in nature. By applying score and embedding-level fusion we demonstrate relative improvements in equal error rate of 17% on NIST SRE 2016 and 10% on SITW over the baseline system.",0.0,4.65
612,2221,3.35    4.25    5.9    5.15    ,Robust Speaker Recognition from Distant Speech under Real Reverberant Environments Using Speaker Embeddings,"This article focuses on speaker recognition using speech acquired using a single distant or far-field microphone in an indoors environment. This study differs from the majority of speaker recognition research, which focuses on speech acquisition over short distances, such as when using a telephone handset or mobile device or far-field microphone arrays, for which beamforming can enhance distant speech signals. We use two large-scale corpora collected by retransmitting speech data in reverberant environments with multiple microphones placed at different distances. We first characterize three different speaker recognition systems ranging from a traditional universal background model (UBM) i-vector system to a state-of-the-art deep neural network (DNN) speaker embedding system with a probabilistic linear discriminant analysis (PLDA) back-end. We then assess the impact of microphone distance and placement, background noise, and loudspeaker orientation on the performance of speaker recognition system for distant speech data. We observe that the recently introduced DNN speaker embed- ding based systems are far more robust compared to i-vector based systems, providing a significant relative improvement of up to 54% over the baseline UBM i-vector system, and 45.5% over prior DNN-based speaker recognition technology.",0.0,4.6625
140,1190,3.7    5    4.05    5.9    ,WaveNet Vocoder with Limited Training Data for Voice Conversion,"This paper investigates the approaches of building WaveNet vocoders with limited training data for voice conversion (VC). Current VC systems using statistical acoustic models always suffer from the quality degradation of converted speech. One of the major causes is the use of hand-crafted vocoders for waveform generation. Recently, with the emergence of WaveNet for waveform modeling, speaker-dependent WaveNet vocoders have been proposed and they can reconstruct speech with better quality than conventional vocoders, such as STRAIGHT. Because training a WaveNet vocoder in the speaker-dependent way requires a relatively large training dataset, it remains a challenge to build a high-quality WaveNet vocoder for VC tasks when the training data of target speakers is limited. In this paper, we propose to build WaveNet vocoders by combining the initialization using a multi-speaker corpus and the adaptation using a small amount of target data, and evaluate this proposed method on the Voice Conversion Challenge (VCC) 2018 dataset which contains approximately 5 minute recordings for each target speaker. Experimental results show that the WaveNet vocoders built using our proposed method outperform conventional STRAIGHT vocoder. Furthermore, our system achieves an average naturalness MOS of 4.13 in VCC 2018, which is the highest among all submitted systems.",0.0,4.6625
234,1353,4.3    3.45    5.9    5    ,Emotion Identification from Raw Speech Signals Using DNNs,"We investigate a number of Deep Neural Network (DNN) architectures for emotion identification with the IEMOCAP database. First we compare different feature extraction front-ends: we compare high-dimensional MFCC input (equivalent to filterbanks), versus frequency-domain and time-domain approaches to learning filters as part of the network. We obtain the best results with the time-domain filter-learning approach. Next we investigated different ways to aggregate information over the duration of an utterance. We tried approaches with a single label per utterance with time aggregation inside the network; and
approaches where the label is repeated for each frame. Having a separate label per frame seemed to work best, and the best architecture that we tried interleaves TDNN-LSTM with
time-restricted self-attention, achieving a weighted accuracy of 70.6%, versus 61.8% for the best previously published system which used 257-dimensional Fourier log-energies as input.",0.0,4.6625
489,1896,4.35    4.2    5.85    4.25    ,Information Encoding by Deep Neural Networks: What Can We Learn?,"The recent advent of deep learning techniques in speech technology
and in particular in automatic speech recognition has
yielded substantial performance improvements. This suggests
that deep neural networks (DNNs) are able to capture structure
in speech data that older methods for acoustic modeling, such
as Gaussian Mixture Models and shallow neural networks fail
to uncover. In image recognition it is possible to link representations
on the first couple of layers in DNNs to structural
properties of images, and to representations on early layers in
the visual cortex. This raises the question whether it is possible
to accomplish a similar feat with representations on DNN
layers when processing speech input. In this paper we present
three different experiments in which we attempt to untangle
how DNNs encode speech signals, and to relate these representations
to phonetic knowledge, with the aim to advance conventional
phonetic concepts and to choose the topology of a
DNNs more efficiently. Two experiments investigate representations
formed by auto-encoders. A third experiment investigates
representations on convolutional layers that treat speech
spectrograms as if they were images. The results lay the basis
for future experiments with recursive networks.",0.0,4.6625
263,1407,4.9    3.8    5.1    4.85    ,Twin Regularization for Online Speech Recognition,"Online speech recognition is crucial for developing natural human-machine interfaces. This modality, however, is significantly more challenging than off-line ASR, since real-time/low-latency constraints inevitably hinder the use of future information, that is known to be very helpful to perform robust predictions. 

A popular solution to mitigate this issue consists of feeding neural acoustic models with context windows that gather some future frames. This introduces a latency which depends on the number of employed look-ahead features. 

This paper explores a different approach, based on estimating the future rather than waiting for it. Our technique  encourages the hidden representations of a unidirectional recurrent network to embed some useful information about the future. Inspired by a recently proposed technique called Twin Networks, we add a regularization term that forces forward hidden states to be as close as possible to cotemporal backward ones, computed by a ""twin"" neural network running backwards in time. 

The experiments, conducted on a number of datasets, recurrent architectures, input features, and acoustic conditions, have shown the effectiveness of this approach. One important advantage is that our method does not introduce any additional computation at test time if compared to standard unidirectional recurrent networks.",0.0,4.6625
45,1019,5.75    4.45    4.95    3.5    ,Annotator Trustability-based Cooperative Learning Solutions for Intelligent Audio Analysis,"A broad range of artificially intelligent applications are nowadays available resulting in a need for masses of labelled data for the underlying machine learning models. This annotated data, however, is scarce and expensive to obtain from expert-like annotators. Crowdsourcing has been shown as a viable alternative, but it has to be carried out with adequate quality control to obtain reliable labels. Whilst crowdsourcing allows for the rapid collection of large-scale annotations, another technique called Cooperative Learning, aims at reducing the overall annotation costs, by learning to select only the most important instances for manual annotation. In this regard, we investigate the advantages of this approach and combine crowdsourcing with different iterative cooperative learning paradigms for audio data annotation, incorporating an annotator trustability score to reduce the labelling effort needed and, at the same time, to achieve better classification results. Key experimental results on an emotion recognition task show a considerable relative annotation reduction compared to a ‘non-intelligent’ approach of up to 85.3%. Moreover, the proposed trustability-based methods reach an unweighted average recall of 74.8%, while the baseline approach peaks at 61.2%. Therefore, the proposed trustability-based approaches efficiently reduce the manual annotation load, as well as improving the model.",0.0,4.6625
181,1262,3.15    5.8    4.95    4.75    ,Building State-of-the-art Distant Speech Recognition Using the CHiME-4 Challenge with a Setup of Speech Enhancement Baseline,"This paper describes a new baseline system for automatic speech recognition (ASR) in the CHiME-4 challenge to promote the development of noisy ASR in speech processing communities by providing 1) state-of-the-art system with a simplified single system comparable to the complicated top systems in the challenge, 2) publicly available and reproducible recipe through the main repository in the Kaldi speech recognition toolkit. The proposed system adopts generalized eigenvalue beamforming with bidirectional long short-term memory (LSTM) mask estimation. We also propose to use a time delay neural network (TDNN) based on the lattice-free version of the maximum mutual information (LF-MMI) trained with augmented all six microphones plus the enhanced data after beamforming. Finally, we use a LSTM language model for lattice and n-best re-scoring.  The final system achieved 2.74% WER for the real test set in the 6-channel track, which corresponds to the 2nd place in the challenge. In addition, the proposed baseline recipe includes four different speech enhancement measures, short-time objective intelligibility measure (STOI), extended STOI (eSTOI), perceptual evaluation of speech quality (PESQ) and speech distortion ratio (SDR) for the simulation test set. 
Thus, the recipe also provides an experimental platform for speech enhancement studies with these performance measures.",0.0,4.6625
606,2204,5.7    5.05    4.25    3.65    ,Efficient Voice Trigger Detection for Low Resource Hardware,"We describe the architecture of an always-on keyword spotting (KWS) system for battery-powered mobile devices used to initiate an interaction with the device. An always-available voice assistant needs a carefully designed voice keyword detector to satisfy the power and computational constraints of battery powered devices. We employ a multi-stage system that uses a low-power primary stage to decide when to run a more accurate (but more power-hungry) secondary detector. We describe a straightforward primary detector and explore variations that result in very useful reductions in computation (or increased accuracy for the same computation). By reducing the set of target labels from three to one per phone, and reducing the rate at which the acoustic model is operated, the compute rate can be reduced by a factor of six while maintaining the same accuracy.",0.0,4.6625
239,1370,5    5.05    4.05    4.55    ,Filter Sampling and Combination CNN (FSC-CNN): a Compact CNN Model for Small-footprint ASR Acoustic Modeling Using Raw Waveforms,"Learning an ASR acoustic model directly from raw waveforms using CNNs has proved to be effective, where convolutional layers with learnable filters are able to automatically extract useful features. However, these filters, with independent parameters, can be highly redundant resulting in inefficient systems. In this paper, we propose a novel method to generate CNN filter parameters by first sampling from a low-dimensional parameter space and then using a trainable scalar vector to perform a linear combination. This filter sampling and combination method (denoted as FSC) not only naturally enforces parameter sharing in the low-dimensional sampling space, but also adds to the learning capacity of filters. The FSC-CNN model has a significantly smaller number of parameters and is more efficient compared to conventional CNN models, which makes it feasible for small-footprint ASR. Experimental results on the WSJ LVCSR task show that FSC-CNNs are able to achieve a WER of 3.67 with a standard decoder set-up with only 1.19M nonlinear-layer parameters (better than a strong baseline CNN model with 3.2x more parameters). It also outperforms a CNN model with a similar number of parameters by a relative improvement of 10.26%.",0.0,4.6625000000000005
153,1223,4.15    3.3    5.15    4.95    5.35    5.1    ,A New Framework for Supervised Speech Enhancement in the Time Domain,"This work proposes a new learning framework that uses a loss function in the frequency domain to train a convolutional neural network (CNN) in the time domain. At the training time, an extra operation is added after the speech enhancement network to convert the estimated signal in the time domain to the frequency domain. This operation is differentiable and is used to train the system with a loss in the frequency domain. This
proposed approach replaces learning in the frequency domain, i.e., short-time Fourier transform (STFT) magnitude estimation, with learning in the original time domain. The proposed method is a spectral mapping approach in which the CNN first generates
a time domain signal then computes its STFT that is used for spectral mapping. This way the CNN can exploit the additional domain knowledge about calculating the STFT magnitude from
the time domain signal. Experimental results demonstrate that the proposed method substantially outperforms the other methods of speech enhancement. The proposed approach is easy to implement and applicable to related speech processing tasks that require spectral mapping or time-frequency (T-F) masking.",0.0,4.666666666666667
133,1173,4.1    4.9    5    ,Improved Epoch Extraction from Telephonic Speech Using Chebfun and Zero Frequency Filtering,"Epoch in speech, represent the instant where maximum excitation at the vocal tract is obtained. Existing epoch extraction algorithms are capable of accurately extracting epoch information from clean speech signals. However, epoch extraction of band limited signals such as telephonic speech is challenging due to the attenuation of the fundamental frequency components. The present work is focused on improving the performance of epoch extraction from telephonic speech signals by exploiting the properties of Chebyshev polynomial interpolation and by reinforcing the frequency components around the fundamental frequency through the Hilbert envelope (HE). The proposed algorithm brings a refinement of the existing Zero Frequency Filtering (ZFF) method by incorporating Chebyshev interpolation. The proposed refinements to the ZFF algorithm confirmed to provide improved  epoch identification rate, identification accuracy, reduced miss rate and false alarm rate. The epoch identification rate of the proposed method is observed to be better than existing methods like Dynamic Programming Phase Slope Algorithm (DYPSA), Speech Event Detection using the Residual Excitation And a Mean-based Signal (SEDREAMS), Dynamic Plosion Index (DPI) and Single Pole Filtering (SPF) methods for telephonic speech quality.",0.0,4.666666666666667
600,2186,5.25    4.9    5.1    3.45    ,Patient Privacy in Paralinguistic Tasks,"Recent developments in cryptography and, in particular in Fully Homomorphic Encryption (FHE), have allowed for the development of new privacy preserving machine learning schemes. 
    In this paper, we show how these schemes can be applied to the automatic assessment of speech affected by medical conditions, allowing for patient privacy in diagnosis and monitoring scenarios. More specifically, we present results for the assessment of the degree of Parkinson’s Disease, the detection of a Cold, and both the detection and assessment of the degree of Depression.
    To this end, we use a neural network in which all operations are performed in an FHE context. This implies replacing the activation functions by linear and second degree polynomials, as only additions and multiplications are viable. Furthermore, to guarantee that the inputs of these activation functions fall within the convergence interval of the approximation, a batch normalization layer is introduced before each activation function. After training the network with unencrypted data, the resulting model is then employed in an encrypted version of the network, to produce encrypted predictions.
    Our tests show that the use of this framework yields results with little to no performance degradation, in comparison to the baselines produced for the same datasets.",0.0,4.675
238,1369,3    4.85    5.85    5    ,Training Recurrent Neural Network through Moment Matching for NLP Applications,Recurrent neural network (RNN) is conventionally trained in the supervised mode but used in the free-running mode for inferences on testing samples.  The supervised mode takes ground truth token values as RNN inputs but the free-running mode can only use self-predicted token values as surrogating inputs. Such inconsistency inevitably results in poor generalizations of RNN on out-of-sample data. We propose a moment matching (MM) training strategy to alleviate such inconsistency by simultaneously taking these two distinct modes and their corresponding dynamics into consideration. Our MM-RNN shows significant  performance improvements over existing approaches when tested on practical NLP applications including logic form generation and image captioning.,0.0,4.675
20,67,5    4.3    4.35    5.05    ,Articulatory Features for ASR of Pathological Speech,"In this work, we investigate the joint use of articulatory and acoustic features for automatic speech recognition (ASR) of pathological speech. Despite long-lasting efforts to build speaker- and text-independent ASR systems for people with dysarthria, the performance of state-of-the-art systems is still considerably lower on this type of speech than on normal speech. The most prominent reason for the inferior performance is the high variability in pathological speech that is characterized by the spectrotemporal deviations caused by articulatory impairments due to various etiologies. To cope with this high variation, we propose to use speech representations which utilize articulatory information together with the acoustic properties. A designated acoustic model, namely a fused-feature-map convolutional neural network (fCNN), which performs frequency convolution on acoustic features and time convolution on articulatory features is trained and tested on a Dutch and a Flemish pathological speech corpus. The ASR performance of fCNN-based ASR system using joint features is compared to other neural network architectures such conventional CNNs and time-frequency convolutional networks (TFCNNs) in several training scenarios.",0.0,4.675
147,1209,4.3    5.05    4.1    5.25    ,Triplet Loss Based Cosine Similarity Metric Learning for Text-independent Speaker Recognition,"Deep neural network based speaker embeddings become increasingly popular in the text-independent speaker recognition task. In contrast to a generatively trained i-vector extractor, a DNN speaker embedding extractor is usually trained discriminatively in the closed set classification scenario using softmax. The problem we addressed in the paper is choosing a dnn based speaker embedding backend solution for speaker verification scoring. There are several options to perform speaker verification in the dnn embedding space. One of them is using a simple heuristic speaker similarity metric for the scoring (e.g. cosine metric).  Similarly in the i-vector based systems, the standard Linear Discriminant Analisys (LDA) followed by the Probabilistic Linear Discriminant Analisys (PLDA) can be used for segregating speaker information. As an alternative, the discriminative metric learning approach can be considered. This work demonstrates that performance of deep speaker embeddings based systems can be improved by using Cosine Similarity Metric Learning (CSML) with the triplet loss training scheme. Results obtained on Speakers in the Wild and NIST SRE 2016 evaluation sets demonstrate superiority and robustness of CSML based systems.",0.0,4.675
664,2371,4.35    4.8    4.55    5    ,Estimation of the Asymmetry Parameter of the Glottal Flow Waveform Using the Electroglottographic Signal,"Glottal activity information can be very important in several speech processing applications, such as in speech therapy, voice disorder diagnosis, voice transformation and text-to-speech synthesis. However, the use of algorithms for estimating glottal parameters from the speech signal is very limited in those applications because of problems with robustness and accuracy. For this reason, current research studies of the glottal source are usually constrained to isolated speech sounds or short segments of speech recorded in controlled conditions and methods requiring manual intervention. An alternative way to obtain more accurate and reliable glottal parameter estimates is to use other recording equipment besides the audio microphone. Electroglottography is the most popular non-invasive measurement of vocal fold motion. It has been widely used to estimate the glottal opening and closing instants, but it does not provide direct information about the other important glottal parameters. This paper proposes an automatic method for estimation of the glottal parameters from the electroglottographic signal that permits to measure an additional parameter related to the asymmetry of the glottal flow pulse. This is a very important characteristic correlated with voice quality and widely studied in voice source analysis, commonly represented by the speed quotient parameter.",0.0,4.675
290,1462,4.9    4.7    4.85    4.25    ,Improving Gender Identification in Movie Audio Using Cross-Domain Data,"Gender identification from audio is an important task for quantitative gender analysis in multimedia, and to improve tasks like speech recognition. Robust gender identification requires speech segmentation that relies on accurate voice activity detection (VAD). These tasks are challenging in movie audio due to diverse and often noisy acoustic conditions. In this work, we acquire VAD labels for movie audio by aligning it with subtitle text, and train a recurrent neural network model for VAD. Subsequently, we apply transfer learning to predict gender using feature embeddings obtained from a model pre-trained for large-scale audio classification. In order to account for the diverse acoustic conditions in movie audio, we use audio clips from YouTube labeled for gender. We compare the performance of our proposed method with baseline experiments that were setup to assess the importance of feature embeddings and training data used for gender identification task.     For systematic evaluation, we extend an existing benchmark dataset for movie VAD, to include precise gender labels. The VAD system shows comparable results to state-of-the-art in movie domain. The proposed gender identification system outperforms existing baselines,  achieving an accuracy of 85\% for movie audio. We have made the data and related code publicly available\footnote{\hyperlink{https://github.com/usc-sail/mica-gender-from-audio}}.",0.0,4.675000000000001
460,1825,5.8    3.2    5.05    ,Measuring the Band Importance Function for Mandarin Chinese with an Bayesian Adaptive Procedure,"A speech intelligibility index (SII) based band importance function (BIF) for Mandarin monosyllabic words spoken by a female speaker was derived with an adaptive procedure in this work. The adaptive procedure, namely the quick-band-importance-function (qBIF) procedure, optimized the stimulus on each trial according listeners’ performance on proceeding trials in an iterative fashion. This method greatly improved the efficiency of data collection. Test-retest experiments were conducted and confirmed the reliability of this adaptive procedure at a group level. The BIF derived in this work showed generally consistence with the BIF derived with the traditional paradigm with noticeable differences at certain frequencies.",0.0,4.683333333333334
198,1293,4.25    4.25    5.05    5.2    ,A Case Study on the Importance of Belief State Representation for Dialogue Policy Management,"A key component of task-oriented dialogue systems is the belief state representation, since it directly affects the policy learning efficiency. In this paper, we propose a novel, binary, compact, yet scalable belief state representation. We compare the standard verbose belief state representation (268 dimensions) with the domain-independent representation (57 dimensions) and the proposed representation (13 or 4 dimensions). To test those representations, the recently introduced Advantage Actor Critic (A2C) algorithm is exploited. The latter has not been tested before for any representation apart from the verbose one. We study the effect of the belief state representation within A2C under 0%, 15%, 30%, and 45% semantic error rate and conclude that the novel binary representation in general outperforms both the domain-independent and the verbose belief state representation. Further, the robustness of the binary representation is tested under more realistic scenarios with mismatched semantic error rates, within the A2C and DQN algorithms. The results indicate that the proposed compact, binary representation performs better or similarly to the other representations, being an efficient and promising alternative to the full belief.",0.0,4.6875
314,1518,4.85    4    5.9    4    ,Language Features for Automated Evaluation of Cognitive Behavior Psychotherapy Sessions,"Cognitive Behavior Therapy (CBT) is a psychotherapy treatment that uses cognitive change strategies to address mental health problems. Quality assessment of a CBT session is traditionally addressed by human raters who evaluate recorded sessions along specific behavioral codes, a cost prohibitive and time consuming method. In this work we examine how linguistic features can be effectively used to develop an automatic competency rating tool for CBT. We explore both standard, widely-used lexical features and domain-specific ones, adapting methods which have been successfully used in similar psychotherapy session coding tasks. Experiments are conducted on manual transcripts of CBT sessions and on automatically derived ones, thus introducing an end-to-end approach. Our results suggest that a real-world system could be developed to automatically evaluate CBT sessions to assist training, supervision, or quality assurance of services.",0.0,4.6875
642,2310,5.05    5.05    5.15    3.5    ,Improving Response Time of Active Speaker Detection Using Visual Prosody Information Prior to Articulation,"Natural multi-party interaction commonly involves turning one's gaze towards the speaker who has the floor. Implementing virtual agents or robots who are able to engage in natural conversations with humans therefore requires enabling machines to exhibit this form of communicative behaviour. This task is called active speaker detection. In this paper, we propose a method for active speaker detection using visual prosody (lip and head movements) information before and after speech articulation to decrease the machine response time; and also demonstrate the discriminating power of visual prosody before and after speech articulation for active speaker detection.  The results show that the visual prosody information one second before articulation is helpful in detecting the active speaker.  Lip movements provide better results than head movements, and fusion of both improves accuracy.  We have also used visual prosody information of the first second of the speech utterance and found that it provides more accurate results than one second before articulation. We conclude that the fusion of lip movements from both regions (the first one second of speech and the one second before articulation) improves the accuracy of active speaker detection.",0.0,4.6875
383,1662,4.7    4.15    4.2    5.75    ,Exemplar-Based Spectral Detail Compensation for Voice Conversion,"Most voice conversion (VC) systems are established under the vocoder-based VC framework. When performing spectral conversion (SC) under this framework, the low-dimensional spectral features, such as mel-ceptral coefficients (MCCs), are often adopted to represent the high-dimensional spectral envelopes. The joint density Gaussian mixture model (GMM)-based SC method with the STRAIGHT vocoder is a well-known representative. Although it is reasonably effective, the loss of spectral details in the converted spectral envelopes inevitably deteriorates speech quality and similarity. To overcome this problem, we propose a novel exemplar-based spectral detail compensation method for VC. In the offline stage, the paired dictionaries of source spectral envelopes and target spectral details are constructed. In the online stage, the locally linear embedding (LLE) algorithm is applied to predict the target spectral details from the source spectral envelopes, and then, the predicted spectral details are used to compensate the converted spectral envelopes obtained by a baseline GMM-based SC method with the STRAIGHT vocoder. Experimental results show that the proposed method can notably improve the baseline system in terms of objective and subjective tests.",0.0,4.7
251,1386,5.1    3.5    5    5.2    ,Automatic Assessment of L2 English Word Prosody Using Weighted Distances of F0 and Intensity Contours,"In the current paper, an automatic prosody assessment method for learners of English using a weighted comparison of fundamental frequency (F0) and intensity contours is proposed. Patterns of F0 and intensity of learners are compared to that of native using a proposed metric -- a weighted distance -- in which the error around the high values of prosodic features have more weight in the computation of the final distance. Gold-standard native references are built using the k-means clustering algorithm. Therefore, we also propose a data-driven criterion called weighted variance based on the weighted similarity within the whole set of native utterances to determine the optimal number of clusters k. In comparison with baseline contour comparison metrics which resulted in a subjective-objective score correlation of 0.278, our method combining the proposed metric and criterion led to a final subjective-objective score correlation of 0.304. In comparison, subjective scores correlated at 0.480.",0.0,4.7
304,1494,5.1    4.7    4.1    4.95    ,Decision-level Feature Switching as a Paradigm for Replay Attack Detection,"A pre-recorded audio sample of an authentic speaker presented to a voice-based biometric system is termed as a replay attack. Such attacks can be detected by identifying the characteristics of the recording device and environment. An analysis of different recording devices indicates that each recording device affects the spectrum differently. It is also observed that each feature captures specific characteristics of recording devices. In
particular, Mel Filterbank Slope (MFS) captures low-frequency information corresponding to that of the low-quality recording devices, while Linear Filterbank Slope (LFS) captures high-frequency information which corresponds to that of a high-quality recording device. The proposed approach uses MFS and LFS along with Mel Frequency Cepstral Coefficients (MFCC) and Constant-Q Cepstral Coefficients (CQCC) in a Decision-level Feature Switching (DLFS) paradigm to determine whether a given utterance is spoofed. The obtained results surpass the state-of-the-art Light Convolutional Neural Network (LCNN) based replay detection system with a relative improvement of 7.43% on the ASV-spoof-2017 evaluation dataset.",0.0,4.7125
450,1805,4.1    4.85    5    4.9    ,Sub-band Envelope Features Using Frequency Domain Linear Prediction for Short Duration Language Identification,"Mismatch between training and testing utterances can significantly degrade the performance of language identification (LID) systems, especially in the case of short duration utterances. This work explores the hypothesis that long-term trends are less affected by this mismatch compared to short-term features. In particular, it proposes the use of features based on temporal envelopes within sub-bands. In this work, the temporal envelopes are obtained using linear prediction in the frequency domain. These envelopes are then transformed into cepstral features. The proposed features are then used as a front-end to a bidirectional long short term memory recurrent neural network to identify languages. Experimental evaluations on the AP17-OLR dataset under different conditions indicate that the proposed features exhibit substantially greater robustness under different noise and mismatch conditions, compared to baseline features. Specifically, the proposed features outperform state-of-the-art bottleneck features and show a relative improvement of 38.4% averaged across the test set.",0.0,4.7125
252,1387,5.1    4.35    4.4    5    ,Exploring the Relationship between Conic Affinity of NMF Dictionaries and Speech Enhancement Metrics,"Nonnegative Matrix Factorization(NMF) has been successfully used in speech enhancement. In the training phase NMF produces speech and noise  dictionaries, whose elements are non-negative, while in the testing phase it estimates a non-negative activation matrix to express the enhanced speech signal as a conic combination of those dictionaries. This nonnegativity property enables us to interpret them as convex polyhedral cones that lie in the positive orthant. Conic affinity could be useful when designing NMF-based systems for unseen noise conditions, which operate by selecting an appropriate noise dictionary amongst a pool of potential candidates. To that end, we examine two conic affinity measures, one based on cosine similarity, while the other is based on euclidean distance from a point to a cone. Moreover, we construct an algorithm to show that conic affinity correlates with speech enhancement performance metrics.",0.0,4.7125
276,1438,4.75    5.1    4.3    ,Auxiliary Feature Based Adaptation of End-to-end ASR Systems,"Acoustic model adaptation has been widely used to adapt models to speakers or environments. For example, appending auxiliary features representing speakers such as i-vectors to the input of a deep neural network (DNN) is an effective way to realize unsupervised adaptation of DNN-hybrid automatic speech recognition (ASR) systems. Recently, end-to-end (E2E) models have been proposed as an alternative to conventional DNN-hybrid ASR systems. E2E models map a speech signal to a sequence of characters or words using a single neural network, which greatly simplifies the ASR pipeline. However, adaptation of E2E models has received little attention yet. In this paper, we investigate auxiliary feature based adaptation for encoder-decoder E2E models. We employ a recently proposed sequence summary network to compute auxiliary features instead of i-vectors, as it can be easily integrated into E2E models and keep the ASR pipeline simple. Indeed, the sequence summary network allows the auxiliary feature extraction module to be a part of the computational graph of the E2E model. We demonstrate that the proposed adaptation scheme consistently improves recognition performance of three publicly available recognition tasks.",0.0,4.716666666666666
552,2053,4.35    5.9    3.9    ,Who Are You Listening to? towards a Dynamic Measure of Auditory Attention to Speech-on-speech.,"When studying speech-on-speech perception, even when participants are explicitly instructed to focus selectively on a single voice, they can spuriously find themselves listening to the wrong voice. These paradigms generally do not allow to infer, retrospectively, which of the speakers was listened to, at different times during presentation.  The present study sought to develop a psychophysical test paradigm, and a set of speech stimuli to that purpose. In this paradigm, after listening to two simultaneous stories, the participant had to identify, among a set of words, those that were present in the target story. Target and masker stories were presented dichotically or diotically. F0 and vocal-tract length were manipulated in order to parametrically vary the distance between the target and masker voices. Consistent with the hypothesis that correct-identification performance for target words depends on selective attention, performance decreases with the distance between the target and masker voices. These results indicate that the paradigm and stimuli described here can be used to infer which voice a participant is listening to in concurrent-speech listening experiments.",0.0,4.716666666666667
65,1049,4.85    4.2    5    4.85    ,Acoustic Modeling with DFSMN-CTC and Joint CTC-CE Learning,"Recently, the connectionist temporal classification (CTC) based acoustic models have achieved comparable or even better performance, with much higher decoding efficiency, than the conventional hybrid systems in LVCSR tasks. For CTC-based models, it usually uses the LSTM-type networks as acoustic models.  However, LSTMs are computationally expensive and sometimes difficult to train with CTC criterion. In this paper, inspired by the recent DFSMN works, we propose to replace the LSTMs with DFSMN in CTC-based acoustic modeling and explore how this type of non- recurrent models behave when trained with CTC loss.  We have evaluated the performance of DFSMN-CTC using both context-independent (CI)  and context-dependent (CD) phones as target labels in many LVCSR tasks with various amount of training data. Experimental results shown that DFSMN-CTC acoustic models using either CI-Phones or CD-Phones can significantly outperform the conventional hybrid models that trained with CD-Phones and cross-entropy (CE) criterion. Moreover, a novel joint CTC and CE training method is proposed, which enables to improve the stability of CTC training and performance. In a 20000 hours Mandarin recognition task, joint CTC-CE trained DFSMN can achieve a 11.0% and 30.1% relative performance improvement compared to DFSMN-CE models in a normal and fast speed test set respectively.",0.0,4.725
605,2196,5    4.35    4.9    4.65    ,Integrating Neural Network Based Beamforming and Weighted Prediction Error Dereverberation,"The WPE algorithm has proven to be a very successful dereverberation method for the REVERB challenge. Likewise, neural network based mask estimation for beamforming demonstrated very good noise suppression in the CHiME 3 and CHiME 4 challenges. Recently, it has been shown that this estimator can also be trained to perform dereverberation and denoising jointly. However, up to now a comparison of a neural beamformer and WPE is still missing, so is an investigation into a combination of the two. Therefore, we here provide an extensive evaluation of both and consequently propose variants to integrate deep neural network based beamforming with WPE. For these integrated variants we identify a consistent WER reduction on two distinct databases. In particular, our study shows that deep learning based beamforming benefits from a model-based dereverberation technique (i.e. WPE) and vice versa. Our key findings are: (a) Neural beamforming yields the lower WER in comparison to WPE the more channels and noise are present. (b) Integration of WPE and a neural beamformer consistently outperforms all stand-alone systems.",0.0,4.725
311,1514,4.15    4.9    4.75    5.1    ,Detection of Dementia from Responses to Atypical Questions Asked by Embodied Conversational Agents,"Detection of dementia requires examinations, such as blood tests and functional magnetic resonance imaging (fMRI), that can be very stressful for the patient. Previous studies proposed screenings for easy detection of dementia that utilized acoustic and language information derived from conversations between patients and medical staff. Although these studies demonstrated effectiveness in automatically detecting dementia, the tasks used were created based on neuropsychological tests. The effect of habituation on this limited variety of tasks might have a negative impact on routine dementia screening. We propose a method to detect dementia using responses to more atypical questions asked by embodied conversational agents. Through consultations with neuropsychologists, we created a total of 13 questions. The embodied conversational agent obtained answers to these questions from 24 participants (12 dementia and 12 non-dementia). We recorded their responses and extracted speech and language features. We classified the two groups (dementia/non-dementia) by a machine learning algorithm (support vector machines and logistic regression) using the extracted features. The results showed a 0.95 detection performance in the area under the curve of the receiver operating characteristic (AUROC). This result demonstrates that our system using atypical questions can detect dementia.",0.0,4.725
161,1234,4.9    5.85    3.45    ,Single-channel Speech Dereverberation via Generative Adversarial Training,"In this paper, we propose a single-channel speech dereverberation system (DeReGAT) based on convolutional, bidirectional long short-term memory and deep feed-forward neural network (CBLDNN) with generative adversarial training (GAT). In order to obtain better speech quality instead of only minimizing a mean square error (MSE), GAT is employed to make the dereverberated speech indistinguishable form the clean samples. Besides, our system can deal with wide range reverberation and be well adapted to variant environments. The experimental results show that the proposed model outperforms weighted prediction error (WPE) and deep neural network-based systems. In addition, DeReGAT is extended to an online speech dereverberation scenario, which reports comparable performance with the offline case.",0.0,4.733333333333333
67,1055,4.9    5.85    4.3    3.9    ,Low-Latency Neural Speech Translation,"Through the development of neural machine translation, the quality of machine translation systems was improved significantly. By exploiting advancements in deep learning, systems are now able to better approximate the complex mapping from source sentences to target sentences.

But with this ability, also new challenges arise. An example is the translation of partial sentences in low-latency speech translation. Since the model has only seen complete sentences in training, it will always try to generate a complete sentence although the input is only a partial sentence. 

We show that NMT systems can be adapted to scenarios where no task-specific training data is available. Furthermore, this is possible without losing its ability to translate the original training data. We achieve this by creating artificial data and by using multi-task learning. Thereby, we are able to reduce the number of corrections displayed during step-wise building of the output by 45 without a decrease of the translation quality.",0.0,4.7375
50,1025,5    5.1    4.05    4.8    ,Compression of End-to-End Models,"End-to-end models, which output text directly given speech using a single neural
network, have been shown to be competitive with conventional speech
recognition models containing separate acoustic, pronunciation, and language model
components. Such models do not require additional resources for decoding and
are typically much smaller than conventional models. This makes them
particularly attractive in the context of on-device speech recognition where
both small memory footprint and low power consumption are critical. This work
explores the problem of compressing end-to-end models with the goal of
satisfying device constraints without sacrificing model accuracy. We evaluate
matrix factorization, knowledge distillation, and parameter sparsity to
determine the most effective methods given constraints such as a fixed parameter
budget.",0.0,4.7375
310,1511,4.75    5.2    4.05    4.95    ,EMPHASIS: an Emotional Phoneme-based Acoustic Model for Speech Synthesis System,"We present EMPHASIS, an emotional phoneme-based acoustic model for speech synthesis system. EMPHASIS includes a phoneme duration prediction model and an acoustic parameter prediction model. It uses a CBHG-based regression network to model the dependencies between linguistic features and acoustic features. We modify the input and output layer structures of the network to improve the performance. For the linguistic features, we apply a feature grouping strategy to enhance emotional and prosodic features. The acoustic parameters are designed to be suitable for the regression task and waveform reconstruction. EMPHASIS can synthesize speech in real-time and generate expressive interrogative and exclamatory speech with high audio quality. EMPHASIS is designed to be a multi-lingual model and can synthesize Mandarin-English speech for now. In the experiment of emotional speech synthesis, it achieves better subjective results than other real-time speech synthesis systems.",0.0,4.7375
503,1929,5.75    4.95    4.9    3.35    ,VoxCeleb2: Deep Speaker Recognition,"The objective of this paper is speaker recognition under noisy and
unconstrained conditions.

We make two key contributions. First, we introduce a very large-scale speaker recognition dataset collected from open-source media. Using a fully automated pipeline, we curate VoxCeleb2
which contains over a million utterances from over 6,000 speakers. This is several times larger than any publicly available speaker recognition dataset.

Second, we develop and compare Convolutional Neural Network (CNN) models and training strategies that can effectively recognise identities from voice under various conditions.
The models trained on the VoxCeleb2 dataset surpass the
performance of previous works on a benchmark dataset by a significant
margin.",0.0,4.7375
232,1351,4.85    4.1    4.1    5.9    ,A Deep Learning Method for Pathological Voice Detection Using Convolutional Deep Belief Networks,"Automatically detecting pathological voice disorders such as vocal cord paralysis or Reinke’s edema is an important medical classification problem. While deep learning techniques have achieved significant progress in the speech recognition field, there has been less research work in the area of pathological voice disorders detection. A novel system for pathological voice detection using Convolutional Neural Network (CNN) as the basic architecture is presented in this work.  The novel system uses spectrograms of normal and pathological speech recordings as the input to the network. Initially Convolutional Deep Belief Network (CDBN) are used to pre-train the weights of CNN system. This acts as a generative model to explore the structure of the input data using statistical methods. Then a CNN is trained using supervised back-propagation learning algorithm to fine tune the weights.  Results show that a small amount of data can be used to achieve good results in classification with this deep learning approach.  A performance analysis of the novel method is provided using real data from the Saarbrucken Voice database.",0.0,4.7375
259,1403,4.85    3.1    5.85    5.15    ,Output-Gate Projected Gated Recurrent Unit for Speech Recognition,"In this paper, we describe the work on accelerating decoding speed while improving the decoding accuracy. Firstly, we propose an architecture which we call Projected Gated Recurrent Unit (PGRU) for automatic speech recognition (ASR) tasks, and show that the PGRU could outperform the standard GRU consistently. Secondly, in order to improve the PGRU's generalization, especially for large-scale ASR task, the Output-gate PGRU (OPGRU) is proposed. Finally, time delay neural network (TDNN) and normalization skills are found  to be beneficial to the proposed projected-based GRU. The finally proposed unidirectional TDNN-OPGRU acoustic model achieves 3.3% / 4.5% relative reduction in word error rate (WER) compared with bidirectional projected LSTM (BLSTMP) on Eval2000 / RT03 test sets. Meanwhile, TDNN-OPGRU acoustic model speeds up the decoding speed by around 2.6 times compared with BLSTMP.",0.0,4.7375
684,2418,5.05    3.75    4.25    5.9    ,Vocal Biomarkers for Cognitive Performance Estimation in a Working Memory Task,"The ability to non-invasively estimate cognitive fatigue and workload as contributing factors to cognitive performance has value for planning and decision making surrounding human participation in cognitively demanding situations and environments.  Growing evidence supports the use of speech as an effective modality for assessing cognitive fatigue and workload, while also being operationally appropriate in a wide variety of environments. To assess ability to discriminate changes in cognitive fatigue and load from speech, features that measure speech onset time, speaking rate, voice quality, and vocal tract coordination from the delta-mel-cepstrum are evaluated on two independent data sets that employ the same auditory working memory task. Feature effect sizes due to fatigue were generally larger than those due to load. Speech onset time, speaking rate, and vocal tract coordination features show strong potential for speech-based fatigue estimation.",0.0,4.737500000000001
280,1447,5    4.2    5.8    4    ,Iterative Learning of Speech Recognition Models for Air Traffic Control,"Automatic Speech Recognition (ASR) has recently proved to be a useful tool to reduce the workload of air traffic controllers leading to significant gains in operational efficiency. Air Traffic Control (ATC) systems in operation rooms around the world generate large amounts of untranscribed speech and radar data each day, which can be utilized to build and improve ASR models. In this paper, we propose an iterative approach that utilizes increasing amounts of untranscribed data to incrementally build the necessary ASR models for an ATC operational area. Our approach uses a semi-supervised learning framework to combine speech and radar data to iteratively update the acoustic model, language model and command prediction model (i.e. prediction of possible commands from radar data for a given air traffic situation) of an ASR system. Starting with seed models built with a limited amount of manually transcribed data, we simulate an operational scenario to adapt and improve the models through semi-supervised learning. Experiments on two independent ATC areas (Vienna and Prague) demonstrate the utility of our proposed methodology that can scale to operational environments with minimal manual effort for learning and adaptation.",0.0,4.75
106,1124,4.05    5.05    5    4.9    ,Improved ASR for Under-resourced Languages through Multi-task Learning with Acoustic Landmarks,"Furui first demonstrated that the identity of both consonant and vowel can be perceived from the C-V transition; later, Stevens proposed that acoustic landmarks are the primary cues for speech perception, and that steady-state regions are secondary or supplemental.  Acoustic landmarks are perceptually salient, even in a language one doesn't speak, and it has been demonstrated that non-speakers of the language can identify features such as the primary articulator of the landmark.  These factors suggest a strategy for developing language-independent automatic speech recognition: landmarks can potentially be learned once from a suitably labeled corpus and rapidly applied to many other languages. This paper proposes enhancing the cross-lingual portability of a neural network by using landmarks as the secondary task in multi-task learning (MTL). The network is trained in a well-resourced source language with both phone and landmark labels (English), then adapted to an under-resourced target language with only word labels (Iban).  Landmark-tasked MTL reduces source-language phone error rate by 2.9% relative, and reduces target-language word error rate by 1.9%-5.9% depending on the amount of target-language training data.  These results suggest that landmark-tasked MTL causes the DNN to learn hidden-node features that are useful for cross-lingual adaptation.",0.0,4.75
738,2559,4.75    4.95    4.3    5    ,Detecting Media Sound Presence in Acoustic Scenes,Using speech to interact with electronic devices and access services is becoming increasingly common. Using such applications in our households poses new challenges for speech and audio processing algorithms as these applications should perform robustly in a number of scenarios. Media devices are very commonly present in such scenarios and can interfere with the user-device communication by contributing to the noise or simply by being mistaken as user issued voice commands. Detecting the presence of media sounds in the environment can help avoid such issues. In this work we propose a method for this task based on a parallel CNN-GRU-FC classifier architecture which relies on multi-channel information to discriminate between media and live sources. Experiments performed using 378 hours of in-house audio recordings collected by volunteers show an F1 score of 71\% with a recall of 72\% in detecting active media sources. The use of information from multiple channels gave a relative improvement of 16\% to the F1 score when compared to using information from only a single channel.,0.0,4.75
56,1035,5.25    3.9    4.95    4.9    ,Contextual Slot Carryover for Disparate Schemas,"In the slot-filling paradigm,  where a user can refer back to slots in the context during the conversation, the goal of the contextual understanding system is to resolve the referring expressions to the appropriate slots in the context. In large-scale multi-domain systems, this presents two challenges - scaling to a very large and potentially unbounded set of slot values, and dealing with diverse schemas. We present a neural network architecture that addresses the slot value scalability challenge by reformulating the contextual interpretation as a decision to  carryover a slot from a set of possible candidates. To deal with heterogenous schemas, we introduce a simple data-driven method for transforming the candidate slots. Our experiments show that our approach can scale to multiple domains and provides competitive results over a strong baseline.",0.0,4.75
717,2495,5.1    4.9    4.1    4.9    ,Estimation of Fundamental Frequency from Singing Voice Using Harmonics of Impulse-like Excitation Source,"This paper focuses on the problem of estimating fundamental frequency from singing voice. Estimation of fundamental frequency is a well studied topic in the speech research community. From the recent studies on fundamental frequency estimation from singing voice with state-of-art methods proposed for speech, there exists a significant gap in accuracy for singing voice. This is mainly because of the wider and rapid variations in pitch in singing voice compared to that in speech. To overcome this, in this paper we propose a method to derive the fundamental frequency from singing voice by exploiting the harmonics of impulse-like excitation in sequence of glottal cycles. The proposed method is compared with the eight state-of-art methods such as YIN, SWIPE, YAAPT, RAPT, SRH, SFF_CEP, PEFAC and SHRP on the LYRICS singing database. From the experimental results, it is observed that the accuracy of fundamental frequency by the proposed method is better than many state-of-art methods in various singing categories and laryngeal mechanisms.",0.0,4.75
656,2355,4.75    4.35    5    4.9    ,The PRIORI Emotion Dataset: Linking Mood to Emotion Detected In-the-Wild,"Bipolar Disorder is a chronic psychiatric illness characterized by pathological mood swings associated with severe disruptions in emotion regulation. Clinical monitoring of mood is key to the care of these dynamic and incapacitating mood states. Frequent and detailed monitoring improves clinical sensitivity to detect mood state changes, but typically requires costly and limited resources. Speech characteristics change during both depressed and manic states, suggesting automatic methods applied to the speech signal can be effectively used to monitor mood state changes.  However, speech is modulated by many factors, which renders mood state prediction challenging.  We hypothesize that emotion can be used as an intermediary step to improve mood state prediction.  This paper presents critical steps in developing this pipeline, including (1) a new in the wild emotion dataset, the PRIORI Emotion Dataset, collected from everyday smartphone conversational speech recordings, (2) activation/valence emotion recognition baselines on this dataset (PCC of 0.71 and 0.41, respectively), and (3) significant correlation between predicted emotion and mood state for individuals with bipolar disorder. This provides evidence and a working baseline for the use of emotion as a meta-feature for mood state monitoring.",0.0,4.75
372,1638,4.2    4.8    4.95    5.05    ,Pitch or Phonation: on the Glottalization in Tone Productions in the Ruokeng Hui Chinese Dialect,"This paper examines the interplay of glottalization and tones in tonal phonology of the Ruokeng Hui Chinese. Acoustic data from 10 native speakers were analyzed in terms of pitch (F0), duration, H1-H2, H1-A1/2/3, CPP, HNR, SHR, etc. Fine-grained phonetic details reveal the interactions between phonation and tones and shed light on the ongoing tonal change from a glottalized tone to a plain high falling tone in the Ruokeng dialect.",0.0,4.75
449,1804,4.85    4.05    5    5.15    ,Employing Phonetic Information in DNN Speaker Embeddings to Improve Speaker Recognition Performance,"The recent speaker embeddings framework has been shown to provide excellent performance on the task of text-independent speaker recognition. The framework is based on a deep neural network (DNN) trained to directly discriminate between speakers from traditional acoustic features such as Mel frequency cepstral coefficients. Prior studies on speaker recognition have found that phonetic information is valuable in the task of speaker identification, with systems being based on either bottleneck features (BFs) or tied-triphone state posteriors from a DNN trained for the task of speech recognition. In this paper, we analyze the role of phonetic BFs for DNN embeddings and explore methods to enhance the BFs further. Experimental results show that exploiting phonetic information encoded in BFs is very valuable for DNN speaker embeddings. Enriching the BFs using a cascaded DNN multi-task architecture is also shown to provide further improvements to the speaker embedding system.",0.0,4.762499999999999
303,1487,5.15    4.85    4.05    5    ,Whispered Speech to Neutral Speech Conversion Using Bidirectional LSTMs,"We propose a bidirectional long short-term memory (BLSTM) based whispered speech to neutral speech conversion system that employs the STRAIGHT speech synthesizer. We use a BLSTM to map the spectral features of whispered speech to those of neutral speech. Three other BLSTMs are employed to predict the pitch, periodicity levels and the voiced/unvoiced phoneme decisions from the spectral features of whispered speech. We use objective measures to quantify the quality of the predicted spectral features and excitation parameters, using data recorded from six subjects, in a four fold setup. We find that the temporal smoothness of the spectral features predicted using the proposed BLSTM based system is statistically more compared to that predicted using deep neural network based baseline schemes. We also observe that while the performance of the proposed system is comparable to the baseline scheme for pitch prediction, it is superior in terms of classifying voicing decisions and predicting periodicity levels. From subjective evaluation via listening test, we find that the proposed method is chosen as the best performing scheme 26.61% (absolute) more often than the best baseline scheme. This reveals that the proposed method yields a more natural sounding neutral speech from whispered speech.",0.0,4.7625
506,1938,4.3    5.25    4.15    5.35    ,Category Similarity in Multilingual Pronunciation Training,"Learners with different native languages (L1) meet different challenges when they learn a foreign language (L2). The Speech Learning Model and the Perceptual Assimilation Model PAM-L2 have led to important insights about these challenges. Among other things, they have shown that the learnability of L2 sounds depends on their similarity to sounds in the L1: L2 sounds are more likely to lead to the formation of new phonetic categories if they differ strongly from L1 categories than if they are similar. The similarity of sounds is hard to quantify objectively, especially if the aim is to do this for many L1-L2 pairs. This limits the models’ practical applicability.

The multilingual pronunciation training platform CALST offers exercises for all new L2 sounds. Two implementations of category (dis)similarity are proposed to identify new sounds, one at the level of functional similarity maintaining all L2 phonemic contrasts, the other based on a more fine-grained, multilingual similarity measure, where L2 sounds are considered new if they can contrast phonemically with the most similar L1 sound in any one language. This level of granularity reflects phonetically salient differences between sounds which, when perceived and produced adequately, suffice for high intelligibility and comprehensibility in L2.",0.0,4.7625
667,2377,4.2    4.4    5.2    5.25    ,Bubble Cooperative Networks for Identifying Important Speech Cues,"Predicting the intelligibility of noisy recordings is difficult and most current algorithms treat all speech energy as equally important to intelligibility. Our previous work on human perception used a listening test paradigm and correlational analysis to show that some energy is more important to intelligibility than other energy. In this paper, we propose a system called the Bubble Cooperative Network (BCN), which aims to predict important areas of individual utterances directly from clean speech. Given such a prediction, noise is added to the utterance in unimportant regions and then presented to a recognizer.  The  BCN is trained with a loss that encourages it to add as much noise as possible while preserving recognition performance, encouraging it to identify important regions precisely and place the noise everywhere else.  Empirical evaluation shows that the BCN can obscure 97.7\% of the spectrogram with noise while maintaining recognition accuracy for a simple speech recognizer that compares a noisy test utterance with a clean reference utterance.  The masks predicted by a single BCN on several utterances show patterns that are similar to analyses derived from human listening tests that analyze each utterance separately, while exhibiting better generalization and less context-dependence than previous approaches.",0.0,4.7625
630,2288,4.9    5.15    4.85    4.15    ,Speaker-specific Structure in German Voiceless Stop Voice Onset Times,"Voice onset time (VOT), a primary cue for voicing in many languages including English and German, is known to vary greatly between speakers, but also displays robust within-speaker consistencies, at least in English. The current analysis extends these findings to German. VOT measures were investigated from voiceless alveolar and velar stops in CV syllables cued by a visual prompt in a cue-distractor task. Comparably to English, a considerable portion of German VOT variability can be attributed to the syllable’s vowel length and the stop’s place of articulation. Individual differences in VOT still remain irrespective of speech rate. However, significant correlations across places of articulation and between speaker-specific mean VOTs and standard deviations indicate that talkers employ a relatively unified VOT profile across places of articulation. This could allow listeners to more efficiently adapt to speaker-specific realisations.",0.0,4.7625
313,1516,4.1    5.15    4    5.8    ,A Knowledge Driven Structural Segmentation Approach for Play-Talk Classification During Autism Assessment,"Automatically segmenting conversational audio into semantically relevant components has both computational and analytical significance. In this paper, we segment play activities and conversational portions interspersed during clinically administered interactions between a psychologist and a child with autism spectrum disorder (ASD). We show that various acoustic-prosodic and turn-taking features commonly used in the literature differ between these segments, and hence can possibly influence further inference tasks. We adopt a two-step approach for the segmentation problem by taking advantage of the structural relation between the two segments. First, we use a supervised machine learning algorithm to estimate class posteriors at frame-level. Next, we use an explicit-duration hidden Markov model (EDHMM) to align the states using the posteriors from the previous step. The durational distributions for both play and talk regions are learnt from training data and modeled using the EDHMM. Our results show that speech features can be used to successfully discriminate between play and talk activities, each providing important insights into the child’s condition.",0.0,4.7625
368,1630,5.05    4.2    5.05    ,Automatic Speech Assessment for People with Aphasia Using TDNN-BLSTM with Multi-Task Learning,"This paper describes an investigation on automatic speech assessment for people with aphasia (PWA) using a DNN based automatic speech recognition (ASR) system. The main problems being addressed are the lack of training speech in the intended application domain and the relevant degradation of ASR performance for impaired speech of PWA. We adopt the TDNN-BLSTM structure for acoustic modeling and apply the technique of multi-task learning with large amount of domain-mismatched data. This leads to a significant improvement on the recognition accuracy, as compared with a conventional single-task learning DNN system. To facilitate the extraction of robust text features for quantifying language impairment in PWA speech, we propose to incorporate N-best hypotheses and confusion network representation of the ASR output. The severity of impairment is predicted from text features and supra-segmental duration features using different regression models. Experimental results show a high correlation of 0.842 between the predicted severity level and the subjective Aphasia Quotient score.",0.0,4.766666666666667
659,2360,4.85    5.05    4.2    5    ,Implementing Fusion Techniques for the Classification of Paralinguistic Information,"TThis work tests several classification techniques and acoustic features and further combines them using late fusion to classify paralinguistic information for the ComParE 2018 challenge. We use Multiple Linear Regression (MLR) with Ordinary Least Squares (OLS) analysis to select the most informative features for Self-Assessed Affect (SSA) sub-Challenge. We also propose to use raw-waveform convolutional neural networks (CNN) in the context of three paralinguistic sub-challenges. By using combined evaluation split for estimating codebook, we obtain better representation for Bag-of-Audio-Words approach. We preprocess the speech to vocalized segments to improve classification performance. For fusion of our leading classification techniques, we use weighted late fusion approach applied for confidence scores. We use two mismatched evaluation phases by changing the training and development sets, and this estimates the optimal fusion weight. Weighted late fusion provides better performance on development sets in comparison with baseline techniques. Raw-waveform techniques perform comparable to the baseline.",0.0,4.7749999999999995
492,1899,5.7    3.45    4.2    5.75    ,Analysis of Breathiness in Contextual Vowel of Voiceless Nasals in Mizo,"This study analyses the source characteristics of voiced and
voiceless nasals in Mizo, a Tibeto-Burman language spoken in
North-East India. Mizo is one of the few languages that has
voiced and voiceless nasals in its phoneme inventory. This analysis is motivated by the interaction between breathiness and
nasality reported in a number of speech perception studies using
synthetic stimuli. However, there are no studies examining this
interaction in vowels after voiced and voiceless nasals. Existing
research has also documented the interaction between breathy
phonation and vowel height. The current study is an acoustic
analysis of breathiness in high and low vowels following voiced
and voiceless nasals in Mizo. The acoustic parameter measures
are: H1H2 ratio, spectral balance (SB), strength of excitation
(SoE), and waveform peak factor (WPF). The values obtained
for all the four acoustic measures suggest that vowels following voiceless nasals exhibit stronger acoustic characteristics associated with breathy phonation than vowels following voiced
nasals. In addition, the degree of acoustic breathiness is affected
by vowel height.",0.0,4.775
438,1776,5.2    4.95    4.35    4.6    ,Empirical Analysis of Score Fusion Application to Combined Neural Networks for Open Vocabulary Spoken Term Detection,"System combination, which combines the outputs of multiple systems or internal representations, is a powerful method to improve the performance of machine learning tasks and has been widely adopted in recent knowledge transfer learning. In this study, to describe how to extract effective knowledge from an ensemble of neural networks, we first examine several score fusions from an ensemble of neural networks tasked with open vocabulary spoken term detection, where the class probability of the neural network is utilized as a similarity metric; then, we investigate the trade-off between confusion and dark knowledge. From the experimental evaluation on open vocabulary spoken term detection, we obtain 2.09% absolute gain as compared to the best result from single systems. Furthermore, the performance gains achieved via score fusion of class probabilities exactly match the mathematical inequality for sum and power means results, and that the gain achieved via summation of class probabilities is consistently better than that achieved via score fusion of power means. The experimental analysis confirms that summation, which enhances the discriminative capability of the superior class probability, can implement smoothed probability distribution to yield more effective dark knowledge, while adequately suppressing undesirable effects.",0.0,4.775
475,1858,4.85    4.4    6    3.85    ,Towards Temporal Modelling of Categorical Speech Emotion Recognition,"To model the categorical speech emotion recognition task in a temporal manner, the first challenge arising is how to transfer the categorical label for each utterance into a label
sequence. To settle this, we make a hypothesis that an utterance is consisting of emotional and non-emotional segments, and these non-emotional segments correspond to silent regions, short pauses, transitions between phonemes, unvoiced phonemes, etc.
With this hypothesis, we propose to treat an utterance's label sequence as a chain of two states: the emotional state denoting the emotional frame and Null denoting the non-emotional frame. Then, we exploit a recurrent neural network based connectionist temporal classification model to automatically label and align an utterance's emotional segments with emotional labels, while non-emotional segments with Nulls. Experimental results on the IEMOCAP corpus validate our hypothesis and also demonstrate the effectiveness of our proposed method compared to the state-of-the-art algorithms.",0.0,4.775
631,2289,4.95    4.15    4.9    5.1    ,Integrated Presentation Attack Detection and Automatic Speaker Verification: Common Features and Gaussian Back-end Fusion,"The vulnerability of automatic speaker verification (ASV) systems to spoofing is widely acknowledged. Recent years have seen an intensification in research efforts to develop spoofing countermeasures, also known as presentation attack detection (PAD) systems. Much of this work has involved the exploration of features that discriminate reliably between bona fide and spoofed speech. While there are grounds to use different front-ends for ASV and PAD systems (they are different tasks) the use of a single front-end has obvious benefits, not least convenience and computational efficiency, especially when ASV and PAD are combined.  This paper investigates the performance of a variety of different features used previously for both ASV and PAD and assesses their performance when combined for both tasks. The paper also presents a Gaussian back-end fusion approach to system combination. In contrast to cascaded architectures, it relies upon the modelling of the two-dimensional score distribution stemming from the combination of ASV and PAD in parallel. This approach to combination is shown to generalise particularly well across independent ASVspoof 2017 v2.0 development and evaluation datasets.",0.0,4.775
261,1405,5    5.05    4.95    4.1    ,A Convolutional Recurrent Neural Network for Real-Time Speech Enhancement,"Many real-world applications of speech enhancement, such as hearing aids and cochlear implants, desire real-time processing, with no or low latency. In this paper, we propose a novel convolutional recurrent network (CRN) to address real-time monaural speech enhancement. We incorporate a convolutional encoder-decoder (CED) and long short-term memory (LSTM) into the CRN architecture, which leads to a causal system that is naturally suitable for real-time processing. Moreover, the proposed model is noise- and speaker-independent, i.e. noise types and speakers can be different between training and test. Our experiments suggest that the CRN leads to consistently better objective intelligibility and perceptual quality than an existing LSTM based model. Moreover, the CRN has much fewer trainable parameters.",0.0,4.775
332,1547,4.2    5.05    5    4.85    ,Monaural Multi-Talker Speech Recognition with Attention Mechanism and Gated Convolutional Networks,"To improve the speech recognition accuracy under the multi-talker scenario, we propose a novel model architecture that incorporates the attention mechanism and gated convolutional network (GCN) into our previously developed permutation invariant training based multi-talker speech recognition system (PIT-ASR). The new architecture has three components: an encoding transformer, an attention module and a frame-level senone predictor. The encoding transformer first transforms a mixed speech sequence into a sequence of embedding vectors. Then the attention mechanism extracts individual context vectors from this embedding sequence for different speaker sources. Finally the predictor generates the senone posteriors for all speaker sources independently with the knowledge from the context vectors. To get better embedding representations we explore gated convolutional networks in the encoding transformer. The experimental results on the artificially mixed two-talker WSJ0 corpus show that our proposed model can reduce the word error rate (WER) by more than 15% relatively compared to our previous PIT-ASR system.",0.0,4.775
399,1696,5    5.1    4.25    ,On Learning Vocal Tract System Related Speaker Discriminative Information from Raw Signal Using CNNs,"In a recent work, we have shown that speaker verification systems can be built where both features and classifiers are directly learned from the raw speech signal with convolutional neural networks (CNNs). In this framework, the training phase also decides the block processing through cross validation. It was found that the first convolution layer, which processes about 20 ms speech, learns to model fundamental frequency information.
In the present paper, inspired from speech recognition studies, we build further on that framework to design a CNN-based system, which models sub-segmental speech (about 2ms speech)
in the first convolution layer, with an hypothesis that such a system should learn vocal tract system related speaker discriminative information. Through experimental studies on Voxforge corpus and analysis on American vowel dataset, we show that the proposed system (a) indeed focuses on formant regions, (b) yields competitive speaker verification system and (c) is complementary to the CNN-based system that models fundamental frequency information.",0.0,4.783333333333333
481,1872,4.15    5    5.2    ,Learning Spontaneity to Improve Emotion Recognition in Speech,"We investigate the effect and usefulness of spontaneity (i.e. whether a given speech is spontaneous or not) in speech in the context of emotion recognition. We hypothesize that emotional content in speech is interrelated with its spontaneity, and use spontaneity classification as an auxiliary task to the problem of emotion recognition. We propose two supervised learning settings that utilize spontaneity to improve speech emotion recognition: a hierarchical model that performs spontaneity detection before performing emotion recognition, and a multitask learning model that jointly learns to recognize both spontaneity and emotion. Through various experiments on the well-known IEMOCAP database, we show that by using spontaneity detection as an additional task, significant improvement can be achieved over emotion recognition systems that are unaware of spontaneity. We achieve state-of-the-art emotion recognition accuracy (4-class, 69.1%) on the IEMOCAP database outperforming several relevant and competitive baselines.",0.0,4.783333333333334
151,1214,5    4.1    5.1    4.95    ,Exploration of Local Speaking Rate Variations in Mandarin Read Speech,"This paper explores speaking rate variation in Mandarin read speech. In contrast to assuming that each utterance is generated in a constant or global speaking rate, this study seeks to estimate local speaking rate for each prosodic unit in an utterance. The exploration is based on the existing speaking rate-dependent hierarchical prosodic model (SR-HPM) which sophisticatedly describes the relationship between speaking rate, prosodic-acoustic features, hierarchical prosodic structure and linguistic features. The main idea is to obtain local speaking rate of each prosodic unit by considering effects of lexical information of tone and base syllable type, prosodic structure, and speaking rates of the higher prosodic units. The significance of the estimated local speaking rate is exemplified by analysis on the results of the speaking rate estimation on a speech corpus uttered by a professional female announcer. By conducting prosody generation experiment based on the SR-HPM, the generated prosody with local speaking rate variations is proved to be more vivid than the one with a constant speaking rate.",0.0,4.7875
105,1122,5.1    4.15    4.75    5.15    ,Contextual Language Model Adaptation for Conversational Agents,"Statistical language models (LM) play a key role in Automatic
Speech Recognition (ASR) systems used by conversational
agents. These ASR systems should provide a high accuracy
under a variety of speaking styles, domains, vocabulary and argots.
In this paper, we present a DNN-based method to adapt the
LM to each user-agent interaction based on generalized contextual
information, by predicting an optimal, context-dependent
set of LM interpolation weights. We show that this framework
for contextual adaptation provides accuracy improvements under
different possible mixture LM partitions that are relevant for
both (1) Goal-oriented conversational agents where it’s natural
to partition the data by the requested application and for (2)
Non-goal oriented conversational agents where the data can be
partitioned using topic labels that come from predictions of a
topic classifier. We obtain a relative WER reduction of 3% with
a 1-pass decoding strategy and 6% in a 2-pass decoding framework,
over an unadapted model. We also show up to a 15%
relative WER reduction in recognizing named entities which is
of significant value for conversational ASR systems.",0.0,4.7875
439,1777,5    4.9    4.25    5    ,Attention-based End-to-End Models for Small-Footprint Keyword Spotting,"In this paper, we propose an attention-based end-to-end neural approach for small-footprint keyword spotting (KWS), which aims to simplify the pipelines of building a production-quality KWS system. Our model consists of an encoder and an attention mechanism. The encoder transforms the input signal into a high level representation using RNNs. Then the attention mechanism weights the encoder features and generates a fixed-length vector. Finally, by linear transformation and softmax function, the vector becomes a score used for keyword detection. We also evaluate the performance of different encoder architectures, including LSTM, GRU and CRNN. Experiments on real-world wake-up data show that our approach outperforms the recent Deep KWS approach by a large margin and the best performance is achieved by CRNN. To be more specific, with ~84K parameters, our attention-based model achieves 1.02% false rejection rate (FRR) at 1.0 false alarm (FA) per hour.",0.0,4.7875
411,1727,4.95    4.05    5.15    5    ,Character-level Language Modeling with Gated Hierarchical Recurrent Neural Networks,"Recurrent neural network (RNN)-based language models are widely used for speech recognition and translation applications. We propose a gated hierarchical recurrent neural network (GHRNN) and apply it to the character-level language modeling. GHRNN consists of multiple RNN units that operate with different time scales, and the frequency of operation at each unit is controlled by the learned gates from training data.  In our model, GHRNN learns the hierarchical structure of character, sub-word, and word. Timing gates are included in the hierarchical connections to control the operating frequency of these units.  The performance was measured for Penn Treebank and Wikitext-2 datasets. Experimental results showed lower bit per character (BPC) when compared to simply layered or skip-connected RNN models. Also, when a continuous cache model is added, the BPC of 1.192 is recorded, which is comparable to the state of the art result.",0.0,4.7875
652,2338,4.2    4.85    5.1    5    ,A Simple Model for Detection of Rare Sound Events,"We propose a simple recurrent model for detecting rare sound events, when the time boundaries of events are available for training. Our model optimizes the combination of an utterance-level loss, which classifies whether an event occurs in an utterance, and a frame-level loss, which classifies whether each frame corresponds to the event when it does occur. The two losses make use of a shared vectorial representation the event, and are connected by an attention mechanism. We demonstrate our model on Task 2 of the DCASE 2017 challenge, and achieve competitive performance.",0.0,4.7875
470,1845,5    4.3    4.7    5.15    ,Reducing Interference with Phase Recovery in DNN-based Monaural Singing Voice Separation,"State-of-the-art methods for monaural singing voice separation consist in estimating the magnitude spectrum of the voice in the short-time Fourier transform (STFT) domain by means of deep neural networks (DNNs). The resulting magnitude estimate is then combined with the mixture's phase to retrieve the complex-valued STFT of the voice, which is further synthesized into a time-domain signal. However, when the sources overlap in time and frequency, the STFT phase of the voice differs from the mixture's phase, which results in interference and artifacts in the estimated signals. In this paper, we investigate on recent phase recovery algorithms that tackle this issue and can further enhance the separation quality. These algorithms exploit phase constraints that originate from a sinusoidal model or from consistency, a property that is a direct consequence of the STFT redundancy. Experiments conducted on real music songs show that those algorithms are efficient for reducing interference in the estimated voice compared to the baseline approach.",0.0,4.7875
350,1589,4.1    5.85    5.05    4.15    ,Knowledge Distillation for Sequence Model,"Knowledge distillation, or teacher-student training, has been effectively used to improve the performance of a relatively simpler deep learning model (the student) using a more complex model (the teacher). It is usually done by minimizing the Kullback-Leibler divergence (KLD) between the output distributions of the student and the teacher at each frame. However, the gain from frame-level knowledge distillation is limited for sequence models such as Connectionist Temporal Classification (CTC), due to the mismatch between the sequence-level criterion used in teacher model training and the frame-level criterion used in distillation. In this paper, sequence-level knowledge distillation is proposed to achieve better distillation performance. Instead of calculating a teacher posterior distribution given the feature vector of the current frame, sequence training criterion is employed to calculate the posterior distribution given the whole utterance and the teacher model. Experiments are conducted on both English Switchboard corpus and a large Chinese corpus. The proposed approach achieves significant and consistent improvements over the traditional frame-level knowledge distillation using both labeled and unlabeled data.",0.0,4.7875
342,1568,4.05    5.1    5.05    4.95    ,Variational Autoencoders for Learning Latent Representations of Speech Emotion: a Preliminary Study,"Learning the latent representation of data in unsupervised fashion is a very interesting process that provides relevant features for enhancing the performance of a classifier. For speech emotion recognition tasks, generating effective features is crucial.  Currently, handcrafted features are mostly used for speech emotion recognition, however, features learned automatically using deep learning have shown strong success in many problems, especially in image processing.  In particular, deep generative models such as Variational Autoencoders (VAEs) have gained enormous success in generating features for natural images. Inspired by this, we propose VAEs for deriving the latent representation of speech signals and use this representation to classify emotions. To the best of our knowledge, we are the first to propose VAEs for speech emotion classification. Evaluations on the IEMOCAP dataset demonstrate that features learned by VAEs can produce state-of-the-art results for speech emotion classification.",0.0,4.7875
571,2090,3.4    4.9    5    5.85    ,Analysis of the Effect of Speech-Laugh on Speaker Recognition System,"A robust speaker recognition system should be able to recognize a speaker despite all the possible variations in speaker's speech. A common variation of the neutral speech is speech-laugh, which occurs when a person is speaking and laughing, simultaneously. In this paper, we show that speech-laugh significantly degrades the performance of an i-vector based speaker recognition system. Further, we show that laughter and neutral speech contain complementary speaker information, which can be combined to improve the performance of the speaker recognition system for speech-laugh scenarios. Using AMI meeting corpus database, we show that by including neutral speech and laughter in enrollment phase, the performance of the system in the speech-laugh scenarios can be relatively improved by 36% in EER.",0.0,4.7875
516,1958,5.75    5.05    4.3    4.05    ,Identification and Classification of Fricatives in Speech Using Zero Time Windowing Method,"Fricatives are produced by creating a turbulence in the air--flow by passing it through a stricture in the vocal tract cavity. Fricatives are characterized by their noise--like behavior, which makes it difficult to analyze. Difference in the place of articulation leads to different classes of fricatives. Identification of fricative segment boundaries in speech helps in improving the performance of several applications. The present study attempts towards the identification and classification of fricative segments in continuous speech, based on the statistical behavior of instantaneous spectral characteristics. The proposed method uses parameters such as the dominant resonance frequencies, the center of gravity along with the statistical moments of the spectrum obtained using the zero time windowing (ZTW) method. The ZTW spectra exhibits a high temporal resolution and therefore gives accurate segment boundaries in speech. The proposed algorithm is tested on the TIMIT dataset for English language. A high identification rate of 97.5% is achieved for segment boundaries of the sibilant fricative class. Voiced nonsibilants show a lower identification rate than their voiceless counterparts due to their vowel--like spectral characteristics. A high classification rate of 93.2% is achieved between sibilants and nonsibilants.",0.0,4.7875000000000005
17,62,4.85    4.35    5    5    ,Entity-Aware Language Model as an Unsupervised Reranker,"In language modeling, it is difficult to incorporate entity relationships from
a knowledge-base. One solution is to use a reranker trained with global features, in which global features are derived from n-best lists. However, training such a reranker requires manually annotated n-best lists, which is expensive to obtain. We propose a method based on the contrastive estimation method that alleviates the need for such data. Experiments in the music domain demonstrate that global features, as well as features extracted from an external knowledge-base, can be incorporated into our reranker. Our final model, a simple ensemble of a language model and reranker, achieves a 0.44\% absolute word error rate improvement over an LSTM language model on the blind test data.",0.0,4.8
722,2508,5    4.95    4.45    ,Role of Regularization in the Prediction of Valence from Speech,"Regularization plays a key role in improving the prediction of emotions using attributes such as arousal, valence and dominance. Regularization is particularly important with deep neural networks (DNNs), which  have millions of parameters. While previous studies have reported competitive performance for arousal and dominance, the prediction results for valence using acoustic features are significantly lower. We hypothesize that higher regularization can lead to better results for valence. This study focuses on exploring the role of dropout as a form of regularization for valence suggesting the need for higher regularization. We analyze the performance of regression models for valence, arousal and dominance as a function of the dropout probability. We observe that the optimum dropout rates are consistent for arousal and dominance. However, the optimum dropout rate for valence is higher. To understand the need for higher regularization for valence, we perform an empirical analysis to explore the nature of emotional cues conveyed in speech. We compare regression models with speaker-dependent and speaker-independent partitions for training and testing. The experimental evaluation suggests stronger speaker dependent traits for valence. We conclude that higher regularization is needed for valence to force the network to learn global patterns that generalize across speakers.",0.0,4.8
644,2321,5    4.2    5.2    ,Denoising and Raw-waveform Networks for Weakly-Supervised Gender Identification on Noisy Speech,"This paper presents a raw-waveform neural network and uses it along with a denoising network for clustering in weakly-supervised learning scenarios under extreme noise conditions. Specifically, we consider language independent gender identification on a set of varied noise conditions and signal to noise ratios (SNRs). We formulate the denoising problem as a source separation task and train the system using a discriminative criterion in order to enhance output SNRs. A denoising recurrent neural network (RNN) is first trained on a small subset (roughly one-fifth) of the data for learning a speech-specific mask. The denoised speech signal is then directly fed as input to a raw-waveform convolutional neural network (CNN) trained with denoised speech. We evaluate the standalone performance of denoiser in terms of various signal-to-noise measures and discuss its contribution towards robust gender identification. An absolute improvement of 11.06% and 13.33% is achieved by the combined pipeline over the i-vector SVM baseline system for 0 dB and -5 dB SNR conditions, respectively. We further analyse the information captured by the first CNN layer in both noisy and denoised speech.",0.0,4.8
593,2162,3.45    4.35    5.8    5.6    ,Leveraging Translations for Speech Transcription in Low-resource Settings,"Recently proposed data collection frameworks for endangered language documentation aim not only to collect speech in the language of interest, but also to collect translations into a high-resource language that will render the collected resource interpretable. We focus on this scenario and explore whether we can improve transcription quality under these extremely low-resource settings with the assistance of text translations. We present a neural multi-source model and evaluate several variations of it on three low-resource datasets. We find that our multi-source model with shared attention outperforms the baselines, reducing transcription character error rate by up to 12.3%.",0.0,4.8
366,1626,4.25    5.8    5.1    4.05    ,Multilingual Grapheme-to-Phoneme Conversion with Global Character Vectors,"Multilingual grapheme-to-phoneme (G2P) models are useful for multilingual speech synthesis because one model simultaneously copes with multilingual words. We propose a G2P
model that combines global character vectors (GCVs) with bidirectional recurrent neural networks (BRNNs) and enables the direct conversion of text (as a sequence of characters) to pronunciation. GCVs are distributional, real-valued representations of characters and their contextual interactions that can be learned from a large-scale text corpus in an unsupervised manner. With the flexibility of learning GCVs from plain text resources, this method has an advantage: it enables monolingual G2P (MoG2P) and multilingual G2P (MuG2P) conversion.
We experiment in four languages (Japanese, Korean, Thai, and Chinese) with learning language-dependent (LD) and language-independent (LI) GCVs and then build MoG2P and MuG2P models with two-hidden-layer BRNNs. Our results show that both LD- and LI-GCV-based MoG2P models, whose performances are equivalent, achieved better than 97.7% syllable accuracy, which is a relative improvement from 27% to 90% depending on the language in comparison with Mecab-based models. As for MuG2P, the accuracy is around 98%, which is a slightly degraded performance compared to MoG2P. The proposed method also has the potential of the G2P conversion of non-normalized words, achieving 80% accuracy in Japanese.",0.0,4.8
362,1613,4.85    4.95    4.35    5.05    ,Epoch Extraction from Pathological Children Speech Using Single Pole Filtering Approach,"The instant of significant excitation of the vocal tract system is referred to the epoch of the speech signal. The presence of high pitch and aperiodicity are the major challenges for the epoch extraction from  the speech of pathological children. In this work,  impulse-like characteristics of epochs derived from single pole filter based time-frequency representation are exploited to propose an epoch extraction algorithm for the pathological children speech. The sharp transitions present in the single pole filtered envelope at the epochs are enhanced using multi-scale product computation. Further, the combined evidence derived from the multi-scale product of the filtered envelopes at different frequencies is  used to locate the epochs. The proposed algorithm is evaluated over the Saarbruecken Voice Database containing pathological children speech and simultaneously recorded electroglottographic signals. The proposed method showed better identification accuracy for pathological children speech when compared to state-of-the-art techniques.",0.0,4.8
91,1099,5.15    5.05    4.2    4.8    ,"The Role of Cognate Words, POS Tags, and Entrainment in Code-Switching","The linguistic or contextual stimuli that elicit code-switching are largely unknown, despite the fact that these are of key importance to understanding mixed language and building tools that can handle it. In this paper, we test the following hypotheses proposed in linguistics literature: first, that cognate stimuli are directly correlated to code-switching; second, that syntactic information facilitates or inhibits code switching; and third that speakers {\it entrain} to one another in code-switching in conversation between bilinguals. In order to test these hypotheses, we built a lexical database of cognate pairs for English Spanish. Using statistical significance tests on a corpus of conversational code-switched English Spanish, we found that a) there is strong statistical evidence that cognates and switches occur simultaneously in the same utterance and that cognates facilitate switching when they precede a code-switch, b) there is strong statistical evidence of the relationship between part-of-speech tags and code-switching and c) speakers tend to show converging entrainment behavior with respect to their rate of code-switching in conversation.",0.0,4.8
532,2003,5    5.25    4.15    ,Far-Field Speech Recognition Using Multivariate Autoregressive Models,"Automatic speech recognition in far-field reverberant environments is challenging even with the state-of-the-art recognition systems. The main issues are artifacts in the signal due to the long-term reverberation that results in temporal smearing. The autoregressive modeling approach to speech feature extraction involves representing the high energy regions of the signal which are less susceptible to noise. In this paper, we propose a novel method of speech feature extraction using multivariate AR modeling (MAR) of temporal envelopes. The sub-band  discrete cosine transform coefficients obtained from multiple speech bands are used in a multivariate linear prediction setting to derive features for speech recognition. For single channel far-field speech recognition, the features are derived using multi-band linear prediction. In the case of multi-channel far-field speech recognition, we use the multi-channel data in the MAR framework. We perform several speech recognition experiments in the REVERB Challenge database for single and multi-microphone settings. In these experiments, the proposed feature extraction method provides significant improvements over baseline methods (average relative improvements of 9.7% and 3.9% in single microphone conditions for clean and multi-conditions respectively and 6.3% in multi-microphone conditions). The results with clean training on single microphone conditions further illustrates the effectiveness of the MAR features.",0.0,4.8
728,2523,5.1    3.3    6    ,Automated Classification of Children’S Linguistic versus Non-Linguistic Vocalisations,"A key outstanding task for speech technology involves dealing with non-standard speakers, notably young children. Distinguishing children's linguistic from non-linguistic vocalisations is crucial for a number of applied and fundamental research goals, and yet there are few systems available for such a classification. This paper investigates two large-scale frame-level acoustic feature sets  (eGeMAPS and ComParE16) followed by a dynamic model (GRU-RNN), and two kinds of derived static feature sets on the segment level (functional-based and Bag of Audio Words) combined with a static model (SVM), and automatically learnt representations directly from original raw voice signals by using an end-to-end system, which are compared against a simple phonetically-inspired baseline. These are applied to a large database of children's vocalisations (total N = 6,298) drawn from daylong recordings gathered in Namibia, Bolivia, and Vanuatu. All of the systems outperform the baseline, with the highest performance in the test set for GRU-RNN using ComParE16 features. We identify promising paths of further research, including the application of a finer-grained classification of children's vocalisations onto these data, and the exploration of other feature systems.",0.0,4.8
292,1465,5.15    5    4.05    5    ,Fusing Text-dependent Word-level i-Vector Models to Screen ‘at Risk’ Child Speech,"Speech sound disorders (SSDs) are the most prevalent type of communication disorder among preschoolers. The earlier an SSD is identified, the earlier an intervention can be provided to potentially reduce the social/academic impact of the disorder. The challenge, lies in early identification of such disorders. In this study 29 carefully selected words  were produced by 165 children from 3-6 years of age. The audio recordings, were collected by parents using a mobile application /platform. ""Ground truth"" child status as 'typically developing' vs 'at risk' was based on a percentage of consonants correct-revised growth curve model. State-of-the-art speech processing/speaker recognition models were employed along with our clinical group verification framework. Results showed that text-dependent i-Vector models were superior to both text dependent and text-independent Gaussian Mixture Models (GMMs) for correct classification of children. Fusing individual word, i-Vector models provides insight into word and consonant groupings that are more indicative of 'at risk' child speech.",0.0,4.8
285,1455,5.05    5    4.1    5.05    ,Encoding Individual Acoustic Features Using Dyad-Augmented Deep Variational Representations for Dialog-level Emotion Recognition,"Face-to-face dyadic spoken dialog is a fundamental unit of human interaction. Despite numerous empirical evidences in demonstrating interlocutor's behavior dependency in dyadic interactions, few technical works exist in leveraging the unique pattern of dynamics in task of advancing emotion recognition during face-to-face settings. In this work, we propose a framework of encoding an individual's acoustic features with dyad-augmented deep networks. The dyad-augmented deep networks includes a general variational deep Gaussian Mixture embedding network and a dyad-specific fine-tuned network. Our framework utilizes the augmented dyad-specific feature space to incorporate the unique behavior pattern emerged when two people interact. We perform dialog-level emotion regression tasks in both the CreativeIT and the NNIME databases. We obtain affect regression accuracy of 0.544 and 0.387 for activation and valence in the CreativeIT database (a relative improvement of 4.41% and 4.03% compared to using features without augmenting the dyad-specific representation), and we obtain 0.700 and 0.604 (4.48% and 4.14% relative improvement) for regressing activation and valence in the NNIME database.",0.0,4.8
57,1037,4.9    5.65    4.05    4.6    ,Stream Attention for Distributed Multi-Microphone Speech Recognition,"Exploiting multiple microphones has been a widely-used strategy for robust automatic speech recognition (ASR).
Particularly, in a general hands-free scenario, acquisition of speech usually happens using a set of distributed microphones or arrays simultaneously. 
Each microphone or array (defined as a stream) carries a different quality of information.
The technique of stream fusion is beneficial to provide the best distant recognition performance against the effects of potential disturbances such as noise, reverberation, as well as the speaker movement. 

In this work, we propose a stream attention framework to improve the far-field ASR performance in the distributed multi-microphone configuration. 
Frame-level attention vectors have been derived by predicting the ASR performance of the acoustic modeling of individual streams using the posterior probabilities from the classifier. 
They are used to characterize the amount of useful information each stream contributes, for the purpose of an efficient and better-performing decoding scheme.
In this paper, we investigate the ASR performance measures using our proposed stream attention system on real recorded datasets, Mixer-6 and DIRHA-WSJ. 
The experimental results show that the proposed framework yields substantial improvements in word error rate (WER) compared to conventional strategies.",0.0,4.800000000000001
387,1671,5.1    5.8    5.15    5.25    2.75    ,Waveform to Single Sinusoid Regression to Estimate the F0 Contour from Noisy Speech Using Recurrent Deep Neural Networks,"The fundamental frequency (F0) represents pitch in speech that determines prosodic characteristics of speech and is needed in various tasks for speech analysis and synthesis. Despite decades of research on this topic, F0 estimation at low signal-to-noise ratios (SNRs) in unexpected noise conditions remains difficult. This work proposes a new approach to noise robust F0 estimation using a recurrent neural network (RNN) trained in a supervised manner. Recent studies employ deep neural networks (DNNs) for F0 tracking as a frame-by-frame classification task into quantised frequency states but we propose waveform-to-sinusoid regression instead to achieve both noise robustness and accurate estimation with increased frequency resolution.

Experimental results with PTDB-TUG corpus contaminated by additive noise (NOISEX-92) demonstrate that the proposed method improves gross pitch error (GPE) rate and fine pitch error (FPE) by more than 35 % at SNRs between -10 dB and +10 dB compared with well-known noise robust F0 tracker, PEFAC. Furthermore, the proposed method also outperforms state-of-the-art DNN-based approaches by more than 15 % in terms of both FPE and GPE rate over the preceding SNR range.",0.0,4.81
9,47,4.25    5.15    5    4.85    ,Categorical vs Dimensional Perception of Italian Emotional Speech,"Culture and measurement strategies are influential factors when evaluating the perception of emotion in speech. However, multilingual databases suitable for such a study are missing, and there is no agreement on the most suitable emotional model. To address this gap, we present EmoFilm, a new multilingual emotional speech corpus, consisting of 1115 English, Spanish, and Italian emotional utterances extracted from 43 films and 207 speakers. We have performed a within-culture categorical vs dimensional perceptual evaluation, employing 225 native Italian listeners, who evaluated the Italian section of the database with the emotional states of anger, sadness, happiness, fear, and contempt. The aim of this study is to assess whether the emotional model (categorical or dimensional), taken as  reference for measurement, influences a listener's perception of emotional speech, and —to what extent— both models are complementary or not. We show that the measurement strategy chosen does influence a listener's response, especially for some emotions, e.g. contempt. The confusion patterns typical of a categorical evaluation are not always mirrored by the dimensional assessment.",0.0,4.8125
515,1955,4.3    3.15    5.95    5.85    ,Visual Speech Enhancement,"When video is shot in noisy environment, the voice of a speaker seen in the video can be enhanced using the visible mouth movements, reducing background noise.
While most existing methods use audio-only inputs, improved performance is obtained with our visual speech enhancement, based on an audio-visual neural network.

We include in the training data videos to which we added the voice of the target speaker as background noise. Since the audio input is not sufficient to separate the voice of a speaker from his own voice, the trained model better exploits the visual input and generalizes well to different noise types.

The proposed model outperforms prior audio visual methods on two public lipreading datasets. It is also the first to be demonstrated on a dataset not designed for lipreading, such as the weekly addresses of Barack Obama.",0.0,4.8125
587,2138,4.9    4.85    5.25    4.25    ,Factorized Deep Neural Network Adaptation for Automatic Scoring of L2 Speech in English Speaking Tests,"Speaker adaptation has been shown to be effective on speech recognition and evaluation of L2 speech. However, other factors, such as environments and foreign accents, can affect the speech signal in addition to speakers. Factorizing the speaker, environment and other acoustic factors is crucial in evaluating L2 speech to effectively reduce acoustic mismatch between train and test conditions. In this study, we investigate the effects of deep neural network factorized adaptation techniques on L2 speech assessment in real speaking tests. Through recognition and automatic scoring experiments on L2 speech, we demonstrate that factorized fMLLR and iVector based DNN adaptation can better utilize adaptation data to efficiently adapt to complex speaker and environment conditions. Combining the factored components of iVectors and fMLLR transforms can further improve robustness of DNN models in speech recognition and automatic scoring of L2 speech in dynamic environments.",0.0,4.8125
27,83,5.7    3.95    4.95    4.65    ,Deep Speech Denoising with Vector Space Projections,"We propose an algorithm to denoise speakers from a single microphone in the presence of non-stationary and dynamic noise. Our approach is inspired by the recent success of neural network models separating speakers from other speakers and singers from instrumental accompaniment. Unlike prior art, we leverage embedding spaces produced with source-contrastive estimation, a technique derived from negative sampling techniques in natural language processing, while simultaneously obtaining a continuous inference mask. Our embedding space directly optimizes for the discrimination of speaker and noise by jointly modeling their characteristics. This space is generalizable in that it is not speaker or noise specific and is capable of denoising speech even if the model has not seen the speaker in the training set. Parameters are trained with dual objectives: one that promotes a selective bandpass filter that eliminates noise at time-frequency positions that exceed signal power, and another that proportionally splits time-frequency content between signal and noise. We compare to state of the art algorithms as well as traditional sparse non-negative matrix factorization solutions. The resulting algorithm avoids severe computational burden by providing a more intuitive and easily optimized approach, while achieving competitive accuracy.",0.0,4.8125
480,1869,5.2    4.85    5    4.2    ,An Investigation of Convolution Attention Based Models for Multilingual Speech Synthesis of Indian Languages,"In this paper we investigate multi-speaker, multi-lingual speech synthesis for 4 Indic languages (Hindi, Marathi, Gujarathi, Bengali) as well as English in a fully convolutional attention based model. We show how factored embeddings can allow cross lingual transfer, and investigate methods to adapt the model in a low resource scenario for the case of Marathi and Gujarati.  We also show results on how effectively the model scales to a new language and how much data is required to train the system on a new language.",0.0,4.8125
284,1454,4.05    5.1    5.85    4.25    ,The Voices Obscured in Complex Environmental Settings (VOICES) Corpus,"This paper introduces the Voices Obscured In Complex Environmental Settings (VOICES) corpus, a freely available dataset under Creative Commons BY 4.0. This dataset will promote speech and signal processing research of speech recorded by far-field microphones in noisy room conditions. Publicly available speech corpora are mostly composed of isolated speech at close-range microphony. A typical approach to better represent realistic scenarios, is to convolve clean speech with noise and simulated room response for model training. Despite these efforts, model performance degrades when tested against uncurated speech in natural conditions. For this corpus, audio was recorded in furnished rooms with background noise played in conjunction with foreground speech selected from the LibriSpeech corpus. Multiple sessions were recorded in each room to accommodate for all foreground speech-background noise combinations. Audio was recorded using twelve microphones placed throughout the room, resulting in 120 hours of audio per microphone. This work is a multi-organizational effort led by SRI International and Lab41 with the intent to push forward state-of-the-art distant microphone approaches in signal processing and speech recognition.",0.0,4.8125
730,2527,5.1    4.1    5.2    4.9    ,Conversational Analysis Using Utterance-level Attention-based Bidirectional Recurrent Neural Networks,"Recent approaches for dialogue act recognition have shown that context from preceding utterances is important to classify the subsequent one. It was shown that the performance improves rapidly when the context is taken into account. We propose an utterance-level attention-based bidirectional recurrent neural network (Utt-Att-BiRNN) model to analyze the importance of preceding utterances to classify the current one. In our setup, the BiRNN is given the input set of current and preceding utterances. Our model outperforms previous models that use only preceding utterances as context on the used corpus. Another contribution of our research is a mechanism to discover the amount of information in each utterance to classify the subsequent one and to show that context-based learning not only improves the performance but also achieves higher confidence in the recognition of dialogue acts. We use character- and word-level features to represent the utterances.
The results are presented for character and word feature representations and as an ensemble model of both representations. We found that when classifying short utterances, the closest preceding utterances contribute to a higher degree.",0.0,4.824999999999999
733,2532,6    4.85    3.3    5.15    ,"On the Relationship between Glottal Pulse Shape and Its Spectrum: Correlations of Open Quotient, Pulse Skew and Peak Flow with Source Harmonic Amplitudes","This paper explores the relationship between the glottal pulse amplitude (Up) and the amplitude of the first harmonic (H1), as well as the combined effects of Up, the open quotient (Oq) and degree of pulse asymmetry/skew (Rk) on the low end of the source spectrum. This serves to elucidate their relationship to the H1-H2 estimate, widely used to make inferences on changes in Oq and voice quality. It has been suggested that H1 is mainly determined by Up and that the pulse shape has a relatively small impact. To investigate this, a series of glottal pulses were generated using the LF model, where Up was kept constant, while Oq and Rk were systematically varied. The resulting harmonic amplitudes of these pulses show that Up is not the sole determinant of H1. Rather, H1 is highly dependent on Oq and to a certain degree also on Rk.  Although the effects of these parameters on the lowest harmonics is rather complex, we find that the H1-H2 measure is broadly correlated with Oq. However, there is also a strong effect of differences in glottal skew, particularly at high Oq values, which could invalidate inferences on Oq and voice quality from estimates of H1-H2.",0.0,4.824999999999999
653,2341,5.1    4.1    5    5.1    ,Speech2Vec: a Sequence-to-Sequence Framework for Learning Word Embeddings from Speech,"In this paper, we propose a novel deep neural network architecture, Speech2Vec, for learning fixed-length vector representations of audio segments excised from a speech corpus, where the vectors contain semantic information pertaining to the underlying spoken words, and are close to other vectors in the embedding space if their corresponding underlying spoken words are semantically similar. The proposed model can be viewed as a speech version of Word2Vec. Its design is based on a RNN Encoder-Decoder framework, and borrows the methodology of skipgrams or continuous bag-of-words for training. Learning word embeddings directly from speech enables Speech2Vec to make use of the semantic information carried by speech that does not exist in plain text. The learned word embeddings are evaluated and analyzed on 13 widely used word similarity benchmarks, and outperform word embeddings learned by Word2Vec from the transcriptions.",0.0,4.824999999999999
172,1247,5.05    4.35    4.9    5    ,Training Augmentation Using Adversarial Examples for Robust Speech Recognition,"This paper explores the use of adversarial examples in training speech recognition systems to increase robustness of deep neural network acoustic models. During training, the fast gradient sign method is used to generate adversarial examples augmenting the original training data. Different from conventional data augmentation based on data transformations, the examples are dynamically generated based on current acoustic model parameters. We assess the impact of adversarial data augmentation in experiments on the Aurora-4 and CHiME-4 single-channel tasks, showing improved robustness against noise and channel variation. Further improvement is obtained when combining adversarial examples with teacher/student training, leading to a 23% relative word error rate reduction on Aurora-4.",0.0,4.824999999999999
166,1241,5    5.1    4.3    4.9    ,Neural Language Codes for Multilingual Acoustic Models,"Multilingual Speech Recognition is one of the most costly AI problems, because each language (7,000+) and even different accents require their own acoustic models to obtain best recognition performance.  Even though they all use the same phoneme symbols, each language and accent imposes its own coloring or “twang”.  Many adaptive approaches have been proposed, but they require further training, additional data and generally are inferior to monolingually trained models.  In this paper, we propose a different approach that uses a large multilingual model that is modulated by the codes generated by an ancillary network that learns to code useful differences between the “twangs” or human language. 

We use Meta-Pi networks to have one network (the language code net) gate the activity of neurons in another (the acoustic model nets).  Our results show that during recognition multilingual Meta-Pi networks quickly adapt to the proper language coloring without retraining or new data, and perform better than monolingually trained networks.
The model was evaluated by training acoustic modeling nets and modulating language code nets jointly and optimize them for best recognition performance.",0.0,4.824999999999999
674,2397,5    4.35    4.05    5.9    ,Predicting Arousal and Valence from Waveforms and Spectrograms Using Deep Neural Networks,"Automatic recognition of spontaneous emotion in conversational speech is an important yet challenging problem. In this paper, we propose a deep neural network model to track continuous emotion changes in the arousal-valence two-dimensional space by combining inputs from raw waveform signals and spectrograms, both of which have been shown to be useful in the emotion recognition task. The neural network architecture contains a set of convolutional neural network (CNN) layers and bidirectional long short-term memory (BLSTM) layers to account for both temporal and spectral variation and model contextual content. Experimental results of predicting valence and arousal on the SEMAINE database and the RECOLA database show that the proposed model significantly outperforms model using hand-engineered features, by exploiting waveforms and spectrograms as input. We also compare the effects of waveforms vs. spectrograms and find that waveforms are better at capturing arousal, while spectrograms are better at capturing valence. Moreover, combining information from both inputs provides further improvement to the performance.",0.0,4.824999999999999
124,1153,5.2    5.1    5.15    3.85    ,State Gradients for RNN Memory Analysis,"We present a framework for analyzing what the state in RNNs remembers from its input embeddings. Our approach is inspired by backpropagation, in the sense that we compute the gradients of the states with respect to the input embeddings. The gradient matrix is decomposed with Singular Value Decomposition to analyze which directions in the embedding space are best transferred to the hidden state space, characterized by the largest singular values. We apply our approach to LSTM language models and investigate to what extent and for how long certain classes of words are remembered on average for a certain corpus. Additionally, the extent to which a specific property or relationship is remembered by the RNN can be tracked by comparing a vector characterizing that property with the direction(s) in embedding space that are best preserved in hidden state space.",0.0,4.825
709,2473,4.25    5.1    5.75    4.2    ,ISI ASR System for the Low Resource Speech Recognition Challenge for Indian Languages,"This paper describes the ISI ASR system used to generate ISI's submissions across Gujarati, Tamil and Telugu speech recognition tasks as part of the Low Resource Speech Recognition Challenge for Indian Languages. The key constraints on this task were limited training data, and the restriction that no external data be used. The ISI ASR system leverages our earlier work on data augmentation and dropout approaches and current work on multilingual training within a Eesen based end-to-end Long Short Term Memory (LSTM) based automatic speech recognition (ASR) system trained with the Connectionist Temporal Classification (CTC) loss criterion, and demonstrates, to the best of our knowledge, one of the first times such systems have been applied to low resource languages with performance comparable and some cases better than hybrid DNN systems. 

Our best monolingual systems show between 6.5% to 25.5% relative reduction in word error rate (WER) compared to the challenge organizer's Time Delay Neural Network (TDNN) based baseline WERs. We further extend these systems with multilingual training approaches that lead to an additional 4.5% to 11.1% relative reduction in WER as measured on the development set.",0.0,4.825
601,2187,5.25    5.2    5.15    3.7    ,Monitoring Infant’S Emotional Cry in Domestic Environments Using the Capsule Network Architecture,"Automated recognition of an infant's cry from audio can be considered as a preliminary step for the applications like remote baby monitoring. In this paper, we implemented a recently introduced deep learning topology called capsule network (CapsNet) for the cry recognition problem. A capsule in the CapsNet, which is defined as a new representation, is a group of neurons whose activity vector represents the probability that the entity exists. Active capsules at one level make predictions, via transformation matrices, for the parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We employed spectrogram representations from the short segments of an audio signal as an input of the CapsNet. For experimental evaluations, we apply the proposed method on INTERSPEECH 2018 computational paralinguistics challenge (ComParE), crying sub-challenge, which is a three-class classification task using an annotated database (CRIED). Provided audio samples contains recordings from 20 healthy infants and categorized into the three classes namely neutral, fussing and crying. We show that multi-layer CapsNet outperforms baseline performance on CRIED corpus and is considerably better than a conventional convolutional net.",0.0,4.825
240,1371,5.25    5.1    5.35    3.6    ,Impact of Aliasing on Deep CNN-Based End-to-End Acoustic Models,"A recent trend in audio and speech processing is to learn target labels directly from raw waveforms rather than hand-crafted acoustic features. Previous work has shown that deep convolutional neural networks (CNNs) as front-end can learn effective representations from the raw waveform. However, due to the large dimension of raw audio waveforms, pooling layers are usually used aggressively between temporal convolutional layers. In essence, these pooling layers perform operations that are similar to signal downsampling, which may lead to temporal aliasing according to the Nyquist-–Shannon sampling theorem. This paper explores, using a series of experiments, if and how this aliasing effect impacts modern deep CNN-based models.",0.0,4.825
459,1823,4.25    4.35    5.9    ,Gaussian Process Neural Networks for Speech Recognition,"Deep neural networks (DNNs) play an important role in state-of-the-art speech recognition systems. One important issue associated with DNNs and artificial neural networks in general is the selection of suitable model structures, for example, the form of hidden node activation functions to use. Due to lack of automatic model selection techniques, the choice of activation functions has been largely empirically based. In addition, the use of deterministic, fixed-point parameter estimates is prone to over-fitting when given limited training data. In order to model both models’ structural and parametric uncertainty, a novel form of DNN architecture using non-parametric activation functions based on Gaussian process (GP), Gaussian process neural networks (GPNN), is proposed in this paper. Initial experiments conducted on the ARPA Resource Management task suggest that the proposed GPNN acoustic models outperformed the baseline sigmoid activation based DNN by 3.40% to 24.25% relatively in terms of word error rate. Consistent performance improvements over the DNN baseline were also obtained by varying the number of hidden nodes and the number of spectral basis functions.",0.0,4.833333333333333
435,1769,5.2    4.25    5.05    ,Deep Discriminative Embeddings for Duration Robust Speaker Verification,"The embedding-based deep convolution neural networks (CNNs) have demonstrated effective for text-independent speaker verification systems with short utterances. However, the duration robustness of the existing deep CNNs based algorithms has not been investigated when dealing with utterances of arbitrary duration. To improve robustness of embedding-based deep CNNs for longer duration utterances, we propose a novel algorithm to learn more discriminative utterance-level embeddings based on the Inception-ResNet speaker classifier. Specifically, the discriminability of embeddings is enhanced by reducing intra-speaker variation with center loss, and simultaneously increasing inter-speaker discrepancy with softmax loss. To further improve system performance when long utterances are available, at test stage long utterances are segmented into shorter ones, where utterance-level speaker embeddings are extracted by an average pooling layer. Experimental results show that when cosine distance is employed as the measure of similarity for a trial, the proposed method outperforms ivector/PLDA framework for short utterances and is effective for long utterances.",0.0,4.833333333333333
496,1908,5.1    5.05    5.75    3.45    ,Revealing Spatiotemporal Brain Dynamics of Speech Production Based on EEG and Eye Movement,"To understand the neural circuitry associated with speech production in oral reading, it is essential to describe the whole-range spatiotemporal brain dynamics in the processes including visual word recognition, orthography-phonology mapping, semantic accessing, speech planning, articulation, self-monitoring, etc. This has turned out to be extremely difficult because of demanding resolution in both spatial and temporal domains and advanced algorithms to eliminate severe contamination by articulatory movements. To tackle this hard target, we recruited 16 subjects in a sentence reading task and measured multimodal signals of electroencephalography (EEG), eye movement, and speech simultaneously. The onset/offset of gazing and utterance were used for segmenting brain activation stages. Cortical modeling of causal interactions among anatomical regions was conducted on EEG signals through (i) independent component analysis to identify cortical regions of interest (ROIs); (ii) multivariate autoregressive modeling of representative cortical activity from each ROI; and (iii) quantification of the dynamic causal interactions among ROIs using the Short-time direct Directed Transfer function. The resulting brain dynamic model reveals a widely connected bilateral organization with left-lateralized semantic, orthographic and phonological sub-networks, right-lateralized prosody and motor sequencing sub-networks, and bi-lateralized auditory and multisensory integration sub-networks that cooperate along interlaced and paralleled temporal stages for speech processing.",0.0,4.8374999999999995
544,2029,4.35    4.8    4.2    6    ,Classification of Huntington's Disease Using Acoustic and Lexical Features,"Speech is a critical biomarker for Huntington Disease (HD), with changes in speech increasing in severity as the disease progresses.  Speech analyses are currently conducted using either transcriptions created manually by trained professionals or using global rating scales.  Manual transcription is both expensive and time-consuming and global rating scales may lack sufficient sensitivity and fidelity. Ultimately, what is needed is an unobtrusive measure that can cheaply and continuously track disease progression.  We present first steps towards the development of such a system, demonstrating the ability to automatically differentiate between healthy controls and individuals with HD using speech cues.  The results provide evidence that objective analyses can be used to support clinical diagnoses, moving towards the tracking of symptomatology outside of laboratory and clinical environments.",0.0,4.8374999999999995
40,1010,4.9    3.55    5.75    5.15    ,Learning Acoustic Word Embeddings with Temporal Context for Query-by-Example Speech Search,"We propose to learn acoustic word embeddings with temporal context for query-by-example (QbE) speech search. The temporal context includes the leading and trailing word sequences of a word. We assume that there exist spoken word pairs in the training database. We pad the word pairs with their original temporal context to form fixed-length speech segment pairs. We obtain the acoustic word embeddings through a deep convolutional neural network (CNN) which is trained on the speech segment pairs with a triplet loss. By shifting a fixed-length analysis window through the search content, we obtain a running sequence of embeddings. In this way, searching for the spoken query is equivalent to the matching of acoustic word embeddings. The experiments show that our proposed acoustic word embeddings learned with temporal context are effective in QbE speech search. They outperform the state-of-the-art frame-level feature representations and reduce run-time computation since no dynamic time warping is required in QbE speech search. We also find that it is important to have sufficient speech segment pairs to train the deep CNN for effective acoustic word embeddings.",0.0,4.8375
152,1222,5.1    5.65    3.5    5.1    ,An Active Feature Transformation Method for Attitude Recognition of Video Bloggers,"Video blogging is a form of unidirectional communication where a video blogger expresses his/her opinion about different issues. The success of a video blog is measured using metrics like the number of views and comments by online viewers. Researchers have highlighted the importance of non-verbal behaviours (e.g. attitudes) in the context of video blogging and showed that it correlates with the level of attention (number of views) gained by a video blog. Therefore, an automatic attitude recognition system can help potential video bloggers to train their attitudes. It can also be useful in developing video blogs summarization and search tools. This study proposes a novel Active Feature Transformation (AFT) method for automatic recognition of attitudes (a form of non-verbal behaviour) in video blogs. The proposed method transforms the Mel-frequency Cepstral Coefficient (MFCC) features for the classification task. The Principal Component Analysis (PCA) transformation is also used for comparison. Our results show that AFT outperforms PCA in terms of accuracy and dimensionality reduction for attitude recognition using linear discrimination analysis, 1-nearest neighbour and decision tree classifiers.",0.0,4.8375
442,1788,4.2    5.95    3.4    5.8    ,Siamese Recurrent Auto-encoder Representation for Query-by-Example Spoken Term Detection,"With the explosive development of human-computer speech interaction, spoken term detection is widely required and has attracted increasing interest. In this paper, we propose a weak supervised approach using Siamese recurrent auto-encoder (RAE) to represent speech segments for query-by-example spoken term detection (QbyE-STD). The proposed approach exploits word pairs that contain different instances of the same/different word content as input to train the Siamese RAE.  The encoder last hidden state vector of Siamese RAE is used as the feature for QbyE-STD, which is a fixed dimensional embedding feature containing mostly semantic content related information. The advantages of the proposed approach are: 1) extracting more compact feature with fixed dimension while keeping the semantic information for STD; 2) the extracted feature can describe the sequential phonetic structure of similar sounds to degree, which can be applied for zero-resource QbyE-STD.  Evaluations on real scene Chinese speech interaction data and TIMIT confirm the effectiveness and efficiency of the proposed approach compared to the conventional ones.",0.0,4.8375
691,2432,4.25    5.25    5    4.85    ,Speaker Adaptation and Adaptive Training for Jointly Optimised Tandem Systems,"Speaker independent (SI) Tandem systems trained by joint optimisation of bottleneck (BN) deep neural networks (DNNs) and Gaussian mixture models (GMMs) have been found to produce similar word error rates (WERs) to Hybrid DNN systems. A key advantage of using GMMs is that existing speaker adaptation methods, such as maximum likelihood linear regression (MLLR), can be used which to account for diverse speaker variations and improve system robustness. This paper investigates speaker adaptation and adaptive training (SAT) schemes for jointly optimised Tandem systems. Adaptation techniques investigated include constrained MLLR (CMLLR) transforms based on BN features for SAT as well as MLLR and parameterised sigmoid functions for unsupervised test-time adaptation. Experiments using English multi-genre broadcast (MGB3) data show that CMLLR SAT yields a 4% relative WER reduction over jointly trained Tandem and Hybrid SI systems, and further reductions in WER are obtained by system combination.",0.0,4.8375
504,1933,4.3    4.95    5    5.1    ,Demonstrating and Modelling Systematic Time-varying Annotator Disagreement in Continuous Emotion Annotation,"Continuous emotion recognition (CER) is the task of determining the emotional content of speech from audio or multimedia recordings. Training targets for machine learning must be generated by human annotation, generally as a time series of emotional parameter values. In typical contemporary CER systems and challenges, the mean over a pool of annotators is taken to represent this ground truth, but is this an appropriate model for the emotional content of speech? Using the RECOLA dataset, the primary contribution of this research is to show that a correlation exists between the time-varying disagreement from independent groups of annotators. Because the groups are completely isolated except via the speech signal, this agreement-about-disagreement demonstrates that there is a component of annotator disagreement which arises systematically from the signal itself, which qualitatively implies that the perceived emotional content of speech can exhibit some degree of inherent ambiguity. Additionally, we show that these human annotations exhibit a degree of temporal smoothness. Neither of these characteristics is represented by the standard series-of-means ground-truth model, so we propose two alternative ground-truth models: a mean-variance model that incorporates ambiguity, and a more general Gaussian process model that incorporates ambiguity and temporal smoothness in a well-defined probability distribution.",0.0,4.8375
710,2474,3.4    5.1    5.8    5.05    ,An Investigation of Non-linear I-vectors for Speaker Verification,"Speaker verification becomes increasingly important due to the popularity of speech assistants and smart home. i-vectors are used broadly for this topic, which use factor analysis to model the shift of average parameter in Gaussian Mixture Models. Recently by the progress of deep learning, high-level non-linearity improves results in many areas. In this paper we proposed a new framework of i-vectors which uses stochastic gradient descent to solve the problem of i-vectors. From our preliminary results stochastic gradient descent can get same performance as expectation-maximization algorithm. However, by backpropagation the assumption can be more flexible, so both linear and non-linear assumption is possible in our framework. From our result, both maximum a posteriori estimation and maximum likelihood lead to slightly better result than conventional i-vectors and both linear and non-linear system has similar performance.",0.0,4.8375
625,2269,5    5    4.95    4.4    ,Deep Personality Recognition for Deception Detection,"Researchers in both psychology and computer science have suggested that modeling individual differences may improve the performance of automatic deception detection systems. In this study, we fuse a personality classifier with a deception classifier and evaluate various ways to combine the two tasks, either as a single network with shared layers, or by feeding the results of the personality classifier into the deception classifier. We show that including personality recognition improves the performance of deception detection on the Columbia Cross-Cultural Deception (CXD) corpus by more than 6% relative, achieving new state-of-the-art results on classification of phrase-like units in this corpus.",0.0,4.8375
294,1472,5.85    4.15    4.2    5.15    ,BLSTM-CRF Based End-to-End Prosodic Boundary Prediction with Context Sensitive Embeddings in a Text-to-Speech Front-End,"In this paper, we propose a language-independent end-to-end architecture for prosodic boundary prediction based on BLSTM-CRF. The proposed architecture has three components, word embedding layer, BLSTM layer and CRF layer. The word embedding layer is employed to learn the task-specific embeddings for prosodic boundary prediction. The BLSTM layer can efficiently use both past and future input features, while the CRF layer can efficiently use sentence level information. We integrate these three components and learn the whole process end-to-end. In addition, we investigate both character-level embeddings and context sensitive embeddings to this model, and employ an attention mechanism for combining alternative word-level embeddings. By using an attention mechanism, the model is able to decide how much information to use from each level of embeddings. Objective evaluation results show the proposed BLSTM-CRF architecture achieves the best results on both Mandarin and English datasets, with an absolute improvement of 3.21% and 3.74% in F1 score, respectively, for intonational phrase prediction, compared to previous state-of-the-art method (BLSTM). The subjective evaluation results further indicate the effectiveness of the proposed methods.",0.0,4.8375
564,2080,5.15    4.75    5.2    4.25    ,Investigating Objective Intelligibility in Real-Time EMG-to-Speech Conversion,"This paper presents an analysis of the influence of various system parameters on the output quality of our neural network based real-time EMG-to-Speech conversion system. This EMG-to-Speech system allows for the direct conversion of facial surface electromyographic signals into audible speech in real time, allowing for a closed-loop setup where users get direct audio feedback. Such a setup opens new avenues for research and applications through co-adaptation approaches.
In this paper, we evaluate the influence of several parameters on the output quality, such as time context, EMG-Audio delay, network-, training data- and Mel spectrogram size. The resulting output quality is evaluated based on the objective output quality measure STOI.",0.0,4.8375
317,1522,4.2    5    5.2    5    ,All-Conv Net for Bird Activity Detection: Significance of Learned Pooling,"Bird activity detection (BAD) deals with the task of predicting
the presence or absence of bird vocalizations in a given audio
recording. In this paper, we propose an all-convolutional neural
network (all-conv net) for bird activity detection. All the
layers of this network including pooling and dense layers are
implemented using convolution operations. The pooling operation
implemented by convolution is termed as learned pooling.
This learned pooling takes into account the inter feature-map
correlations which are ignored in traditional max-pooling. This
helps in learning a pooling function which aggregates the complementary
information in various feature maps, leading to better
bird activity detection. Experimental observations confirm
this hypothesis. The performance of the proposed all-conv net
is evaluated on BAD Challenge 2017 dataset. The proposed
all-conv net achieves state-of-art performance with a simple architecture
and does not employ any data pre-processing or data
augmentation techniques.",0.0,4.85
150,1212,5.1    5.85    4.9    3.55    ,Segmental Encoder-Decoder Models for Large Vocabulary Automatic Speech Recognition,"It has been known for a long time that the classic Hidden-Markov-Model (HMM) derivation for speech recognition contains assumptions such as independence of observation vectors and weak duration modeling that are practical but unrealistic. When using the hybrid approach this is amplified by trying to fit a discriminative model into a generative one. Hidden Conditional Random Fields (CRFs) and segmental models (e.g. Semi-Markov CRFs / Segmental CRFs) have been proposed as an alternative, but for a long time have failed to get traction until recently. In this paper we explore different length modeling approaches for segmental models, their relation to attention-based systems. Furthermore we show experimental results on a handwriting recognition task and to the best of our knowledge the first reported results on the Switchboard 300h speech recognition corpus using this approach.",0.0,4.85
167,1242,4.1    5.1    5.2    5    ,An Attention Pooling Based Representation Learning Method for Speech Emotion Recognition,"This paper proposes an attention pooling based representation
learning method for speech emotion recognition (SER). The
emotional representation is learned in an end-to-end fashion by
applying a deep convolutional neural network (CNN) directly
to spectrograms extracted from speech utterances. Motivated
by the success of GoogLeNet, two groups of filters with different
shapes are designed to capture both temporal and frequency
domain context information from the input spectrogram.
The learned features are concatenated and fed into the subsequent
convolutional layers. To learn the final emotional representation,
a novel attention pooling method is further proposed.
Compared with the existing pooling methods, such as
max-pooling and average-pooling, the proposed attention pooling
can effectively incorporate class-agnostic bottom-up, and
class-specific top-down, attention maps. We conduct extensive
evaluations on benchmark IEMOCAP data to assess the effectiveness
of the proposed representation. Results demonstrate a
recognition performance of 71.8% weighted accuracy (WA) and
68% unweighted accuracy (UA) over four emotions, which outperforms
the state-of-the-art method by about 3% absolute for
WA and 4% for UA.",0.0,4.85
281,1450,4.8    4.7    5    4.9    ,Improving DNNs Trained with Non-Native Transcriptions Using Knowledge Distillation and Target Interpolation,"Often, it is quite hard to find native transcribers in under-resourced languages. However, Turkers (crowd workers) available in online marketplaces can serve as valuable alternative resources by providing transcriptions in the target language. Since the Turkers may neither speak nor have any familiarity with the target language, their transcriptions are non-native by nature and are usually filled with incorrect labels. After some post-processing, these transcriptions can be converted to Probabilistic Transcriptions (PT). Conventional Deep Neural Networks (DNN) trained using PTs do not necessarily improve error rates over Gaussian Mixture Models (GMMs) due to the presence of label noise. Previously reported results have demonstrated some success by adopting Multi-Task Learning (MTL) training for PTs. In this study, we report further improvements using Knowledge Distillation (KD) and Target Interpolation (TI) to alleviate transcription errors in PTs. In the KD method, knowledge is transfered from a well-trained multilingual DNN to the target language DNN trained using PTs. In the TI method, the confidences of the labels provided by PTs are modified using the confidences of the target language DNN. Results show an average absolute improvement in phone error rates (PER) by about 1.9% across Swahili, Amharic, Dinka, and Mandarin using each proposed method.",0.0,4.85
330,1544,5.25    4.95    5.1    4.1    ,Gated Recurrent Unit Based Acoustic Modeling with Future Context,"The use of future contextual information is typically shown to be helpful for acoustic modeling. However, for the recurrent neural network (RNN), it's not so easy to model the future temporal context effectively, meanwhile keep lower model latency. In this paper, we attempt to design a RNN acoustic model that being capable of utilizing the future context effectively and directly, with the model latency and computation cost as low as possible. The proposed model is based on the minimal gated recurrent unit (mGRU) with an input projection layer inserted in it. Two context modules, temporal encoding and temporal convolution, are specifically designed for this architecture to model the future context. Experimental results on the Switchboard task and an internal Mandarin ASR task show that, the proposed model performs much better than long short-term memory (LSTM) and mGRU models, whereas enables online decoding with a maximum latency of 170 ms. This model even outperforms a very strong baseline, TDNN-LSTM, with smaller model latency and almost half less parameters.",0.0,4.85
523,1973,3.35    5.1    5.9    5.05    ,Phonological Posterior Hashing for Query by Example Spoken Term Detection,"State of the art query by example spoken term detection (QbE-STD) systems in zero-resource conditions rely on representation of speech in terms of sequences of class-conditional posterior probabilities estimated by deep neural network (DNN). The posteriors are often used for pattern matching or dynamic time warping (DTW). Exploiting posterior probabilities as speech representation propounds diverse advantages in a classification system. One key property of the posterior representations is that they admit a highly effective hashing strategy that enables indexing a large audio archive in divisions for reducing the search complexity. Moreover, posterior indexing leads to a compressed representation and enables pronunciation dewarping and partial detection with no need for DTW. We exploit these characteristics of the posterior space in the context of redundant hash addressing for query-by-example spoken term detection (QbE-STD). We evaluate the QbE-STD system on AMI corpus and demonstrate that tremendous speedup and superior accuracy is achieved compared to the state-of-the-art pattern matching solution based on DTW. The system has the potential to enable massively large scale spoken query detection.",0.0,4.85
302,1486,5.15    5    5.1    4.15    ,Densely Connected Networks for Conversational Speech Recognition,"In this paper we show how we have achieved the state-of-the-art performance on the industry-standard NIST 2000 Hub5 English evaluation set. We propose densely connected LSTMs (namely, dense LSTMs), inspired by the densely connected convolutional neural networks recently introduced for image classification tasks. It is shown that the proposed dense LSTMs would provide more reliable performance as compared to the conventional, residual LSTMs as more LSTM layers are stacked in neural networks. With RNN-LM rescoring and lattice combination on the 5 systems (including 2 dense LSTM based systems) trained across three different phone sets, Capio's conversational speech recognition system has obtained 5.0% and 9.1% on Switchboard and CallHome, respectively.",0.0,4.85
125,1154,4.9    5.9    4.35    4.25    ,Waveform-Based Speaker Representations for Speech Synthesis,"Speaker adaptation is a key aspect of building a range of speech processing systems, for example personalised speech synthesis. For deep-learning based approaches, the model parameters are hard to interpret, making speaker adaptation more challenging. One widely used method to address this problem is to extract a fixed length vector as speaker representation, and use this as an additional input to the task-specific model. This allows speaker-specific output to be generated, without modifying the model parameters. However, the speaker representation is often extracted in a task-independent fashion. This allows the same approach to be used for a range of tasks, but the extracted representation is unlikely to be optimal for the specific task of interest. Furthermore, the features from which the speaker representation is extracted are usually pre-defined, often a standard speech representation. This may limit the available information that can be used. In this paper, an integrated optimisation framework for building a task specific speaker representation, making use of all the available information, is proposed. Speech synthesis is used as the example task. The speaker representation is derived from raw waveform, incorporating text information via an attention mechanism. This paper evaluates and compares this framework with standard task-independent forms.",0.0,4.85
468,1841,5.15    5.1    4.2    4.95    ,Estimation of the Number of Speakers with Variational Bayesian PLDA in the DIHARD Diarization Challenge.,"This paper focuses on the estimation of the number of speakers for diarization in the context of the DIHARD Challenge at InterSpeech 2018. This evaluation seeks the improvement of the diarization task in challenging corpora (Youtube videos, meetings, court audios, etc), containing an undetermined number of speakers with different relevance in terms of speech contributions. 
Our proposal for the challenge is a system based on the i-vector PLDA paradigm: Given some initial segmentation of the input audio  we extract i-vector representations for each acoustic fragment. These i-vectors are clustered with a Fully Bayesian PLDA. This model, a generative model with latent variables as speaker labels, produces the diarization labels by means of Variational Bayes iterations. The number of speakers is decided by comparing multiple hypotheses according to different information criteria. These criteria are developed around the Evidence Lower Bound (ELBO) provided by our PLDA.",0.0,4.85
4,41,5    4.9    4.75    4.75    ,Improved Supervised Locality Preserving Projection for I-vector Based Speaker Verification,"A Supervised Locality Preserving Projection (SLPP) method is employed for
channel compensation in an i-vector based speaker verification system. SLPP
preserves more important local information by weighing both the within- and
between-speaker nearby data pairs based on the similarity matrices. In this
paper, we propose an improved SLPP (P-SLPP) to enhance the channel compensation
ability. First, the conventional Euclidean distance in conventional SLPP is
replaced with Probabilistic Linear Discriminant Analysis (PLDA) scores.
Furthermore, the weight matrices of P-SLPP are generated by using the relative
PLDA scores of within- and between-speaker pairs. Experiments are carried out
on the five common conditions of NIST 2012 speaker recognition evaluation (SRE)
core sets. The results show that SLPP and the proposed P-SLPP outperform all
other state-of-the-art channel compensation methods. Among these methods,
P-SLPP achieves the best performance.",0.0,4.85
619,2246,5    4.25    5.05    5.1    ,Domain Adaptation Using Factorized Hidden Layer for Robust Automatic Speech Recognition,"Domain robustness is a challenging problem for automatic speech recognition (ASR). In this paper, we consider speech data collected for different applications as separate domains and investigate the robustness of acoustic models trained on multi-domain data on unseen domains. Specifically, we use Factorized Hidden Layer (FHL) as a compact low-rank representation to adapt a multi-domain ASR system to unseen domains. Experimental results on two unseen domains show that FHL is a more effective adaptation method compared to selectively fine-tuning part of the network, without dramatically increasing the model parameters. Furthermore, we found that using singular value decomposition to initialize the low-rank bases of an FHL model leads to a faster convergence and improved performance.",0.0,4.85
402,1706,5.2    4.75    5.25    4.2    ,Improving Mongolian Phrase Break Prediction by Using Syllable and Morphological Embeddings with BiLSTM Model,"In the speech synthesis systems, the phrase break (PB) prediction is the first and most important step. Recently, the state-of-the-art PB prediction systems mainly rely on word embeddings. However this method is not fully applicable to Mongolian language, because its word embeddings are inadequate trained, owing to the lack of resources. In this paper, we introduce a bidirectional Long Short Term Memory (BiLSTM) model which combined word embeddings with syllable and morphological embedding representations to provide richer and multi-view information which leverages the agglutinative property. Experimental results show the proposed method outperforms compared systems which only used the word embeddings. In addition, further analysis shows that it is quite robust to the Out-of-Vocabulary (OOV) problem owe to the refined word embedding. The proposed method achieves the state-of-the-art performance in the Mongolian PB prediction.",0.0,4.85
740,2566,4.45    5.1    4.9    4.95    ,Efficient Implementation of the Room Simulator for Training Deep Neural Network Acoustic Models,"In this paper, we describe how to efficiently implement an acoustic room simulator to generate large-scale simulated data for training deep neural networks. Even though Google Room Simulator in [1] was shown to be quite effective in reducing the Word Error Rates (WERs) for far-field applications by generating simulated far-field training sets, it requires a very large number of FFTs. Room Simulator used approximately 80 % of CPU usage in our CPU/GPU training architecture [2]. In this work, we implement an efficient OverLap Addition (OLA) based filtering using the open-source FFTW3 library. Further, we investigate the effects of the Room Impulse Response (RIR) lengths. Experimentally, we conclude that we can cut the tail portions of RIRs whose power is less than 20 dB below the maximum power without sacrificing the speech recognition accuracy. However, we observe that cutting RIR tail more than this threshold harms the speech recognition accuracy for rerecorded test sets. Using these approaches, we were able to reduce CPU usage for the room simulator portion down to 9.69 % in CPU/GPU training architecture. Profiling result shows that we obtain 22.4 times speed-up on a single machine and 37.3 times speed up on Google’s distributed training infrastructure.",0.0,4.8500000000000005
466,1836,5    5.55    4    ,Automatic Speech Recognition and Topic Identification from Speech for Almost-Zero-Resource Languages,"Automatic speech recognition (ASR) systems often need to be developed for extremely low-resource languages to serve end-uses such as audio content categorization and search. While universal phone recognition is natural to consider when no transcribed speech is available to train an ASR system in a language, adapting universal phone models using very small amounts (minutes rather than hours) of transcribed speech also needs to be studied, particularly with state-of-the-art DNN-based acoustic models.  The DARPA LORELEI program provides a framework for such very-low-resource ASR studies, and provides an extrinsic metric for evaluating ASR performance in a humanitarian assistance, disaster relief setting. This paper presents our Kaldi-based systems for the program, which employ a universal phone modeling approach to ASR, and describes recipes for very rapid adaptation of this universal ASR system. The results we obtain significantly outperform results obtained by many competing approaches on the NIST LoReHLT 2017 Evaluation datasets.",0.0,4.8500000000000005
115,1138,5.05    4.4    5    5    ,Multi-modal Attention Mechanisms in LSTM and Its Application to Acoustic Scene Classification,"Neural network architectures such as long short-term memory (LSTM) have been proven to be powerful models for processing sequences including text, audio and video. On the basis of vanilla LSTM, multi-modal attention mechanisms are proposed in this paper to synthesize the time and semantic information of input sequences. First, we reconstruct the forget and input gates of the LSTM unit from the perspective of attention model in the temporal dimension. Then the memory content of the LSTM unit is recalculated using a cluster-based attention mechanism in semantic space. Experiments on acoustic scene classification tasks show performance improvements of the proposed methods when compared with vanilla LSTM. The classification errors on LITIS ROUEN dataset and DCASE2016 dataset are reduced by 16.5% and 7.7% relatively. We get a second place in the Kaggle's YouTube-8M video understanding challenge, and multi-modal attention based LSTM model is one of our best-performing single systems.",0.0,4.8625
217,1320,5.15    5.1    6    3.2    ,Prosodic Focus Acquisition in French Early Cochlear Implanted Children,"Cochlear implanted (CI) children display an array of speech production and perception problems. No study has evaluated the specific use of prosody regarding information structure in the discourse, in French speaking early CI children. 

This study aims to evaluate prosody production in these children, to determine whether they show prosodic effect on word duration.

We conducted a cross-sectional study of 10 prelingually hearing impaired French speaking children (4-7 years old), without comorbidities, CI before the age of 18 months between 2009 and 2012. The speech production task consisted in playing a computer-based semi-structured game, where children interacted with their caregiver. Results were interpreted according to both chronological age and hearing age (HA).

In our series, 6- and 7-year old children (HA<6.2 years) showed stronger lengthening of the focused word in the corrective narrow focus condition than in the contrastive narrow focus which in turn was stronger than in broad focus condition. Only 7-year old children adopted a strategy similar to that of adults, lengthening the end-phrase adjective to preserve the typical phrasing pattern of French.

This study shows for the first time that early CI children are able to acquire important intonation structure features comparable to adult patterns.",0.0,4.8625
553,2057,5    4.95    5.25    4.25    ,Subword and Crossword Units for CTC Acoustic Models,"This paper proposes a novel approach to create a unit set for CTC based speech recognition systems. By using Byte Pair Encoding we learn a unit set of an arbitrary size on a given training text. In contrast to using characters or words as units this allows us to find a good trade-off between the size of our unit set and the available training data. We investigate both Crossword units, that may span multiple word, and Subword units. By evaluating these unit sets with decodings methods using a separate language model we are able to show improvements over a purely character based unit set.",0.0,4.8625
94,1103,5.15    5.05    5.05    4.2    ,Double Joint Bayesian Modeling of DNN Local I-Vector for Text Dependent Speaker Verification with Random Digit Strings,"Double joint Bayesian is a recently introduced analysis method that models and explores multiple information explicitly from the samples to improve the verification performance. It was recently applied to voice pass phrase verification, result in better results on text dependent speaker verification task. However little is known about its effectiveness in other challenging situations such as speaker verification for short, text-constrained test utterances, e.g. random digit strings. Contrary to conventional joint Bayesian method that cannot make full use of multi-view information, double joint Bayesian can incorporate both intra-speaker/digit and inter-speaker/digit variation, and calculated the likelihood to describe whether the features having all labels consistent or not. We show that double joint Bayesian outperforms conventional method on modeling DNN local (digit-dependent) i-vectors for speaker verification with random prompted digit strings. Since the strength of both double joint Bayesian and conventional DNN local i-vector appear complementary, the combination significantly outperforms either of its components.",0.0,4.8625
969,1512,6    5.75    4.05    3.65    ,Speaker Adaptation for End-to-End Models,"End-to-end (E2E) systems are emerging trends in automatic speech recognition (ASR), which allow for capturing long range context and optimizing all the parameters in the whole system using a single criterion. The excessive reliance on the immense training data presents a critical challenge for speaker adaptation of the E2E systems. In this work, we present our studies in speak adaptation of acoustic-to-word connectionist temporal classification (CTC) system. We propose two approaches, Kullback-Leibler (KL) regularization and multi-task learning (MTL), to address the data sparsity issue in the speaker adaptation. Experiments on the short message dictation (SMD) task demonstrated that the MTL adaptation outperformed the KL regularization. In particular, the MTL adaptation produced  8.8% and 4.0% relative word error rate reduction over the baseline for supervised and unsupervised adaptation, respectively.",1.0,4.8625
8,46,4.65    5    5.7    4.1    ,Dithered Quantization for Frequency-Domain Speech and Audio Coding,"A common issue in coding speech and audio in the frequency domain, which appears with decreasing bitrate, is that quantization levels become increasingly sparse. With low accuracy,  high-frequency components are typically quantized to zero, which leads to a muffled output signal and musical noise. Band-width extension and noise-filling methods attempt to treat the problem by inserting noise of similar energy as the original signal, at the cost of low signal to noise ratio. Dithering methods however provide an alternative approach, where both accuracy and energy are retained. We propose a hybrid coding approach where low-energy samples are quantized using dithering, instead of the conventional uniform quantizer. For dithering, we apply 1\,bit quantization in a randomized sub-space. We further show that the output energy can be adjusted to the desired level using a scaling parameter. Objective measurements and listening tests demonstrate the advantages of the proposed methods.",0.0,4.862500000000001
319,1524,5.1    5.95    4.9    3.5    ,Deep Convolutional Neural Network with Scalogram for Audio Scene Modeling,"Deep learning has improved the performance of acoustic scene classification recently. However, learning is usually based on short-time Fourier transform and hand-tailored filters. Learning directly from raw signals has remained a big challenge. In this paper, we proposed an approach to learning audio scene patterns from scalogram, which is extracted from raw signal with simple wavelet transforms. The experiments were conducted on DCASE2016 dataset. We compared scalogram with classical Mel energy, which showed that multi-scale feature led to an obvious accuracy increase. The convolutional neural network integrated with maximum-average downsampled scalogram achieved an accuracy of 90.5% in the evaluation step in DCASE2016.",0.0,4.862500000000001
205,1301,5.1    4.45    4.85    5.1    ,Multi-channel Attention for End-to-End Speech Recognition,"Recent end-to-end models for automatic speech recognition use sensory attention to integrate multiple input channels within a single neural network. However, these attention models are sensitive to the ordering of the channels used during training. This work proposes a sensory attention mechanism that is invariant to the channel ordering and only increases the overall parameter count by 0.09%. We demonstrate that even without re-training, our attention-equipped end-to-end model is able to deal with arbitrary numbers of input channels during inference. In comparison to a recent related model with sensory attention, our model when tested on the real noisy recordings from the multi-channel CHiME-4 dataset, achieves a relative character error rate (CER) improvement of 40.3% to 42.9%. In a two-channel configuration experiment, the attention signal allows the lower signal-to-noise ratio (SNR) sensor to be identified with 97.7% accuracy.",0.0,4.875
697,2443,4.5    4.9    5.05    5.05    ,Acoustic-Prosodic Indicators of Deception and Trust in Interview Dialogues,"We analyze a set of acoustic-prosodic features in both truthful and deceptive responses to interview questions, identifying differences between truthful and deceptive speech.  We also study the perception of deception, identifying acoustic-prosodic characteristics of speech that is perceived as truthful or deceptive by interviewers. In addition to studying differences across all speakers, we identify individual variations in deception production and perception across gender and native language. 
We conduct machine learning classification experiments aimed at distinguishing between truthful and deceptive speech, using acoustic-prosodic features.  We also explore methods of leveraging individual traits for deception classification.  Our results show that acoustic-prosodic features are highly effective at classifying deceptive speech. Our best classifier achieved an F1-score of 72.77, well above both the random baseline and above human performance at this task.
  This work advances our understanding of deception production and perception, and has implications for automatic deception detection and the development of synthesized speech that is trustworthy.",0.0,4.875
566,2082,4.25    6    4.2    5.1    ,Memory Time Span in LSTMs for Multi-Speaker Source Separation,"With deep learning approaches becoming state-of-the-art in many speech (as well as non-speech) related machine learning tasks, efforts are being taken to delve into the neural networks which are often considered as a black box. In this paper it is analyzed how recurrent neural network (RNNs) cope with temporal dependencies by determining the relevant memory time span in a long short-term memory (LSTM) cell. This is done by leaking the state variable with a controlled lifetime and evaluating the task performance. This technique can be used for any task to estimate the time span the LSTM exploits in that specific scenario. The focus in this paper is on the task of separating speakers from overlapping speech. We discern two effects: A long term effect, probably due to speaker characterization and a short term effect, probably exploiting phone-size formant tracks.",0.0,4.887499999999999
433,1766,5    5.2    4.1    5.25    ,"Investigation on Estimation of Sentence Probability by Combining Forward, Backward and Bi-directional LSTM-RNNs","A combination of forward and backward long short-term memory (LSTM) recurrent neural network (RNN) language models
is a popular model combination approach to improve the estimation of the sequence probability in the second pass N-best list
rescoring in automatic speech recognition (ASR). In this work,
we further push such an idea by proposing a combination of
three models: a forward LSTM language model, a backward
LSTM language model and a bi-directional LSTM based gap
completion model. We derive such a combination method from
a forward backward decomposition of the sequence probability.
We carry out experiments on the Switchboard speech recognition task. While we empirically find that such a combination
gives slight improvements in perplexity over the combination
of forward and backward models, we finally show that a combination of the same number of forward models gives the best perplexity and word error rate (WER) overall.",0.0,4.887499999999999
5,42,5    4.25    5.1    5.2    ,Attentive Sequence-to-Sequence Learning for Diacritic Restoration of YorùBá Language Text,"Yorùbá is a widely spoken West African language with a writing system rich in tonal and orthographic diacritics. With very few exceptions, diacritics are omitted from electronic texts, due to limited device and application support. Diacritics provide morphological information, are crucial for lexical disambiguation, pronunciation and are vital for any Yorùbá text-to-speech (TTS), automatic speech recognition (ASR) and natural language processing (NLP) tasks. Reframing Automatic Diacritic Restoration (ADR) as a machine translation task, we experiment with two different attentive Sequence-to-Sequence neural models to process undiacritized text. On our evaluation dataset, this approach produces diacritization error rates of less than 5\%. We have released pre-trained models, datasets and source-code as an open-source project to advance efforts on Yorùbá TTS, ASR and NLP.",0.0,4.8875
696,2441,3.5    5.15    5.9    5    ,Investigations on Data Augmentation and Loss Functions for Deep Learning Based Speech-Background Separation,"A successful deep learning-based method for separation of a speech signal from an interfering background audio signal is based on neural network prediction of time-frequency masks which multiply noisy signal's short-time Fourier transform (STFT) to yield the STFT of an enhanced signal. In this paper, we investigate training strategies for mask-prediction-based speech-background separation systems. First, we examine the impact of mixing speech and noise files on the fly during training, which enables models to be trained on virtually infinite amount of data. We also investigate the effect of using a novel signal-to-noise ratio related loss function, instead of mean-squared error which is prone to scaling differences among utterances. We evaluate bi-directional long-short term memory (BLSTM) networks as well as a combination of convolutional and BLSTM (CNN+BLSTM) networks for mask prediction and compare performances of real and complex-valued mask prediction. Data-augmented training combined with a novel loss function yields significant improvements in signal to distortion ratio (SDR) and perceptual evaluation of speech quality (PESQ) as compared to the best published result on CHiME-2 medium vocabulary data set when using a CNN+BLSTM network.",0.0,4.8875
727,2522,5.15    4.35    5.05    5    ,Detecting Depression with Audio/Text Sequence Modeling of Interviews,"Medical professionals diagnose depression by interpreting the responses of individuals to a variety of questions, probing lifestyle changes and ongoing thoughts. Like professionals, an effective automated agent must understand that responses to queries have varying prognostic value. In this study we demonstrate an automated depression-detection algorithm that models interviews between an individual and agent and learns from sequences of questions and answers without the need to perform explicit topic modeling of the content. We utilized data of 142 individuals undergoing depression screening, and modeled the interactions with audio and text features in a Long-Short Term Memory (LSTM) neural network model to detect depression. Our results were comparable to methods that explicitly modeled the topics of the questions and answers which suggests that depression can be detected through sequential modeling of an interaction, with minimal information on the structure of the interview.",0.0,4.8875
410,1725,5.65    5    4.85    4.05    ,The Role of Temporal Variation in Narrative Organization,"The aim of this study was to see if temporal variation can be considered a robust cue in the discourse structuring process. If so, at what level (s) of the discursive structure does it operate? In a bottom-up corpus-based approach, we analyze a 58-minute corpus of 60 natural French speech narratives. First, the corpus was segmented at the phonemic, syllabic, lexical and inter-pausal unit levels. Second, a narrative segmentation was applied using the criteria of Labov’s evaluative model, which is based on semantic and informational criteria. Duration data was then extracted automatically at each level of granularity. The mapping of discourse segmentation to acoustic-phonetic analyses was made on two structural levels: micro and macro. A positive effect of local temporal variation in discourse structuring was not found, however, the existence of a link between the narrative internal segmentation and speech rate variation was identified. This variation is long-term, progressive, and gradual which suggests a manipulation of this feature. In relation to the content, temporal values can be seen as contextual cues: relevant information is presented with a slower speech rate; while minor content is presented with a faster speech rate.",0.0,4.8875
216,1319,4.75    5.75    4.35    4.4    5    5.1    ,Anomaly Detection Approach for Pronunciation Verification of Disordered Speech Using Speech Attribute Features,"The automatic assessment of speech is a powerful tool in computer aided speech therapy system. However, the lack of sufficient annotated disordered speech data is a serious impediment to achieving the accurate detection of pronunciation errors. To handle this deficiency, in this paper, we used the novel approach of tackling pronunciation verification as an anomaly detection problem. We achieved this by modeling only the correct pronunciation of each individual phoneme with a one-class SVM trained using a set of speech attributes features, namely the manner and place of articulation. 
These features are extracted from a bank of pre-trained DNN speech attributes classifiers. The one-class SVM model classifies each phoneme production as normal (correct) or an anomaly (incorrect). We evaluated the system using both native speech with artificial errors and disordered speech collected from children with apraxia of speech and compared it with the DNN Goodness of Pronunciation (GOP) algorithm. The results show that our approach reduces the false-rejection rates by around 35% when applied on disordered speech.",0.0,4.891666666666667
598,2174,5.2    4.25    5.2    4.95    ,Neural MultiVoice Models for Expressing Novel Personalities in Dialog,"Natural language generators for task-oriented dialog should be able
to vary the style of the output utterance while still effectively
realizing the system dialog actions and their associated semantics.
While the use of neural generation for training the response
generation component of conversational agents promises to simplify the
process of producing high quality responses in new domains, to our
knowledge, there has been very little investigation of neural
generators for task-oriented dialog that can vary their response
style, and we know of no experiments on models that can generate
responses that are different in style from those seen during training,
while still maintaining semantic fidelity to the input meaning
representation.  Here, we show that a model that is trained to achieve
a single stylistic personality target can produce outputs that combine
stylistic targets. We carefully evaluate the multivoice outputs for
both semantic fidelity and for similarities to and differences from
the linguistic features that characterize the original training
style. We show that contrary to our predictions, the learned models
do not always simply interpolate model parameters, but rather produce
styles that are distinct, and novel from the personalities
they were trained on.",0.0,4.8999999999999995
86,1089,5    5.1    4.6    ,Acoustic Modeling with Densely Connected Residual Network for Multichannel Speech Recognition,"Motivated by recent advances in computer vision society, in this paper, an acoustic model called Densely Connected Residual Network (DenseRNet) for multichannel speech recognition is presented. To combine the strength of both DenseNet and ResNet, DenseRNet adopts the ""building block"" of ResNet with different convolutional layers, receptive field sizes and growth rates as basic components to be densely connected to form the so-called denseR blocks. Furthermore, by concatenating the feature maps of all preceding layers as inputs, DenseRNet can not only strengthen gradients back-propagation for vanishing-gradient problem, but also exploit multi-resolution feature maps. Preliminary experimental results on CHiME-3 corpus have shown that DenseRNet achieves a word error rate (WER) of 7.58% on the beamforming-enhanced speech with the six channel real test data by cross entropy criteria training while WER is 10.23% for the official baseline. Besides, additional experimental results are also presented to demonstrate that DenseRNet exhibits the robustness to beamforming-enhanced speech as well as near, far-field one.",0.0,4.8999999999999995
692,2434,5.1    4.35    5.15    5    ,Visual Recognition of Continuous Cued Speech Using a Tandem CNN-HMM Approach,"This study addresses the problem of automatic recognition of
Cued Speech (CS), a visual mode of communication for hearing
impaired people in which a complete phonetic repertoire is ob-
tained by combining lip movements with hand cues. In the pro-
posed system, the dynamic of visual features extracted from lip
and hand images using convolutional neural networks (CNN)
are modeled by a set of hidden Markov models (HMM), for
each phonetic context (tandem architecture). CNN-based fea-
ture extraction is compared to an unsupervised approach based
on the principal component analysis. A novel temporal segmen-
tation of hand streams is used to train CNNs efficiently. Differ-
ent strategies for combining the extracted visual features within
the HMM decoder are investigated. Experimental evaluation
is carried on an audiovisual dataset (containing only continu-
ous French sentences) recorded specifically for this study. In its
best configuration, and without exploiting any dictionary or lan-
guage model, the proposed tandem CNN-HMM architecture is
able to identify correctly more than 73% of the phoneme (62%
when considering insertion errors).",0.0,4.9
588,2148,5.85    4.25    5.1    4.4    ,Full Bayesian Hidden Markov Model Variational Autoencoder for Acoustic Unit Discovery,"The invention of the Variational Autoencoder enables the application of Neural Networks to a wide range of tasks in unsupervised learning, including the field of Acoustic Unit Discovery (AUD). The recently proposed Hidden Markov Model Variational Autoencoder (HMMVAE) allows a joint training of a neural network based feature extractor and a structured prior for the latent space given by a Hidden Markov Model.
It has been shown that the HMMVAE significantly outperforms pure GMM-HMM based systems on the AUD task.
However, the HMMVAE cannot autonomously infer the number of acoustic units and thus relies on the GMM-HMM system for initialization.
This paper introduces the Bayesian Hidden Markov Model Variational Autoencoder (BHMMVAE) which solves these issues by embedding the HMMVAE in a Bayesian framework with a Dirichlet Process Prior for the distribution of the acoustic units, and diagonal or full-covariance Gaussians as emission distributions.
Experiments on Timit and Xitsonga show that the BHMMVAE is able to autonomously infer a reasonable number of acoustic units, can be initialized without supervision by a GMM-HMM system, achieves computationally efficient stochastic variational inference by using natural gradient descent, and, additionally, improves the AUD performance over the HMMVAE.",0.0,4.9
138,1185,5.2    4.4    5.05    4.95    ,GlobalTIMIT: Acoustic-Phonetic Datasets for the World’S Languages,"Although the TIMIT acoustic-phonetic dataset ([1], [2]) was created three decades ago, it remains in wide use, with more than 20000 Google Scholar references, and more than 1000 since 2017. Despite TIMIT’s antiquity and relatively small size, inspection of these references shows that it is still used in many research areas: speech recognition, speaker recognition, speech synthesis, speech coding, speech enhancement, voice activity detection, speech perception, overlap detection and source separation, diagnosis of speech and language disorders, and linguistic phonetics, among others.
Nevertheless, comparable datasets are not available even for other widely-studied languages, much less for under-documented languages and varieties. Therefore, we have developed a method for creating TIMIT-like datasets in new languages with modest effort and cost, and we have applied this method in standard Thai, standard Mandarin Chinese, English from Chinese L2 learners, the Guanzhong dialect of Mandarin Chinese, and the Ga language of West Africa. Other collections are planned or underway.
The resulting datasets will be published through the LDC, along with instructions and open-source tools for replicating this method in other languages, covering the steps of sentence selection and assignment to speakers, speaker recruiting and recording, proof-listening, and forced alignment.",0.0,4.9
521,1970,5.1    5.05    4.25    5    5.1    ,On the Application and Compression of Deep Time Delay Neural Network for Embedded Statistical Parametric Speech Synthesis,"Acoustic models based on long short-term memory (LSTM) recurrent neural networks (RNNs) were applied to statistical parametric speech synthesis (SPSS) and shown significant improvements. However, the model complexity and inference time cost of RNNs are much higher than feed-forward neural networks (FNN) due to the sequential nature of the learning algorithm, thus limiting its usage in many runtime applications. In this paper, we explore a novel application of deep time delay neural network (TDNN) for embedded SPSS, which requires low disk footprint, memory and latency. The TDNN could model long short-term temporal dependencies with inference cost comparable to standard FNN. Temporal subsampling enabled by TDNN could reduce computational complexity. Then we compress deep TDNN using singular value decomposition (SVD) to further reduce model complexity, which are motivated by the goal of building embedded SPSS systems which can be run efficiently on mobile devices. Both objective and subjective experimental results show that, the proposed deep TDNN with SVD compression could generate synthesized speech with better speech quality than FNN and comparable speech quality to LSTM,  while drastically reduce model complexity and speech parameter generation time.",0.0,4.9
530,1991,3.6    5.1    5    5.9    ,Rapid Style Adaptation Using Residual Error Embedding for Expressive Speech Synthesis,"Synthesizing expressive speech with appropriate prosodic variations, e.g., various styles, still has much room for improvement. Previous methods have explored to use manual annotations as conditioning attributes to provide variation information. However, the related training data are expensive to obtain and the annotated style codes can be ambiguous and unreliable. In this paper, we explore utilizing the residual error as conditioning attributes. The residual error is the difference between the prediction of a trained average model and the ground truth. We encode the residual error into a style embedding via a neural network-based error encoder. The embedding is then fed to the target synthesis model to provide information for modeling various style distributions more accurately. The average model and the error encoder are jointly optimized with the target synthesis model. Our proposed method has two advantages: 1) the embedding is automatically learned with no need of manual annotations, which helps overcome data sparsity and ambiguity limitations; 2) For any unseen audio utterance, the style embedding can be efficiently generated. This enables rapid adaptation to the desired style to be achieved with only one adaptation utterance. Experimental results show that our method outperforms the baseline in speech quality and style similarity.",0.0,4.9
63,1046,4.35    5    5.25    5    ,Bone-Conduction Sensor Assisted Noise Estimation for Improved Speech Enhancement,"State-of-the-art noise power spectral density (PSD) estimation techniques for speech enhancement utilize the so-called speech presence probability (SPP). However, in highly non-stationary environments, SPP-based techniques could still suffer from inaccurate estimation, leading to significant amount of residual noise or speech distortion. In this paper, we propose to improve speech enhancement by deploying the bone-conduction (BC) sensor, which is known to be relatively insensitive to the environmental noise compared to the regular air-conduction (AC) microphone. A strategy is suggested to utilized the BC sensor characteristics for assisting the AC microphone in better SPP-based noise estimation. To our knowledge, no previous work has incorporated the BC sensor in this noise estimation aspect. Consequently, the proposed strategy can possibly be combined with other BC sensor assisted speech enhancement techniques. We show the feasibility and potential of the proposed method for improving the enhanced speech quality by both objective and subjective tests.",0.0,4.9
485,1884,4.15    5.2    4.35    5.9    ,Global Snr Estimation of Speech Signals Using Entropy and Uncertainty Estimates from Dropout Networks,"This paper demonstrates two novel methods to estimate the global SNR of speech signals. In both methods, Deep Neural Network-Hidden Markov Model (DNN-HMM) acoustic model used in speech recognition systems is leveraged for the additional task of SNR estimation. In the first method, SNR is estimated using the entropy of the posterior distribution obtained from DNN of an ASR system. Recent work on bayesian deep learning has shown that a DNN-HMM trained with dropout can be used to estimate model uncertainty by approximating it as a deep Gaussian process. In the second method, this approximation is used to obtain model uncertainty estimates. Noise specific regressors are used to predict the SNR from the entropy and model uncertainty. The DNN-HMM is trained on GRID corpus and tested on different noise profiles from the DEMAND noise database at SNR levels ranging from -10 dB to 30 dB.",0.0,4.9
272,1430,5    4.5    5.05    5.05    ,Neural Error Corrective Language Models for Automatic Speech Recognition,"We present novel neural network based language models that can correct automatic speech recognition (ASR) errors by using speech recognizer output as a context. These models, called neural error corrective language models (NECLMs), utilizes ASR hypotheses of a target utterance as a context for estimating the generative probability of words. NECLMs are expressed as conditional generative models composed of an encoder network and a decoder network. In the models, the encoder network constructs context vectors from N-best lists and ASR confidence scores generated in a speech recognizer. The decoder network rescores recognition hypotheses by computing a generative probability of words using the context vectors so as to correct ASR errors. We evaluate the proposed models in Japanese lecture ASR tasks. Experimental results show that NECLM achieve better ASR performance than a state-of-the-art ASR system that incorporate a convolutional neural network acoustic model and a long short-term memory recurrent neural network language model.",0.0,4.9
464,1832,4.9    5    4.85    4.9    ,Investigation on Joint Representation Learning for Robust Feature Extraction in Speech Emotion Recognition,"Speech emotion recognition (SER) is a challenging task due to its difficulty in finding proper representations for emotion embedding in speech. Recently, Convolutional Recurrent Neural Network (CRNN), which is combined by convolution neural network and recurrent neural network, is popular in this field and achieves state-of-art on related corpus. However, most of work on CRNN only utilizes simple spectral information, which is not capable to capture enough emotion characteristics for the SER task. In this work, we investigate two joint representation learning structures based on CRNN aiming at capturing richer emotional information from speech. Cooperating the handcrafted high-level statistic features with CRNN, a two-channel SER system (HSF-CRNN) is developed to jointly learn the emotion-related features with better discriminative property. Furthermore, considering that the time duration of speech segment significantly affects the accuracy of emotion recognition, another two-channel SER system is proposed where CRNN features extracted from different time scale of spectrogram segment are used for joint representation learning. The systems are evaluated over Atypical Affect Challenge of ComParE2018 and IEMOCAP corpus. Experimental results show that our proposed systems outperform the plain CRNN.",0.0,4.9125
123,1152,4.3    5.15    5.15    5.1    ,Temporal Transformer Networks for Acoustic Scene Classification,"Neural networks have been proven to be powerful models for acoustic scene classification tasks, but are still limited by the lack of ability to be temporally invariant to the audio data. In this paper, a novel temporal transformer module is proposed to allow the temporal manipulation of data in neural networks. This module is composed of a Fourier transform layer for feature maps and a learnable feature reduction layer, and can be inserted into existing convolutional neural network (CNN) and Long short-term memory (LSTM) models. Experiments on LITIS Rouen dataset and DCASE2016 dataset show that the proposed method leads to a significant improvement when compared with the existing neural networks. Our approach is able to perform significantly better than the state-of-the-art result on LITIS Rouen dataset, obtaining a relative reduction of 23.6% on classification error.",0.0,4.925
409,1722,3.3    4.95    5.5    5.95    ,CNN Based Query by Example Spoken Term Detection,"In this work, we address the problem of query by example spoken term detection (QbE-STD) in zero-resource scenario. State of the art solutions usually rely on dynamic time warping (DTW) based template matching. In contrast, we propose here to tackle the problem as binary classification of images. Similar to the DTW approach, we rely on deep neural network (DNN) based posterior probabilities as feature vectors. The posteriors from a spoken query and a test utterance are used to compute frame-level similarities in a matrix form. This matrix contains somewhere a quasi-diagonal pattern if the query occurs in the test utterance. We propose to use this matrix as an image and train a convolutional neural network (CNN) for identifying the pattern and make a decision about the occurrence of the query. This language independent system is evaluated on SWS 2013 and is shown to give 10% relative improvement over a highly competitive baseline system based on DTW.
Experiments on QUESST 2014 database gives similar improvements showing that the approach generalizes to other database as well.",0.0,4.925
621,2256,5.85    5    3.6    5.25    ,Is ATIS Too Shallow to Go Deeper for Benchmarking Spoken Language Understanding Models?,"The recent gold rush on Deep Neural Network (DNN) models for handling all kinds of supervised learning tasks on image, audio and text data has increased the need for benchmark datasets that can reliably assess the pros and cons of each new method. One of the most well-known dataset for benchmarking SLU systems is the ATIS corpus, containing utterances from spoken dialogs about several air travel planning scenarios.
Since the introduction of this corpus in 1990, numerous machine learning methods have been applied with a steady improvement in performance, leading to an accuracy of about 95\% with current neural models.
Since this achievement, more and more complex DNN models have been applied to ATIS leading in some cases to further improvements although rather limited in terms of error reduction.
Following a previous study performed on the same corpus by Tur-et-al., we propose in this paper to investigate the latest results obtained on ATIS from a qualitative point of view and answer the two following questions: what kind of qualitative improvement brought DNN models to SLU on the ATIS corpus? Is there anything left, from a qualitative point of view, in the remaining 5\% of errors made by current state-of-the-art models?",0.0,4.925
121,1150,5    4.85    5    4.85    ,A Shifted Delta Coefficient Objective for Monaural Speech Separation Using Multi-task Learning,"This paper addresses the problem of monaural speech separation for simultaneous speakers. Recent studies such as uPIT, cuPIT-Grid LSTM and their variants have advanced the state-of-the-art separation models. Delta and acceleration coefficients are typically used in the objective function to capture short time dynamics. We consider that such coefficients don't benefit from the temporal information over a long range such as phoneme and syllable. In this paper, we propose a shifted delta coefficient (SDC) objective to explore the temporal information over a long range of the spectral dynamics. The SDC ensures the temporal continuity of output frames within the same speaker. In addition, we propose a novel multi-task learning framework, that we call SDC-MTL, by extending the SDC objective with a subtask of predicting the time-frequency labels ({silence, single, overlapped}) of the mixture. The experimental results show 11.7% and 3.9% relative improvements on WSJ0-2mix dataset under open conditions over the uPIT and cuPIT-Grid LSTM baselines. A further analysis shows 17.8% and 6.2% relative improvements with speakers of same gender.",0.0,4.925
81,1081,4.95    5    4.25    5.5    ,Exploiting Speaker and Phonetic Diversity of Mismatched Language Resources for Unsupervised Subword Modeling,"This study addresses the problem of learning robust frame-level feature representation for unsupervised subword modeling in the zero-resource scenario. Robustness of the learned features is achieved through effective speaker adaptation and exploiting cross-lingual phonetic knowledge. For speaker adaptation, an out-of-domain automatic speech recognition (ASR) system is used to estimate fMLLR features for untranscribed speech of target zero-resource languages. The fMLLR features  are applied in multi-task learning of a deep neural network (DNN) to further obtain phonetically discriminative and speaker-invariant bottleneck features (BNFs). Frame-level labels for DNN training can be acquired based on two approaches:  Dirichlet process Gaussian mixture model (DPGMM) clustering, and out-of-domain ASR decoding. Moreover, system fusion is performed by concatenating BNFs extracted by different DNNs. Our methods are evaluated by ZeroSpeech 2017 Track one, where the performance is evaluated by ABX minimal pair discriminability. Experimental results demonstrate that: (1) Using an out-of-domain ASR system to perform speaker adaptation of zero-resource speech is effective and efficient; (2) Our system achieves highly competitive performance to state of the art; (3) System fusion could improve feature representation capability.",0.0,4.925
143,1202,5.15    5.15    5.1    4.35    ,Phoneme-to-Articulatory Mapping Using Bidirectional Gated RNN,"Deriving articulatory dynamics from the acoustic speech signal has been addressed in several speech production studies. In this paper, we investigate whether it is possible to predict articulatory dynamics from phonetic information without having the acoustic speech signal. The input data may be considered as not sufficiently rich acoustically, as probably there is no explicit coarticulation information but we expect that the phonetic sequence provides compact yet rich knowledge.
Motivated by the recent success of deep learning techniques used in the acoustic-to-articulatory inversion, we have experimented around the bidirectional gated recurrent neural network architectures. We trained these models with an EMA corpus, and have obtained good performances similar to the state-of-the-art articulatory inversion from LSF features, but using only the phoneme labels and durations.",0.0,4.9375
519,1966,5.05    4.8    5    4.9    ,Building a Unified Code-Switching ASR System for South African Languages,"We present our first efforts towards building a single multilingual automatic speech recognition (ASR) system that can process code-switching (CS) speech in five languages spoken within the same population. This contrasts with related prior work which focuses on the recognition of CS speech in bilingual scenarios. Recently, we have compiled a small five-language corpus of South African soap opera speech which contains examples of CS between 5 languages occurring in various contexts such as using English as the matrix language and switching to other indigenous languages. The ASR system presented in this work is trained on 4 corpora containing English-isiZulu, English-isiXhosa, English-Setswana and English-Sesotho CS speech. The interpolation of multiple language models trained on these language pairs enables the ASR system to hypothesize mixed word sequences from these 5 languages. We evaluate various state-of-the-art acoustic models trained on this 5-lingual training data and report ASR accuracy and language recognition performance on the development and test sets of the South African multilingual soap opera corpus.",0.0,4.9375
256,1399,4.9    5.9    3.05    5.9    ,Effectiveness of Voice Quality Features in Detecting Depression,"Automatic assessment of depression from speech signals is affected by variabilities in acoustic content and speakers. In this study, we focused on addressing these variabilities. We used a database comprised of recordings of interviews from a large number of female speakers: 735 individuals suffering from depressive (dysthymia and major depression) and anxiety disorders (generalized anxiety disorder, panic disorder with or without agoraphobia) and 953 healthy individuals. Leveraging this unique and extensive database, we built an i-vector framework. In order to capture various aspects of speech signals, we used voice quality features in addition to conventional cepstral features. The features (F0, F1, F2, F3, H1-H2, H2-H4, H4-H2k, A1, A2, A3, and CPP) were inspired by a psychoacoustic model of voice quality [1]. An i-vector-based system using Mel Frequency Cepstral Coefficients (MFCCs) and another using voice quality features was developed. Voice quality features performed as well as MFCCs. A score-level fusion was then used to combine these two systems, resulting in a 6% relative improvement in accuracy in comparison with the i-vector system based on MFCCs alone. The system was robust even when the duration of the utterances was shortened to 10 seconds.",0.0,4.9375
134,1174,5.15    5.15    5    3.4    6    ,Using Pupillometry to Measure the Cognitive Load of Synthetic Speech,"It is common to evaluate synthetic speech using listening tests in which intelligibility is measured by asking listeners to tran- scribe the words heard, and naturalness is measured using Mean Opinion Scores. But, for real-world applications of synthetic speech, the effort (cognitive load) required to understand the synthetic speech may be a more appropriate measure. Cognitive load has been investigated in the past, when rule-based speech synthesizers were popular, but there is little or no recent work using state-of-the-art text-to-speech. Studies on the understand- ing of natural speech have shown that the pupil dilates when increased mental effort is exerted to perform a task. We use pupillometry to measure the cognitive load of synthetic speech submitted to two of the Blizzard Challenge evaluations. Our results show that pupil dilation is sensitive to the quality of syn- thetic speech. In all cases, synthetic speech imposes a higher cognitive load than natural speech. Pupillometry is therefore proposed as a sensitive measure that can be used to evaluate synthetic speech.",0.0,4.9399999999999995
479,1866,5.2    4.75    4.9    ,Multi-task Learning with Augmentation Strategy for Acoustic-to-word Attention-based Encoder-decoder Speech Recognition,"In this paper, we propose a novel training strategy for attention-based encoder-decoder acoustic-to-word end-to-end systems. Accuracy of end-to-end systems has greatly improved thanks to careful tuning of model structure and the introduction of novel training strategies to stabilize training. For example, multi-task learning using a shared-encoder is often used to escape from bad local optima. However, multi-task learning usually relies on a linear interpolation of the losses for each sub-task and consequently, the shared-encoder is not optimized for each task. To solve the above problem, we propose a multi-task learning with augmentation strategy. We augment the training data by creating multiple copies of the original training data to suit different output targets associated with each sub-task. We use each target loss sequentially to update the parameters of the shared-encoder so as to enhance the versatility of capturing acoustic features. This strategy enables better learning of the shared-encoder as each task is trained with a dedicated loss. The parameters of the word-decoder are jointly updated via the shared-encoder when optimizing the word prediction task loss. We evaluate our proposal on various speech data sets, and show that our models achieve lower word error rates than both single-task and conventional multi-task approaches.",0.0,4.95
51,1026,4.05    5    5.65    5.1    ,Postfiltering with Complex Spectral Correlations for Speech and Audio Coding,"State-of-the-art speech codecs achieve a good compromise between quality, bitrate and complexity. However, retaining performance outside the target bitrate range remains challenging. To improve performance, many codecs use pre- and post-filtering techniques to reduce the perceptual effect of quantization-noise. In this paper, we propose a postfiltering method to attenuate quantization noise which uses the complex spectral correlations of speech signals. Since conventional speech codecs cannot transmit information with temporal dependencies as transmission errors could result in severe error propagation, we model the correlation offline and employ them at the decoder, hence removing the need to transmit any side information. Objective evaluation indicates an average 4 dB improvement in the perceptual SNR of signals using the context-based post-filter, with respect to the noisy signal, and an average 2 dB improvement relative to the conventional Wiener filter. These results are confirmed by an improvement of up to 30 MUSHRA points in a subjective listening test.",0.0,4.95
594,2165,4.4    5.1    4.9    5.4    ,Creak in the Respiratory Cycle,"Creakiness is a well-known turn-taking cue and has been observed to systematically accompany phrase and turn ends in several languages. In Estonian, creaky voice is frequently used by all speakers without any obvious evidence for its systematic use as a turn-taking cue. Rather, it signals a lack of prominence and is favored by lengthening and later timing in phrases. In this paper, we analyze the occurrence of creak with respect to properties of the respiratory cycle. We show that creak is more likely to accompany longer exhalations. Furthermore, the results suggest there is little difference in lung volume values regardless of the presence of creak, indicating that creaky voice might be employed to preserve air over the course of longer utterances. We discuss the results in connection to processes of speech planning in spontaneous speech.

Index Terms: respiration, creaky voice, spontaneous Estonian, multiparty conversation, speech planning",0.0,4.95
706,2464,4.05    4.1    5.9    5.8    ,Predicting Categorical Emotions by Jointly Learning Primary and Secondary Emotions through Multitask Learning,"Detection of human emotions is an essential part of affect-aware human-computer interaction (HCI). In daily conversations, the preferred way of describing affects is by using categorical emotion labels (e.g., sad, anger, surprise). In categorical emotion classification, multiple descriptors (with different degrees of relevance) can be assigned to a sample. Perceptual evaluations have relied on primary and secondary emotions to capture the ambiguous nature of spontaneous recordings. Primary emotion is the most relevant category felt by the evaluator. Secondary emotions capture other emotional cues also conveyed in the stimulus. In most cases, the labels collected from the secondary emotions are discarded, since assigning a single class label to a sample is preferred from an application perspective. In this work, we take advantage of both types of annotations to improve the performance of emotion classification. We collect the labels from all the annotations available for a sample and generate primary and secondary emotion labels. A classifier is then trained using multitask learning with both primary and secondary emotions. We experimentally show that considering secondary emotion labels during the learning process leads to relative improvements of 7.9% in F1-score for an 8-class emotion classification task.",0.0,4.9624999999999995
429,1757,5.1    4.8    4.85    5.1    ,A New Glottal Neural Vocoder for Speech Synthesis,"Direct modeling of waveform generation for speech synthesis, e.g. WaveNet, has made significant progress on improving the naturalness and clarity of TTS. Such deep neural network-based models can generate highly realistic speech but at high computational and memory costs. We propose here a novel neural glottal vocoder which tends to bridge the gap between the traditional parametric vocoder and end-to-end speech sample generation. In the analysis, speech signals are decomposed into corresponding glottal source signals and vocal tract filters by the glottal inverse filtering. Glottal pulses are parameterized into energy, DCT coefficients (shape) and phase. The phase trajectory of successive glottal pulses is rendered with a trainable weighting matrix to keep a smooth pitch synchronous phase trajectory. We design a hybrid, i.e., both feed-forward and recurrent, neural network to reconstruct the glottal waveform including the optimized weighting matrix. Speech is then synthesized by filtering the generated glottal waveform with the vocal tract filter. The new neural glottal vocoder can generate high-quality speech with efficient computations. Subjective tests show that it gets an MOS score of 4.12 and 75\% preference over the conventional glottal vocoder with a perceived quality comparable to WaveNet and natural recording in analysis-by-synthesis.",0.0,4.9624999999999995
112,1132,5.1    4.8    5.75    4.2    ,Emotion Recognition from Human Speech Using Temporal Information and Deep Learning,"Emotion recognition by machine is a challenging task, but it has great potential to make empathic human-machine communications possible. In conventional approaches that consist of feature extraction and classifier stages, extensive studies have devoted their effort to developing good feature representations, but relatively little effort was made to make proper use of the important temporal information in these features. In this paper, we propose a model combining features known to be useful for emotion recognition and deep neural networks to exploit temporal information when recognizing emotion status. A benchmark evaluation on EMO-DB demonstrates that the proposed model achieves a state-of-the- art performance of 88.9% recognition rate.",0.0,4.9624999999999995
488,1893,4.65    5.15    4.95    5.1    ,Diarization Is Hard: Some Experiences and Lessons Learned for the JHU Team in the Inaugural DIHARD Challenge,"We describe in this paper the experiences of the Johns Hopkins University team during the inaugural DIHARD diarization evaluation.  This new task provided microphone recordings in a variety of difficult conditions and challenged researchers to fully consider all speaker activity, without the currently typical practices of unscored collars or ignored overlapping speaker segments.  This paper explores several key aspects of currently state-of-the-art diarization methods, such as training data selection, signal bandwidth for feature extraction, representations of speech segments (i-vector versus x-vector), and domain-adaptive processing.  In the end, our best system clustered x-vector embeddings trained on wideband microphone data followed by Variational-Bayesian refinement, and a speech activity detector specifically trained for this task with in-domain data was found to be the best performing.  After presenting these decisions and their final result, we discuss lessons learned and remaining challenges within the lens of this new approach to diarization performance measurement.",0.0,4.9625
128,1159,5    4.95    4.25    5.65    ,Word Emphasis Prediction for Expressive Text to Speech,"Word emphasis prediction is an important part of expressive prosody generation in modern Text-To-Speech (TTS) systems. We present a method for predicting emphasized words for expressive TTS, based on a Deep Neural Network (DNN). We show that the presented method outperforms machine learning methods based on hand-crafted features in terms of objective metrics such as precision and recall. Using a listening test, we further demonstrate that the contribution of the predicted emphasized words to the expressiveness of the synthesized speech is subjectively perceivable.",0.0,4.9625
327,1541,4.25    5    5.8    4.8    ,Development of the CUHK Dysarthric Speech Recognition System for the UA Speech Corpus,"Dysarthric speech recognition is a highly challenging task. The articulatory motor control problems associated with neuro-motor conditions produces large mismatch against normal speech. In addition, such data is difficult to collect in large quantities. This paper presents an initial attempt at the Chinese University of Hong Kong to develop an automatic speech recognition (ASR) system for the Universal Access Speech (UASpeech) task. A range of deep neural network (DNN) acoustic models and their more advanced variants based on time delayed neural networks (TDNNs) and long short-term memory recurrent neural networks (LSTM-RNNs) were developed.  Speaker adaptation by learning hidden unit contributions (LHUC) was used.  A semi-supervised complementary auto-encoder system was further constructed to improve the bottleneck feature extraction.  Two out-of-domain ASR systems separately trained on broadcast news and switchboard data were cross domain adapted to the UASpeech data and used in system combination. The final combined system gave an overall word accuracy of 69.4\% on the 16-speaker test set.",0.0,4.9625
495,1907,4.3    5.3    5.1    5.15    ,Reconstructing Neutral Speech from Tracheoesophageal Speech,"In this work, we propose a tracheoesophageal (TE) speech to neutral speech conversion system using data collected from a laryngectomee. In laryngectomees, in the absence of vocal folds, it is the vibration of the esophagus that gives rise to a low-frequency pitch during speech production. This pitch is manifested as impulse-like noise in the recorded speech. We propose a method to first ‘whisperize’ the TE speech prior to the linear predictive coding (LPC) based synthesis which uses pitch derived from the energy contour. In order to perform ‘whisperization’, we model the LPC residual signal as the sum of white noise and impulses introduced by the esophageal vibrations. We model these impulses and white noise using Bernoulli-Gaussian distribution and Gaussian distribution, respectively. The strength and location of the impulses are estimated using Gibbs sampling in order to remove the impulse-like noise from speech to obtain whispered speech. Subjective evaluation via listening test reveals that the ‘whisperization’ step in the proposed method aids in synthesizing a more natural sounding neutral speech. A different listening test shows that the listeners prefer the synthesized speech from the proposed method ∼ 93% (absolute) times more than the best baseline scheme.",0.0,4.9625
550,2043,4.8    4.85    5.25    ,State of Mind: Classification through Self-reported Affect and Word Use in Speech.,"Human state–of-mind (SOM; e.g.: perception, cognition, attention) constantly shifts due to internal and external demands. Mental health is influenced by the habitual use of either adaptive or maladaptive SOM. Therefore, the training of conscious regulation of SOM could be promising in self-help (e- and m-health), blended care and psychotherapy. The presented study indicates that SOM can be influenced by telling personal narratives. Furthermore, SOM and narrative sentiment (positive vs. negative) can be predicted through word use. Such results lay the groundwork for the development of applications that analyse text and speech for: i) the early detection of mental health; ii) the early detection of maladaptive changes in emotion dynamics; (iii) the use of personal narratives to improve emotion regulation skills; iv) the distribution of tailored interventions; and finally, v) evaluation of therapy outcome.",0.0,4.966666666666666
509,1942,5.25    4.95    4.7    ,Fearless Steps: Apollo-11 Corpus Advancements for Speech Technologies from Earth to the Moon,"The Apollo Program is one the most significant benchmarks for technology and innovation in human history. The previously introduced UTD-CRSS Apollo initiative resulted in the digitization of the original analog audio tapes recorded during the Apollo Space Missions. This entire speech data is now being made publicly available with the release of the Fearless Steps Corpus. This corpus consists of a cumulative 19,000 hours of conversational speech spanning over thirty time-synchronized channels. With over six hundred speakers, the corpus has a rich collection of information which can be beneficial for research and advancement in the Speech and Language Community. Recent efforts on this data have led to the generation of pipeline diarization transcripts for the entire Speech Corpus. Research has also been done to address speech and natural language tasks such as speech activity detection, speech recognition, and sentiment analysis. This paper provides an overview of the Fearless-Steps Corpus as well as a summary of previous research work achieved and highlights the factors that make the processing of this data a challenging problem. To initiate further development of algorithms on this Corpus, five challenge tasks are also organized. We also describe the challenge tasks with their associated transcriptions.",0.0,4.966666666666666
320,1526,3.25    5.8    5.85    ,Keyword Based Speaker Localization: Localizing a Target Speaker in a Multi-speaker Environment,"Speaker localization is a hard task, especially in adverse environmental conditions involving reverberation and noise. In this work we introduce the new task of localizing the speaker who uttered a given keyword, e.g., the wake-up word of a distant-microphone voice command system, in the presence of overlapping speech. We employ a convolutional neural network based localization system and investigate multiple identifiers as additional inputs to the system in order to characterize this speaker. We conduct experiments using ground truth identifiers which are obtained assuming the availability of clean speech and also in realistic conditions where  the identifiers are computed from the corrupted speech. We find that the identifier consisting of the ground truth time-frequency mask corresponding to the target speaker provides the best localization performance and we propose methods to estimate such a mask in adverse reverberant and noisy conditions using the considered keyword.",0.0,4.966666666666667
437,1773,5.45    5.1    5.1    5.15    4.05    ,PhaseNet: Discretized Phase Modeling with Deep Neural Networks for Audio Source Separation,"Previous research on audio source separation based on deep neural networks (DNNs) mainly focuses on estimating the magnitude spectrum of target sources and typically, phase of the mixture signal is combined with the estimated magnitude spectra in an ad-hoc way. Although recovering target phase is assumed to be important for the improvement of separation quality, it can be difficult to handle the periodic nature of the phase with the regression approach. Unwrapping phase is one way to eliminate the phase discontinuity, however, it increases the range of value along with the times of unwrapping, making it difficult for DNNs to model. 
To overcome this difficulty, we propose to treat the phase estimation problem as a classification problem by discretizing phase values and assigning class indices to them. 
Experimental results show that our classification-based approach 1) successfully recovers the phase of the target source in the discretized domain, 2) improves signal-to-distortion ratio (SDR) over the regression-based approach in both speech enhancement task and music source separation (MSS) task, and 3) outperforms state-of-the-art MSS.",0.0,4.970000000000001
163,1238,4.15    5.65    5.1    5    ,Deep Learning in Paralinguistic Recognition Tasks: Are Hand-crafted Features Still Relevant?,"In the past, the performance of machine learning algorithms depended heavily on the representation of the data. Well-designed features therefore played a key role in speech and paralinguistic recognition tasks. Consequently, engineers have put a great deal of work into manually designing large and complex acoustic feature sets. With the emergence of Deep Neural Networks (DNNs), however, it is now possible to automatically infer higher abstractions from simple spectral representations or even learn directly from raw waveforms. This raises the question if (complex) hand-crafted features will still be needed in the future. We take this year's INTERSPEECH Computational Paralinguistic Challenge as an opportunity to approach this issue by means of two corpora -- Atypical Affect and Crying. At first, we train a Recurrent Neural Network (RNN) to evaluate the performance of several hand-crafted feature sets of varying complexity. Afterwards, we make the network do the feature engineering all on its own by prefixing a stack of convolutional layers. Our results show that there is no clear winner (yet). This creates room to discuss chances and limits of either approach.",0.0,4.975
223,1336,5.1    5    5.65    4.15    ,Automatically Measuring L2 Speech Fluency without the Need of ASR: a Proof-of-concept Study with Japanese Learners of French,"This research work investigates the possibility of using automatic acoustic measures to assess speech fluency in the context of second language (L2) acquisition.
To this end, three experts rated speech recordings of Japanese learners of French who were instructed to read aloud a 21-sentence-long text. A Forward-Backward Divergence Segmentation (FBDS) algorithm was used to segment speech recordings (sentences) into acoustically homogeneous units at a subphonemic scale. The FBDS processing results were used — along with more classic measures such as raw percentage of speech and length/standard deviation of silent pauses — to estimate speech rate and regularity of speech rate, while a formant tracking algorithm was used to estimate speech fluidity (i.e., quality of coarticulation). A step-by-step multiple linear regression was finally computed to predict the experts’ mean fluency ratings.
Results show that FBDS-derived measures, raw percentage of speech, and standard deviation of the first formant curve derivative can be combined together to calculate accurate estimates of speakers’ fluency scores (R = .92;P < .001). As only low-level signal features were used in the study, the method could also be relevant for the assessment of speakers of other target languages, as well as for the assessment of disordered speech.",0.0,4.975
49,1024,3.95    4.3    5.9    5.75    ,"Effective Acoustic Cue Learning Is Not Just Statistical, It Is Discriminative","A growing statistical learning literature suggests that listeners extract statistical information from the linguistic environment. However, distributional frequency may be insufficient for important but relatively low-frequency cues. Acquisition of linguistic knowledge may rely not merely on co-occurrences but on predictive relationships between cues and their outcomes. The present study investigates effects of predictive temporal cue structure on acquisition of a non-native acoustic cue dimension.

During training, native English speakers saw coloured shape objects and heard spoken Min Chinese words with six different lexical tones. Tones were the only reliable cue to identifying the associated object. Words also contained a salient cue that did not discriminate between objects. Three tones occurred with high-frequency and three with low-frequency in training. The critical manipulation was the presentation order: either words, containing complex cue structure, preceded object outcomes (discriminative order) or objects preceded words (non-discriminative order). 

Generalised linear mixed models showed accuracy was significantly higher in the discriminative order than the non-discriminative order. These results demonstrate that predictive cue structure can facilitate acquisition of a non-native cue dimension. Feedback from prediction error drives learners to ignore salient non-discriminative cues and effectively learn to use the target cue dimension.",0.0,4.975
436,1772,5.7    5.9    5.1    4    4.2    ,Recognition of Echolalic Autistic Child Vocalisations Utilising Convolutional Recurrent Neural Networks,"Autism spectrum conditions (ASC) are a set of neuro-developmental conditions partly characterised by difficulties with communication. Individuals with ASC can show a variety of atypical speech behaviours, including echolalia or the `echoing' of another's speech. We herein introduce a new dataset of 15 Serbian ASC children in a human-robot interaction scenario, annotated for the presence of echolalia amongst other ASC vocal behaviours. From this, we propose a four-class classification problem and investigate the suitability of applying a 2D convolutional neural network augmented with a recurrent neural network with bidirectional long short-term memory cells to solve the proposed task of echolalia recognition. In this approach, log Mel-spectrograms are first generated from the audio recordings and then fed as input into the convolutional layers to extract high-level spectral features. The subsequent recurrent layers are applied to learn the long-term temporal context from the obtained features. Finally, we use a feed forward neural network with softmax activation to classify the dataset. To evaluate the performance of our deep learning approach, we use leave-one-subject-out cross-validation. Key results presented indicate the suitability of our approach by achieving a classification accuracy of 83.5% unweighted average recall.",0.0,4.98
309,1509,3.95    5    6    ,Noise Robust Acoustic to Articulatory Speech Inversion,"In previous work, we have shown that using articulatory features derived from a speech inversion system trained using synthetic data can significantly improve the robustness of an automatic speech recognition (ASR) system. This paper presents results from the first of two steps needed for exploring if the same will hold true for a speech inversion system trained with natural speech. Specifically, we developed a noise robust multi-speaker acoustic to articulatory speech inversion system. A feed forward neural network was trained using contextualized mel-frequency cepstral coefficients (MFCC) as the input acoustic features and six tract-variable (TV) trajectories as the output articulatory features. Experiments were performed on the U. Wisc. X-ray Microbeam (XRMB) database with 8 noise types artificially added at 5 different SNRs. Performance of the system was measured by computing the correlation between estimated and actual TVs. The performance of the multi-condition trained system was compared to the clean-speech trained system. The effect of speech enhancement on TV estimation was also evaluated. Experiments showed a 10% relative improvement in correlation over the baseline clean-speech trained system.",0.0,4.983333333333333
699,2453,5.25    5.1    4.6    ,Semantic Lattice Processing in Contextual Automatic Speech Recognition for Google Assistant,"Recent interest in intelligent assistants has increased demand
for Automatic Speech Recognition (ASR) systems that
can utilize contextual information to adapt to the user's preferences
or the current device state. For example, a user might be
more likely to refer to their favorite songs when giving a ""music
playing"" command or request to watch a movie starring a particular
favorite actor when giving a ""movie playing"" command.
Similarly, when a device is in a ""music playing"" state, a user is
more likely to give volume control commands.

In this paper, we explore using semantic information inside
the ASR word lattice by employing Named Entity Recognition
(NER) to identify and boost contextually relevant paths in order
to improve speech recognition accuracy. We use broad semantic
classes comprising millions of entities, such as songs
and musical artists, to tag relevant semantic entities in the lattice.
We show that our method reduces Word Error Rate (WER)
by 12.0% relative on a Google Assistant ""media playing"" commands
test set, while not affecting WER on a test set containing
commands unrelated to media.",0.0,4.983333333333333
26,79,5.85    4.95    4.25    4.9    ,Lattice-free State-level Minimum Bayes Risk Training of Acoustic Models,"Lattice-free maximum mutual information (LF-MMI) training, which enables MMI-based acoustic model training without any lattice generation procedure, has recently been proposed. Although LF-MMI showed high accuracy in many tasks, its MMI criterion does not necessarily maximize the speech recognition accuracy. In this work, we propose a lattice-free state-level minimum Bayes risk training (LF-sMBR), which maximizes state-level expected accuracy without relying on a lattice generation procedure. As is the case with the LF-MMI, LF-sMBR avoids redundant lattice generation by exploiting forward-backward calculation on phone N-gram space, which enables a much simpler and faster training based on an sMBR criterion. We found that special care for silence phones was essential for improving the accuracy by LF-sMBR. In our experiments on the AMI, CSJ, and Librispeech corpora, LF-sMBR achieved small but consistent improvements over LF-MMI AMs, showing state-of-the-art results for each test set.",0.0,4.987500000000001
318,1523,5.05    5.05    5.7    4.15    ,Automatic Assessment of Individual Culture Attribute of Power Distance Using a Social Context-Enhanced Prosodic Network Representation,"Culture is a collective social norm of human societies that often influences a person's values, thoughts, and social behaviors during interactions at an individual level. In this work, we present a computational analysis toward automatic assessing an individual's culture attribute of power distance, i.e., a measure of his/her belief about status, authority and power in organizations, by modeling their expressive prosodic structures during social encounters with people of different power status. Specifically, we propose a center-loss embedded network architecture to jointly consider the effect of social interaction contexts on individuals' prosodic manifestations in order to learn an enhanced representation for power distance recognition. Our proposed prosodic network achieves an overall accuracy of 78.6% in binary classification task of recognizing high versus low power distance. Our experiment demonstrates an improved discriminability (17.6% absolute improvement) over prosodic neural network without social context enhancement. Further visualization reveals that the diversity in the prosodic manifestation for individuals with low power distance seems to be higher than those of high power distance.",0.0,4.987500000000001
241,1372,5.8    5    4.95    4.25    ,The University of Birmingham 2018 Spoken CALL Shared Task Systems,"This paper describes the systems developed by the University of Birmingham for the 2018 CALL Shared Task (ST) challenge. The task is to perform automatic assessment of grammatical and linguistic aspects of English spoken by German-speaking Swiss teenagers. Our developed systems consist of two components, automatic speech recognition (ASR) and text processing (TP). We explore several ways of building a DNN-HMM ASR system using out-of-domain AMI speech corpus
plus a limited amount of ST data. In development experiments on the initial ST data, our final ASR system achieved the word-error-rate (WER) of 12.00%, compared to 14.89% for the official ST baseline DNN-HMM system. The WER of 9.28% was achieved on the test set data. For TP component, we first post-process the ASR output to deal with hesitations and then pass this to a template-based grammar, which we expanded from the provided baseline. We also developed a TP system based on machine learning methods, which enables to better accommodate variability of spoken language. We also fused outputs from several systems using a linear logistic regression. Our best system submitted to the challenge achieved F-measure of 0.914, D of 10.764 and Dfull score of 5.691 on the final test set.",0.0,5.0
389,1676,4.9    5.8    4.3    ,Analyzing EEG Signals in Auditory Speech Comprehension Using Temporal Response Functions and Generalized Additive Models,"Analyzing EEG signals recorded while participants are listening
to continuous speech with the purpose of testing linguistic
hypotheses is complicated by the fact that the signals simultaneously
reflect exogenous acoustic excitation and endogenous
linguistic processing. This makes it difficult to trace subtle differences
that occur in mid-sentence position. We apply an analysis
based on multivariate temporal response functions to uncover
subtle mid-sentence effects. This approach is based on
a per-stimulus estimate of the response of the neural system to
speech input. Analyzing EEG signals predicted on the basis
of the response functions might then bring to light conditionspecific
differences in the filtered signals. We validate this approach
by means of an analysis of EEG signals recorded with
isolated word stimuli. Then, we apply the validated method to
the analysis of the responses to the same words in the middle of
meaningful sentences.",0.0,5.0
714,2484,5.25    4.8    4.95    5    ,Articulation-to-Speech Synthesis Using Articulatory Flesh Point Sensors’ Orientation Information,"Articulation-to-speech (ATS) synthesis generates audio waveform directly from articulatory information. Current works in ATS used articulatory movement information (spatial coordinates) only. The orientation information of articulatory flesh points has rarely been used, although some devices (e.g., electromagnetic articulography) provide that. Previous work indicated that orientation information contains significant information for speech production.  In this paper, we explored the performance of applying orientation information of flesh points on articulators (i.e., tongue, lips and jaw) in ATS. Experiments using articulators' movement information with or without orientation information were conducted using standard deep neural networks (DNNs) and long-short term memory-recurrent neural networks (LSTM-RNNs). Both objective and subjective evaluations indicated that adding orientation information of flesh points on articulators in addition to movement information generated higher quality speech output than using movement information only.",0.0,5.0
448,1802,4.95    4.25    5.9    4.9    ,Quality-Net: an End-to-End Non-intrusive Speech Quality Assessment Model Based on BLSTM,"Nowadays, most of the objective speech quality assessment tools (e.g., perceptual evaluation of speech quality (PESQ)) are based on the comparison of the degraded/processed speech with its clean counterpart. The need of a “golden” reference considerably restricts the practicality of such assessment tools in real-world scenarios since the clean reference usually cannot be accessed. On the other hand, human beings can readily evaluate the speech quality without any reference (e.g., mean opinion score (MOS) tests), implying the existence of an objective and non-intrusive (no clean reference needed) quality assessment mechanism. In this study, we propose a novel end-to-end, non-intrusive speech quality evaluation model, termed Quality-Net, based on bidirectional long short-term memory. The evaluation of utterance-level quality in Quality-Net is based on the frame-level assessment. Frame constraints and sensible initializations of forget gate biases are applied to learn meaningful frame-level quality assessment from the utterance-level quality label. Experimental results show that Quality-Net can yield high correlation to PESQ (0.9 for the noisy speech and 0.84 for the speech processed by speech enhancement). We believe that Quality-Net has potential to be used in a wide variety of applications of speech signal processing.",0.0,5.0
635,2297,4.25    4.45    5.9    5.4    ,On the Difficulties of Automatic Speech Recognition for Kindergarten-Aged Children,"Automatic speech recognition (ASR) systems for children have lagged behind in performance when compared to adult ASR. The exact problems and evaluation methods for child ASR have not yet been fully investigated. Recent work from the robotics community suggests that ASR for kindergarten speech is especially difficult, even though this age group may benefit most from voice-based educational and diagnostic tools. Our study focused on ASR performance for specific grade levels (K-10) using a word identification task. Grade-specific ASR systems were evaluated, with particular attention placed on the evaluation of kindergarten-aged children (5-6 years old). Experiments included investigation of grade-specific interactions with triphone models using feature space maximum likelihood linear regression (fMLLR), vocal tract length normalization (VTLN), and subglottal resonance (SGR) normalization. Our results indicate that kindergarten ASR performs dramatically worse than even 1st grade ASR, likely due to large speech variability at that age. As such, ASR systems may require targeted evaluations on kindergarten speech rather than being evaluated under the guise of ""child ASR."" Additionally, results show that systems trained in matched conditions on kindergarten speech may be less suitable than mismatched-grade training with 1st grade speech. Finally, we analyzed the phonetic errors made by the kindergarten ASR.",0.0,5.0
471,1846,4.75    5.2    5.05    ,Modulation Dynamic Features for the Detection of Replay Attacks,"The development of automatic systems that can detect replayed speech has emerged as a significant research challenge for securing voice biometric systems and is the focus of this paper. Specifically, this paper proposes two novel features to capture the static and  dynamic characteristics of the signal from the modulation spectrum , which complement short term spectral features for use in replay detection. The modulation spectral centroid frequency feature is proposed as a vector representation of the first order spectral moments of the modulation spectrum. In conjunction to this, the long term spectral average serves to capture the static characteristics of the modulation spectrum. The proposed system, employing a GMM back-end, was evaluated on the ASVSpoof 2017 dataset and found to yield an EER of 6.54%.",0.0,5.0
748,2598,5.9    4.55    4.45    5.1    ,Acoustic Analysis of Whispery Voice Disguise in Mandarin Chinese,"This paper investigates the auditory and acoustical characteristics of whispery disguised voice, and compares the patterns with those of normal (non-disguised) voices. It also evaluates effects of whispery disguise on forensic voice comparison. Recordings of eleven male college students’ normal voices and whispery disguised voices were collected. All their normal and whisper speech was acoustically analyzed and compared. The parameters including average syllable duration, intensity, vowel formant frequencies, and long term average spectrum (LTAS) were measured and statistically analyzed. The effect of whispery voice disguise on speaker recognition by auditory perception and an automatic system were evaluated. Correlation and regression analyses were made on the parameters of whispery voice and normal voice. These simple regression models can be used for parameter compensation in forensic casework.",0.0,5.0
370,1635,4.35    5.05    5.55    5.1    ,Speaker-independent Raw Waveform Model for Glottal Excitation,"Recent speech technology research has seen a growing interest in using WaveNets as statistical vocoders, i.e., generating speech waveforms from acoustic features. These models have been shown to improve the generated speech quality over classical vocoders in many tasks, such as text-to-speech synthesis and voice conversion. Furthermore, conditioning WaveNets with acoustic features allows sharing the waveform generator model across multiple speakers without additional speaker codes. However, multi-speaker WaveNet models require large amounts of training data and computation to cover the entire acoustic space. This paper proposes leveraging the source-filter model of speech production to more effectively train a speaker-independent waveform generator with limited resources. We present a multi-speaker ’GlotNet’ vocoder, which utilizes a WaveNet to generate glottal excitation waveforms, which are then used to excite the corresponding vocal tract filter to produce speech. Listening tests show that the proposed model performs favourably to a direct WaveNet vocoder trained with the same model architecture and data.",0.0,5.012499999999999
2,38,5.2    4.9    5.15    4.8    ,Conditional End-to-End Audio Transformations,"We present an end-to-end method for transforming audio from one style to another. For the case of speech, by conditioning on speaker identities, we can train a single model to transform words spoken by multiple people into multiple target voices. For the case of music, we can specify musical instruments and achieve the same result. Architecturally, our method is a fully-differentiable sequence-to-sequence model based on convolutional and hierarchical recurrent neural networks. It is designed to capture long-term acoustic dependencies, requires minimal post-processing, and produces realistic audio transforms. Ablation studies confirm that our model can separate acoustic properties from musical and language content at different receptive fields. Empirically, our method achieves competitive performance on community-standard datasets.",0.0,5.0125
277,1439,5    4.15    5.95    4.95    ,Error Modeling via Asymmetric Laplace Distribution for Deep Neural Network Based Single-Channel Speech Enhancement,"The minimum mean squared error (MMSE) as a conventional training criterion for deep neural network (DNN) based speech enhancement has been found many problems. In our recent work, a maximum likelihood (ML) approach to parameter learning by modeling the prediction error vector as a Gaussian density was proposed. In this study, our preliminary statistical analysis reveals the super-Gaussianity and asymmetricity of the prediction error distribution. Consequently, we adopt the asymmetric Laplace distribution (ALD) instead of the Gaussian distribution (GD) to model the prediction error vectors. Then the new derivation for optimizing the the proposed ML-ALD-DNN with both DNN and ALD parameters is presented. Moreover, we can well interpret the asymmetry parameter of ALD as the balance control between noise reduction and speech preservation from both formulations and experiments. This implies that the customization of DNN models for the different noise types and levels is possible by the setting of the asymmetry parameter. Finally, our ML-ALD-DNN approach achieves better STOI and SSNR measures over both MMSE-DNN and ML-GD-DNN approaches.",0.0,5.0125
507,1939,5.65    4.2    4.4    5.8    ,Air-Tissue Boundary Segmentation in Real-Time Magnetic Resonance Imaging Video Using Semantic Segmentation with Fully Convolutional Networks,"In this paper, we propose a new technique for the segmentation of the Air-Tissue Boundaries (ATBs) in the vocal tract from the real-time magnetic resonance imaging (rtMRI) videos of the upper airway in the midsagittal plane. The proposed technique uses the approach of semantic segmentation using the Deep learning architecture called Fully Convolutional Networks (FCN). The architecture takes an input image and produces images of the same size with air and tissue class labels at each pixel. These output images are post processed using morphological filling and image smoothing to predict realistic ATBs. The performance of the predicted contours is evaluated using Dynamic Time Warping (DTW) distance between the manually annotated ground truth contours and the predicted contours. Four fold experiments with four subjects from USC-TIMIT corpus (with ~2900 training images in every fold) demonstrate that the proposed FCN based approach has 8.87% and $9.65% lesser average error than the baseline Maeda Grid based scheme, for the lower and upper ATBs respectively. In addition, the proposed FCN based rtMRI segmentation achieves an average pixel classification accuracy of 99.05% across all subjects.",0.0,5.0125
268,1423,4.85    5.05    5.15    ,End-to-end Speech Recognition Using Lattice-free MMI,"We present our work on end-to-end training of acoustic models using the
lattice-free maximum mutual information (LF-MMI) objective function in the context
of hidden Markov models.
By end-to-end training, we mean flat-start training of a single DNN in one stage without using
any previously trained models, forced alignments, or building state-tying decision trees.
We use full biphones to enable context-dependent modeling without trees, and show that
our end-to-end LF-MMI approach can achieve comparable results to regular
 LF-MMI on well-known large vocabulary tasks.
We also compare with other end-to-end methods such as CTC
in character-based and lexicon-free settings and show 5 to 25 percent relative reduction in word error
rates on different large vocabulary tasks while using significantly smaller models.",0.0,5.016666666666667
14,57,4.1    5.05    5.9    5.05    ,Investigating the Effect of Audio Duration on Dementia Detection Using Acoustic Features,"This paper presents recent progress toward our goal to enable area-wide pre-screening methods for the early detection of dementia based on automatically processing conversational speech of a representative group of more than 200 subjects. We focus on conversational speech since it is the natural form of communication that can be recorded unobtrusively, without adding stress to subjects, and without the need of controlled clinical settings. We describe our unsupervised process chain consisting of voice activity detection and speaker diarization followed by extraction of features and detection of early signs of dementia. The unsupervised system achieves up to 0.645 unweighted average recall (UAR) and compares favorably to a system that was carefully designed on manually annotated data. To further lower the burden for subjects, we investigate UAR over speech duration, and find that about 12 minutes of interview are sufficient to achieve the best UAR.",0.0,5.0249999999999995
585,2130,3.7    5.15    6    5.25    ,Learning Two Tone Languages Enhances the Brainstem Encoding of Lexical Tones,"Auditory brainstem encoding is influenced by experience-dependent factors such as language and music. Tone language speakers exhibit more robust brainstem encoding of lexical tones than non-tone language speakers. Studies suggest that the effects of experience with a tone language generalize to the brainstem encoding of lexical tones from other tone languages. However, the effects of learning two tone languages, with different tonal systems, on brainstem encoding of lexical pitch are unknown. In the current study, we investigated whether or not the experience with two tone languages (Mandarin and Cantonese) enhances the brainstem encoding of lexical pitch, using frequency following response (FFR). Mandarin has four lexical tones- high level, rising, dipping, and falling while Cantonese has a richer tone system with three level tones (high, mid, low), two rising tones (high and low), and one falling tone. We compared speakers fluent in Cantonese vs. those fluent in both Cantonese and Mandarin on their brainstem encoding of Cantonese and Mandarin lexical tones. We found that the Cantonese-Mandarin speakers exhibited more robust brainstem encoding of the lexical tones as compared to Cantonese speakers. From the current findings, we conclude that learning two tone languages may enhance lexical pitch encoding at the brainstem.",0.0,5.025
255,1395,5.1    4.75    5.15    5.1    ,Towards an Unsupervised Entrainment Distance in Conversational Speech Using Deep Neural Networks,"Entrainment is a known adaptation mechanism that causes interaction participants to adapt or synchronize their acoustic characteristics. Understanding how interlocutors tend to adapt to each other's speaking style through entrainment involves measuring a range of acoustic features and comparing those via multiple signal comparison methods. In this work, we present a turn-level distance measure obtained in an unsupervised manner using a Deep Neural Network (DNN) model, which we call Neural Entrainment Distance (NED). This metric establishes a framework that learns an embedding from the population-wide entrainment in an unlabeled training corpus. We use the framework for a set of acoustic features and validate the measure experimentally by showing its efficacy in distinguishing real conversations from fake ones created by randomly shuffling speaker turns. Moreover, we show real world evidence of the validity of the proposed measure. We find that high value of NED is associated with high ratings of emotional bond in suicide assessment interviews, which is consistent with prior studies.",0.0,5.025
732,2530,4.9    4.3    5.9    5    ,An Empirical Analysis of the Correlation of Syntax and Prosody,"The relation of syntax and prosody (the syntax-prosody interface) has
been an active area of research, mostly in linguistics and typically
studied under controlled conditions.  More recently, prosody has also
been successfully used in the data-based training of syntax parsers.
However, there is a gap between the controlled and detailed study of
the individual effects between syntax and prosody and the large-scale
application of prosody in syntactic parsing with only a shallow
analysis of the respective influences.  In this paper, we close the
gap by investigating the significance of correlations of prosodic
realization with specific syntactic functions using linear mixed
effects models in a very large corpus of read-out German encyclopedic
texts.  Using this corpus, we are able to analyze prosodic structuring
performed by a diverse set of speakers while they try to optimize
factual content delivery.  After normalization by speaker, we obtain
significant effects, e.g. confirming that the subject function, as
compared to the object function, has a positive effect on pitch and
duration of a word, but a negative effect on loudness.",0.0,5.025
687,2423,5    4.85    5.1    5.15    ,A Priori SNR Estimation Based on a Recurrent Neural Network for Robust Speech Enhancement,"Speech enhancement under highly non-stationary noise conditions remains a
challenging problem. Classical methods typically attempt to identify a frequency-domain optimal gain function that suppresses noise in noisy speech. These algorithms typically produce artifacts such as “musical noise” that are detrimental to machine and human understanding, largely due to inaccurate estimation of noise power spectra. The
optimal gain function is commonly referred to as the ideal ratio mask (IRM) in neural-network-based systems, and the goal becomes estimation of the IRM from the short-time Fourier transform amplitude of degraded speech. While these data-driven techniques are able to enhance speech quality with reduced artifacts, they are frequently not robust to types of noise that they had not been exposed to in the training process.
In this paper, we propose a novel recurrent neural network (RNN) that bridges the gap
between classical and neural-network-based methods. By reformulating the classical
decision-directed approach, the a priori and a posteriori SNRs become latent variables in the RNN, from which the frequency-dependent estimated likelihood of speech presence is used to update recursively the latent variables. The proposed method provides substantial enhancement of speech quality and objective accuracy in machine interpretation of speech.",0.0,5.025
742,2577,4.35    5.75    5    ,Multi-frame Quantization of LSF Parameters Using a Deep Autoencoder and Pyramid Vector Quantizer,"This paper presents a multi-frame quantization of line spectral frequency (LSF) parameters using a deep autoencoder (DAE) and pyramid vector quantizer (PVQ). The object is to provide sophisticated LSF quantization for the ultra-low bit rate speech coders with moderate delay. For the compression and de-correlation of multiple LSF frames, a DAE possessing linear coder-layer units with Gaussian noise is used. The DAE demonstrates a high degree of modelling flexibility for multiple LSF frames. To quantize the coder-layer vector effectively, a PVQ is considered. Comparing the discrete cosine model (DCM), the DAE-based compression shows better modelling accuracy of multi-frame LSF parameters and possesses an advantage in that the coder-layer dimensions could be any value. The compressed coder-layer dimensions of the DAE govern the trade-off between the modelling distortion and the coder-layer quantization distortion. The experimental results show that the proposed algorithm with determined optimal coder-layer dimension outperforms the DCM-based multi-frame LSF quantization approach in terms of spectral distortion (SD) performance and robustness across different speech segments.",0.0,5.033333333333333
547,2032,5.25    4.9    4.95    ,VoiceGuard: Secure and Private Speech Processing,"With the advent of smart-home devices providing voice-based interfaces, such as Amazon Alexa or Apple Siri, voice data is constantly transferred to cloud services for automated speech recognition or speaker verification.

While this development enables intriguing new applications, it also poses significant risks: Voice data is highly sensitive since it contains biometric information of the speaker as well as the spoken words. This data may be abused if not protected properly, thus the security and privacy of billions of end-users is at stake.

We tackle this challenge by proposing an architecture, dubbed VoiceGuard, that efficiently protects the speech processing task inside a trusted execution environment (TEE). Our solution preserves the privacy of users while at the same time it does not require the service provider to reveal model parameters. Our architecture can be extended to enable user-specific models, such as feature transformations (including fMLLR), i-vectors, or model transformations (e.g., custom output layers). It also generalizes to secure on-premise solutions, allowing vendors to securely ship their models to customers.

We provide a proof-of-concept implementation and evaluate it on the Resource Management and WSJ speech recognition tasks isolated with Intel SGX, a widely available TEE implementation, demonstrating even real time processing capabilities.",0.0,5.033333333333334
528,1988,5.9    4.2    5    ,A Multitask Learning Approach to Assess the Dysarthria Severity in Patients with Parkinson's Disease,"Parkinson's disease is a neurodegenerative disorder characterized by a variety of motor and non-motor symptoms. Particularly, several speech impairments appear in the initial stages of the disease, which affect aspects related to respiration and the movement of muscles and limbs in the vocal tract. Most of the studies in the literature aim to assess only one specific task from the patients, such as the classification of patients vs. healthy speakers, or the assessment of the neurological state of the patients. This study proposes a multitask learning approach based on convolutional neural networks to assess at the same time several speech deficits of the patients.  A total of eleven speech aspects are considered, including difficulties of the patients to move articulators such as lips, palate, tongue, and larynx. 
 According to the results, the proposed approach improves the generalization of the convolutional network, producing more representative feature maps to assess the different speech symptoms of the patients. The multitask learning scheme improves in of up to 4% the average accuracy relative to single networks trained to assess each individual speech aspect.",0.0,5.033333333333334
583,2128,3.5    5.7    5.1    5.85    ,Fast Variational Bayes for Heavy-tailed PLDA Applied to I-vectors and X-vectors,"The standard state-of-the-art backend for text-independent
speaker recognizers that use i-vectors or x-vectors is Gaussian
PLDA (G-PLDA), assisted by a Gaussianization step involving
length normalization. G-PLDA can be trained with both gener-
ative or discriminative methods. It has long been known that
heavy-tailed PLDA (HT-PLDA), applied without length nor-
malization, gives similar accuracy, but at considerable extra
computational cost. We have recently introduced a fast scor-
ing algorithm for a discriminatively trained HT-PLDA back-
end. This paper extends that work by introducing a fast, vari-
ational Bayes, generative training algorithm. We compare old
and new backends, with and without length-normalization, with
i-vectors and x-vectors, on SRE’10, SRE’16 and SITW.",0.0,5.0375
633,2293,5.9    5.8    3.35    5.1    ,Tone Recognition Using Lifters and CTC,"In this paper, we present a new method for recognizing tones in continuous speech for tonal languages. The method works by converting the speech signal to a cepstrogram, extracting a sequence of cepstral features using a convolutional neural network, and predicting the underlying sequence of tones using a connectionist temporal classification (CTC) network. The performance of the proposed method is evaluated on a freely available Mandarin Chinese speech corpus, AISHELL-1, and is shown to outperform the existing techniques in the literature in terms of tone error rate (TER).",0.0,5.0375
1,34,5.25    4.95    5.05    4.95    ,Real-Time Scoring of an Oral Reading Assessment on Mobile Devices,"We discuss the real-time scoring logic for a self-administered oral reading assessment on mobile devices (Moby.Read) to measure the three components of children's oral reading fluency skills: words correct per minute, expression, and comprehension. Critical techniques that make the assessment real-time on-device are discussed in detail. We propose the idea of producing comprehension scores by measuring the semantic similarity between the prompt and the retelling response utilizing the recent advance of document embeddings in natural language processing. By combining features derived from word embedding with the normalized number of common types, we achieved a human-machine correlation coefficient of 0.90 at the participant level for comprehension scores, which was better than the human inter-rater correlation 0.88. We achieved a better human-machine correlation coefficient than that of the human inter-rater in expression scores too. Experimental results demonstrate that Moby.Read can provide highly accurate words correct per minute, expression and comprehension scores in real-time, and validate the use of machine scoring methods to automatically measure oral reading fluency skills.",0.0,5.05
651,2335,5    4.95    5.15    5.1    ,Combining Natural Gradient with Hessian Free Methods for Sequence Training,"This paper presents a new optimisation approach to train Deep Neural Networks (DNNs) with discriminative sequence criteria. At each iteration, the method combines information from the Natural Gradient (NG) direction with local curvature information of the error surface that enables better paths on the parameter manifold to be traversed. The method has been applied within a Hessian Free (HF) style optimisation framework to sequence train both standard fully-connected DNNs and Time Delay Neural Networks as speech recognition acoustic mod- els. The efficacy of the method is shown using experiments on a Multi-Genre Broadcast (MGB) transcription task and neural networks using sigmoid and ReLU activation functions have been investigated. It is shown that for the same number of updates this proposed approach achieves larger reductions in the word error rate (WER) than both NG and HF, and also leads to a lower WER than standard stochastic gradient descent.",0.0,5.05
244,1375,5    5    5.75    4.45    ,Prediction of Subjective Listening Effort from Acoustic Data with Non-Intrusive Deep Models,"The effort of listening to spoken language is a highly important perceptive measure for the design of speech enhancement algorithms and hearing-aid processing. In previous research, we proposed a model that quantifies the phoneme output probabilities obtained from a deep neural net (DNN), which resulted in accurate predictions for unseen speech samples. However, high correlations between subjective ratings and model output were observed in known noise types, which is an unrealistic assumption in real-life scenarios. This paper explores non-intrusive listening effort prediction in unseen noisy environments. A set of different noise types are used for training a standard automatic speech recognition (ASR) system. Model predictions are produced by measuring the mean temporal distance of phoneme vectors from the DNN and compared to subjective ratings of hearing-impaired and normal-hearing listener responses group in three databases that cover a variety of noise types and signal enhancement algorithms. We obtain an average correlation of 0.88 and outperform three baseline measures in most conditions.",0.0,5.05
104,1121,5.6    5    5.25    4.35    ,Voice Conversion with Conditional SampleRNN,"Here we present a novel approach to conditioning the SampleRNN [1] generative model for voice conversion (VC). Conventional methods for VC modify the perceived speaker identity by converting between source and target acoustic features. Our approach focuses on preserving voice content and depends on the generative network to learn voice style. We first train a multi-speaker SampleRNN model conditioned on linguistic features, pitch contour, and speaker identity using a multi-speaker speech corpus. Voice-converted speech is generated using linguistic features and pitch contour extracted from the source speaker, and the target speaker identity. We demonstrate that our system is capable of many-to-many voice conversion without requiring parallel data, enabling broad applications. Subjective evaluation demonstrates that our approach outperforms conventional VC methods.",0.0,5.05
111,1131,5.05    5.1    4.8    5.25    ,A Voice Conversion Framework with Tandem Feature Sparse Representation and Speaker-Adapted WaveNet Vocoder,"The statistical approach to voice conversion typically consists of a feature conversion module followed by a vocoder. The exemplar-based sparse representation marks a success in feature conversion when we only have a very limited amount of training data. While parametric vocoder is generally designed to simulate the mechanics of the human speech generation process under certain simplification assumptions, it doesn't work consistently well for all target applications. In this paper, we study two effective ways to make use of the limited amount of training data for voice conversion. Firstly, we study a novel technique for sparse representation that augments the spectral features with phonetic information, or Tandem Features. Secondly, we study the use of WaveNet vocoder that can be trained on multi-speaker and target speaker data to improve the vocoding quality. We evaluate that the proposed strategy with Tandem Features and WaveNet vocoder, and show that it provides performance improvement consistently over the traditional sparse representations framework in objective and subjective evaluations.",0.0,5.05
637,2299,5.75    5.05    4.9    4.5    ,Classification of Nonverbal Human Produced Audio Events: a Pilot Study,"The accurate classification of nonverbal human produced audio events opens the door to numerous applications beyond health monitoring. Voluntary events, such as tongue clicking and teeth chattering, may lead to a novel way of silent interface command. Involuntary events, such as coughing and clearing the throat, may advance the current state-of-the-art in hearing health research. The challenge of such applications is the balance between the processing capabilities of a small intra-aural device and the accuracy of classification. In this pilot study, 10 nonverbal audio events are captured inside the ear canal blocked by an intra-aural device.  The performance of three classifiers is investigated:  Gaussian Mixture Model (GMM), Support Vector Machine and  Multi-Layer Perceptron. Each classifier is trained using three different feature vector structures constructed using the mel-frequency cepstral (MFCC) coefficients and their derivatives. Fusion of the MFCCs with the auditory-inspired amplitude modulation features (AAMF) is also investigated. Classification is compared between binaural and monaural training sets as well as for noisy and clean conditions. The highest accuracy is achieved at 75.45% using the GMM classifier with the binaural MFCC+AAMF clean training set. Accuracy of 73.47% is achieved by training and testing the classifier with the binaural clean and noisy dataset.",0.0,5.050000000000001
689,2429,6    5.9    3.2    5.1    ,Infant Emotional Outbursts Detection in Infant-parent Spoken Interactions,"Detection of infant emotional outbursts, such as crying,  in large corpora of recorded infant speech, is essential to the study of dyadic social process, by which infants learn to identify and regulate their own emotions.  Such large corpora now exist with the advent of LENA speech monitoring systems, but are not labeled for emotional outbursts. This paper reports on our efforts to manually code child utterances as being of type ""laugh"", ""cry"", ""fuss"", ""babble"" and ""hiccup"", and to develop algorithms capable of performing the same task automatically. Human labelers achieve much higher rates of inter-coder agreement for some of these categories than for others. Linear discriminant analysis (LDA) achieves better accuracy on tokens that have been coded by two human labelers than on tokens that have been coded by only one labeler, but the difference is not as much as we expected, suggesting that the acoustic and contextual features being used by human labelers are not yet available to the LDA.  Convolutional neural network and hidden markov model achieve better accuracy than LDA, but worse F-score, because they over-weight the prior.  Discounting the transition probability does not solve the problem.",0.0,5.050000000000001
401,1705,5.1    5.05    5.1    5    ,Deep Convex Representations: Feature Representations for Bioacoustics Classification,"In this paper, a deep convex matrix factorization framework is
proposed for bioacoustics classification. Archetypal analysis, a
form of convex non-negative matrix factorization, is used for
acoustic modelling at each level of this framework. At first
level, the input feature matrix is factorized into an archetypal
dictionary and corresponding convex representations. The representation
matrix obtained at the first level is further factorized
into a dictionary and convex representations at second level.
This hierarchical factorization continues until a desired depth
is achieved. We observe that the dictionaries at different levels
model complimentary information present in the data. The
atoms of the dictionary learned at the first layer lie on convex
hull of the data, thus try to model the extremal behaviour. On
the contrary, atoms of the deeper dictionaries lie on the convex
hull as well as inside the convex hull. Hence, these dictionaries
can simultaneously model the extremal and average behaviour
of the data. The convex representations obtained from these
deeper dictionaries are referred as deep convex representations.
Due to inherent sparsity, they result in efficient classification
performance. Through experiments on two available bioacoustics
datasets, we show that the proposed approach yield comparable
or better results than state-of-art approaches.",0.0,5.0625
596,2172,5    5.05    5.1    5.1    ,The EURECOM Submission to the First DIHARD Challenge,"The first DIHARD challenge aims to promote speaker diarization research and to foster progress in domain robustness. This paper reports EURECOM's submission to the DIHARD challenge. It is based upon a low-resource, domain-robust binary key approach to speaker modelling. New contributions include the use of an infinite impulse response - constant Q Mel-frequency cepstral coefficient (ICMC) front-end, a clustering selection / stopping criterion algorithm based on spectral clustering and a mechanism to detect single-speaker trials. Experimental results obtained using the standard DIHARD database show that the contributions reported in this paper deliver relative improvements of 39% in terms of the diarization error rate over the baseline algorithm. An absolute DER of 29% on the evaluation set compares favourably with those of competing systems, especially given that the binary key system is highly efficient, running 63 times faster than real-time.",0.0,5.0625
560,2072,5    4.5    5.75    5    ,Neural Response Development During Distributional Learning,"We investigated online electrophysiological components of distributional learning, specifically of tones by listeners of a non-tonal language. German listeners were presented with a bimodal distribution of syllables with lexical tones from a synthesized continuum based on Cantonese level tones. Tones were presented in sets of four standards (within-category tokens) followed by a deviant (across-category token). Mismatch negativity (MMN) was measured. Earlier behavioral data showed that exposure to this bimodal distribution improved both categorical perception and perceptual acuity for level tones [1]. In the present study we present analyses of the electrophysiological response recorded during this exposure, i.e. the development of the MMN response during distributional learning. This development over time is analyzed using Generalized Additive Mixed Models and results showed that  the MMN amplitude increased for both within- and across-category tokens, reflecting higher perceptual acuity accompanying category formation. This is evidence that learners zooming in on phonological categories undergo neural changes associated with more accurate phonetic perception.",0.0,5.0625
456,1819,4.85    5.15    5.6    4.65    ,Deep Siamese Architecture Based Replay Detection for Secure Voice Biometrics,"Replay attacks are the simplest and the most easily accessible form of spoofing attacks on voice biometric systems and can be hard to detect by systems designed to identify spoofing attacks based on synthesised speech. In this paper, we propose a novel approach to evaluate the similarities between pairs of speech samples to detect replayed speech based on a suitable embedding learned by deep Siamese architectures. Specifically, we train a deep Siamese network to identify pairs of genuine speech samples and pairs of replayed speech samples as being ‘similar’ and mixed pairs of genuine and replayed speech to be identified as ‘dissimilar’. Siamese networks are particularly suited to this task and have been shown to be effective in problems where intra-class variability is large and the number of training samples per class is relatively small. The internal low-dimensional embedding learnt by the Siamese network to accomplish this task is then used as the basis for replay detection. The proposed approach outperforms state-of-the-art systems when evaluated on the ASVspoof 2017 challenge corpus without relying on fusion with other sub-systems.",0.0,5.0625
44,1018,5.25    5    5.85    4.15    ,Detection of Glottal Closure Instants in Degraded Speech Using Single Frequency Filtering Analysis,"Impulse-like characteristics of excitation occur at the glottal closure instant (GCI) due to sharp closure of the vibrating vocal folds in each glottal cycle. The GCIs are detected from the excitation component of the speech signal, and the excitation component is derived using inverse filtering or its variants. In this paper we propose a method for GCI detection based on single frequency filtering (SFF) of the speech signal. The SFF output has high signal-to-noise ratio (SNR) property in speech regions. The variance (across frequency) contour computed from the SFF output show rapid changes around the GCIs, and these rapid changes can be observed even when the speech signal is degraded. Thus the GCI locations can be extracted even from degraded speech using the SFF analysis. The robustness of the method is demonstrated for several cases of degradation of speech signal.",0.0,5.0625
201,1296,4.35    5.9    4.95    ,Single-Channel Dereverberation Using Direct MMSE Optimization and Bidirectional LSTM Networks,"Dereverberation is useful in hands-free communication and voice controlled devices for distant speech acquisition. Single-channel dereverberation can be achieved by applying a time-frequency (TF) mask to the short-time Fourier transform (STFT) representation of a reverberant signal. Recent approaches have used deep neural networks (DNNs) to estimate such masks. Previously proposed DNN-based mask estimation methods train a DNN to minimize the mean-squared-error (MSE) between the desired and estimated masks. Recent TF mask estimation methods for signal separation directly minimize instead the MSE between the desired and estimated STFT magnitudes. We apply this direct optimization concept to dereverberation. Moreover, as reverberation exceeds the duration of a single STFT frame, we propose to use a bidirectional long short-term memory (LSTM) network which is able to take the relation between multiple STFT frames into account. We evaluated our method for different reverberation times and source-microphone distances using simulated as well as measured room impulse responses of different rooms. An evaluation of the proposed method and a comparison with a state-of-the-art method demonstrate the superiority of our approach and its robustness to different acoustic conditions.",0.0,5.066666666666666
554,2061,4.35    5.05    4.9    6    ,Dictionary Augmented Sequence-to-Sequence Neural Network for Grapheme to Phoneme Prediction,"Both automatic speech recognition and text to speech systems need accurate pronunciations, typically obtained by using both a lexicon dictionary and a grapheme to phoneme (G2P) model. G2P typically struggle with predicting pronunciations for tail words, and we hypothesized that one reason is because they try to discover general pronunciation rules without using prior knowledge of the pronunciation of related words. Our new approach expands a sequence-to-sequence G2P model by injecting prior knowledge. In addition, our model can be updated without having to retrain a system. We show that our new model has significantly better performance for German, both on a tightly controlled task and on our real-world system. Finally, the simplification of the system allows for faster and easier scaling to other languages.",0.0,5.074999999999999
417,1742,5.75    5.05    5.1    4.4    ,Speaker Diarization with Enhancing Speech for the First DIHARD Challenge,"We design a novel speaker diarization system for the first DIHARD challenge by integrating several important modules of speech denoising, speech activity detection (SAD), i-vector design, and scoring strategy. One main contribution is the proposed long short-term memory (LSTM) based speech denoising model. By fully utilizing the diversified simulated training data and advanced network architecture using progressive multitask learning with dense structure, the denoising model demonstrates the strong generalization capability to realistic noisy environments. The enhanced speech can boost the performance for the subsequent SAD, segmentation and clustering. To the best of our knowledge, this is the first time we show significant improvements of deep learning based single-channel speech enhancement over state-of-the-art diarization systems in highly mismatch conditions. For the design of i-vector extraction, we adopt a residual convolutional neural network trained on large dataset including more than 30,000 people. Finally, by score fusion of different i-vectors based on all these techniques, our systems yield diarization error rates (DERs) of 24.56% and 36.05% on the evaluation sets of Track1 and Track2, which are both in the second place among 14 and 11 participating teams, respectively.",0.0,5.075
725,2516,4.15    5.75    4.4    6    ,DNN Driven Speaker Independent Audio-Visual Mask Estimation for Speech Separation,"Human auditory cortex excels at selectively suppressing background noise to focus on a target speaker. The process of selective attention in the brain is known to contextually exploit the available audio and visual cues to better focus on target speaker while filtering out other noises. In this study, we propose a novel deep neural network (DNN) based audiovisual (AV) mask estimation model. The proposed AV mask estimation model contextually integrates the temporal dynamics of both audio and noise-immune visual features for improved mask estimation and speech separation. For optimal AV features extraction and ideal binary mask (IBM) estimation, a hybrid DNN architecture is exploited to leverages the complementary strengths of a stacked long short term memory (LSTM) and convolution LSTM network. The comparative simulation results in terms of speech quality and intelligibility demonstrate significant performance improvement of our proposed AV mask estimation model as compared to audio-only and visual-only mask estimation approaches for both speaker dependent and independent scenarios.",0.0,5.075
35,995,5.25    5    5.15    4.9    ,UltraFit: a Speaker-friendly Headset for Ultrasound Recordings in Speech Sciences,"UltraFit is a headset for Ultrasound Tongue Imaging (UTI)
printed in Nylon; altogether, it weighs about 350 g. It was
developed through an iterative process of rapid prototyping a
proof of concept, asking for feedback from researchers and subjects
of the experiments, and instantly incorporating changes
based on their feedback into the design. We evaluated the UltraFit
headset by recording a speaker using an optical marker
tracking system that provides sub-millimeter tracking accuracy.
We show that the overall error range of the headset movement
for this speaker lies within 3mm with most errors lying in a
1-2mm range. This makes the headset potentially suitable for
speech science applications. Furthermore, we analyze the superior
usability of the headset compared to other existing designs
and describe the headsets development process.",0.0,5.075
357,1603,5.1    5    5    5.2    ,Permutation Invariant Training of Generative Adversarial Network for Monaural Speech Separation,"We explore generative adversarial networks (GANs) for speech separation, particularly with permutation invariant training (SSGAN-PIT). Prior work demonstrates that GANs can be implemented for suppressing additive noise in noisy speech waveform and improving perceptual speech quality. In this work, we train GANs for speech separation which enhances multiple speech sources simultaneously with the permutation issue addressed by the utterance level PIT in the training of the generator network. We propose operating GANs on the power spectrum domain instead of waveforms to reduce computation. To better explore time dependencies, recurrent neural networks
(RNNs) with long short-term memory (LSTM) are adopted for both generator and discriminator in this study. We evaluated SSGAN-PIT on the WSJ0 two-talker mixed speech separation task and found that SSGAN-PIT outperforms SSGAN without PIT and the neural networks based speech separation with or without PIT. The evaluation confirms the feasibility of the proposed model and training approach for efficient speech separation. The convergence behavior of permutation invariant training and adversarial training are analyzed.",0.0,5.075
569,2087,5.95    5.05    4.9    4.4    ,FACTS: a Hierarchical Task-based Control Model of Speech Incorporating Sensory Feedback,"We present a computational model of speech motor control that integrates vocal tract state prediction with sensory feedback. This hierarchical model, called FACTS, incorporates both a high-level and low-level controller. The high-level controller orchestrates linguistically-relevant speech tasks, which are represented as desired constrictions along the vocal tract (e.g., closure of the lips). The output of the high-level controller is passed to a low-level controller that can issue motor commands at the level of the speech articulators in order to accomplish the desired constrictions. In order to generate these articulatory motor commands, this low-level articulatory controller relies on an estimate of the current state of the vocal tract. This estimate combines internal predictions about the consequences of issued motor commands with auditory and somatosensory feedback from the vocal tract using an Unscented Kalman Filter based state estimation method. FACTS is able to replicate important aspects of human speech behavior, in that it reproduces: (i) stable speech behavior in the presence of noisy motor and sensory systems, (ii) partial acoustic compensation to auditory feedback perturbations,  (iii) complete compensations to mechanical perturbations only when they interfere with current production goals, and (iv) the observed relationship between sensory acuity and response to sensory perturbations.",0.0,5.075
103,1120,5.1    5.15    5.1    4.95    ,Multiple Instance Deep Learning for Weakly Supervised Small-Footprint Audio Event Detection,"State-of-the-art audio event detection (AED) systems rely on supervised learning using strongly labeled data. However, this dependence severely limits scalability to large-scale datasets where fine resolution annotations are too expensive to obtain. In this paper, we propose a small-footprint multiple instance learn- ing (MIL) framework for multi-class AED using weakly annotated labels. The proposed MIL framework uses audio embed- dings extracted from a pre-trained convolutional neural network as input features. We show that by using audio embeddings the MIL framework can be implemented using a simple DNN with performance comparable to recurrent neural networks.
We evaluate our approach by training an audio tagging system using a subset of AudioSet, which is a large collection of weakly labeled YouTube video excerpts. Combined with a late- fusion approach, we improve the F1 score of a baseline audio tagging system by 17%. We show that audio embeddings extracted by the convolutional neural networks significantly boost the performance of all MIL models. This framework reduces the model complexity of the AED system and is suitable for applications where computational resources are limited.",0.0,5.075
590,2155,5    4.3    5.9    5.15    ,A Lightly Supervised Approach to Detect Stuttering in Children's Speech,"In speech pathology, new assistive technologies using ASR and machine learning approaches are being developed for detecting speech disorder events. Classically-trained ASR model tends to remove disfluencies from spoken utterances, due to its focus on producing clean and readable text output. However, diagnostic systems need to be able to track speech disfluencies, such as stuttering events, in order to determine the severity level of stuttering. To achieve this, ASR systems must be adapted to recognise full verbatim utterances, including pseudo-words and non-meaningful part-words. This work proposes a training regime to address this problem, and preserve a full verbatim output of stuttering speech. We use a lightly-supervised approach using task-oriented lattices to recognise the stuttering speech of children performing a standard reading task. This approach improved the WER by 27.8% relative to a baseline that uses word-lattices generated from the original prompt. The improved results preserved 63% of stuttering events (including sound, word, part-word and phrase repetition, and revision). This work also proposes a separate correction layer on top of the ASR that detects prolongation events (which are poorly recognised by the ASR). This increases the percentage of preserved stuttering events to 70%.",0.0,5.0875
723,2512,3.95    5.7    4.9    5.8    ,An Ultrasound Study of Gemination in Coronal Stops in Eastern Oromo,"This study extends the use of ultrasound methodology to stops in Eastern Oromo (Cushitic; Ethiopia) to examine the link between gemination, laryngeal features, and tongue shape.

Ultrasound data were collected from 5 native speakers of Eastern Oromo. Tokens consisted of 12 repetitions per speaker of [tʰ, t’, d, ɗ] and six of [ttʰ, tt’, dd, ɗɗ] in the environment of a_a. Tongue images at the point of maximum constriction during the stop closure were traced following Kochetov et al. (2014) and their coordinates submitted to linear mixed effects models.

Results indicated differences in tongue shape between singletons and geminates, especially for ejectives and implosives. Singleton ejectives displayed raised tongue bodies not found in geminate ejectives. Singleton implosives resembled voiceless stops, but geminate implosives were variably produced with tongue body raising.

I suggest that the results can be attributed to fortition in geminates. Tongue body raising in singleton ejectives may be an enhancement strategy to the ejective contrast that is not necessary in longer geminates. The singleton implosive resembling a voiceless aspirated stop is predicted by Lloret (1994) while the geminate tongue body raising may be retraction, c.f. Payne (2006). The results support a link between tongue, larynx, and gemination.",0.0,5.0875
74,1065,4.15    5.15    5.05    6    ,Regional Variation of /R/ in Swiss German Dialects,"German-speaking Europe is known to feature substantial regional variation in the articulation of /r/. According to historical atlases, this is particularly true for the most southwestern fringe of the region, i.e. German-speaking Switzerland. Large-scale, multilocality studies that show an updated picture of regional variation in this region are lacking, however. To this end, we coded /r/s of almost 3,000 speakers from 438 localities on a predominantly auditory basis, using data crowdsourced through a smartphone app. We report substantial regional variation, with uvular articulations especially dominant in the Northwest and the Northeast and alveolar -  particularly tapped - articulations prevalent in the Midlands. We further provide exemplary evidence of an urban ([ʁ]) vs. rural stratification ([ɾ]) in the Northwest. This contribution further discusses (a) issues related to the coding of /r/, given the volatile articulatory and acoustic properties of /r/s and (b) the benefits and pitfalls of the crowdsourcing methodology applied more generally.",0.0,5.0875
720,2502,4.85    6    4.35    5.15    ,Analysis and Detection of Phonation Mode in Singing Voice Using Excitation Source Features,"In this study, classification of the phonation modes in singing voice is carried out. Phonation modes in singing voice can be described using four categories: breathy, neutral, flow and pressed phonations. Previous studies on the classification of phonation modes use voice quality features derived from inverse filtering which lack in accuracy. This is due to difficulty in deriving the excitation source features using inverse filtering from singing voice. We propose to use the excitation source features that are derived directly from the signal. It is known that, the characteristics of the excitation source vary in different phonation types due to the vibration of the vocal folds together with the respiratory effort (lungs effort). In the present study, we are exploring excitation source features derived from the modified zero frequency filtering (ZFF) method. Apart from excitation source features, we also explore cepstral coefficients derived from single frequency filtering (SFF) method for the analysis and classification of phonation types in singing voice.",0.0,5.0875
82,1085,4.1    5.1    6    5.2    ,"Automatic Speech Recognition System Development in the ""Wild""","The standard framework for developing an automatic speech recognition (ASR) system is to generate training and development data for building the system, and evaluation data for the final performance analysis. All the data is assumed to come from the domain of interest. Though this framework is matched to some tasks, it is more challenging for systems that are required to operate over broad domains, or where the ability to collect the required data is limited. This paper discusses ASR work performed under the IARPA MATERIAL program, which is aimed at cross-language information retrieval, and examines this challenging scenario. In terms of available data, only limited narrow-band conversational telephone speech data was provided. However, the system is required to operate over a range of domains, including broadcast data. As no data is available for the broadcast domain, this paper proposes an approach for system development based on scraping ""related"" data from the web, and using ASR system confidence scores as the primary metric for developing the acoustic and language model components. As an initial evaluation of the approach, the Swahili development language is used, with the final system performance assessed on the IARPA MATERIAL Analysis Pack 1 data.",0.0,5.1
678,2409,5    5.1    5.85    4.45    ,Cycle-Consistent Speech Enhancement,"Feature mapping using deep neural networks is an effective approach for single-channel speech enhancement. Noisy features are transformed to the enhanced ones through a mapping network and the mean square errors between the enhanced and clean features are minimized. In this paper, we propose a cycle-consistent speech enhancement (CSE) in which an additional inverse mapping network is introduced to reconstruct the noisy features from the enhanced ones. A cycle-consistent constraint is enforced to minimize the reconstruction loss. Similarly, a backward cycle of mappings is performed in the opposite direction with the same networks and losses. With cycle-consistency, the speech structure is well preserved in the enhanced features while noise is effectively reduced such that the feature mapping network generalizes better to unseen data. In cases where only unparalleled noisy and clean data is available for training, two discriminator networks are used to distinguish the reconstructed clean and noisy features from the real ones. The discrimination losses are jointly optimized with reconstruction losses through adversarial multi-task learning. Evaluated on the CHiME-3 dataset, the proposed CSE achieves 19.60% and 6.69% relative word error rate improvements respectively when using or without using parallel clean and noisy speech data.",0.0,5.1
34,993,5.1    5.1    5.1    5.1    ,Attentive Statistics Pooling for Deep Speaker Embedding,"This paper proposes attentive statistics pooling for deep speaker embedding in text-independent speaker verification. In conventional speaker embedding, frame-level features are averaged over all the frames of a single utterance to form an utterance-level feature. Our method utilizes an attention mechanism to give different weights to different frames and generates not only weighted means but also weighted standard deviations. In this way, it can capture long-term variations in speaker characteristics more effectively. An evaluation on the NIST SRE 2012 and the VoxCeleb data sets shows that it reduces equal error rates (EERs) from the conventional method by 7.5% and 8.1%, respectively.",0.0,5.1
348,1583,5.25    4.9    5.1    5.15    ,Computational Modeling of Conversational Humor in Psychotherapy,"Humor is an important social construct that serves several roles in human communication. Though subjective, it is culturally ubiquitous and is often used to diffuse tension, specially in intense conversations such as those in psychotherapy sessions. Automatic recognition of humor has been of considerable interest in the natural language processing community thanks to its relevance in conversational agents. In this work, we present a model for humor recognition in Motivational Interviewing based psychotherapy sessions. We use a Long Short Term Memory (LSTM) based recurrent neural network sequence model trained on dyadic conversations from psychotherapy sessions and our model outperforms a standard baseline with linguistic humor features.",0.0,5.1
224,1339,5.1    5.2    5.1    5    ,A GPU-based WFST Decoder with Exact Lattice Generation,"We describe initial work on an extension of the Kaldi toolkit
that supports weighted finite-state transducer (WFST) decoding
on Graphics Processing Units (GPUs). We implement token
recombination as an atomic GPU operation in order to fully
parallelize the Viterbi beam search, and propose a dynamic load
balancing strategy for more efficient token passing scheduling
among GPU threads. We also redesign the exact lattice
generation and lattice pruning algorithms for better utilization
of the GPUs. Experiments on the Switchboard corpus show
that the proposed method achieves identical 1-best results and
lattice quality in recognition and confidence measure tasks,
while running 3 to 15 times faster than the single process Kaldi
decoder. The above results are reported on different GPU
architectures. Additionally we obtain a 46-fold speedup with
sequence parallelism and multi-process service (MPS) in GPU.",0.0,5.1
543,2028,5.15    4.4    4.95    5.9    ,AVA-Speech: a Densely Labeled Dataset of Speech Activity in Movies,"Speech activity detection (or endpointing) is an important processing step for applications such as speech recognition, language identification and speaker diarization. Both audio- and vision-based approaches have been used for this task in various settings, often tailored toward end applications. However, much of the prior work reports results in synthetic settings, on task-specific datasets, or on datasets that are not openly available. This makes it difficult to compare approaches and understand their strengths and weaknesses. In this paper, we describe a new dataset which we will release publicly containing densely labeled speech activity in YouTube videos, with the goal of creating a shared, available dataset for this task. The labels in the dataset annotate three different speech activity conditions: clean speech, speech co-occurring with music, and speech co-occurring with noise, which enable analysis of model performance in more challenging conditions based on the presence of overlapping noise. We report benchmark performance numbers on AVA-Speech using off-the-shelf, state-of-the-art audio and vision models that serve as a baseline to facilitate future research.",0.0,5.1
97,1108,5.2    5.9    4.1    5.2    ,Tongue Segmentation with Geometrically Constrained Snake Model,"Articulatory visualization aims at providing precise visual information of the speech organs (tongue, lips, and velum) that accompany with speech signals. It is often critical in fundamental studies and certain applications. To construct an articulatory visualization system, the profile of the speech organs must be segmented from images acquired by various types of medical equipments. In this paper, a geometrically constrained snake model is proposed to segment tongue profiles from mid-sagittal MRI to deal with the situation in which the tongue contacts with the surrounding structures and the target object with inhomogeneity nature. The result indicates that the proposed method improves segmentation performance significantly compared with the traditional snake model.",0.0,5.1000000000000005
565,2081,5.15    4.4    5.75    ,Implementing DIANA to Model Isolated Auditory Word Recognition in English,"DIANA, an end-to-end computational model of spoken word recognition, was previously used to simulate auditory lexical decision experiments in Dutch. A single test conducted for North American English showed promising results as well. However, this simulation used a relatively small amount of data collected in the pilot phase of the Massive Auditory Lexical Decision (MALD) project. Additionally, already existing acoustic models were implemented. In this paper, we expand the analysis of MALD data by including a larger sample of both stimuli and participants. Acknowledging that most speech humans hear is conversational speech, we also test new acoustic models created using spontaneous speech corpora. Simulations successfully replicate expected trends in word competition and show plausible competitors as the signal unfolds, but acoustic model accuracy should be improved. Despite the number of responses per word being relatively small (never more than five), correlations between model estimates and participants' responses are moderate. Future directions in acoustic model training and simulating MALD data are discussed.",0.0,5.1000000000000005
508,1940,4.6    5.05    5    5.8    5.1    ,Integrating Spectral and Spatial Features for Multi-Channel Speaker Separation,"This paper tightly integrates spectral and spatial information for deep learning based multi-channel speaker separation. The key idea is to localize individual speakers so that an enhancement network can be used to separate the speaker from an estimated direction and with specific spectral characteristics. To determine the direction of the speaker of interest, we identify time-frequency (T-F) units dominated by that speaker and only use them for direction of arrival (DOA) estimation. The speaker dominance at each T-F unit is determined by a two-channel permutation invariant training network, which combines spectral and interchannel phase patterns at the input feature level. In addition, beamforming is tightly integrated in the proposed system by exploiting the magnitudes and phase pro-duced by T-F masking based beamforming. Strong separation performance has been observed on a spatialized reverberant version of the wsj0-2mix corpus.",0.0,5.109999999999999
422,1749,5.75    4.15    4.8    5.75    ,BUT System for DIHARD Speech Diarization Challenge 2018,"This paper presents the approach developed by the BUT team for the first DIHARD speech diarization challenge, which is based on our Bayesian Hidden Markov Model with eigenvoice priors system. Besides the description of the approach, we provide a brief analysis of different techniques and data processing methods tested on the development set. We also introduce a simple attempt for overlapped speech detection that we used for attaining cleaner speaker models and reassigning overlapped speech to multiple speakers. 
Finally, we present results obtained on the evaluation set and discuss findings we made during the development phase and with the help of the DIHARD leaderboard feedback.",0.0,5.1125
337,1558,4.45    5    5.9    5.1    ,Machine Speech Chain with One-shot Speaker Adaptation,"In previous work, we developed a closed-loop speech chain model based on deep learning, in which the architecture enabled the automatic speech recognition (ASR) and text-to-speech synthesis (TTS) components to mutually improve their performance. This was accomplished by the two parts teaching each other using both labeled and unlabeled data. This approach could significantly improve model performance within a single-speaker speech dataset, but only a slight increase could be gained in multi-speaker tasks. Furthermore, the model is still unable to handle unseen speakers. In this paper, we present a new speech chain mechanism by integrating a speaker recognition model inside the loop. We also propose extending the capability of TTS to handle unseen speakers by implementing one-shot speaker adaptation. This enables TTS to mimic voice characteristics from one speaker to another with only a one-shot speaker sample, even from a text without any speaker information. In the speech chain loop mechanism, ASR also benefits from the ability to further learn an arbitrary speaker’s characteristics from the generated speech waveform, resulting in a significant improvement in the recognition rate.",0.0,5.1125
419,1744,4.35    5.75    5.3    5.1    ,Imbalance Learning-based Framework for Fear Recognition in the MediaEval Emotional Impact of Movies Task,"Fear recognition, which aims at predicting whether a movie segment can induce fear or not, is a promising area in movie emotion recognition. Research in this area, however, has reached a bottleneck. Difficulties may partly result from the imbalanced database. In this paper, we propose an imbalance learning-based framework for movie fear recognition. A data rebalance module is adopted before classification. Several sampling methods, including the proposed softsampling and hardsampling which combine the merits of both undersampling and oversampling, are explored in this module. Experiments are conducted on the MediaEval 2017 Emotional Impact of Movies Task. Compared with the current state-of-the-art, we achieve an improvement of 8.94% on F1, proving the effectiveness of proposed framework.",0.0,5.125
213,1313,5.1    5.15    5    5.25    ,A Comparison of Speaker-based and Utterance-based Data Selection for Text-to-Speech Synthesis,"Building on previous work in subset selection of training data  for text-to-speech (TTS), this work compares speaker-level and utterance-level selection of TTS training data, using acoustic features to guide selection. We find that speaker-based selection is more effective than utterance-based selection, regardless of whether selection is guided by a single feature or a combination of features. We use US English telephone data collected for automatic speech recognition to simulate the conditions of TTS training on low-resource languages. Our best voice achieves a human-evaluated WER of 29.0% on semantically-unpredictable sentences. This constitutes a significant improvement over our baseline voice trained on the same amount of randomly selected utterances, which performed at 42.4% WER.  In addition to subjective voice evaluations with Amazon Mechanical Turk, we also explored objective voice evaluation using mel-cepstral distortion.  We found that this measure correlates strongly with human evaluations of intelligibility, indicating that it may be a useful method to evaluate or pre-select voices in future work.",0.0,5.125
584,2129,5.75    4.85    4.8    5.1    ,Discourse Marker Detection for Hesitation Events on Mandarin Conversation,The occurrence of hesitation events in spontaneous conversations can be associated with the difficulties in memory recall. One indicator of hesitation in speech in Taiwanese Mandarin is the usage of discourse markers. This paper introduces an approach to the detection of discourse markers that denote hesitation events. We propose a sequential labeling model to detect discourse markers in conversations by taking information on both acoustic level and word level into account. Experimental results show the integration of word-level acoustic feature extraction network significantly enhances the detection performance. Our approach for further applications is also discussed.,0.0,5.125
477,1862,5.4    5.9    4.85    4.35    ,Relating Articulatory Motions in Different Speaking Rates,"Movements of articulators (e.g., tongue, lips and jaw) in different speaking rates are related in a complex manner. In this work, we examine the underlying function to transform articulatory movements involved in producing speech at a neutral speaking rate into those at fast and slow speaking rates (N2F and N2S). For this we use articulatory movement data collected from five subjects using an Electromagnetic articulograph at neutral, fast and slow speaking rates. As candidate transformation functions (TF), we use affine transformations with a diagonal matrix and a full matrix and a nonlinear function modeled by a deep neural network (DNN). Since the duration of an utterance in different speaking rates would typically be unequal, it is required to time align the articulatory movement trajectories, which, in turn, affects the TF learnt. Therefore, we propose an iterative algorithm to alternately optimize for the TF and the time alignments. Subject specific experiments reveal that while N2F transformation can be well described by an affine transformation with a full matrix, N2S transformation is better represented by a more complex nonlinear function modeled by a DNN. This could be because subjects exhibit gross articulatory movements during fast speech and hyper-articulate while producing slow speech.",0.0,5.125
79,1079,5    5.25    5.15    ,Identifying Schizophrenia Based on Temporal Parameters in Spontaneous Speech,"Schizophrenia is a neurodegenerative disease with spectrum disorder,
consisting of groups of different deficits. It is, among other
symptoms, characterized by reduced information processing speed and
deficits in verbal fluency. In this study we focus on the speech
production fluency of patients with schizophrenia compared to
healthy controls. Our aim is to show that a temporal speech
parameter set consisting of articulation tempo, speech tempo and
various pause-related indicators, originally defined for the sake of
early detection of various dementia types such as Mild Cognitive
Impairment and early Alzheimer's Disease, is able to capture
specific differences in the spontaneous speech of the two groups. We
tested the applicability of the temporal indicators by machine
learning (i.e. by using Support-Vector Machines). Our results show
that members of the two speaker groups could be identified with
classification accuracy scores of between 70-80% and F-measure
scores between 81% and 87%. Our detailed examination revealed
that, among the pause-related temporal parameters, the most useful
for distinguishing the two speaker groups were those which took into
account both the silent and filled pauses.",0.0,5.133333333333334
142,1199,5.7    4    5.1    5.75    ,Measuring the Cognitive Load of Synthetic Speech Using a Dual Task Paradigm,"We present a methodology for measuring the cognitive load (listening effort) of synthetic speech using a dual task paradigm. Cognitive load is calculated from changes in a listener’s perfor- mance on a secondary task (e.g., reaction time to decide if a visually-displayed digit is odd or even). Previous related stud- ies have only found significant differences between the best and worst quality systems but failed to separate the systems that lie in between. A paradigm that is sensitive enough to detect differences between state-of-the-art, high quality speech syn- thesizers would be very useful for advancing the state of the art. In our work, four speech synthesis systems from a previ- ous Blizzard Challenge, and the corresponding natural speech, were compared. Our results show that reaction times slow down as speech quality reduces, as we expected: lower qual- ity speech imposes a greater cognitive load, taking resources away from the secondary task. However, natural speech did not have the fastest reaction times. This intriguing result might indicate that, as speech synthesizers attain near-perfect intelli- gibility, this paradigm is measuring something like the listener’s level of sustained attention and not listening effort.",0.0,5.137499999999999
36,996,5    5.1    5.2    5.25    ,Bags in Bag: Generating Context-Aware Bags for Tracking Emotions from Speech,"Whereas systems based on deep learning have been proposed to learn efficient representations of emotional speech data, methods such as Bag-of-Audio-Words (BoAW) have yielded similar or even better performance while providing understandable representations of the data. In those representations, however, context information is overlooked as the BoAW include only local information. In this paper, we propose to learn a novel representation ‘Bag-of-Context-Aware-Words’ that encapsulates the context with neighbouring frames of BoAW; segment-level BoAW are extracted in the first layer which are then utilised to create a final instance-level bag. Such a hierarchical structure of BoAW enables the system to learn representations with context information. To evaluate the effectiveness of the method, we perform extensive experiments on a time- and value-continuous spontaneous emotion database: RECOLA. The results show that, the best segment length for valence is twice of that for arousal, suggesting that the prediction of the emotional valence requires more context information than the prediction of arousal, and the performance obtained on RECOLA with the proposed Bag-of-Context-Aware-Words outperforms all previously reported results.",0.0,5.1375
499,1914,5.85    4    5.75    4.95    ,Evolving Learning for Analysing Mood-Related Infant Vocalisation,"Infant vocalisation analysis plays an important role in the study of the development of pre-speech capability of infants, while machine-based approaches nowadays emerge with an aim to advance such an analysis. However, conventional machine learning techniques require heavy feature-engineering and refined architecture designing. In this paper, we present an evolving learning framework to automate the design of neural network structures for infant vocalisation analysis. In contrast to manually searching by trial and error, we aim to automate the search process in a given space with less interference. This framework consists of a controller and its child networks, where the child networks are built according to the controller's estimation. When applying the framework to the Interspeech 2018 Computational Paralinguistics (ComParE) Crying Sub-challenge, we discover several deep recurrent neural network structures, which are able to deliver competitive results to the best ComParE baseline method.",0.0,5.1375
537,2015,5.05    5.25    5    5.25    ,End-to-end Deep Neural Network Age Estimation,"In this paper, we apply the recently proposed x-vector 
neural network architecture for the task of age estimation. This architecture maps a variable length utterance into a fixed dimensional embedding which
retains the relevant sequence level information. This is achieved by a 
temporal pooling layer. From the embedding, a series of layers is applied to make predictions. The full network is trained end-to-end in a discriminative
fashion.
This kind of network is starting to outperform the
state-of-the-art i-vector embeddings in tasks like speaker and language
recognition. Motivated by this, 
we investigated the optimum way to train x-vectors for the age estimation task. Despite that a regression objective is typical for this task, we found that optimizing a mixture of classification and
regression losses provides better results. We trained our models on the NIST SRE08 dataset and evaluated on SRE10. The proposed 
approach improved mean absolute error (MAE) 
by 12\% w.r.t the i-vector baseline.",0.0,5.1375
482,1873,5.1    4.9    5.9    4.9    4.9    ,Prominence-based Evaluation of L2 Prosody,"Prosody in terms of word and sentence stress is one of the most difficult features for many second language (L2) speakers to learn and it can be hypothesized that assessing the learner's prosodic abilities could provide a good measure for assessing the learners' spoken language skills in general. Automatic assessment is, however, dependent on reliable automatic analyses of prosodic features for comparing the productions between native (L1) and L2 speech. Here we investigate, whether estimated prosodic prominence levels of syllables can be used to predict the prosodic competence of Finnish learners of Swedish. Syllable level prominence was estimated for 99 L2 and 25 native Swedish utterances using continuous wavelet transform analysis with combinations of f0, energy, and duration features. The L2 utterances were assessed by four expert raters using the revised CEFR scale for prosodic features. Correlations of prominence estimates for L2 utterances with estimates for L1 utterances and linguistic stress patterns were used as a measure of prosodic proficiency of the L2 speakers. The results show that these estimates correlate significantly with the assessments of expert raters. Overall, the results provide strong support for the use of the wavelet-based prominence estimation techniques in automatic assessment of L2 proficiency.",0.0,5.140000000000001
286,1456,5    5.2    5.35    5.05    ,ESPnet: End-to-End Speech Processing Toolkit,"This paper introduces a new open source platform for end-to-end speech processing named ESPnet. ESPnet mainly focuses on end-to-end automatic speech recognition (ASR), and adopts widely-used dynamic neural network toolkits, Chainer and PyTorch, as a main deep learning engine. ESPnet also follows the Kaldi ASR toolkit style for data processing, feature extraction/format, and recipes to provide a complete setup for speech recognition and other speech processing experiments. This paper explains a major architecture of this software platform, several important functionalities, which differentiate ESPnet from other open source ASR toolkits, and experimental results with major ASR benchmarks.",0.0,5.1499999999999995
375,1649,3.95    6    5.65    5    ,The Individual and the System: Assessing the Stability of the Output of a Semi-automatic Forensic Voice Comparison System,"Semi-automatic systems based on traditional linguistic-phonetic features are increasingly being used for forensic voice comparison (FVC) casework. In this paper, we examine the stability of the output of a semi-automatic system, based on the long-term formant distributions (LTFDs) of F1, F2, and F3, as the channel quality of the input recordings decreases. Cross-validated, calibrated GMM-UBM log likelihood-ratios (LLRs) were computed for 97 Standard Southern British English speakers under four conditions. In each condition the same speech material was used, but the technical properties of the recordings changed (high quality studio recording, landline telephone recording, high bit-rate GSM mobile telephone recording and low bit-rate GSM mobile telephone recording). Equal error rate (EER) and the log LR cost function (Cllr) were compared across conditions. System validity was found to decrease with poorer technical quality, with the largest differences in EER (21.66%) and Cllr (0.46) found between the studio and the low bit-rate GSM conditions. However, importantly, performance for individual speakers was affected differently by channel quality. Speakers that produced stronger evidence overall were found to be more variable. Mean F3 was also found to be a predictor of LLR variability, however no effects were found based on speakers’ voice quality profiles.",0.0,5.15
288,1459,5.25    4.3    5.1    5.95    ,Fast Derivation of Cross-lingual Document Vectors from Self-attentive Neural Machine Translation Model,"A universal cross-lingual representation of documents, which
can capture the underlying semantics is very useful in many
natural language processing tasks. In this paper, we develop
a new document vectorization method which effectively selects
the most salient sequential patterns from the inputs to create
document vectors via a self-attention mechanism using a neural
machine translation (NMT) model. The model used by our
method can be trained with parallel corpora that are unrelated
to the task at hand. During testing, our method will take a
monolingual document and convert it into a “Neural machine
Translation framework based cross-lingual Document Vector”
(NTDV). NTDV has two comparative advantages.
Firstly, the NTDV can be produced by the forward pass
of the encoder in the NMT, and the process is very fast and
does not require any training/optimization. Secondly, our model
can be conveniently adapted from a pair of existing attention based
NMT models, and the training requirement on parallel
corpus can be reduced significantly. In a cross-lingual document
classification task, our NTDV embeddings surpass the
previous state-of-the-art performance in the English-to-German
classification test, and, to our best knowledge, it also achieves
the best performance among the fast decoding methods in the
German-to-English classification test",0.0,5.15
246,1377,5.25    5.1    5.1    5.15    ,Integrating Recurrence Dynamics for Speech Emotion Recognition,"We investigate the performance of features that can capture nonlinear recurrence dynamics embedded in the speech signal for the task of Speech Emotion Recognition (SER). Reconstruction of the phase space of each speech frame and the computation of its respective Recurrence Plot (RP) reveals complex structures which can be measured by performing Recurrence Quantification Analysis (RQA). These measures are aggregated by using statistical functionals over segment and utterance periods. We report SER results for the proposed feature set on three databases using different classification methods. When fusing the proposed features with traditional feature sets, e.g., [1], we show an improvement in unweighted accuracy of up to 5.7% and 10.7% on Speaker-Dependent (SD) and Speaker-Independent (SI) SER tasks, respectively, over the baseline [1]. Following a segment-based approach we demonstrate state-of-the-art performance on IEMOCAP using a Bidirectional Recurrent Neural Network.",0.0,5.15
127,1158,5.8    5.65    4.95    4.25    ,Self-Attentive Speaker Embeddings for Text-Independent Speaker Verification,"This paper introduces a new method to extract speaker embeddings from a deep neural network (DNN) for text-independent speaker verification. Usually, speaker embeddings are extracted from a speaker-classification DNN that averages the hidden vectors over the frames of a speaker; the hidden vectors produced from all the frames are assumed to be equally important. We relax this assumption and compute the speaker embedding as a weighted average of a speaker's frame-level hidden vectors, and their weights are automatically determined by a self-attention mechanism. The effect of multiple attention heads are also investigated to capture different aspects of a speaker's input speech. Finally, a PLDA classifier is used to compare pairs of embeddings. The proposed self-attentive speaker embedding system is compared with a strong DNN embedding baseline on NIST SRE 2016. We find that the self-attentive embeddings achieve superior performance. Moreover, the improvement produced by the self-attentive speaker embeddings is consistent with both short and long testing utterances.",0.0,5.1625
473,1851,5.1    4.4    5.25    5.9    ,What Do Classifiers Actually Learn? a Case Study on Emotion Recognition Datasets,"In supervised learning, a typical method to ensure that a classifier has desirable generalization properties, is to split the available data into training, validation and test subsets. Given a proper data split, we typically then trust our results on the test data. But what do classifiers actually learn? In this case study we show how important it is to analyze precisely the available data, its inherent dependencies w.r.t. class labels, and present an example of a popular database for speech emotion recognition, where a minor change of the data split results in an accuracy decrease of about 55% absolute, leading to the conclusion that linguistic content has been learned instead of the desired speech emotions.",0.0,5.1625
724,2513,5.8    4.75    4.1    6    ,Truncation and Compression in Southern German and Australian English,"Nuclear pitch accents are realized differently when there is little sonorant material (as in monosyllabic compared to disyllabic words): Southern British English speakers compress rises and falls, while Northern German speakers truncate falls and compress rises [1] (Grabe 1998). This leads to different phonetic surface patterns for final falls. Within these languages, dialectal variation affects alignment and the frequency of occurrence of nuclear tunes. We test whether the differences in compression and truncation use are a stable cross-linguistic phenomenon (and occur in other varieties of English and German) or whether they are limited to the varieties tested in [1]. Here, we investigated productions of rises and falls in Australian English and Southern German in words with different proportions of sonorant material. Australian English speakers compressed rises and falls, while Southern German speakers only compressed rises but truncated falls, consistent with Grabe’s findings for Southern British English and Northern German. This indicates consistent use of strategies within a language, even though the varieties under investigation display other phonetic differences from previous varieties tested. We discuss implications of these findings for automatic labelling..",0.0,5.1625
22,69,4.05    5.9    5.15    5.55    ,Performance Analysis of the 2017 NIST Language Recognition Evaluation,"The 2017 NIST Language Recognition Evaluation (LRE) was held in the autumn of 2017. Similar to past LREs, the basic task in LRE17 was language detection, with an emphasis on discriminating closely related languages (14 in total) selected from 5 language clusters. LRE17 featured several new aspects including: audio data extracted from online videos; a development set for system training and development use; log-likelihood system output submissions; a normalized cross-entropy performance measure as an alternative metric; and, the release of a baseline system developed using the NIST Speaker and Language Recognition Evaluation (SLRE) toolkit for participant use. A total of 18 teams from 25 academic and industrial organizations participated in the evaluation and submitted 79 valid systems under  fixed and open training conditions first introduced in LRE15. In this paper, we report an in-depth analysis of system performance broken down by multiple factors such as data source and gender,  as well as a cross-year performance comparison of leading systems from LRE15 and LRE17 to measure progress over the 2-year period. In addition, we present a comparison of primary versus ""single best"" submissions to understand the effect of fusion on overall performance.",0.0,5.1625
613,2222,4    5.75    5.05    5.9    ,Modeling Self-Reported and Observed Affect from Speech,"Listeners hear joy/sadness and engagement/indifference in speech, even when linguistic content is neutral. We measured audible emotion in spontaneous speech and related it to self-reports of affect in response to questions, such as ``Are you hopeful?"" Spontaneous speech and self-reports were both collected in sessions with an interactive mobile app and used to compare three affect measurements: self-report; listener judgement; and machine score. The app adapted a widely-used measure of affective state to collect self-reported positive/negative affect, and it engaged users in spoken interactions. Each session elicited 11 affect self-reports and captured about 9 minutes of speech; with 118 sessions by psychiatric patients and 227 sessions by non-clinical users.  Speech recordings were evaluated for arousal and valence by clinical experts and by computer analysis of acoustic (non-linguistic) variables. The affect self-reports were reasonably reliable ($\alpha$ 0.73 to 0.84). Combined affect ratings from clinical-expert listeners produced reliable ratings per session ($\alpha$ 0.75 to 0.99), and acoustic feature analysis matched the expert ratings fairly well ($0.36<r<0.72$, mean 0.57), but neither human nor computed scores had high correlation with standard affect self-reported values. These results are discussed in relation to common methods of developing and evaluating affect analysis.",0.0,5.175000000000001
336,1556,5    5    5.9    4.85    ,Pitch Characteristics of L2 English Speech by Chinese Speakers: a Large-scale Study,"AI-powered English learning apps are used by hundreds of millions of people across the globe on a daily basis. This presents a great opportunity for the study of L2 speech. On one hand, the amount of data accessible for research is very large and rapidly growing; on the other hand, new theories and understanding of L2 speech can be continually tested and revised through real-life and real-time applications.
    This paper presents a study of pitch characteristics of L2 English speech using a large-scale dataset from a language learning app. Our dataset contains 180,000 spoken utterances which amount to 240 hours of speech. The results show that compared to L1, L2 English has narrower pitch range and slower rate of pitch change, but more small “ripples” on the pitch contour. The percentage of F0 rise time is higher in L2, and the maximum F0 in an utterance is realized later (with respect to the onset of the word on which the maximum F0 resides). These results suggest that the influence of L1 on L2 prosody is more complex than previously demonstrated, and they shed light on L2 prosody assessment and learning.",0.0,5.1875
76,1074,5.05    5.1    4.75    5.9    ,Structural Effects on Properties of Consonantal Gestures in Tashlhiyt,"Tashlhiyt Berber is a language in which every consonant can take up the nucleus position in a syllable. The present study investigates how gestural properties are modified when the consonants occur in different syllable positions (onset, nucleus, coda). Furthermore, the effect of higher structural components such as morphology on the respective gestural organization patterns are examined. 
Therefore, we collected articulographic data for different consonantal roots, such as /bdg/ and /gzm/ with varying affixes, entailing different syllabification patterns in Tashlhiyt. Consonantal properties in different syllable positions are investigated with respect to their intragestural properties and intergestural properties, i.e. bonding strength. Furthermore, gestural coherence with respect to prefixation were examined. 
Results reveal that consonantal gestures were not modified on the intragestural level in terms of duration, velocity, stiffness or displacement, when the morphological structure was kept constant. However, on the intergestural level syllable relation was encoded, revealing a tighter bonding for onset-nucleus relations than for heterosyllabic sequences. Furthermore, when changing the morphological marker, modifications of intragestural parameters occur, inducing temporal changes of consonantal gestures. We conclude that higher structural components should be taken into account when investigating syllable internal timing patterns.",0.0,5.199999999999999
196,1288,4.85    5.2    5.1    5.65    ,An Interlocutor-Modulated Attentional LSTM for Differentiating between Subgroups of Autism Spectrum Disorder,"Recalling and discussing personal emotional experiences is one of the key procedures in assessing complex affect processing of individuals with Autism Spectrum Disorder (ASD). This procedure is a standard subpart of a diagnostic interview to assess ASD - the Autism Diagnostic Observation Schedule (ADOS). Previous work has demonstrated that the behavior features computed from this procedure in ADOS possess discriminative information between the three distinct ASD subgroups: Autistic Disorder (AD), High Functioning Autism (HFA), and Asperger Syndrome (AS). In this work, we propose an interlocutor-modulated attentional long short term memory network (IM-aLSTM) that models the ASD individual's acoustic features with a novel interlocutor-modulated attention mechanism. Our IM-aLSTM achieves ASD subgroup categorization accuracy of 66.5%, which is a 14% absolute improvement over baseline method on the same database. Our analyses further indicate that the attention weights are concentrated more on interaction segments where the ASD individual is being asked to recall and discuss his/her own negative emotional experiences.",0.0,5.2
347,1581,5.9    5    4.9    5    ,Joint Learning of Domain Classification and Out-of-Domain Detection with Dynamic Class Weighting for Satisficing False Acceptance Rates,"In domain classification for spoken dialog systems, correct detection of out-of-domain (OOD) utterances is crucial because it reduces confusion and unnecessary interaction costs between users and the systems. Previous work usually utilizes OOD detectors that are trained separately from in-domain (IND) classifiers, and confidence thresholding for OOD detection given target evaluation scores. In this paper, we introduce a neural joint learning model for domain classification and OOD detection, where dynamic class weighting is used during the model training to satisfice a given OOD false acceptance rate (FAR) while maximizing the domain classification accuracy. Evaluating on two domain classification tasks for the utterances from a large spoken dialogue system, we show that our approach significantly improves the domain classification performance with satisficing given target FARs.",0.0,5.2
243,1374,5    5    5.15    5.65    ,Prediction of Perceived Speech Quality Using Deep Machine Listening,"Subjective ratings of speech quality (SQ) are essential for evaluating algorithms for speech transmission and enhancement. 
In this paper we explore a non-intrusive model for SQ prediction based on the output of a deep neural net (DNN) from a regular automatic speech recognizer. 
The degradation of phoneme probabilities obtained from the net is quantified with the mean temporal distance proposed earlier for multi-stream ASR. 
The SQ predicted with this method is compared with average subject ratings from the TCD-VoIP speech quality database that covers several effects of SQ degradation that can occur in VoIP applications such as clipping, packet loss, echo effects, background noise, and competing speakers. 
Our approach is tailored to speech and therefore not applicable when quality is degraded by a competing speaker, which is reflected by an insignificant correlation between model output and subjective SQ. 
In all other conditions mentioned above, the model reaches an average correlation of r=0.87, which is higher than the correlation achieved with the baseline ITU-T P.563 (r=0.71) and the American National Standard ANIQUE+ (r=0.75).
Since the most robust ASR system is not necessarily the best model to predict SQ, we investigate the effect of the amount of training data on quality prediction.",0.0,5.2
215,1318,4.35    5    5.8    5.7    ,Semi-supervised Learning for Information Extraction from Dialogue,"In this work we present a method for semi-supervised learning from
transcripts of dialogue between humans.  We consider the scenario in which a
large amount of transcripts are available, and we would like to
extract some semantic information from them; however, only a small
number of transcripts have been labeled with this information.  We present a
method for leveraging the unlabeled data to learn a better model than
could be learned from the labeled data alone.  First, a recurrent neural
network (RNN) encoder-decoder is trained on the task of predicting nearby
turns on the full dialogue corpus; next, the RNN encoder is
reused as a feature representation for the supervised learning problem.
While previous work has explored the use of pre-training for non-dialogue
corpora, our method is specifically geared toward the dialogue use case.
We demonstrate an improvement on a clinical documentation task, particularly
in the regime of small amounts of labeled data.  We compare several
types of encoders, both in the context of a classification task and in a
human-evaluation of their learned representations. We show that our method
significantly improves the classification task in the case where only a small
amount of labeled data is available.",0.0,5.2124999999999995
469,1843,5.05    5.8    5.05    4.95    ,Low Resource Acoustic-to-articulatory Inversion Using Bi-directional Long Short Term Memory,"Estimating articulatory movements from speech acoustic features is known as acoustic-to-articulatory inversion (AAI). Large amount of parallel data from speech and articulatory motion is required for training an AAI model in a subject dependent manner, referred to as subject dependent AAI (SD-AAI). Electromagnetic articulograph (EMA) is a promising technology to record such parallel data, but it is expensive, time consuming and tiring for a subject. In order to reduce the demand for parallel acoustic-articulatory data in the AAI task for a subject, we, in this work, propose a subject-adaptative AAI method (SA-AAI) from an existing AAI model which is trained using large amount of parallel data from a fixed set of subjects. Experiments are performed with 30 subjects’ acoustic-articulatory data and AAI is trained using BLSTM network to examine the amount of data needed from a new target subject for the SA-AAI to achieve an AAI performance equivalent to that of SD-AAI. Experimental results reveal that the proposed SA-AAI performs similar to that of the SD-AAI with ∼62.5% less training data. Among different articulators, the SA-AAI performance for tongue articulators matches with the corresponding SD-AAI performance with only ∼12.5% of the data used for SD-AAI training.",0.0,5.2124999999999995
328,1542,5    5.05    5.9    4.9    ,"The Zurich Corpus of Vowel and Voice Quality, Version 1.0","Existing databases of isolated vowel sounds or vowel sounds embedded in consonantal context generally document only limited variation of basic production parameters. Thus, concerning the possible variation range of vowel and voice quality-related sound characteristics, there is a lack of broad phenomenological and descriptive references that allow for a comprehensive understanding of vowel acoustics and for an evaluation of the extent to which corresponding existing approaches and models can be generalised. In order to contribute to the building up of such references, a novel database of vowel sounds that exceeds any existing collection by size and diversity of vocalic characteristics is presented here, comprised of c. 34 600 utterances of 70 speakers (46 non-professional speakers, children, women and men, and 24 professional actors/actresses and singers of straight theatre, contemporary singing, and European classical singing). The database focuses on sounds of the long Standard German vowels /i–y–e–ø–ɛ–a–o–u/ produced with varying basic production parameters such as phonation type, vocal effort, fundamental frequency, vowel context and speaking or singing style. In addition, a read text and, for professionals, songs are also included. The database is accessible for scientific use, and further extensions are in progress.",0.0,5.2125
702,2457,5.1    6    5    4.75    ,BUT OpenSAT 2017 Speech Recognition System,"The paper describes BUT Automatic Speech Recognition (ASR) systems for
two domains in OpenSAT evaluations: Low Resourced Languages and Public
Safety Communications. The first was challenging due to lack of
training data,  therefore multilingual approaches for BLSTM training were
employed and recently published Residual Memory
Networks requiring less training data were used. Combination of both approaches led to superior
performance. The second domain was challenging due to recording in extreme
conditions: specific channel, speaker under stress, high levels of
noise. A data augmentation process was very  important to get
reasonably good performance.",0.0,5.2125
461,1827,4.95    5    5.9    5    ,Interaction Mechanisms between Glottal Source and Vocal Tract in Pitch Glides,"A computational model for vowel production has been used to simulate rising pitch glides in the time domain. Such glides reveal multi-faceted nonlinear system behaviour when the fundamental frequency f_o is near the first vocal tract resonance f_{R1}. There are multiple physical mechanisms for how the acoustic field in the vocal tract can interact with vocal fold dynamics causing this behaviour. The model used in this work includes the direct impact of the acoustic pressure on the transversal plane of the vocal folds and an acoustic perturbation component to the glottal flow. Simulations indicate that both of these mechanisms, when applied separately, cause similar perturbations in phonation parameters when f_o crosses f_{R1}. Enabling both mechanisms simultaneously tends to make the separately emerging features more prominent. In simulated glottal flow waveforms, the tendency towards a formant ripple increases when acoustic feedback to glottal flow is enabled, whereas the phenomenon occurs more rarely as a result of the direct acoustic pressure to vocal folds. In all cases, the formant ripple is more pronounced for frequencies below f_{R1}.",0.0,5.2125
592,2158,6    4.9    4.1    5.85    ,Semi-tied Units for Efficient Gating in LSTM and Highway Networks,"Gating is a key technique used for integrating information from multiple sources by long short-term memory (LSTM) models and has recently also been applied to other models such as the highway network. Although gating is powerful, it is rather expensive in terms of both computation and storage as each gating unit uses a separate full weight matrix. This issue can be severe since several gates can be used together in e.g. an LSTM cell. This paper proposes a semi-tied unit (STU) approach to solve this efficiency issue, which uses one shared weight matrix to replace those in all the units in the same layer. The approach is termed ""semi-tied'' since extra parameters are used to separately scale each of the shared output values.  These extra scaling factors are associated with the network activation functions and result in the use of parameterised sigmoid, hyperbolic tangent, and rectified linear unit functions. Speech recognition experiments using British English multi-genre broadcast data showed that using STUs can reduce the calculation and storage cost  by a factor of  three for highway networks and four for LSTMs, while giving similar word error rates to the original models.",0.0,5.2125
661,2362,5.9    5.15    5.9    4.2    4.95    ,Improvements to an Automated Content Scoring System for Spoken CALL Responses: the ETS Submission to the Second Spoken CALL Shared Task,"This paper describes the details of the ETS submission to the 2018 Spoken CALL Shared Task.  We employed a system using word and character n-gram features in a random forest machine learning framework based on the system that achieved the second-highest score in the text processing track of the 2017 Spoken CALL Shared Task.  This system was augmented with additional features based on comparing the learner's responses to language models trained on text written by both native English speakers and L1-German English learners.  In addition, we developed a set of sequence-to-label models using bidirectional LSTM-RNNs with an attention layer.  The RNN model predictions were combined with the other feature sets using feature-level and score-level fusion approaches resulting in a best-performing system that achieved a D score of 7.397 on the test set (ranking 5th out of 12 submissions to the text processing track of the Shared Task).  Subsequent experiments resulted in higher D scores when the model parameters were optimized for D score instead of F-score, and the paper presents an error analysis of these models in an attempt to determine which metric is more appropriate for evaluating spoken CALL systems.",0.0,5.220000000000001
645,2323,5.1    4.9    5.85    5.05    ,R-CRNN: Region-based Convolutional Recurrent Neural Network for Audio Event Detection,"This paper proposes a Region-based Convolutional Recurrent Neural Network (R-CRNN) for audio event detection (AED). The proposed network is inspired by Faster-RCNN [1], a well-known region-based convolutional network framework for visual object detection. Different from the original Faster-RCNN, a recurrent layer is added on top of the convolutional network to capture the long-term temporal context from the extracted high-level features. While most of the previous works on AED generate predictions at frame level first, and then use post-processing to predict the onset/offset timestamps of events from a probability sequence; the proposed method generates predictions at event level directly and can be trained end-to-end with a multi-task loss, which optimizes the classification and localization of audio events simultaneously. The proposed method is tested on DCASE 2017 Challenge dataset [2]. To the best of our knowledge, R-CRNN is the best performing single-model method among all methods without using ensembles both on development and evaluation sets. Compared to the other region-based network for AED (R-FCN [3]) with an event-based error rate (ER) of 0.18 on the development set, our method reduced the ER to half.",0.0,5.225
627,2279,5.05    5.05    5.05    5.75    ,End-To-End Audio Replay Attack Detection Using Deep Convolutional Networks with Attention,"With automatic speaker verification (ASV) systems becoming increasingly popular, the development of robust countermeasures against spoofing is needed. Replay attacks pose a significant threat to the reliability of ASV systems because of the relative difficulty in detecting replayed speech and the ease with which such attacks can be mounted. In this paper, we propose an end-to-end deep learning framework for audio replay attack detection. Our proposed approach uses a novel visual attention mechanism on time-frequency representations of utterances based on group delay features, via deep residual learning (an adaptation of ResNet-18 architecture). Using a single model system, we achieve a perfect Equal Error Rate (EER) of 0% on both the development as well as the evaluation set of the ASVspoof 2017 dataset, against a previous best of 0.12% on the development set and 2.76% on the evaluation set reported in the literature. This highlights the efficacy of our feature representation and attention-based architecture in tackling the challenging task of audio replay attack detection.",0.0,5.225
71,1060,4.25    5.1    5.8    5.75    ,Length Contrast and Covarying Features: Whistled Speech as a Case Study,"The status of covarying features to sound contrasts is a long-standing issue in speech: are they deliberately controlled by the speakers, or are they contingent automatic effects required by the defining features? We address this question by drawing parallels between the way gemination is implemented in spoken language and the way it is rendered in whistled speech. Audio materials were collected with five Berber whistlers in Morocco. The spoken and whistled data were composed of pairs of words contrasting singletons to geminates in different word positions. 
Compared to spoken forms, whistling, while adapting to the specific constraints imposed by the medium, transposes the basic strategies used in normal speech. As in normal speech, the primary and most salient acoustic attribute differentiating whistled singletons and geminates is closure duration. But duration is not used alone. Covarying secondary attributes are conveyed which may serve to enhance the primary correlate by contributing additional properties increasing the distance between the two lexical categories. These enhancing correlates may take on distinctive function in cases where the primary correlate is not implemented. This is, for instance, the case of higher frequency values in word-initial position where duration differences cannot be acoustically implemented using whistled speech.",0.0,5.225
233,1352,5.05    4.95    5.85    5.05    ,User-centric Evaluation of Automatic Punctuation in ASR Closed Captioning,"Punctuation of ASR-produced transcripts has received increasing attention in the recent years; RNN-based sequence modelling solutions which exploit textual and/or acoustic features show encouraging performance. Switching the focus from the technical side, qualifying and quantifying the benefits of such punctuation from end-user perspective have not been performed yet exhaustively. The ambition of the current paper is to explore to what extent automatic punctuation can improve human readability and understandability.
The paper presents a user-centric evaluation of a real-time closed captioning system enhanced by a lightweight RNN-based punctuation module. Subjective tests involve both normal hearing and deaf or hard-of-hearing (DHH) subjects. Results confirm that automatic punctuation itself significantly increases understandability, even if several other factors interplay in subjective impression. The perceived improvement is even more pronounced in the DHH group. A statistical analysis is carried out to identify objectively measurable factors which are well reflected by subjective scores.",0.0,5.225
367,1629,6    5.2    4.75    4.95    ,End-to-End Speech Separation with Unfolded Iterative Phase Reconstruction,"This paper proposes an end-to-end approach for single-channel speaker-independent multi-speaker speech separation, where time-frequency (T-F) masking, the short-time Fourier transform (STFT), and its inverse are represented as  layers within a deep network. Previous approaches, rather than computing a loss on the reconstructed signal, used a surrogate loss based on the target STFT magnitudes. This ignores reconstruction error introduced by phase inconsistency. In our approach, the loss function is directly defined on the reconstructed signals, which are optimized for best separation. In addition, we train through unfolded iterations of a phase reconstruction algorithm, represented as a series of STFT and inverse STFT layers.  While mask values are typically limited to lie between zero and one for approaches using the mixture phase for reconstruction, this limitation is less relevant if the estimated magnitudes are to be used together with phase reconstruction. We thus propose several novel activation functions for the output layer of the T-F masking, to allow mask values beyond one. On the publicly-available wsj0-2mix dataset, our approach achieves state-of-the-art 12.6 dB scale-invariant signal-to-distortion ratio (SI-SDR) and 13.1 dB SDR, revealing new possibilities for deep learning based phase reconstruction and representing a fundamental progress towards solving the notoriously-hard cocktail party problem.",0.0,5.225
186,1269,5.15    5.1    4.95    5.75    ,Joint Localization and Classification of Multiple Sound Sources Using a Multi-task Neural Network,"We propose a novel multi-task neural network-based approach for joint sound source localization and speech/non-speech classification in noisy environments. The network takes raw short time Fourier transform as input and outputs the likelihood values for the two tasks, which are used for the simultaneous detection, localization and classification of an unknown number of overlapping sound sources, Tested with real recorded data, our method achieves significantly better performance in terms of speech/non-speech classification and localization of speech sources, compared to method that performs localization and classification separately. In addition, we demonstrate that incorporating the temporal context can further improve the performance.",0.0,5.2375
10,48,6    5.15    5.05    4.75    ,"Cross-language Perception of Mandarin Lexical Tones by Mongolian-speaking Bilinguals in the Inner Mongolia Autonomous Region, China","Mandarin is a representative tonal language with four contrastive tone categories (Tone 1 (T1): high level (ā), Tone 2 (T2): high rising (á), Tone 3 (T3): dipping (ǎ), Tone 4 (T4): high falling (à)). Learning Mandarin tones is known to be difficult for speakers from diverse linguistic backgrounds. The purpose of this research was to examine how native Mongolian-speaking bilinguals perceive Mandarin lexical tones. The 24 (17 females, 7 males) participants studied Mandarin for 15 years on average in the Inner Mongolia Autonomous Region, China. A discrimination experiment was conducted to assess Mongolian bilinguals' perception of six tone pairs (T1-T2, T1-T3, T1-T4, T2-T3, T2-T4, T3-T4). The Mongolian group was less accurate than the control group of ten native Mandarin listeners for all six pairs and the between-group difference was particularly large for T2-T3. However, large individual variation was observed and some Mongolian bilinguals perceived Mandarin tones as accurately as native Mandarin listeners, suggesting that native-like tone perception is attainable in subsequently acquired languages.",0.0,5.2375
412,1728,5.9    3.9    5.25    5.9    ,Analyzing Reaction Time Sequences from Human Participants in Auditory Experiments,"Sequences of reaction times (RT) produced by participants in an
experiment are not only influenced by the stimuli, but by many
other factors as well, including fatigue, attention, experience,
IQ, handedness, etc. These confounding factors result in longterm
effects (such as a participant’s overall reaction capability)
and in short- and medium-time fluctuations in RTs (often referred
to as ’local speed effects’). Because stimuli are usually
presented in a random sequence different for each participant,
local speed effects affect the underlying ’true’ RTs of specific
trials in different ways across participants. To be able to focus
statistical analysis on the effects of the cognitive process under
study, it is necessary to reduce the effect of confounding
factors as much as possible. In this paper we propose and compare
techniques and criteria for doing so, with focus on reducing
(’filtering’) the local speed effects. We show that filtering
matters substantially for the significance analyses of predictors
in linear mixed effect regression models. The performance of
filtering is assessed by the average between-participant correlation
between filtered RT sequences, and by Akaike’s Information
Criterion, an important measure of the goodness-of-fit of
linear mixed effect regression models.",0.0,5.237500000000001
463,1830,4.15    5    5.9    5.9    ,Multi-target Voice Conversion without Parallel Data by Adversarially Learning Disentangled Audio Representations,"Recently, cycle-consistent adversarial network (Cycle-GAN) has been successfully applied to voice conversion to a different speaker without parallel data, although in those approaches an individual model is needed for each target speaker. In this paper, we propose an adversarial learning framework for voice conversion, with which a single model can be trained to convert the voice to many different speakers, all without parallel data, by separating the speaker characteristics from the linguistic content in speech signals. An autoencoder is first trained to extract speaker-independent latent representations and speaker embedding separately using another auxiliary speaker classifier to regularize the latent representation. The decoder then takes the speaker-independent latent representation and the target speaker embedding as the input to generate the voice of the target speaker with the linguistic content of the source utterance. The quality of decoder output is further improved by patching with the residual signal produced by another pair of generator and discriminator. A target speaker set size of 20 was tested in the preliminary experiments, and very good voice quality was obtained. Conventional voice conversion metrics are reported. We also show that the speaker information has been properly reduced from the latent representations.",0.0,5.237500000000001
364,1616,5    5    5.9    5.1    ,Improved Training of End-to-end Attention Models for Speech Recognition,"Sequence-to-sequence attention-based models on subword units
allow simple open-vocabulary end-to-end speech recognition.
In this work, we show that such models can achieve competitive
results on the Switchboard 300h and LibriSpeech 1000h tasks.
In particular, we report the state-of-the-art word error rates (WER)
of 3.54% on the dev-clean and 3.82% on the test-clean evaluation subsets of LibriSpeech.
We introduce a new pretraining scheme by starting with
a high time reduction factor and lowering it during training,
which is crucial both for convergence and final performance.
In some experiments, we also use an auxiliary CTC loss function
to help the convergence. In addition, we train long short-term
memory (LSTM) language models on subword units.
By shallow fusion, we report up to 27% relative improvements in WER
over the attention baseline without a language model.",0.0,5.25
107,1125,5.1    4.9    5.75    ,Self-similarity Matrix Based Intelligibility Assessment of Cleft Lip and Palate Speech,"This work presents a comparison based framework by exploiting the self-similarity matrices matching technique to estimate
the speech intelligibility of cleft lip and palate (CLP) children.
Self-similarity matrix (SSM) of a feature sequence is a square
matrix, which encodes the acoustic-phonetic composition of the
underlying speech signal. Deviations in the acoustic characteristics of underlying sound units due to the degradation of intelligibility will deviate the CLP speech’s SSM structure from
that of normal. This degree of deviations in CLP speech’s SSM
from the corresponding normal speech’s SSM may provide information about the severity profile of speech intelligibility. The
degree of deviations is quantified using the structural similarity
(SSIM) index, which is considered as the representative of objective intelligibility score. The proposed method is evaluated
using two parameterizations of speech signals: Mel-frequency
cepstral coefficients and Gaussian posteriorgrams and compared with dynamic time warping (DTW) based intelligibility
assessment method. The proposed SSM based method shows
the better correlation with the perceptual ratings of intelligibility when compared to the DTW based method.",0.0,5.25
511,1944,5.8    5.05    5.1    5.05    ,Variation in the FACE Vowel across West Yorkshire: Implications for Forensic Speaker Comparisons,"In the field of sociophonetics, research is largely focused on the documentation of regional variability. However, the majority of literature in the United Kingdom often reports on variation at a macro-level (e.g. Northern, Yorkshire, West Yorkshire) rather than at a more local level (e.g. West Yorkshire: Bradford, Calderdale, Kirklees, Leeds, Wakefield). Traditionally, for sociophoneticians, examining regional variation at a broader level is adequate for answering research questions related to language change or more general variation. For practical applications (e.g. forensic, speech technology), however, more fine-grained regional analysis is necessary. This paper analyses over 2000 FACE tokens from three metropolitan boroughs (Bradford, Kirklees and Wakefield) within West Yorkshire, in order to determine the extent to which F1~F3 vary across the region.  Results suggest that for FACE, these three boroughs within West Yorkshire are more regionally stratified than previously acknowledged. These findings are of particular importance to the forensic speech science community, as experts rely on these regional nuances in order to make important judgments related to strength of the speech evidence in a case. Should decisions be made without the greater understanding of local-level variation, the strength of evidence risks being over- or under-estimated.",0.0,5.25
210,1309,5.15    5.8    5.1    5    ,Liulishuo's System for the Spoken CALL Shared Task 2018,"The Spoken CALL (computer-assisted language learning) 2018 shared task requires systems to automatically accept or reject each single-sentence spoken response depending on whether the response is correct given a prompt. Spoken responses are first recognized to texts and then recognition output is classified as accept or reject based on their language and meaning. This paper describes our system for the shared task. We focused on improving speech recognition performance, developing a rich set of features to capture the linguistic and semantic meaning of the responses, and optimizing classification results for various factors (training set, n-best hypotheses of speech recognition, decision threshold, model ensemble). Our system obtains the best performance among participating teams.",0.0,5.262499999999999
572,2096,5.85    5    5.85    4.35    ,Temporal Noise Shaping with Companding,"Audio codecs are typically transform-domain based and efficiently code stationary musical signals but they struggle with speech and signals with dense transients such as applause. The temporal noise shaping (TNS) tool standardized in HE-AAC alleviates the issue of noise unmasking in these troublesome cases via signal-adaptive filtering of the transform domain quantization noise, albeit at the cost of significant additional side information in the bitstream.  We present a novel alternative referred to as companding that involves QMF domain pre- and post-processing around the core transform-domain coding system: prior to transform encoding, the dynamic range of the signal is reduced locally within a QMF time slot and restored again post decoding, which naturally shapes the coding noise temporally. A primary advantage is that the companding function is fixed and hence enables signal-adaptive noise shaping with just 1-2 bits of side-information per frame. Subjective tests illustrate that the proposed tool improves the quality of hard-to-code applause excerpts compared to TNS while achieving comparable performance on speech signals. The coding tool described in this paper is part of the Dolby AC-4 audio coding system standardized by ETSI and included in ATSC 3.0.",0.0,5.262499999999999
677,2403,5    5.15    5.1    5.8    ,An Efficient Approach to Encoding Context for Spoken Language Understanding,"In task-oriented dialogue systems, spoken language understanding,
or SLU, refers to the task of parsing natural language user
utterances into semantic frames. Making use of context from
prior dialogue history holds the key to more effective SLU. State
of the art approaches to SLU use memory networks to encode
context by processing multiple utterances from the dialogue at
each turn, resulting in significant trade-offs between accuracy
and computational efficiency. On the other hand, downstream
components like the dialogue state tracker (DST) already keep
track of the dialogue state, which can serve as a summary of the
dialogue history. In this work, we propose an efficient approach
to encoding context from prior utterances for SLU. More specifically,
our architecture includes a separate recurrent neural network
(RNN) based encoding module that accumulates dialogue
context to guide the frame parsing sub-tasks and can be shared
between SLU and DST. In our experiments, we demonstrate the
effectiveness of our approach on dialogues from two domains.",0.0,5.2625
19,65,6    4    5.8    ,The ‘West Yorkshire Regional English Database’: Investigations into the Generalizability of Reference Populations for Forensic Speaker Comparison Casework,"The West Yorkshire Regional English Database (WYRED) consists of approximately 196 hours of high-quality audio recordings of 180 West Yorkshire (British English) speakers. All participants are male between the ages of 18-30, and are divided evenly (60 per region) across three boroughs within West Yorkshire (Northern England): Bradford, Kirklees, and Wakefield. Speakers participated in four spontaneous speaking tasks. The first two tasks relate to a mock crime where the participant speaks to a police officer (Research Assistant 1) followed by an accomplice (Research Assistant 2). Speakers returned a minimum of a week later at which point they were paired with someone from their borough and recorded having a conversation on any topics they wish. The final task is an experimental task in which speakers are asked to leave a voicemail message related to the fictitious crime from the first recording session. In total, each speaker participated in approximately 1 hour of spontaneous speech recordings. This paper details the design of WYRED, in order to introduce forensic speech science research utilizing this data, and to promote WYRED’s potential application in related research and in forensic speech science casework.",0.0,5.266666666666667
570,2089,5    5    5.95    5.15    ,Loud and Shouted Speech Perception at Variable Distances in a Forest,"To increase the range of modal speech in natural ambient noise, individuals increase their vocal effort and may pass into the ‘shouted speech’ register. To date, most studies concerning the influence of distance on spoken communication in outdoor natural environments have focused on the ‘productive side’ of the human ability to tacitly adjust vocal output to compensate for acoustic losses due to sound propagation. Our study takes a slightly different path as it is based on an adaptive speech production/perception experiment. The setting was an outdoor natural soundscape (a plane forest in altitude). The stimuli were produced live during the interaction: each speaker adapted speech to transmit French disyllabic words in isolation to an interlocutor/listener who was situated at variable distances in the course of the experiment (30m, 60m, 90m). Speech recognition was explored by evaluating the ability of 16 normal-hearing French listeners to recognize these words and their constituent vowels and consonants. Results showed that in such conditions, speech adaptation was rather efficient as word recognition remained around 95% at 30m, 85% at 60m and 75% at 90m. We also observed striking differences in patterns of answers along several lines: different distances, speech registers, vowels and consonants.",0.0,5.275
393,1685,5.1    4.95    5.1    6    ,Robust and Discriminative Speaker Embedding via Intra-Class Distance Variance Regularization,"Learning a good speaker embedding is critical for many speech processing tasks, including recognition, verification, and diarization. To this end, we propose a complementary optimizing goal called intra-class loss to improve deep speaker embeddings learned with triplet loss. This loss function is formulated as a soft constraint on the averaged pair-wise distance between samples from the same class. Its goal is to prevent the scattering of these samples within the embedding space to increase the intra-class compactness.When intra-class loss is jointly optimized with triplet loss, we can observe 2 major improvements: the deep embedding network can achieve a more robust and discriminative representation and the training process is more stable with a faster convergence rate. We conduct experiments on 2 large public benchmarking datasets for speaker verification, VoxCeleb and VoxForge. The results show that intra-class loss helps accelerating the convergence of deep network training and significantly improves the overall performance of the resulted embeddings.",0.0,5.2875
333,1552,5.05    5.2    4.95    6    ,Temporal Attentive Pooling for Acoustic Event Detection,"Deep convolutional neural network (DCNN) based model has been successfully applied to acoustic event detection (AED) due to its efficiency to explore temporal-frequency structure for feature representations. In most studies, the final representation either uses a temporal average- or max- pooling algorithm to accumulate local temporal features as a global representation for event classification. The temporal pooling algorithm in the DCNN is based on the assumption that the target label is assigned to all temporal locations (average pooling) or to only one temporal location with a maximum response (max-pooling). However, the acoustic event labels are holistic descriptions in a semantic level, it is difficult or even impossible to decide features from which temporal locations contribute to the event perception. In this study, we propose a weighted temporal-pooling algorithm to accumulate local temporal features for AED. The pooling algorithm integrates global and local attention modules in a convolutional recurrent neural network to integrate temporal features. Experiments on an AED task were carried out to evaluate the proposed model. Results showed that with the global and local attentions, a large gain was obtained.",0.0,5.3
228,1346,6    4.15    6    5.05    ,Joint Learning of Interactive Spoken Content Retrieval and Trainable User Simulator,"User-machine interaction is crucial for information retrieval, especially for spoken content retrieval, because spoken content is difficult to browse, and speech recognition has a high degree of uncertainty. In interactive retrieval, the machine takes different actions to interact with the user to obtain better retrieval results; here it is critical to select the most efficient action. In previous work, deep Q-learning techniques were proposed to train an interactive retrieval system but rely on a hand-crafted user simulator; building a reliable user simulator is difficult. In this paper, we further improve the interactive spoken content retrieval framework by proposing a learnable user simulator which is jointly trained with interactive retrieval system, making the hand-crafted user simulator unnecessary. The experimental results show that the learned simulated users not only achieve larger rewards than the hand-crafted ones but act more like real users.",0.0,5.3
21,68,5.85    4.95    5.1    ,Vowel Space as a Tool to Evaluate Articulation Problems,"Treatment for oral tumors can lead to long term changes in the anatomy and physiology of the tongue and result in problems with articulation. There are currently no readily available automatic methods to evaluate changes in articulation. We developed a simple \textit{Praat} script which plots and measures vowel space coverage. The script reproduces speaker specific vowel space use and speaking-style dependent vowel reduction in normal speech from a Dutch corpus. Speaker identity and speaking style explain more than 60\% of the variance in the measured area of the vowel triangle. In recordings of patients treated for oral tumors, vowel space use before and after treatment is still significantly correlated. Articulation before and after treatment is evaluated in a listening experiment and from a maximal articulation speed task. Linear models can explain 50-75\% of variance in perceptual ratings and relative articulation rate from values at previous recordings and vowel space measures.",0.0,5.3
156,1226,5.25    5.15    5    5.8    ,Speaker Embedding Extraction with Phonetic Information,"Speaker embeddings achieve promising results on many speaker verification tasks. Phonetic information, as an important component of speech, is rarely considered in the extraction of speaker embeddings. In this paper, we introduce phonetic information to the speaker embedding extraction based on the x-vector architecture. Two methods using phonetic vectors and multi-task learning are proposed. On the Fisher dataset, our best system outperforms the original x-vector approach by 20% in EER, and by 15%, 15% in minDCF08 and minDCF10, respectively. Experiments conducted on NIST SRE10 further demonstrate the effectiveness of the proposed methods.",0.0,5.3
562,2078,6    4.35    6    4.85    ,Talker Diarization in the Wild: the Case of Child-centered Daylong Audio-recordings,"Speaker diarization (answering 'who spoke when') is a  widely researched subject within speech technology. Numerous experiments have been run on datasets built from broadcast news, meeting data, and call centers---the task sometimes appears close to being solved. Much less work has begun to tackle the hardest diarization task of all: spontaneous conversations in real-world settings. Such diarization would be particularly useful for studies of language acquisition, where researchers investigate the speech children produce and hear in their daily lives. In this paper, we study audio gathered with a recorder worn by small children as they went about their normal days. As a result, each child was exposed to different acoustic environments with a multitude of background noises and a varying number of adults and peers. The inconsistency of speech and noise within and across samples poses a challenging task for speaker diarization systems, which we tackled via retraining and data augmentation techniques. We further studied sources of structured variation across raw audio files, including the impact of speaker type distribution, proportion of speech from children, and child age on diarization performance. We discuss the extent to which these findings might generalize to other samples of speech in the wild.",0.0,5.300000000000001
235,1358,5    5.1    6    5.1    ,Impact of Different Speech Types on Listening Effort,"Listeners are exposed to different types of speech in everyday life, from natural speech to speech that has undergone modifications or has been generated synthetically. While many studies have focused on measuring the intelligibility of these distinct speech types, their impact on listening effort is not known. The current study combined an objective measure of intelligibility, a physiological measure of listening effort (pupil size) and listeners' subjective judgements, to examine the impact of four speech types: plain (natural) speech, speech produced in noise (Lombard speech), speech enhanced to promote intelligibility, and synthetic speech. For each speech type, listeners responded to sentences presented in one of three levels of speech-shaped noise. Subjective effort ratings and intelligibility scores showed an inverse ranking across speech types, with synthetic speech being the most demanding and enhanced speech the least. Pupil size measures indicated an increase in listening effort with decreasing signal-to-noise ratio for all speech types apart from synthetic speech, which required significantly more effort at the most favourable noise level. Naturally and artificially modified speech were less effortful than plain speech at the more adverse noise levels. These outcomes indicate a clear impact of speech type on the cognitive demands required for comprehension.",0.0,5.300000000000001
415,1736,4.9    5.15    5.25    5.9    ,UltraSuite: a Repository of Ultrasound and Acoustic Data from Child Speech Therapy Sessions,"We introduce UltraSuite, a curated repository of ultrasound and acoustic data, collected from recordings of child speech therapy sessions. This release includes three data collections, one from typically developing children and two from children with speech sound disorders. In addition, it includes a set of annotations, some manual and some automatically produced, and software tools to process, transform and visualise the data.",0.0,5.300000000000001
420,1746,5.95    5.1    4.9    ,Semi-Supervised End-to-End Speech Recognition,"We propose a novel semi-supervised method for end-to-end automatic speech recognition (ASR).
It can exploit large unpaired speech and text datasets, which require much less human effort to create paired speech-to-text datasets. Our semi-supervised method targets the extraction of an intermediate representation between speech and text data using a shared encoder network. Autoencoding of text data  with this shared encoder improves the feature extraction of text data as well as that of speech data when the intermediate representations of speech and text are similar to each other as an inter-domain feature. In other words, by combining speech-to-text and text-to-text mappings through the shared network, we can improve speech-to-text mapping by learning to reconstruct the unpaired text data in a semi-supervised end-to-end manner. We investigate how to design suitable inter-domain loss, which minimizes the dissimilarity between the encoded speech and text sequences, which originally belong to quite different domains. The experimental results we obtained with our proposed semi-supervised training shows a larger character error rate reduction from 15.8% to 14.4% than a conventional language model integration on the Wall Street Journal dataset.",0.0,5.316666666666667
403,1707,5.1    5.1    5.1    6    ,Visualizing Phoneme Category Adaptation in Deep Neural Networks,"Both human listeners and machines need to adapt their sound categories whenever a new speaker is encountered. This perceptual learning is driven by lexical information. The aim of this paper is two-fold: investigate whether a deep neural network-based (DNN) ASR system can adapt to only a few examples of ambiguous speech as humans have been found to do; investigate a DNN’s ability to serve as a model of human perceptual learning. Crucially, we do so by looking at intermediate levels of phoneme category adaptation rather than at the output level. We visualize the activations in the hidden layers of the DNN during perceptual learning. The results show that, similar to humans, DNN systems learn speaker-adapted phone category boundaries from a few labeled examples. The DNN adapts its category boundaries not only by adapting the weights of the output layer, but also by adapting the implicit feature maps computed by the hidden layers, suggesting the possibility that human perceptual learning might involve a similar nonlinear distortion of a perceptual space that is intermediate between the acoustic input and the phonological categories. Comparisons between DNNs and humans can thus provide valuable insights into the way humans process speech and improve ASR technology.",0.0,5.324999999999999
369,1631,5.8    5.1    4.7    5.75    ,Estimation of Hypernasality Scores from Cleft Lip and Palate Speech,"Hypernasality refers to the perception of excessive nasal resonances in vowels and voiced consonants. Existing speech processing based approaches concentrate only on the classification of speech into normal or hypernasal, which do not give the degree of hypernasality in terms of continuous values like nasometer.   Motivated by the functionality of nasometer, in this work, a method is proposed for the evaluation  of hypernasality. Speech signals representing two extremely opposite cases of nasality are used to develop the acoustic models, where oral sentences (rich in vowels, stops, and fricatives) of normal speakers and nasal sentences (rich in nasals and nasalized vowels) of moderate-severe hypernasal speakers represent the group with minimum and maximum attainable degree of nasality, respectively.  The acoustic features derived from glottal activity regions are used to model the maximum and minimum nasality classes using Gaussian mixture model and deep neural network approaches. The posterior probabilities obtained for nasal sentence class are referred to as hypernasality scores. The scores show the significant correlation (p<0.01) with the perceptual ratings of hypernasality, provided by expert speech-language pathologists. Further, hypernasality scores are used for the detection of hypernasality, and the results are compared with the nasometer based approach.",0.0,5.3374999999999995
158,1230,5.25    5.9    4.35    5.9    ,Time-regularized Linear Prediction for Noise-robust Extraction of the Spectral Envelope of Speech,"Feature extraction of speech signals is typically performed in short-time frames by assuming that the signal is stationary within each frame. For the extraction of the spectral envelope of speech, which conveys the formant frequencies produced by the resonances of the slowly varying vocal tract, an often used frame length is within 20--30 ms. However, this kind of conventional frame-based spectral analysis is oblivious of the broader temporal context of the signal and is prone to degradation by, for example, environmental noise.

In this paper, we propose a new frame-based linear prediction (LP) analysis method that includes a regularization term that penalizes energy differences in consecutive frames of an all-pole spectral envelope model. This integrates the slowly varying nature of the vocal tract as a part of the analysis. Objective evaluations related to feature distortion and phonetic representational capability were performed by studying the properties of the mel-frequency cepstral coefficient (MFCC) representations computed from different spectral estimation methods under noisy conditions using the TIMIT database. The results show that the proposed time-regularized LP approach exhibits superior MFCC distortion behavior while simultaneously having the greatest average separability of different phoneme categories in comparison to the other methods.",0.0,5.35
308,1506,6    4.75    5.85    4.95    ,Multi-task WaveNet: a Multi-task Generative Model for Statistical Parametric Speech Synthesis without Fundamental Frequency Conditions,"This paper introduces an improved generative model for statistical
parametric speech synthesis (SPSS) based on WaveNet under
a multi-task learning framework. Different from the original WaveNet
model, the proposed Multi-task WaveNet employs
the frame-level acoustic feature prediction as the secondary
task and the external fundamental frequency prediction model
for the original WaveNet can be removed. Therefore the improved
WaveNet can generate high-quality speech waveforms
only conditioned on linguistic features. Multi-task WaveNet can
produce more natural and expressive speech by addressing the
pitch prediction error accumulation issue and possesses more
succinct inference procedures than the original WaveNet. Experimental
results prove that the SPSS method proposed in this
paper can achieve better performance than the state-of-the-art
approach utilizing the original WaveNet in both objective and
subjective preference tests.",0.0,5.3875
665,2372,5.35    5.9    5.2    5.15    ,The Effect of Exposure to High Altitude and Heat on Speech Articulatory Coordination,"The effects of altitude and heat on speech articulatory coordination following exercise and approximately three hours of exposure are explored. Recordings of read speech and free response speech before and after exercise in moderate altitude, moderate heat, and both moderate altitude and heat are analyzed using features that characterize articulatory coordination. It is found that 1) moderate altitude causes small changes and moderate heat negligible changes to articulatory coordination features after brief exposure prior to exercise; 2) moderate altitude and heat produce similar large feature changes following exercise and longer exposure; 3) moderate altitude and heat produce larger feature changes in combination than individually immediately following exercise. Finally, using cross-validation training of a statistical classifiers, the features are sufficient to classify the four experimental conditions with an overall accuracy of 0.50, and to detect the presence of any one of the experimental conditions with an accuracy of 0.90.",0.0,5.4
312,1515,5.1    5.75    5.7    5.1    ,An Improved Deep Embedding Learning Method for Short Duration Speaker Verification,"This paper presents an improved deep embedding learning method based on convolutional neural network (CNN) for short-duration speaker verification (SV). Existing deep learning-based SV methods generally extract frontend embeddings from a feed-forward deep neural network, in which the long-term speaker characteristics are captured via a pooling operation over the input speech. The extracted embeddings are then scored via a backend model, such as Probabilistic Linear Discriminative Analysis (PLDA). Two improvements are proposed for frontend embedding learning based on the CNN structure: (1) Motivated by the WaveNet for speech synthesis, dilated filters are designed to achieve a tradeoff between computational efficiency and receptive-filter size; and (2) A novel cross-convolutional-layer pooling method is exploited to capture 1th-order statistics for modelling long-term speaker characteristics. Specifically, the activations of one convolutional layer are aggregated with the guidance of the feature maps from the successive layer. To evaluate the effectiveness of our proposed methods, extensive experiments are conducted on the modified female portion of NIST SRE 2010  evaluations, with conditions ranging from 10s-10s to 5s-4s. Excellent performance has been achieved on each evaluation condition, significantly outperforming existing SV systems using i-vector and d-vector embeddings.",0.0,5.4125
322,1529,5    5.45    5.2    6    ,"Information Structure, Affect, and Prenuclear Prominence in American English","The influence of information structure (givenness, accessibility, newness and focus) on pitch accent assignment and acoustic prominence measures of prenuclear words was investigated for American English speech elicited through read production of mini-stories. Results showed a consistent pattern of accenting the initial content word in the sentence, supporting an analysis of prenuclear accent as structural, or ‘rhythmic’. While no association was observed between IS and accent type (e.g., H*, L*, L+H*, L*+H), the acoustic-phonetic realization of prominence was modulated by information structure. In particular, words that carry contrastive focus generally showed more extreme f0 excursions relative to the average. In addition, there was a strong influence of speaking style or ‘affect’ on both pitch accent type and the acoustic-phonetic realization of prominence. Speakers were more likely to produce L+H* accents in a lively than a neutral speaking style. Differences in affect were also strongly reflected in f0 excursion, duration, and amplitude within the target word. Overall, this study indicates both linguistic (information structure) and paralinguistic (affect) influences on the phonetic implementation of prenuclear prominence, with varying influence of these two factors on the phonological assignment of prenuclear pitch accents.",0.0,5.4125
18,63,4.95    5.9    5.85    4.95    ,Effects of User Controlled Speech Rate on Intelligibility in Noisy Environments,"Talkers intentionally producing high-intelligibility speech for listeners in challenging situations often reduce their speech rate. This study affords listeners fine-grained control over the playback rate of a desired speech signal in varying levels of background noise, and tests listener intelligibility with their preferred and unmodified rates of speech.  We find clear listener preference for decreased rates of speech as background noise increased. However, we also found degraded performance on a speech-in-noise intelligibility test relative to unmodified speech in these same conditions.",0.0,5.4125000000000005
747,2592,5.15    5.95    5.35    5.35    ,Sensorimotor response to tongue displacement imagery by talkers with Parkinson’s disease,"In a previous study, we asked healthy adult speakers to produce
the word head under noise-masked (visual only) conditions and
while watching videos of a 3D tongue avatar that gradually
morphed from producing head to had. Results indicated that
during the visual mismatch phases all participants entrained to
the visually presented word, head, without being aware that
their vowel quality had changed. Here, we explore whether
similar effects occur for individuals with presumed
sensorineural processing disorders, patients with Parkinson’s
disease (PD). We also examine the effects of PD treatment on
this entrainment behavior. Participants were 14 individuals with
PD, with eight in ongoing speech/language therapy, and six
reporting no recent therapy. Participants heard pink noise over
headphones and produced the word head under four viewing
conditions: First, while viewing repetitions of head (baseline);
next, during “morphed” videos shifting gradually from head to
had (ramp); then videos of had (maximum hold); and finally
videos of head (after effects). Analysis with a linear mixedeffects
model indicated a significant F1 difference between
baseline and maximum hold phases for the productions of the
treated PD group, but not for the untreated group. Implications
for the causes and treatment of PD speech disorders are
discussed.",0.0,5.450000000000001
283,1453,5.8    5.25    4.95    5.85    ,Acoustic Modeling from Frequency Domain Representations of Speech,"In recent years, different studies have proposed new methods for DNN-based
feature extraction and joint acoustic model training and feature learning from raw waveform for large vocabulary speech recognition. However, conventional pre-processed methods such as MFCC
and PLP are still preferred in the state-of-the-art speech recognition systems as they are perceived to be more robust. Besides, the raw waveform methods -- most of which are based on the time-domain signal -- do not significantly outperform
the conventional methods.
In this paper, we propose a frequency-domain feature-learning layer which can allow acoustic model
training directly from the waveform. The main distinctions from
previous works are a new normalization block and a short-range constraint on
the filter weights. 
The proposed setup achieves consistent performance improvements compared to the baseline MFCC and log-Mel features as well as other proposed time and frequency domain setups on different LVCSR tasks. Finally, based on the learned filters in our feature-learning layer, we propose a new set of analytic filters using polynomial approximation, which outperforms log-Mel filters significantly while being equally fast.",0.0,5.4625
390,1677,5.05    5.9    5.25    5.65    ,Weighting of Coda Voicing Cues: Glottalisation and Vowel Duration,"Recent research suggests that a trading relationship may exist in speech production between vowel duration and glottalisation as cues to coda stop voicing in Australian English. Younger speakers have been shown to use glottalisation to signal voicelessness more than older speakers who instead make greater use of vowel duration. This suggests a sound change in progress for the voicing cues. In addition, the vowel duration cue to voicing is greater in inherently long vowel contexts compared to inherently short vowel contexts. We report on a perceptual study designed to examine whether the weighting of these two cues found in production is replicated in perception.

Older and younger listeners were presented with audio stimuli co-varying in vowel duration and glottalisation. In accord with findings from production, the vowel duration cue was weaker for contexts containing inherently short vowels than for those containing inherently long vowels. Complementarily, glottalisation had a stronger effect on the perception of coda voicelessness in inherently short vowel contexts. Older and younger listeners did not differ in their use of glottalisation as a perceptual cue to voicelessness despite previously identified age differences in production. This finding raises questions about the link between perception and production in sound change.",0.0,5.4625
434,1768,5.8    4.9    6    5.15    ,"The Fifth `CHiME' Speech Separation and Recognition Challenge: Dataset, Task and Baselines","The CHiME challenge series aims to advance robust automatic speech recognition (ASR) technology by promoting research at the interface of speech and language processing, signal processing, and machine learning. This paper introduces the 5th CHiME Challenge, which considers the task of distant multi-microphone conversational ASR in real home environments. Speech material was elicited using a dinner party scenario with efforts taken to capture data that is representative of natural conversational speech and recorded by 6 Kinect microphone arrays and 4 binaural microphone pairs. The challenge features a single-array track and a multiple-array track and, for each track, distinct rankings will be produced for systems focusing on robustness with respect to distant-microphone capture versus systems attempting to address all aspects of the task including conversational language modeling. We discuss the rationale for the challenge and provide a detailed description of the data collection procedure, the task, and the baseline systems for array synchronization, speech enhancement, and conventional and end-to-end ASR.",0.0,5.4625
432,1764,4.95    5.1    6    5.8    ,Detecting Signs of Dementia Using Word Vector Representations,"Recent approaches to word vector representations, e.g., ‘w2vec’ and ‘GloVe’, have been shown to be powerful methods for capturing the semantics and syntax of words in a text. The approaches model the co-occurrences of words and recent successful applications on written text have shown how the vector representations and their interrelations  represent the meaning or sentiment in the text.  Most applications have targeted written language, however, in this paper, we investigate how these models port to the spoken language domain where the text is the result of (erroneous) automatic speech transcription. In particular, we are interested in the task of detecting signs of dementia in a person’s spoken language. This is motivated by the fact that early signs of dementia are known to affect a person’s ability to express meaning articulately for example when they engage in a conversation – something which is known to be cognitively very demanding. We analyse conversations designed to probe people’s short and long-term memory and propose three different methods for how word vectors may be used in a classification setup.  We show that it is possible to identify dementia from the output of a speech recogniser despite a high occurrence of recognition errors.",0.0,5.4625
741,2572,5.1    5.9    6    5    ,Automatic Glottis Localization and Segmentation in Stroboscopic Videos Using Deep Neural Network,"Exact analysis of the glottal vibration patten is vital for assessing voice pathologies. One of the primary steps in this
 analysis is automatic glottis segmentation, which, in turn, has
 two main parts, namely, glottis localization and the glottis segmentation. In this paper, we propose a deep neural network
 (DNN) based automatic glottis localization and segmentation
 scheme. We pose the problem as a classification problem where
 colors of each pixel and its neighborhood is classified as belonging to inside or outside the glottis region. We further process the classification result to get the biggest cluster, which is
 declared as the segmented glottis. The proposed algorithm is
 evaluated on a dataset comprising of stroboscopic videos from
 18 subjects where the glottis region is marked by the three
 Speech Language Pathologists (SLPs). On average, the proposed DNN based segmentation scheme achieves a localization
 performance of 65.33% and segmentation DICE score of 0.74
 (absolute), which is better than the baseline scheme by 22.66%
 and 0.09 respectively. We also find that the DICE score obtained by the DNN based segmentation scheme correlates well
with the average DICE score computed between annotation provided by any two SLPs suggesting the robustness of the proposed glottis segmentation scheme.",0.0,5.5
31,97,5.7    5.75    5.65    4.95    ,Overview of the 2018 Spoken CALL Shared Task,"We present an overview of the second edition of the Spoken CALL Shared Task. Groups competed on a prompt-response task using English-language data collected, through an online CALL game, from Swiss German teens in their second and third years of learning English. Each item consists of a written German prompt and an audio file containing a spoken response. The task is to accept linguistically correct responses and reject linguistically incorrect ones, with ``linguistically correct'' defined by a gold standard derived from human annotations. Scoring was performed using a metric defined as the ratio of the relative rejection rates on incorrect and correct responses. The second edition received eighteen entries and showed very substantial improvement on the first edition; all entries were better than the best entry from the first edition, and the best score was about four times higher. We present the task, the resources, the results, a discussion of the metrics used, and an analysis of what makes items challenging. In particular, we present quantitative evidence suggesting that incorrect responses are much more difficult to process than correct responses, and that the most significant factor in making a response challenging is its distance from the closest training example.",0.0,5.5125
491,1898,5    5.8    5.75    ,Quaternion Convolutional Neural Networks for End-to-End Automatic Speech Recognition,"Recently, the connectionist temporal classification (CTC) model coupled with recurrent (RNN) or convolutional neural networks (CNN), made it easier to train speech recognition systems in an end-to-end fashion. However in real-valued models, time frame components such as mel-filter-bank energies and the cepstral coefficients obtained from them, together with their  first and second order derivatives, are processed as individual elements, while a natural alternative is to process such components as composed entities. We propose to group such elements in the form of quaternions and to process these quaternions using the established quaternion algebra. Quaternion numbers and quaternion neural networks have shown their efficiency to process multidimensional inputs as entities, to encode internal dependencies, and to solve many tasks with less learning parameters than real-valued models. This paper proposes to integrate multiple feature views in quaternion-valued convolutional neural network (QCNN), to be used for sequence-to-sequence mapping with the CTC model. Promising results are reported using simple QCNNs in phoneme recognition experiments with the TIMIT corpus. More precisely, QCNNs obtain a lower phoneme error rate (PER) with less learning parameters than a competing model based on real-valued CNNs.",0.0,5.516666666666667
301,1485,5.05    5.9    5.9    5.25    ,Layer Trajectory LSTM,"It is popular to stack LSTM layers to get better modeling power, especially when large amount of training data is available. However, an LSTM-RNN with too many vanilla LSTM layers is very hard to train and there still exists the gradient vanishing issue if the network goes too deep.  This issue can be partially solved by adding skip connections between layers, such as residual LSTM. In this paper, we propose a layer trajectory LSTM (ltLSTM) which builds a layer-LSTM using all the layer outputs from a standard multi-layer time-LSTM. This layer-LSTM scans the outputs from time-LSTMs, and uses the summarized layer trajectory  information for final senone classification. The forward-propagation of time-LSTM and layer-LSTM can be handled in two separate threads in parallel so that the network computation time is the same as the standard time-LSTM. With a layer-LSTM running through layers, a gated path is provided from the output layer to the bottom layer, alleviating  the gradient vanishing issue. Trained  with 30 thousand hours of EN-US Microsoft internal data, the proposed ltLSTM performed  significantly better than the standard multi-layer LSTM and residual LSTM, with up to 9.0% relative word error rate reduction across different tasks.",0.0,5.525
287,1457,5.15    6    6    5    ,The Retroflex-dental Contrast in Punjabi Stops and Nasals: a Principal Component Analysis of Ultrasound Images,"Many languages of South Asia show a phonemic contrast between retroflexes and dentals across different manners of articulation. This contrast, however, tends to be less phonetically distinct and more variable in nasals. 

The goal of this paper is to examine the overall similarity of the retroflex-dental contrasts in Punjabi stops and nasals. Ultrasound tongue imaging recordings were obtained from 14 Punjabi speakers producing /ʈ,ɳ,t,n/ in the /ba_ab/ nonsense word context. Selected video frames were fed to a principal component analysis (PCA); the output was used for (1) training a linear discriminant model on one manner that discriminates place and (2) testing it on the other manner.

The results showed 100% correct classification of the contrast (retroflex or dental) in stops and 92% correct classification in nasals in the training data. The classification was much poorer across different manners: on average 67% of stops and 57% of nasals were classified correctly based on training sets with nasals or stops, respectively. In both cases, retroflex responses were more common. 
These results suggest that the tongue configurations for Punjabi retroflex and dental consonants differ by manner of articulation. The contrast is also overall less robust in nasals than in stops, confirming previous reports.",0.0,5.5375
331,1545,5.9    5.3    5.15    5.9    ,Angular Softmax for Short-Duration Text-independent Speaker Verification,"Recently, researchers propose to build deep learning based end-to-end speaker verification (SV) systems and achieve competitive results compared with the standard i-vector approach. In addition to deep learning architectures, optimization metric, such as softmax loss or triplet loss, is important for extracting speaker embeddings which are discriminative and generalizable to unseen speakers. In this paper, angular softmax (A-softmax) loss is introduced to improve speaker embedding quality. It is investigated in two SV frameworks: a CNN based end-to-end SV framework and an i-vector SV framework where deep discriminant analysis is used for channel compensation. Experimental results on a short-duration text-independent speaker verification dataset generated from SRE reveal that A-softmax achieves significant performance improvement compared with other metrics in both frameworks.",0.0,5.5625
85,1088,5.25    5.05    6    6    ,The Conversation Continues: the Effect of Lyrics and Music Complexity of Background Music on Spoken-Word Recognition,"Background music in social interaction settings can hinder conversation. Yet, little is known of how specific properties of music impact speech processing. This paper addresses this knowledge gap by investigating the effect of the 1) complexity of the background music, and 2) the presence versus absence of sung lyrics on spoken-word recognition in background music. To answer these questions, a word identification experiment was run in which Dutch participants listened to Dutch CVC words embedded in stretches of background music in four conditions: low/high complexity and with lyrics/music-only, and at three SNRs. Music stretches with and without lyrics were sampled from the same song in order to control for factors beyond the complexity of the music and the presence of lyrics. The results showed a clear negative impact of more complex music and the presence of lyrics in background music on spoken-word recognition. The results open a path for future work, and suggest that social spaces (e.g., restaurants, cafés and bars) should make careful choices of music to promote conversation.",0.0,5.575
701,2456,5.35    5.1    5.95    5.95    ,Multi-Modal Data Augmentation for End-to-end ASR,"We present a new end-to-end architecture for automatic speech recognition (ASR) that can be trained using symbolic input in addition to the traditional acoustic input. This architecture utilizes two separate encoders: one for acoustic input and another for symbolic input, both sharing the attention and decoder parameters. We call this architecture a multi-modal data augmentation network (MMDA), as it can support multi-modal (acoustic and symbolic) input and enables seamless mixing of large text datasets with significantly smaller transcribed speech corpora during training. We study different ways of transforming large text corpora into a symbolic form suitable for training our MMDA network. Our best MMDA setup obtains small improvements on character error rate (CER), and as much as 7-10\% relative word error rate (WER) improvement over a baseline both with and without an external language model.",0.0,5.5874999999999995
431,1760,5.85    5.2    5.8    ,Picture Naming or Word Reading: Does the Modality Affect Speech Motor Adaptation and Its Transfer?,"Auditory-motor adaptation and transfer paradigms are increasingly used to explore speech motor control as well as phonological representations underlying speech production. Auditory-motor adaptation is generally assumed to occur at the sensory-motor level. However, few studies suggested that linguistic or contextual factors such as the modality of presentation of stimuli influences adaptation. 
The present study investigates the influence of the modality of stimuli presentation (written word vs. a picture representing the same word) on auditory-motor adaptation and transfer. In this speech production experiment, speakers’ auditory feedback was altered online, inducing adaptation. We contrasted the magnitude of adaptation in these two different modalities and we assessed transfer from /pe/ to the French word /epe/ in the same vs. different modality of presentation, using a mixed 2*2 subject design.
The magnitude of adaptation was not different between modalities. This observation contrasts with recent findings showing an effect of the modality (a written word vs. a go signal) on adaptation. Moreover, transfer did occur from one modality to the other, and transfer pattern depended on the modality of transfer stimuli. Overall, the results suggest that picture naming and word reading rely on sensory-motor representations that may be linked to contextual (or surface) characteristics.",0.0,5.616666666666667
12,51,5.9    5.75    5.1    5.9    ,"The INTERSPEECH 2018 Computational Paralinguistics Challenge: Atypical & Self-Assessed Affect, Crying & Heart Beats","The INTERSPEECH 2018 Computational Paralinguistics Challenge addresses four different problems for the first time in a research competition under well-defined conditions: In the Atypical Affect Sub-Challenge, four basic emotions annotated in the speech of handicapped subjects have to be classified; in the Self-Assessed Affect Sub-Challenge, valence scores given by the speakers themselves are used for a three-class classification problem; in the Crying Sub-Challenge, three types of infant vocalisations have to be told apart; and in the Heart Beats Sub-Challenge, three different types of heart beats have to be determined. We describe the Sub-Challenges, their conditions, and baseline feature extraction and classifiers, which include data-learnt (supervised) feature representations by end-to-end learning, the ‘usual’ ComParE and BoAW features, and deep unsupervised representation learning using the auDEEP toolkit for the first time in the challenge series.",0.0,5.6625
250,1384,6    5.95    5.8    5    ,Articulation Rate as a Speaker Discriminant in British English,"Identifying speech parameters that have both a low level of intra-speaker variability and a high level of inter-speaker variability is key when discriminating between individuals in forensic speaker comparison cases. A substantial amount of research in the field of forensic phonetics has been devoted to identifying highly discriminant speaker parameters. To this end, the vast majority of the existing literature has focused solely on vowels and constants. However, the discriminant power of speaking tempo has yet to be examined, despite its broad use in practice and it having been recognized.

This paper examines, for the first time, the discriminant power of articulation rate (AR) in British English. Approximately 3000 local ARs were measured in this study for 100 Southern Standard British English male speakers. In order to assess the evidential value of AR, likelihood ratios were calculated. The results suggest that AR performs well for same speaker comparisons. However, for different speaker comparisons, the system is performing just worse than chance. Overall, it appears that AR may not be the best speaker discriminant, although it is important to still consider AR in forensic speaker comparisons as there may be some individuals for which AR is highly idiosyncratic.",0.0,5.6875
628,2284,5.85    5.6    5.9    5.85    ,Recognizing Overlapped Speech in Meetings: a Multichannel Separation Approach Using Neural Networks,"The goal of this work is to develop a meeting transcription system that can recognize speech even when utterances of different speakers are overlapped. While speech overlaps have been regarded as a major obstacle in accurately transcribing meetings, a traditional beamformer with a single output has been exclusively used because previously proposed speech separation techniques have critical constraints for application to real meetings. This paper proposes a new signal processing module, called an unmixing transducer, and describes its implementation using a windowed BLSTM. The unmixing transducer has a fixed number, say J, of output channels, where J may be different from the number of meeting attendees, and transforms an input multi-channel acoustic signal into J time-synchronous audio streams. Each utterance in the meeting  is separated and emitted from one of the output channels. Then, each output signal can be simply fed to a speech recognition back-end for segmentation and transcription. Our meeting transcription system using the unmixing transducer outperforms a system based on a state-of-the-art neural mask-based beamformer by 10.8%. Significant improvements are observed in overlapped segments. To the best of our knowledge, this is the first report that applies overlapped speech recognition to unconstrained real meeting audio.",0.0,5.800000000000001
254,1392,5.75    6    5.9    5.75    ,Cold Fusion: Training Seq2Seq Models Together with Language Models,"Sequence-to-sequence (Seq2Seq) models with attention have excelled at tasks which involve generating natural language sentences such as machine translation, image captioning and speech recognition. Performance has further been improved by leveraging unlabeled data, often in the form of a language model. In this work, we present the Cold Fusion method, which leverages a pre-trained language model during training, and show its effectiveness on the speech recognition task. We show that Seq2Seq models with Cold Fusion are able to better utilize language information enjoying i) faster convergence and better generalization, and ii) almost complete transfer to a new domain while using less than 10% of the labeled training data.",0.0,5.85
